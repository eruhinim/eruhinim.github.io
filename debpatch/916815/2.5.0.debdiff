diff -Nru fabric-1.14.0/AUTHORS fabric-2.5.0/AUTHORS
--- fabric-1.14.0/AUTHORS	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/AUTHORS	1970-01-01 01:00:00.000000000 +0100
@@ -1,60 +0,0 @@
-The following list contains individuals who contributed nontrivial code to
-Fabric's codebase, ordered by date of first contribution. Individuals who
-submitted bug reports or trivial one-line "you forgot to do X" patches are
-generally credited in the commit log only.
-
-IMPORTANT: as of 2012, this file is historical only and we'll probably stop
-updating it. The changelog and/or Git history is the canonical source for
-thanks, credits etc.
-
-Christian Vest Hansen
-Rob Cowie
-Jeff Forcier
-Travis Cline
-Niklas Lindström
-Kevin Horn
-Max Battcher
-Alexander Artemenko
-Dennis Schoen
-Erick Dennis
-Sverre Johansen
-Michael Stephens
-Armin Ronacher
-Curt Micol
-Patrick McNerthney
-Steve Steiner
-Ali Saifee
-Jorge Vargas
-Peter Ellis
-Brian Rosner
-Xinan Wu
-Alex Koshelev
-Mich Matuson
-Morgan Goose
-Carl Meyer
-Erich Heine
-Travis Swicegood
-Paul Smith
-Alex Koshelev
-Stephen Goss
-James Murty
-Thomas Ballinger
-Rick Harding
-Kirill Pinchuk
-Ales Zoulek
-Casey Banner
-Roman Imankulov
-Rodrigue Alcazar
-Jeremy Avnet
-Matt Chisholm
-Mark Merritt
-Max Arnold
-Szymon Reichmann
-David Wolever
-Jason Coombs
-Ben Davis
-Neilen Marais
-Rory Geoghegan
-Alexey Diyan
-Kamil Kisiel
-Jonas Lundberg
diff -Nru fabric-1.14.0/CHANGELOG.rst fabric-2.5.0/CHANGELOG.rst
--- fabric-1.14.0/CHANGELOG.rst	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/CHANGELOG.rst	1970-01-01 01:00:00.000000000 +0100
@@ -1,4 +0,0 @@
-The CHANGELOG lives in `sites/www/changelog.rst <sites/www/changelog.rst>`_.
-
-An HTML version lives online at `fabfile.org/changelog.html
-<http://fabfile.org/changelog.html>`_.
diff -Nru fabric-1.14.0/codecov.yml fabric-2.5.0/codecov.yml
--- fabric-1.14.0/codecov.yml	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/codecov.yml	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,2 @@
+# No codecov comments at all, please - just the github 'checks' is sufficient
+comment: off
diff -Nru fabric-1.14.0/CONTRIBUTING.rst fabric-2.5.0/CONTRIBUTING.rst
--- fabric-1.14.0/CONTRIBUTING.rst	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/CONTRIBUTING.rst	1970-01-01 01:00:00.000000000 +0100
@@ -1,2 +0,0 @@
-Please see `contribution-guide.org <http://www.contribution-guide.org/>`_ for
-details on what we expect from contributors. Thanks!
diff -Nru fabric-1.14.0/.coveragerc fabric-2.5.0/.coveragerc
--- fabric-1.14.0/.coveragerc	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/.coveragerc	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,3 @@
+[run]
+branch = True
+include = fabric/*
diff -Nru fabric-1.14.0/debian/changelog fabric-2.5.0/debian/changelog
--- fabric-1.14.0/debian/changelog	2017-12-26 01:55:31.000000000 +0000
+++ fabric-2.5.0/debian/changelog	2019-11-28 18:21:31.000000000 +0000
@@ -1,3 +1,23 @@
+fabric (2.5.0-0.1) unstable; urgency=medium
+
+  [ Ondřej Nový ]
+  * d/control: Remove ancient X-Python-Version field
+  * d/copyright: Use https protocol in Format field
+  * d/control: Set Vcs-* to salsa.debian.org
+
+  [ Luca Boccassi ]
+  * Update upstream source from tag 'upstream/2.5.0' (Closes: #930451)
+  * Refresh 0001-Remove-Google-AdSense-tracker-from-docs.patch to remove
+    fuzz
+  * Remove 0002-disable-failed-tests-with-fudge10.patch, no longer
+    required
+  * Convert to python3 and split executable in its own package (Closes:
+    #916815, #936501)
+  * Add Built-Using metadata generated by sphinx
+  * Non-maintainer upload.
+
+ -- Luca Boccassi <bluca@debian.org>  Thu, 28 Nov 2019 18:21:31 +0000
+
 fabric (1.14.0-1) unstable; urgency=medium
 
   * New upstream release.
@@ -5,6 +25,14 @@
 
  -- Andrew Starr-Bochicchio <asb@debian.org>  Mon, 25 Dec 2017 20:55:31 -0500
 
+fabric (1.13.1-4) unstable; urgency=medium
+
+  * debian/rules: Disable test_should_use_sentinel_for_tasks_that_errored
+    The test randomly fails in certain environments causing more
+    too much noise to be useful (Closes: #854686).
+
+ -- Andrew Starr-Bochicchio <asb@debian.org>  Tue, 16 May 2017 22:20:39 -0400
+
 fabric (1.13.1-3) unstable; urgency=medium
 
   * Run tests in verbose mode (Closes: #857094).
diff -Nru fabric-1.14.0/debian/control fabric-2.5.0/debian/control
--- fabric-1.14.0/debian/control	2017-12-25 23:36:46.000000000 +0000
+++ fabric-2.5.0/debian/control	2019-11-28 18:21:31.000000000 +0000
@@ -4,28 +4,27 @@
 Maintainer: Andrew Starr-Bochicchio <asb@debian.org>
 Build-Depends: debhelper (>= 10),
                dh-python,
-               python-alabaster,
-               python-all (>= 2.7),
-               python-crypto,
-               python-fudge,
-               python-nose,
-               python-paramiko,
-               python-setuptools,
-               python-sphinx,
+               python3,
+               python3-alabaster,
+               python3-decorator <!nocheck>,
+               python3-invoke (>= 1.3.0~) <!nocheck>,
+               python3-nose <!nocheck>,
+               python3-paramiko,
+               python3-setuptools,
+               python3-sphinx,
 Standards-Version: 4.1.2
-Vcs-Git: git://anonscm.debian.org/collab-maint/fabric.git
-Vcs-Browser: http://anonscm.debian.org/cgit/collab-maint/fabric.git
+Vcs-Git: https://salsa.debian.org/debian/fabric.git
+Vcs-Browser: https://salsa.debian.org/debian/fabric
 Homepage: http://fabfile.org/
-X-Python-Version: >= 2.5
 
 Package: fabric
 Architecture: all
-Depends: python-crypto,
-         python-paramiko (>= 1.10),
-         python-pkg-resources,
+Depends: python3-fabric (>= ${source:Version}),
+         python3-pkg-resources,
          ${misc:Depends},
-         ${python:Depends},
+         ${python3:Depends},
          ${sphinxdoc:Depends},
+Built-Using: ${sphinxdoc:Built-Using},
 Description: Simple Pythonic remote deployment tool
  Fabric is designed to upload files and run shell commands on a number of
  servers in parallel or serially. These commands are grouped in tasks (which
@@ -33,3 +32,19 @@
  .
  It is similar to Capistrano, except it's implemented in Python and doesn't
  expect you to be deploying Rails applications.
+ .
+ This package contains the binary executable and the documentation.
+
+Package: python3-fabric
+Architecture: all
+Depends: ${misc:Depends},
+         ${python3:Depends},
+Description: Simple Pythonic remote deployment tool
+ Fabric is designed to upload files and run shell commands on a number of
+ servers in parallel or serially. These commands are grouped in tasks (which
+ are regular Python functions) and specified in a 'fabfile.'
+ .
+ It is similar to Capistrano, except it's implemented in Python and doesn't
+ expect you to be deploying Rails applications.
+ .
+ This package contains the Python 3 modules.
diff -Nru fabric-1.14.0/debian/copyright fabric-2.5.0/debian/copyright
--- fabric-1.14.0/debian/copyright	2016-06-12 18:15:32.000000000 +0100
+++ fabric-2.5.0/debian/copyright	2019-11-28 18:20:27.000000000 +0000
@@ -1,4 +1,4 @@
-Format: http://www.debian.org/doc/packaging-manuals/copyright-format/1.0/
+Format: https://www.debian.org/doc/packaging-manuals/copyright-format/1.0/
 Upstream-Name: fabric
 Upstream-Contact: Jeffrey E. Forcier <jeff@bitprophet.org>
 Source: https://github.com/fabric/fabric
diff -Nru fabric-1.14.0/debian/patches/0001-Remove-Google-AdSense-tracker-from-docs.patch fabric-2.5.0/debian/patches/0001-Remove-Google-AdSense-tracker-from-docs.patch
--- fabric-1.14.0/debian/patches/0001-Remove-Google-AdSense-tracker-from-docs.patch	2017-12-26 01:09:36.000000000 +0000
+++ fabric-2.5.0/debian/patches/0001-Remove-Google-AdSense-tracker-from-docs.patch	2019-11-28 18:20:41.000000000 +0000
@@ -6,16 +6,14 @@
  sites/shared_conf.py | 2 +-
  1 file changed, 1 insertion(+), 1 deletion(-)
 
-Index: fabric/sites/shared_conf.py
-===================================================================
---- fabric.orig/sites/shared_conf.py	2017-12-25 20:09:29.533079915 -0500
-+++ fabric/sites/shared_conf.py	2017-12-25 20:09:29.529079734 -0500
-@@ -18,7 +18,7 @@
-     'github_user': 'fabric',
-     'github_repo': 'fabric',
-     'travis_button': True,
--    'analytics_id': 'UA-18486793-1',
-+#    'analytics_id': 'UA-18486793-1',
- 
-     'link': '#3782BE',
-     'link_hover': '#3782BE',
+--- a/sites/shared_conf.py
++++ b/sites/shared_conf.py
+@@ -22,7 +22,7 @@
+     "travis_button": True,
+     "codecov_button": True,
+     "tidelift_url": "https://tidelift.com/subscription/pkg/pypi-fabric?utm_source=pypi-fabric&utm_medium=referral&utm_campaign=docs",
+-    "analytics_id": "UA-18486793-1",
++#    "analytics_id": "UA-18486793-1",
+     "link": "#3782BE",
+     "link_hover": "#3782BE",
+     # Wide enough that 80-col code snippets aren't truncated on default font
diff -Nru fabric-1.14.0/debian/patches/0002-disable-failed-tests-with-fudge10.patch fabric-2.5.0/debian/patches/0002-disable-failed-tests-with-fudge10.patch
--- fabric-1.14.0/debian/patches/0002-disable-failed-tests-with-fudge10.patch	2017-12-26 01:51:37.000000000 +0000
+++ fabric-2.5.0/debian/patches/0002-disable-failed-tests-with-fudge10.patch	1970-01-01 01:00:00.000000000 +0100
@@ -1,169 +0,0 @@
-Description: Disable tests that require Fudge < 1.0 so that we can run the rest
- of the test suite.
-Forwarded: not-needed
-Origin: Fedora, http://pkgs.fedoraproject.org/cgit/rpms/fabric.git/tree/fabric-1.10.2-disable-failed-tests-with-fudge10.patch
-
-Index: fabric/setup.py
-===================================================================
---- fabric.orig/setup.py	2017-12-25 20:50:56.296262849 -0500
-+++ fabric/setup.py	2017-12-25 20:50:56.292262661 -0500
-@@ -44,7 +44,7 @@
-     url='http://fabfile.org',
-     packages=find_packages(),
-     test_suite='nose.collector',
--    tests_require=['nose<2.0', 'fudge<1.0', 'jinja2<3.0'],
-+    tests_require=['nose<2.0', 'fudge', 'jinja2<3.0'],
-     install_requires=install_requires,
-     entry_points={
-         'console_scripts': [
-Index: fabric/tests/test_utils.py
-===================================================================
---- fabric.orig/tests/test_utils.py	2017-12-25 20:50:56.296262849 -0500
-+++ fabric/tests/test_utils.py	2017-12-25 20:50:56.292262661 -0500
-@@ -17,16 +17,6 @@
-     assert_not_contains
- 
- 
--@mock_streams('stderr')
--@with_patched_object(output, 'warnings', True)
--def test_warn():
--    """
--    warn() should print 'Warning' plus given text
--    """
--    warn("Test")
--    eq_("\nWarning: Test\n\n", sys.stderr.getvalue())
--
--
- def test_indent():
-     for description, input_, output_ in (
-         ("Sanity check: 1 line string",
-@@ -72,45 +62,6 @@
-     with settings(abort_exception=TestException):
-         abort("Test")
- 
--@mock_streams('stderr')
--@with_patched_object(output, 'aborts', True)
--def test_abort_message():
--    """
--    abort() should print 'Fatal error' plus exception value
--    """
--    try:
--        abort("Test")
--    except SystemExit:
--        pass
--    result = sys.stderr.getvalue()
--    eq_("\nFatal error: Test\n\nAborting.\n", result)
--
--def test_abort_message_only_printed_once():
--    """
--    abort()'s SystemExit should not cause a reprint of the error message
--    """
--    # No good way to test the implicit stderr print which sys.exit/SystemExit
--    # perform when they are allowed to bubble all the way to the top. So, we
--    # invoke a subprocess and look at its stderr instead.
--    with quiet():
--        result = local("python -m fabric.__main__ -f tests/support/aborts.py kaboom", capture=True)
--    # When error in #1318 is present, this has an extra "It burns!" at end of
--    # stderr string.
--    eq_(result.stderr, "Fatal error: It burns!\n\nAborting.")
--
--@mock_streams('stderr')
--@with_patched_object(output, 'aborts', True)
--def test_abort_exception_contains_separate_message_and_code():
--    """
--    abort()'s SystemExit contains distinct .code/.message attributes.
--    """
--    # Re #1318 / #1213
--    try:
--        abort("Test")
--    except SystemExit as e:
--        eq_(e.message, "Test")
--        eq_(e.code, 1)
--
- @mock_streams('stdout')
- def test_puts_with_user_output_on():
-     """
-@@ -227,44 +178,6 @@
-             error("error message", func=utils.abort, stdout=stdout)
-         assert_contains(stdout, sys.stdout.getvalue())
- 
--    @mock_streams('stdout')
--    @with_patched_object(utils, 'abort', Fake('abort', callable=True,
--        expect_call=True).calls(lambda x: sys.stdout.write(x + "\n")))
--    @with_patched_object(output, 'exceptions', True)
--    @with_patched_object(utils, 'format_exc', Fake('format_exc', callable=True,
--        expect_call=True).returns(dummy_string))
--    def test_includes_traceback_if_exceptions_logging_is_on(self):
--        """
--        error() includes traceback in message if exceptions logging is on
--        """
--        error("error message", func=utils.abort, stdout=error)
--        assert_contains(self.dummy_string, sys.stdout.getvalue())
--
--    @mock_streams('stdout')
--    @with_patched_object(utils, 'abort', Fake('abort', callable=True,
--        expect_call=True).calls(lambda x: sys.stdout.write(x + "\n")))
--    @with_patched_object(output, 'debug', True)
--    @with_patched_object(utils, 'format_exc', Fake('format_exc', callable=True,
--        expect_call=True).returns(dummy_string))
--    def test_includes_traceback_if_debug_logging_is_on(self):
--        """
--        error() includes traceback in message if debug logging is on (backwardis compatibility)
--        """
--        error("error message", func=utils.abort, stdout=error)
--        assert_contains(self.dummy_string, sys.stdout.getvalue())
--
--    @mock_streams('stdout')
--    @with_patched_object(utils, 'abort', Fake('abort', callable=True,
--        expect_call=True).calls(lambda x: sys.stdout.write(x + "\n")))
--    @with_patched_object(output, 'exceptions', True)
--    @with_patched_object(utils, 'format_exc', Fake('format_exc', callable=True,
--        expect_call=True).returns(None))
--    def test_doesnt_print_None_when_no_traceback_present(self):
--        """
--        error() doesn't include None in message if there is no traceback
--        """
--        error("error message", func=utils.abort, stdout=error)
--        assert_not_contains('None', sys.stdout.getvalue())
- 
-     @mock_streams('stderr')
-     @with_patched_object(utils, 'abort', Fake('abort', callable=True,
-Index: fabric/tests/test_network.py
-===================================================================
---- fabric.orig/tests/test_network.py	2017-12-25 20:51:10.356922711 -0500
-+++ fabric/tests/test_network.py	2017-12-25 20:51:34.470052653 -0500
-@@ -232,34 +232,6 @@
-             cache = HostConnectionCache()
-             cache[env.host_string]
- 
--    @with_fakes
--    @raises(NetworkError)
--    def test_connect_does_not_prompt_password_when_ssh_raises_channel_exception(self):
--        def raise_channel_exception_once(*args, **kwargs):
--            if raise_channel_exception_once.should_raise_channel_exception:
--                raise_channel_exception_once.should_raise_channel_exception = False
--                raise ssh.ChannelException(2, 'Connect failed')
--        raise_channel_exception_once.should_raise_channel_exception = True
--
--        def generate_fake_client():
--            fake_client = Fake('SSHClient', allows_any_call=True, expect_call=True)
--            fake_client.provides('connect').calls(raise_channel_exception_once)
--            return fake_client
--
--        fake_ssh = Fake('ssh', allows_any_call=True)
--        fake_ssh.provides('SSHClient').calls(generate_fake_client)
--        # We need the real exceptions here to preserve the inheritence structure
--        fake_ssh.SSHException = ssh.SSHException
--        fake_ssh.ChannelException = ssh.ChannelException
--        patched_connect = patch_object('fabric.network', 'ssh', fake_ssh)
--        patched_password = patch_object('fabric.network', 'prompt_for_password', Fake('prompt_for_password', callable = True).times_called(0))
--        try:
--            connect('user', 'localhost', 22, HostConnectionCache())
--        finally:
--            # Restore ssh
--            patched_connect.restore()
--            patched_password.restore()
--
- 
-     @mock_streams('stdout')
-     @server()
diff -Nru fabric-1.14.0/debian/patches/series fabric-2.5.0/debian/patches/series
--- fabric-1.14.0/debian/patches/series	2017-12-26 01:18:45.000000000 +0000
+++ fabric-2.5.0/debian/patches/series	2019-11-28 18:20:41.000000000 +0000
@@ -1,2 +1 @@
 0001-Remove-Google-AdSense-tracker-from-docs.patch
-0002-disable-failed-tests-with-fudge10.patch
diff -Nru fabric-1.14.0/debian/rules fabric-2.5.0/debian/rules
--- fabric-1.14.0/debian/rules	2017-03-08 01:09:51.000000000 +0000
+++ fabric-2.5.0/debian/rules	2019-11-28 18:21:22.000000000 +0000
@@ -3,11 +3,12 @@
 # Uncomment this to turn on verbose mode.
 #export DH_VERBOSE=1
 
-export PYBUILD_TEST_ARGS=-v --exclude=test_nested_execution_with_explicit_ports
+export PYBUILD_TEST_ARGS=-v --ignore=test_nested_execution_with_explicit_ports --ignore=test_should_use_sentinel_for_tasks_that_errored
 export PYBUILD_NAME=fabric
+export PYBUILD_INSTALL_ARGS=--install-scripts=../fabric/usr/bin
 
 %:
-	dh $@ --with python2,sphinxdoc --buildsystem=pybuild
+	dh $@ --with python3,sphinxdoc --buildsystem=pybuild
 
 override_dh_auto_install:
 	dh_auto_install
@@ -16,3 +17,4 @@
 override_dh_auto_clean:
 	dh_auto_clean
 	rm -rf sites/docs/build/
+	rm -rf fabric.egg-info/
diff -Nru fabric-1.14.0/dev-requirements.txt fabric-2.5.0/dev-requirements.txt
--- fabric-1.14.0/dev-requirements.txt	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/dev-requirements.txt	2019-08-06 23:57:28.000000000 +0100
@@ -1,19 +1,26 @@
-# You should already have the dev version of Paramiko and your local Fabric
-# checkout installed! Stable Paramiko may not be sufficient!
-
-# Test runner/testing utils
-nose<2.0
-# Rudolf adds color to the output of 'fab test'. This is a custom fork
-# addressing Python 2.7 and Nose's 'skip' plugin compatibility issues.
--e git+https://github.com/bitprophet/rudolf#egg=rudolf
-# Mocking library
-Fudge<1.0
-# Documentation generation
-Sphinx==1.3.4
-releases>=1.2.0,<2
-invoke>=0.12,<0.13
-invocations>=0.12,<0.13
-alabaster>=0.6.1
-semantic_version==2.4
+# Invoke implicitly required by self/pip install -e .
+# Invocations for common project tasks
+invocations>=1.0,<2.0
+# Invoke from git, temporarily?
+-e git+https://github.com/pyinvoke/invoke#egg=invoke
+# pytest-relaxed for test organization, display etc tweaks
+pytest-relaxed>=1.0.1,<1.1
+pytest==3.2.5
+# pytest-cov for coverage
+pytest-cov==2.5.1
+six==1.10.0
+# Mock for test mocking
+mock==2.0.0
+# Linting!
+flake8==3.6.0
+# Coverage!
+coverage==3.7.1
+codecov==1.6.3
+# Documentation tools
+sphinx>=1.4,<1.7
+alabaster==0.7.12
+releases>=1.5,<2.0
+# Release tools
+semantic_version>=2.4,<2.5
 wheel==0.24
-twine==1.5
+twine==1.11.0
diff -Nru fabric-1.14.0/fabfile.py fabric-2.5.0/fabfile.py
--- fabric-1.14.0/fabfile.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/fabfile.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,24 +0,0 @@
-"""
-Fabric's own fabfile.
-"""
-
-from __future__ import with_statement
-
-import nose
-
-from fabric.api import task
-
-
-@task(default=True)
-def test(args=None):
-    """
-    Run all unit tests and doctests.
-
-    Specify string argument ``args`` for additional args to ``nosetests``.
-    """
-    # Default to explicitly targeting the 'tests' folder, but only if nothing
-    # is being overridden.
-    tests = "" if args else " tests"
-    default_args = "-sv --with-doctest --nologcapture --with-color %s" % tests
-    default_args += (" " + args) if args else ""
-    nose.core.run_exit(argv=[''] + default_args.split())
diff -Nru fabric-1.14.0/fabric/api.py fabric-2.5.0/fabric/api.py
--- fabric-1.14.0/fabric/api.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/fabric/api.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,18 +0,0 @@
-"""
-Non-init module for doing convenient * imports from.
-
-Necessary because if we did this in __init__, one would be unable to import
-anything else inside the package -- like, say, the version number used in
-setup.py -- without triggering loads of most of the code. Which doesn't work so
-well when you're using setup.py to install e.g. ssh!
-"""
-
-from fabric.context_managers import (cd, hide, settings, show, path, prefix,
-    lcd, quiet, warn_only, remote_tunnel, shell_env)
-from fabric.decorators import (hosts, roles, runs_once, with_settings, task,
-        serial, parallel)
-from fabric.operations import (require, prompt, put, get, run, sudo, local,
-    reboot, open_shell)
-from fabric.state import env, output
-from fabric.utils import abort, warn, puts, fastprint
-from fabric.tasks import execute
diff -Nru fabric-1.14.0/fabric/auth.py fabric-2.5.0/fabric/auth.py
--- fabric-1.14.0/fabric/auth.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/fabric/auth.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,19 +0,0 @@
-"""
-Common authentication subroutines. Primarily for internal use.
-"""
-
-
-def get_password(user, host, port, login_only=False):
-    from fabric.state import env
-    from fabric.network import join_host_strings
-    host_string = join_host_strings(user, host, port)
-    sudo_password = env.sudo_passwords.get(host_string, env.sudo_password)
-    login_password = env.passwords.get(host_string, env.password)
-    return login_password if login_only else sudo_password or login_password
-
-
-def set_password(user, host, port, password):
-    from fabric.state import env
-    from fabric.network import join_host_strings
-    host_string = join_host_strings(user, host, port)
-    env.password = env.passwords[host_string] = password
diff -Nru fabric-1.14.0/fabric/colors.py fabric-2.5.0/fabric/colors.py
--- fabric-1.14.0/fabric/colors.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/fabric/colors.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,53 +0,0 @@
-"""
-.. versionadded:: 0.9.2
-
-Functions for wrapping strings in ANSI color codes.
-
-Each function within this module returns the input string ``text``, wrapped
-with ANSI color codes for the appropriate color.
-
-For example, to print some text as green on supporting terminals::
-
-    from fabric.colors import green
-
-    print(green("This text is green!"))
-
-Because these functions simply return modified strings, you can nest them::
-
-    from fabric.colors import red, green
-
-    print(red("This sentence is red, except for " + \
-          green("these words, which are green") + "."))
-
-If ``bold`` is set to ``True``, the ANSI flag for bolding will be flipped on
-for that particular invocation, which usually shows up as a bold or brighter
-version of the original color on most terminals.
-
-.. versionchanged:: 1.11
-    Added support for the shell env var ``FABRIC_DISABLE_COLORS``; if this
-    variable is present and set to any non-empty value, all colorization driven
-    by this module will be skipped/disabled.
-"""
-
-import os
-
-def _wrap_with(code):
-
-    def inner(text, bold=False):
-        c = code
-
-        if os.environ.get('FABRIC_DISABLE_COLORS'):
-            return text
-
-        if bold:
-            c = "1;%s" % c
-        return "\033[%sm%s\033[0m" % (c, text)
-    return inner
-
-red = _wrap_with('31')
-green = _wrap_with('32')
-yellow = _wrap_with('33')
-blue = _wrap_with('34')
-magenta = _wrap_with('35')
-cyan = _wrap_with('36')
-white = _wrap_with('37')
diff -Nru fabric-1.14.0/fabric/config.py fabric-2.5.0/fabric/config.py
--- fabric-1.14.0/fabric/config.py	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/fabric/config.py	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,321 @@
+import copy
+import errno
+import os
+
+from invoke.config import Config as InvokeConfig, merge_dicts
+from paramiko.config import SSHConfig
+
+from .runners import Remote
+from .util import get_local_user, debug
+
+
+class Config(InvokeConfig):
+    """
+    An `invoke.config.Config` subclass with extra Fabric-related behavior.
+
+    This class behaves like `invoke.config.Config` in every way, with the
+    following exceptions:
+
+    - its `global_defaults` staticmethod has been extended to add/modify some
+      default settings (see its documentation, below, for details);
+    - it triggers loading of Fabric-specific env vars (e.g.
+      ``FABRIC_RUN_HIDE=true`` instead of ``INVOKE_RUN_HIDE=true``) and
+      filenames (e.g. ``/etc/fabric.yaml`` instead of ``/etc/invoke.yaml``).
+    - it extends the API to account for loading ``ssh_config`` files (which are
+      stored as additional attributes and have no direct relation to the
+      regular config data/hierarchy.)
+    - it adds a new optional constructor, `from_v1`, which :ref:`generates
+      configuration data from Fabric 1 <from-v1>`.
+
+    Intended for use with `.Connection`, as using vanilla
+    `invoke.config.Config` objects would require users to manually define
+    ``port``, ``user`` and so forth.
+
+    .. seealso:: :doc:`/concepts/configuration`, :ref:`ssh-config`
+
+    .. versionadded:: 2.0
+    """
+
+    prefix = "fabric"
+
+    @classmethod
+    def from_v1(cls, env, **kwargs):
+        """
+        Alternate constructor which uses Fabric 1's ``env`` dict for settings.
+
+        All keyword arguments besides ``env`` are passed unmolested into the
+        primary constructor, with the exception of ``overrides``, which is used
+        internally & will end up resembling the data from ``env`` with the
+        user-supplied overrides on top.
+
+        .. warning::
+            Because your own config overrides will win over data from ``env``,
+            make sure you only set values you *intend* to change from your v1
+            environment!
+
+        For details on exactly which ``env`` vars are imported and what they
+        become in the new API, please see :ref:`v1-env-var-imports`.
+
+        :param env:
+            An explicit Fabric 1 ``env`` dict (technically, any
+            ``fabric.utils._AttributeDict`` instance should work) to pull
+            configuration from.
+
+        .. versionadded:: 2.4
+        """
+        # TODO: automagic import, if we can find a way to test that
+        # Use overrides level (and preserve whatever the user may have given)
+        # TODO: we really do want arbitrary number of config levels, don't we?
+        # TODO: most of these need more care re: only filling in when they
+        # differ from the v1 default. As-is these won't overwrite runtime
+        # overrides (due to .setdefault) but they may still be filling in empty
+        # values to stomp on lower level config levels...
+        data = kwargs.pop("overrides", {})
+        # TODO: just use a dataproxy or defaultdict??
+        for subdict in ("connect_kwargs", "run", "sudo", "timeouts"):
+            data.setdefault(subdict, {})
+        # PTY use
+        data["run"].setdefault("pty", env.always_use_pty)
+        # Gateway
+        data.setdefault("gateway", env.gateway)
+        # Agent forwarding
+        data.setdefault("forward_agent", env.forward_agent)
+        # Key filename(s)
+        if env.key_filename is not None:
+            data["connect_kwargs"].setdefault("key_filename", env.key_filename)
+        # Load keys from agent?
+        data["connect_kwargs"].setdefault("allow_agent", not env.no_agent)
+        data.setdefault("ssh_config_path", env.ssh_config_path)
+        # Sudo password
+        data["sudo"].setdefault("password", env.sudo_password)
+        # Vanilla password (may be used for regular and/or sudo, depending)
+        passwd = env.password
+        data["connect_kwargs"].setdefault("password", passwd)
+        if not data["sudo"]["password"]:
+            data["sudo"]["password"] = passwd
+        data["sudo"].setdefault("prompt", env.sudo_prompt)
+        data["timeouts"].setdefault("connect", env.timeout)
+        data.setdefault("load_ssh_configs", env.use_ssh_config)
+        data["run"].setdefault("warn", env.warn_only)
+        # Put overrides back for real constructor and go
+        kwargs["overrides"] = data
+        return cls(**kwargs)
+
+    def __init__(self, *args, **kwargs):
+        """
+        Creates a new Fabric-specific config object.
+
+        For most API details, see `invoke.config.Config.__init__`. Parameters
+        new to this subclass are listed below.
+
+        :param ssh_config:
+            Custom/explicit `paramiko.config.SSHConfig` object. If given,
+            prevents loading of any SSH config files. Default: ``None``.
+
+        :param str runtime_ssh_path:
+            Runtime SSH config path to load. Prevents loading of system/user
+            files if given. Default: ``None``.
+
+        :param str system_ssh_path:
+            Location of the system-level SSH config file. Default:
+            ``/etc/ssh/ssh_config``.
+
+        :param str user_ssh_path:
+            Location of the user-level SSH config file. Default:
+            ``~/.ssh/config``.
+
+        :param bool lazy:
+            Has the same meaning as the parent class' ``lazy``, but
+            additionally controls whether SSH config file loading is deferred
+            (requires manually calling `load_ssh_config` sometime.) For
+            example, one may need to wait for user input before calling
+            `set_runtime_ssh_path`, which will inform exactly what
+            `load_ssh_config` does.
+        """
+        # Tease out our own kwargs.
+        # TODO: consider moving more stuff out of __init__ and into methods so
+        # there's less of this sort of splat-args + pop thing? Eh.
+        ssh_config = kwargs.pop("ssh_config", None)
+        lazy = kwargs.get("lazy", False)
+        self.set_runtime_ssh_path(kwargs.pop("runtime_ssh_path", None))
+        system_path = kwargs.pop("system_ssh_path", "/etc/ssh/ssh_config")
+        self._set(_system_ssh_path=system_path)
+        self._set(_user_ssh_path=kwargs.pop("user_ssh_path", "~/.ssh/config"))
+
+        # Record whether we were given an explicit object (so other steps know
+        # whether to bother loading from disk or not)
+        # This needs doing before super __init__ as that calls our post_init
+        explicit = ssh_config is not None
+        self._set(_given_explicit_object=explicit)
+
+        # Arrive at some non-None SSHConfig object (upon which to run .parse()
+        # later, in _load_ssh_file())
+        if ssh_config is None:
+            ssh_config = SSHConfig()
+        self._set(base_ssh_config=ssh_config)
+
+        # Now that our own attributes have been prepared & kwargs yanked, we
+        # can fall up into parent __init__()
+        super(Config, self).__init__(*args, **kwargs)
+
+        # And finally perform convenience non-lazy bits if needed
+        if not lazy:
+            self.load_ssh_config()
+
+    def set_runtime_ssh_path(self, path):
+        """
+        Configure a runtime-level SSH config file path.
+
+        If set, this will cause `load_ssh_config` to skip system and user
+        files, as OpenSSH does.
+
+        .. versionadded:: 2.0
+        """
+        self._set(_runtime_ssh_path=path)
+
+    def load_ssh_config(self):
+        """
+        Load SSH config file(s) from disk.
+
+        Also (beforehand) ensures that Invoke-level config re: runtime SSH
+        config file paths, is accounted for.
+
+        .. versionadded:: 2.0
+        """
+        # Update the runtime SSH config path (assumes enough regular config
+        # levels have been loaded that anyone wanting to transmit this info
+        # from a 'vanilla' Invoke config, has gotten it set.)
+        if self.ssh_config_path:
+            self._runtime_ssh_path = self.ssh_config_path
+        # Load files from disk if we weren't given an explicit SSHConfig in
+        # __init__
+        if not self._given_explicit_object:
+            self._load_ssh_files()
+
+    def clone(self, *args, **kwargs):
+        # TODO: clone() at this point kinda-sorta feels like it's retreading
+        # __reduce__ and the related (un)pickling stuff...
+        # Get cloned obj.
+        # NOTE: Because we also extend .init_kwargs, the actual core SSHConfig
+        # data is passed in at init time (ensuring no files get loaded a 2nd,
+        # etc time) and will already be present, so we don't need to set
+        # .base_ssh_config ourselves. Similarly, there's no need to worry about
+        # how the SSH config paths may be inaccurate until below; nothing will
+        # be referencing them.
+        new = super(Config, self).clone(*args, **kwargs)
+        # Copy over our custom attributes, so that the clone still resembles us
+        # re: recording where the data originally came from (in case anything
+        # re-runs ._load_ssh_files(), for example).
+        for attr in (
+            "_runtime_ssh_path",
+            "_system_ssh_path",
+            "_user_ssh_path",
+        ):
+            setattr(new, attr, getattr(self, attr))
+        # Load SSH configs, in case they weren't prior to now (e.g. a vanilla
+        # Invoke clone(into), instead of a us-to-us clone.)
+        self.load_ssh_config()
+        # All done
+        return new
+
+    def _clone_init_kwargs(self, *args, **kw):
+        # Parent kwargs
+        kwargs = super(Config, self)._clone_init_kwargs(*args, **kw)
+        # Transmit our internal SSHConfig via explicit-obj kwarg, thus
+        # bypassing any file loading. (Our extension of clone() above copies
+        # over other attributes as well so that the end result looks consistent
+        # with reality.)
+        new_config = SSHConfig()
+        # TODO: as with other spots, this implies SSHConfig needs a cleaner
+        # public API re: creating and updating its core data.
+        new_config._config = copy.deepcopy(self.base_ssh_config._config)
+        return dict(kwargs, ssh_config=new_config)
+
+    def _load_ssh_files(self):
+        """
+        Trigger loading of configured SSH config file paths.
+
+        Expects that ``base_ssh_config`` has already been set to an
+        `~paramiko.config.SSHConfig` object.
+
+        :returns: ``None``.
+        """
+        # TODO: does this want to more closely ape the behavior of
+        # InvokeConfig.load_files? re: having a _found attribute for each that
+        # determines whether to load or skip
+        if self._runtime_ssh_path is not None:
+            path = self._runtime_ssh_path
+            # Manually blow up like open() (_load_ssh_file normally doesn't)
+            if not os.path.exists(path):
+                msg = "No such file or directory: {!r}".format(path)
+                raise IOError(errno.ENOENT, msg)
+            self._load_ssh_file(os.path.expanduser(path))
+        elif self.load_ssh_configs:
+            for path in (self._user_ssh_path, self._system_ssh_path):
+                self._load_ssh_file(os.path.expanduser(path))
+
+    def _load_ssh_file(self, path):
+        """
+        Attempt to open and parse an SSH config file at ``path``.
+
+        Does nothing if ``path`` is not a path to a valid file.
+
+        :returns: ``None``.
+        """
+        if os.path.isfile(path):
+            old_rules = len(self.base_ssh_config._config)
+            with open(path) as fd:
+                self.base_ssh_config.parse(fd)
+            new_rules = len(self.base_ssh_config._config)
+            msg = "Loaded {} new ssh_config rules from {!r}"
+            debug(msg.format(new_rules - old_rules, path))
+        else:
+            debug("File not found, skipping")
+
+    @staticmethod
+    def global_defaults():
+        """
+        Default configuration values and behavior toggles.
+
+        Fabric only extends this method in order to make minor adjustments and
+        additions to Invoke's `~invoke.config.Config.global_defaults`; see its
+        documentation for the base values, such as the config subtrees
+        controlling behavior of ``run`` or how ``tasks`` behave.
+
+        For Fabric-specific modifications and additions to the Invoke-level
+        defaults, see our own config docs at :ref:`default-values`.
+
+        .. versionadded:: 2.0
+        """
+        # TODO: hrm should the run-related things actually be derived from the
+        # runner_class? E.g. Local defines local stuff, Remote defines remote
+        # stuff? Doesn't help with the final config tree tho...
+        # TODO: as to that, this is a core problem, Fabric wants split
+        # local/remote stuff, eg replace_env wants to be False for local and
+        # True remotely; shell wants to differ depending on target (and either
+        # way, does not want to use local interrogation for remote)
+        # TODO: is it worth moving all of our 'new' settings to a discrete
+        # namespace for cleanliness' sake? e.g. ssh.port, ssh.user etc.
+        # It wouldn't actually simplify this code any, but it would make it
+        # easier for users to determine what came from which library/repo.
+        defaults = InvokeConfig.global_defaults()
+        ours = {
+            # New settings
+            "connect_kwargs": {},
+            "forward_agent": False,
+            "gateway": None,
+            # TODO 3.0: change to True and update all docs accordingly.
+            "inline_ssh_env": False,
+            "load_ssh_configs": True,
+            "port": 22,
+            "run": {"replace_env": True},
+            "runners": {"remote": Remote},
+            "ssh_config_path": None,
+            "tasks": {"collection_name": "fabfile"},
+            # TODO: this becomes an override/extend once Invoke grows execution
+            # timeouts (which should be timeouts.execute)
+            "timeouts": {"connect": None},
+            "user": get_local_user(),
+        }
+        merge_dicts(defaults, ours)
+        return defaults
diff -Nru fabric-1.14.0/fabric/connection.py fabric-2.5.0/fabric/connection.py
--- fabric-1.14.0/fabric/connection.py	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/fabric/connection.py	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,993 @@
+from contextlib import contextmanager
+from threading import Event
+
+try:
+    from invoke.vendor.six import StringIO
+    from invoke.vendor.decorator import decorator
+    from invoke.vendor.six import string_types
+except ImportError:
+    from six import StringIO
+    from decorator import decorator
+    from six import string_types
+import socket
+
+from invoke import Context
+from invoke.exceptions import ThreadException
+from paramiko.agent import AgentRequestHandler
+from paramiko.client import SSHClient, AutoAddPolicy
+from paramiko.config import SSHConfig
+from paramiko.proxy import ProxyCommand
+
+from .config import Config
+from .exceptions import InvalidV1Env
+from .transfer import Transfer
+from .tunnels import TunnelManager, Tunnel
+
+
+@decorator
+def opens(method, self, *args, **kwargs):
+    self.open()
+    return method(self, *args, **kwargs)
+
+
+def derive_shorthand(host_string):
+    user_hostport = host_string.rsplit("@", 1)
+    hostport = user_hostport.pop()
+    user = user_hostport[0] if user_hostport and user_hostport[0] else None
+
+    # IPv6: can't reliably tell where addr ends and port begins, so don't
+    # try (and don't bother adding special syntax either, user should avoid
+    # this situation by using port=).
+    if hostport.count(":") > 1:
+        host = hostport
+        port = None
+    # IPv4: can split on ':' reliably.
+    else:
+        host_port = hostport.rsplit(":", 1)
+        host = host_port.pop(0) or None
+        port = host_port[0] if host_port and host_port[0] else None
+
+    if port is not None:
+        port = int(port)
+
+    return {"user": user, "host": host, "port": port}
+
+
+class Connection(Context):
+    """
+    A connection to an SSH daemon, with methods for commands and file transfer.
+
+    **Basics**
+
+    This class inherits from Invoke's `~invoke.context.Context`, as it is a
+    context within which commands, tasks etc can operate. It also encapsulates
+    a Paramiko `~paramiko.client.SSHClient` instance, performing useful high
+    level operations with that `~paramiko.client.SSHClient` and
+    `~paramiko.channel.Channel` instances generated from it.
+
+    .. _connect_kwargs:
+
+    .. note::
+        Many SSH specific options -- such as specifying private keys and
+        passphrases, timeouts, disabling SSH agents, etc -- are handled
+        directly by Paramiko and should be specified via the
+        :ref:`connect_kwargs argument <connect_kwargs-arg>` of the constructor.
+
+    **Lifecycle**
+
+    `.Connection` has a basic "`create <__init__>`, `connect/open <open>`, `do
+    work <run>`, `disconnect/close <close>`" lifecycle:
+
+    - `Instantiation <__init__>` imprints the object with its connection
+      parameters (but does **not** actually initiate the network connection).
+
+        - An alternate constructor exists for users :ref:`upgrading piecemeal
+          from Fabric 1 <from-v1>`: `from_v1`
+
+    - Methods like `run`, `get` etc automatically trigger a call to
+      `open` if the connection is not active; users may of course call `open`
+      manually if desired.
+    - Connections do not always need to be explicitly closed; much of the
+      time, Paramiko's garbage collection hooks or Python's own shutdown
+      sequence will take care of things. **However**, should you encounter edge
+      cases (for example, sessions hanging on exit) it's helpful to explicitly
+      close connections when you're done with them.
+
+      This can be accomplished by manually calling `close`, or by using the
+      object as a contextmanager::
+
+        with Connection('host') as c:
+            c.run('command')
+            c.put('file')
+
+    .. note::
+        This class rebinds `invoke.context.Context.run` to `.local` so both
+        remote and local command execution can coexist.
+
+    **Configuration**
+
+    Most `.Connection` parameters honor :doc:`Invoke-style configuration
+    </concepts/configuration>` as well as any applicable :ref:`SSH config file
+    directives <connection-ssh-config>`. For example, to end up with a
+    connection to ``admin@myhost``, one could:
+
+    - Use any built-in config mechanism, such as ``/etc/fabric.yml``,
+      ``~/.fabric.json``, collection-driven configuration, env vars, etc,
+      stating ``user: admin`` (or ``{"user": "admin"}``, depending on config
+      format.) Then ``Connection('myhost')`` would implicitly have a ``user``
+      of ``admin``.
+    - Use an SSH config file containing ``User admin`` within any applicable
+      ``Host`` header (``Host myhost``, ``Host *``, etc.) Again,
+      ``Connection('myhost')`` will default to an ``admin`` user.
+    - Leverage host-parameter shorthand (described in `.Config.__init__`), i.e.
+      ``Connection('admin@myhost')``.
+    - Give the parameter directly: ``Connection('myhost', user='admin')``.
+
+    The same applies to agent forwarding, gateways, and so forth.
+
+    .. versionadded:: 2.0
+    """
+
+    # NOTE: these are initialized here to hint to invoke.Config.__setattr__
+    # that they should be treated as real attributes instead of config proxies.
+    # (Additionally, we're doing this instead of using invoke.Config._set() so
+    # we can take advantage of Sphinx's attribute-doc-comment static analysis.)
+    # Once an instance is created, these values will usually be non-None
+    # because they default to the default config values.
+    host = None
+    original_host = None
+    user = None
+    port = None
+    ssh_config = None
+    gateway = None
+    forward_agent = None
+    connect_timeout = None
+    connect_kwargs = None
+    client = None
+    transport = None
+    _sftp = None
+    _agent_handler = None
+
+    @classmethod
+    def from_v1(cls, env, **kwargs):
+        """
+        Alternate constructor which uses Fabric 1's ``env`` dict for settings.
+
+        All keyword arguments besides ``env`` are passed unmolested into the
+        primary constructor.
+
+        .. warning::
+            Because your own config overrides will win over data from ``env``,
+            make sure you only set values you *intend* to change from your v1
+            environment!
+
+        For details on exactly which ``env`` vars are imported and what they
+        become in the new API, please see :ref:`v1-env-var-imports`.
+
+        :param env:
+            An explicit Fabric 1 ``env`` dict (technically, any
+            ``fabric.utils._AttributeDict`` instance should work) to pull
+            configuration from.
+
+        .. versionadded:: 2.4
+        """
+        # TODO: import fabric.state.env (need good way to test it first...)
+        # TODO: how to handle somebody accidentally calling this in a process
+        # where 'fabric' is fabric 2, and there's no fabric 1? Probably just a
+        # re-raise of ImportError??
+        # Our only requirement is a non-empty host_string
+        if not env.host_string:
+            raise InvalidV1Env(
+                "Supplied v1 env has an empty `host_string` value! Please make sure you're calling Connection.from_v1 within a connected Fabric 1 session."  # noqa
+            )
+        # TODO: detect collisions with kwargs & except instead of overwriting?
+        # (More Zen of Python compliant, but also, effort, and also, makes it
+        # harder for users to intentionally overwrite!)
+        connect_kwargs = kwargs.setdefault("connect_kwargs", {})
+        kwargs.setdefault("host", env.host_string)
+        shorthand = derive_shorthand(env.host_string)
+        # TODO: don't we need to do the below skipping for user too?
+        kwargs.setdefault("user", env.user)
+        # Skip port if host string seemed to have it; otherwise we hit our own
+        # ambiguity clause in __init__. v1 would also have been doing this
+        # anyways (host string wins over other settings).
+        if not shorthand["port"]:
+            # Run port through int(); v1 inexplicably has a string default...
+            kwargs.setdefault("port", int(env.port))
+        # key_filename defaults to None in v1, but in v2, we expect it to be
+        # either unset, or set to a list. Thus, we only pull it over if it is
+        # not None.
+        if env.key_filename is not None:
+            connect_kwargs.setdefault("key_filename", env.key_filename)
+        # Obtain config values, if not given, from its own from_v1
+        # NOTE: not using setdefault as we truly only want to call
+        # Config.from_v1 when necessary.
+        if "config" not in kwargs:
+            kwargs["config"] = Config.from_v1(env)
+        return cls(**kwargs)
+
+    # TODO: should "reopening" an existing Connection object that has been
+    # closed, be allowed? (See e.g. how v1 detects closed/semi-closed
+    # connections & nukes them before creating a new client to the same host.)
+    # TODO: push some of this into paramiko.client.Client? e.g. expand what
+    # Client.exec_command does, it already allows configuring a subset of what
+    # we do / will eventually do / did in 1.x. It's silly to have to do
+    # .get_transport().open_session().
+    def __init__(
+        self,
+        host,
+        user=None,
+        port=None,
+        config=None,
+        gateway=None,
+        forward_agent=None,
+        connect_timeout=None,
+        connect_kwargs=None,
+        inline_ssh_env=None,
+    ):
+        """
+        Set up a new object representing a server connection.
+
+        :param str host:
+            the hostname (or IP address) of this connection.
+
+            May include shorthand for the ``user`` and/or ``port`` parameters,
+            of the form ``user@host``, ``host:port``, or ``user@host:port``.
+
+            .. note::
+                Due to ambiguity, IPv6 host addresses are incompatible with the
+                ``host:port`` shorthand (though ``user@host`` will still work
+                OK). In other words, the presence of >1 ``:`` character will
+                prevent any attempt to derive a shorthand port number; use the
+                explicit ``port`` parameter instead.
+
+            .. note::
+                If ``host`` matches a ``Host`` clause in loaded SSH config
+                data, and that ``Host`` clause contains a ``Hostname``
+                directive, the resulting `.Connection` object will behave as if
+                ``host`` is equal to that ``Hostname`` value.
+
+                In all cases, the original value of ``host`` is preserved as
+                the ``original_host`` attribute.
+
+                Thus, given SSH config like so::
+
+                    Host myalias
+                        Hostname realhostname
+
+                a call like ``Connection(host='myalias')`` will result in an
+                object whose ``host`` attribute is ``realhostname``, and whose
+                ``original_host`` attribute is ``myalias``.
+
+        :param str user:
+            the login user for the remote connection. Defaults to
+            ``config.user``.
+
+        :param int port:
+            the remote port. Defaults to ``config.port``.
+
+        :param config:
+            configuration settings to use when executing methods on this
+            `.Connection` (e.g. default SSH port and so forth).
+
+            Should be a `.Config` or an `invoke.config.Config`
+            (which will be turned into a `.Config`).
+
+            Default is an anonymous `.Config` object.
+
+        :param gateway:
+            An object to use as a proxy or gateway for this connection.
+
+            This parameter accepts one of the following:
+
+            - another `.Connection` (for a ``ProxyJump`` style gateway);
+            - a shell command string (for a ``ProxyCommand`` style style
+              gateway).
+
+            Default: ``None``, meaning no gatewaying will occur (unless
+            otherwise configured; if one wants to override a configured gateway
+            at runtime, specify ``gateway=False``.)
+
+            .. seealso:: :ref:`ssh-gateways`
+
+        :param bool forward_agent:
+            Whether to enable SSH agent forwarding.
+
+            Default: ``config.forward_agent``.
+
+        :param int connect_timeout:
+            Connection timeout, in seconds.
+
+            Default: ``config.timeouts.connect``.
+
+        .. _connect_kwargs-arg:
+
+        :param dict connect_kwargs:
+            Keyword arguments handed verbatim to
+            `SSHClient.connect <paramiko.client.SSHClient.connect>` (when
+            `.open` is called).
+
+            `.Connection` tries not to grow additional settings/kwargs of its
+            own unless it is adding value of some kind; thus,
+            ``connect_kwargs`` is currently the right place to hand in paramiko
+            connection parameters such as ``pkey`` or ``key_filename``. For
+            example::
+
+                c = Connection(
+                    host="hostname",
+                    user="admin",
+                    connect_kwargs={
+                        "key_filename": "/home/myuser/.ssh/private.key",
+                    },
+                )
+
+            Default: ``config.connect_kwargs``.
+
+        :param bool inline_ssh_env:
+            Whether to send environment variables "inline" as prefixes in front
+            of command strings (``export VARNAME=value && mycommand here``),
+            instead of trying to submit them through the SSH protocol itself
+            (which is the default behavior). This is necessary if the remote
+            server has a restricted ``AcceptEnv`` setting (which is the common
+            default).
+
+            The default value is the value of the ``inline_ssh_env``
+            :ref:`configuration value <default-values>` (which itself defaults
+            to ``False``).
+
+            .. warning::
+                This functionality does **not** currently perform any shell
+                escaping on your behalf! Be careful when using nontrivial
+                values, and note that you can put in your own quoting,
+                backslashing etc if desired.
+
+                Consider using a different approach (such as actual
+                remote shell scripts) if you run into too many issues here.
+
+            .. note::
+                When serializing into prefixed ``FOO=bar`` format, we apply the
+                builtin `sorted` function to the env dictionary's keys, to
+                remove what would otherwise be ambiguous/arbitrary ordering.
+
+            .. note::
+                This setting has no bearing on *local* shell commands; it only
+                affects remote commands, and thus, methods like `.run` and
+                `.sudo`.
+
+        :raises ValueError:
+            if user or port values are given via both ``host`` shorthand *and*
+            their own arguments. (We `refuse the temptation to guess`_).
+
+        .. _refuse the temptation to guess:
+            http://zen-of-python.info/
+            in-the-face-of-ambiguity-refuse-the-temptation-to-guess.html#12
+
+        .. versionchanged:: 2.3
+            Added the ``inline_ssh_env`` parameter.
+        """
+        # NOTE: parent __init__ sets self._config; for now we simply overwrite
+        # that below. If it's somehow problematic we would want to break parent
+        # __init__ up in a manner that is more cleanly overrideable.
+        super(Connection, self).__init__(config=config)
+
+        #: The .Config object referenced when handling default values (for e.g.
+        #: user or port, when not explicitly given) or deciding how to behave.
+        if config is None:
+            config = Config()
+        # Handle 'vanilla' Invoke config objects, which need cloning 'into' one
+        # of our own Configs (which grants the new defaults, etc, while not
+        # squashing them if the Invoke-level config already accounted for them)
+        elif not isinstance(config, Config):
+            config = config.clone(into=Config)
+        self._set(_config=config)
+        # TODO: when/how to run load_files, merge, load_shell_env, etc?
+        # TODO: i.e. what is the lib use case here (and honestly in invoke too)
+
+        shorthand = self.derive_shorthand(host)
+        host = shorthand["host"]
+        err = "You supplied the {} via both shorthand and kwarg! Please pick one."  # noqa
+        if shorthand["user"] is not None:
+            if user is not None:
+                raise ValueError(err.format("user"))
+            user = shorthand["user"]
+        if shorthand["port"] is not None:
+            if port is not None:
+                raise ValueError(err.format("port"))
+            port = shorthand["port"]
+
+        # NOTE: we load SSH config data as early as possible as it has
+        # potential to affect nearly every other attribute.
+        #: The per-host SSH config data, if any. (See :ref:`ssh-config`.)
+        self.ssh_config = self.config.base_ssh_config.lookup(host)
+
+        self.original_host = host
+        #: The hostname of the target server.
+        self.host = host
+        if "hostname" in self.ssh_config:
+            # TODO: log that this occurred?
+            self.host = self.ssh_config["hostname"]
+
+        #: The username this connection will use to connect to the remote end.
+        self.user = user or self.ssh_config.get("user", self.config.user)
+        # TODO: is it _ever_ possible to give an empty user value (e.g.
+        # user='')? E.g. do some SSH server specs allow for that?
+
+        #: The network port to connect on.
+        self.port = port or int(self.ssh_config.get("port", self.config.port))
+
+        # Gateway/proxy/bastion/jump setting: non-None values - string,
+        # Connection, even eg False - get set directly; None triggers seek in
+        # config/ssh_config
+        #: The gateway `.Connection` or ``ProxyCommand`` string to be used,
+        #: if any.
+        self.gateway = gateway if gateway is not None else self.get_gateway()
+        # NOTE: we use string above, vs ProxyCommand obj, to avoid spinning up
+        # the ProxyCommand subprocess at init time, vs open() time.
+        # TODO: make paramiko.proxy.ProxyCommand lazy instead?
+
+        if forward_agent is None:
+            # Default to config...
+            forward_agent = self.config.forward_agent
+            # But if ssh_config is present, it wins
+            if "forwardagent" in self.ssh_config:
+                # TODO: SSHConfig really, seriously needs some love here, god
+                map_ = {"yes": True, "no": False}
+                forward_agent = map_[self.ssh_config["forwardagent"]]
+        #: Whether agent forwarding is enabled.
+        self.forward_agent = forward_agent
+
+        if connect_timeout is None:
+            connect_timeout = self.ssh_config.get(
+                "connecttimeout", self.config.timeouts.connect
+            )
+        if connect_timeout is not None:
+            connect_timeout = int(connect_timeout)
+        #: Connection timeout
+        self.connect_timeout = connect_timeout
+
+        #: Keyword arguments given to `paramiko.client.SSHClient.connect` when
+        #: `open` is called.
+        self.connect_kwargs = self.resolve_connect_kwargs(connect_kwargs)
+
+        #: The `paramiko.client.SSHClient` instance this connection wraps.
+        client = SSHClient()
+        client.set_missing_host_key_policy(AutoAddPolicy())
+        self.client = client
+
+        #: A convenience handle onto the return value of
+        #: ``self.client.get_transport()``.
+        self.transport = None
+
+        if inline_ssh_env is None:
+            inline_ssh_env = self.config.inline_ssh_env
+        #: Whether to construct remote command lines with env vars prefixed
+        #: inline.
+        self.inline_ssh_env = inline_ssh_env
+
+    def resolve_connect_kwargs(self, connect_kwargs):
+        # Grab connect_kwargs from config if not explicitly given.
+        if connect_kwargs is None:
+            # TODO: is it better to pre-empt conflicts w/ manually-handled
+            # connect() kwargs (hostname, username, etc) here or in open()?
+            # We're doing open() for now in case e.g. someone manually modifies
+            # .connect_kwargs attributewise, but otherwise it feels better to
+            # do it early instead of late.
+            connect_kwargs = self.config.connect_kwargs
+        # Special case: key_filename gets merged instead of overridden.
+        # TODO: probably want some sorta smart merging generally, special cases
+        # are bad.
+        elif "key_filename" in self.config.connect_kwargs:
+            kwarg_val = connect_kwargs.get("key_filename", [])
+            conf_val = self.config.connect_kwargs["key_filename"]
+            # Config value comes before kwarg value (because it may contain
+            # CLI flag value.)
+            connect_kwargs["key_filename"] = conf_val + kwarg_val
+
+        # SSH config identityfile values come last in the key_filename
+        # 'hierarchy'.
+        if "identityfile" in self.ssh_config:
+            connect_kwargs.setdefault("key_filename", [])
+            connect_kwargs["key_filename"].extend(
+                self.ssh_config["identityfile"]
+            )
+
+        return connect_kwargs
+
+    def get_gateway(self):
+        # SSH config wins over Invoke-style config
+        if "proxyjump" in self.ssh_config:
+            # Reverse hop1,hop2,hop3 style ProxyJump directive so we start
+            # with the final (itself non-gatewayed) hop and work up to
+            # the front (actual, supplied as our own gateway) hop
+            hops = reversed(self.ssh_config["proxyjump"].split(","))
+            prev_gw = None
+            for hop in hops:
+                # Short-circuit if we appear to be our own proxy, which would
+                # be a RecursionError. Implies SSH config wildcards.
+                # TODO: in an ideal world we'd check user/port too in case they
+                # differ, but...seriously? They can file a PR with those extra
+                # half dozen test cases in play, E_NOTIME
+                if self.derive_shorthand(hop)["host"] == self.host:
+                    return None
+                # Happily, ProxyJump uses identical format to our host
+                # shorthand...
+                kwargs = dict(config=self.config.clone())
+                if prev_gw is not None:
+                    kwargs["gateway"] = prev_gw
+                cxn = Connection(hop, **kwargs)
+                prev_gw = cxn
+            return prev_gw
+        elif "proxycommand" in self.ssh_config:
+            # Just a string, which we interpret as a proxy command..
+            return self.ssh_config["proxycommand"]
+        # Fallback: config value (may be None).
+        return self.config.gateway
+
+    def __repr__(self):
+        # Host comes first as it's the most common differentiator by far
+        bits = [("host", self.host)]
+        # TODO: maybe always show user regardless? Explicit is good...
+        if self.user != self.config.user:
+            bits.append(("user", self.user))
+        # TODO: harder to make case for 'always show port'; maybe if it's
+        # non-22 (even if config has overridden the local default)?
+        if self.port != self.config.port:
+            bits.append(("port", self.port))
+        # NOTE: sometimes self.gateway may be eg False if someone wants to
+        # explicitly override a configured non-None value (as otherwise it's
+        # impossible for __init__ to tell if a None means "nothing given" or
+        # "seriously please no gatewaying". So, this must always be a vanilla
+        # truth test and not eg "is not None".
+        if self.gateway:
+            # Displaying type because gw params would probs be too verbose
+            val = "proxyjump"
+            if isinstance(self.gateway, string_types):
+                val = "proxycommand"
+            bits.append(("gw", val))
+        return "<Connection {}>".format(
+            " ".join("{}={}".format(*x) for x in bits)
+        )
+
+    def _identity(self):
+        # TODO: consider including gateway and maybe even other init kwargs?
+        # Whether two cxns w/ same user/host/port but different
+        # gateway/keys/etc, should be considered "the same", is unclear.
+        return (self.host, self.user, self.port)
+
+    def __eq__(self, other):
+        if not isinstance(other, Connection):
+            return False
+        return self._identity() == other._identity()
+
+    def __lt__(self, other):
+        return self._identity() < other._identity()
+
+    def __hash__(self):
+        # NOTE: this departs from Context/DataProxy, which is not usefully
+        # hashable.
+        return hash(self._identity())
+
+    def derive_shorthand(self, host_string):
+        # NOTE: used to be defined inline; preserving API call for both
+        # backwards compatibility and because it seems plausible we may want to
+        # modify behavior later, using eg config or other attributes.
+        return derive_shorthand(host_string)
+
+    @property
+    def is_connected(self):
+        """
+        Whether or not this connection is actually open.
+
+        .. versionadded:: 2.0
+        """
+        return self.transport.active if self.transport else False
+
+    def open(self):
+        """
+        Initiate an SSH connection to the host/port this object is bound to.
+
+        This may include activating the configured gateway connection, if one
+        is set.
+
+        Also saves a handle to the now-set Transport object for easier access.
+
+        Various connect-time settings (and/or their corresponding :ref:`SSH
+        config options <ssh-config>`) are utilized here in the call to
+        `SSHClient.connect <paramiko.client.SSHClient.connect>`. (For details,
+        see :doc:`the configuration docs </concepts/configuration>`.)
+
+        .. versionadded:: 2.0
+        """
+        # Short-circuit
+        if self.is_connected:
+            return
+        err = "Refusing to be ambiguous: connect() kwarg '{}' was given both via regular arg and via connect_kwargs!"  # noqa
+        # These may not be given, period
+        for key in """
+            hostname
+            port
+            username
+        """.split():
+            if key in self.connect_kwargs:
+                raise ValueError(err.format(key))
+        # These may be given one way or the other, but not both
+        if (
+            "timeout" in self.connect_kwargs
+            and self.connect_timeout is not None
+        ):
+            raise ValueError(err.format("timeout"))
+        # No conflicts -> merge 'em together
+        kwargs = dict(
+            self.connect_kwargs,
+            username=self.user,
+            hostname=self.host,
+            port=self.port,
+        )
+        if self.gateway:
+            kwargs["sock"] = self.open_gateway()
+        if self.connect_timeout:
+            kwargs["timeout"] = self.connect_timeout
+        # Strip out empty defaults for less noisy debugging
+        if "key_filename" in kwargs and not kwargs["key_filename"]:
+            del kwargs["key_filename"]
+        # Actually connect!
+        self.client.connect(**kwargs)
+        self.transport = self.client.get_transport()
+
+    def open_gateway(self):
+        """
+        Obtain a socket-like object from `gateway`.
+
+        :returns:
+            A ``direct-tcpip`` `paramiko.channel.Channel`, if `gateway` was a
+            `.Connection`; or a `~paramiko.proxy.ProxyCommand`, if `gateway`
+            was a string.
+
+        .. versionadded:: 2.0
+        """
+        # ProxyCommand is faster to set up, so do it first.
+        if isinstance(self.gateway, string_types):
+            # Leverage a dummy SSHConfig to ensure %h/%p/etc are parsed.
+            # TODO: use real SSH config once loading one properly is
+            # implemented.
+            ssh_conf = SSHConfig()
+            dummy = "Host {}\n    ProxyCommand {}"
+            ssh_conf.parse(StringIO(dummy.format(self.host, self.gateway)))
+            return ProxyCommand(ssh_conf.lookup(self.host)["proxycommand"])
+        # Handle inner-Connection gateway type here.
+        # TODO: logging
+        self.gateway.open()
+        # TODO: expose the opened channel itself as an attribute? (another
+        # possible argument for separating the two gateway types...) e.g. if
+        # someone wanted to piggyback on it for other same-interpreter socket
+        # needs...
+        # TODO: and the inverse? allow users to supply their own socket/like
+        # object they got via $WHEREEVER?
+        # TODO: how best to expose timeout param? reuse general connection
+        # timeout from config?
+        return self.gateway.transport.open_channel(
+            kind="direct-tcpip",
+            dest_addr=(self.host, int(self.port)),
+            # NOTE: src_addr needs to be 'empty but not None' values to
+            # correctly encode into a network message. Theoretically Paramiko
+            # could auto-interpret None sometime & save us the trouble.
+            src_addr=("", 0),
+        )
+
+    def close(self):
+        """
+        Terminate the network connection to the remote end, if open.
+
+        If no connection is open, this method does nothing.
+
+        .. versionadded:: 2.0
+        """
+        if self.is_connected:
+            self.client.close()
+            if self.forward_agent and self._agent_handler is not None:
+                self._agent_handler.close()
+
+    def __enter__(self):
+        return self
+
+    def __exit__(self, *exc):
+        self.close()
+
+    @opens
+    def create_session(self):
+        channel = self.transport.open_session()
+        if self.forward_agent:
+            self._agent_handler = AgentRequestHandler(channel)
+        return channel
+
+    def _remote_runner(self):
+        return self.config.runners.remote(self, inline_env=self.inline_ssh_env)
+
+    @opens
+    def run(self, command, **kwargs):
+        """
+        Execute a shell command on the remote end of this connection.
+
+        This method wraps an SSH-capable implementation of
+        `invoke.runners.Runner.run`; see its documentation for details.
+
+        .. warning::
+            There are a few spots where Fabric departs from Invoke's default
+            settings/behaviors; they are documented under
+            `.Config.global_defaults`.
+
+        .. versionadded:: 2.0
+        """
+        return self._run(self._remote_runner(), command, **kwargs)
+
+    @opens
+    def sudo(self, command, **kwargs):
+        """
+        Execute a shell command, via ``sudo``, on the remote end.
+
+        This method is identical to `invoke.context.Context.sudo` in every way,
+        except in that -- like `run` -- it honors per-host/per-connection
+        configuration overrides in addition to the generic/global ones. Thus,
+        for example, per-host sudo passwords may be configured.
+
+        .. versionadded:: 2.0
+        """
+        return self._sudo(self._remote_runner(), command, **kwargs)
+
+    def local(self, *args, **kwargs):
+        """
+        Execute a shell command on the local system.
+
+        This method is effectively a wrapper of `invoke.run`; see its docs for
+        details and call signature.
+
+        .. versionadded:: 2.0
+        """
+        # Superclass run() uses runners.local, so we can literally just call it
+        # straight.
+        return super(Connection, self).run(*args, **kwargs)
+
+    @opens
+    def sftp(self):
+        """
+        Return a `~paramiko.sftp_client.SFTPClient` object.
+
+        If called more than one time, memoizes the first result; thus, any
+        given `.Connection` instance will only ever have a single SFTP client,
+        and state (such as that managed by
+        `~paramiko.sftp_client.SFTPClient.chdir`) will be preserved.
+
+        .. versionadded:: 2.0
+        """
+        if self._sftp is None:
+            self._sftp = self.client.open_sftp()
+        return self._sftp
+
+    def get(self, *args, **kwargs):
+        """
+        Get a remote file to the local filesystem or file-like object.
+
+        Simply a wrapper for `.Transfer.get`. Please see its documentation for
+        all details.
+
+        .. versionadded:: 2.0
+        """
+        return Transfer(self).get(*args, **kwargs)
+
+    def put(self, *args, **kwargs):
+        """
+        Put a remote file (or file-like object) to the remote filesystem.
+
+        Simply a wrapper for `.Transfer.put`. Please see its documentation for
+        all details.
+
+        .. versionadded:: 2.0
+        """
+        return Transfer(self).put(*args, **kwargs)
+
+    # TODO: yield the socket for advanced users? Other advanced use cases
+    # (perhaps factor out socket creation itself)?
+    # TODO: probably push some of this down into Paramiko
+    @contextmanager
+    @opens
+    def forward_local(
+        self,
+        local_port,
+        remote_port=None,
+        remote_host="localhost",
+        local_host="localhost",
+    ):
+        """
+        Open a tunnel connecting ``local_port`` to the server's environment.
+
+        For example, say you want to connect to a remote PostgreSQL database
+        which is locked down and only accessible via the system it's running
+        on. You have SSH access to this server, so you can temporarily make
+        port 5432 on your local system act like port 5432 on the server::
+
+            import psycopg2
+            from fabric import Connection
+
+            with Connection('my-db-server').forward_local(5432):
+                db = psycopg2.connect(
+                    host='localhost', port=5432, database='mydb'
+                )
+                # Do things with 'db' here
+
+        This method is analogous to using the ``-L`` option of OpenSSH's
+        ``ssh`` program.
+
+        :param int local_port: The local port number on which to listen.
+
+        :param int remote_port:
+            The remote port number. Defaults to the same value as
+            ``local_port``.
+
+        :param str local_host:
+            The local hostname/interface on which to listen. Default:
+            ``localhost``.
+
+        :param str remote_host:
+            The remote hostname serving the forwarded remote port. Default:
+            ``localhost`` (i.e., the host this `.Connection` is connected to.)
+
+        :returns:
+            Nothing; this method is only useful as a context manager affecting
+            local operating system state.
+
+        .. versionadded:: 2.0
+        """
+        if not remote_port:
+            remote_port = local_port
+
+        # TunnelManager does all of the work, sitting in the background (so we
+        # can yield) and spawning threads every time somebody connects to our
+        # local port.
+        finished = Event()
+        manager = TunnelManager(
+            local_port=local_port,
+            local_host=local_host,
+            remote_port=remote_port,
+            remote_host=remote_host,
+            # TODO: not a huge fan of handing in our transport, but...?
+            transport=self.transport,
+            finished=finished,
+        )
+        manager.start()
+
+        # Return control to caller now that things ought to be operational
+        try:
+            yield
+        # Teardown once user exits block
+        finally:
+            # Signal to manager that it should close all open tunnels
+            finished.set()
+            # Then wait for it to do so
+            manager.join()
+            # Raise threading errors from within the manager, which would be
+            # one of:
+            # - an inner ThreadException, which was created by the manager on
+            # behalf of its Tunnels; this gets directly raised.
+            # - some other exception, which would thus have occurred in the
+            # manager itself; we wrap this in a new ThreadException.
+            # NOTE: in these cases, some of the metadata tracking in
+            # ExceptionHandlingThread/ExceptionWrapper/ThreadException (which
+            # is useful when dealing with multiple nearly-identical sibling IO
+            # threads) is superfluous, but it doesn't feel worth breaking
+            # things up further; we just ignore it for now.
+            wrapper = manager.exception()
+            if wrapper is not None:
+                if wrapper.type is ThreadException:
+                    raise wrapper.value
+                else:
+                    raise ThreadException([wrapper])
+
+            # TODO: cancel port forward on transport? Does that even make sense
+            # here (where we used direct-tcpip) vs the opposite method (which
+            # is what uses forward-tcpip)?
+
+    # TODO: probably push some of this down into Paramiko
+    @contextmanager
+    @opens
+    def forward_remote(
+        self,
+        remote_port,
+        local_port=None,
+        remote_host="127.0.0.1",
+        local_host="localhost",
+    ):
+        """
+        Open a tunnel connecting ``remote_port`` to the local environment.
+
+        For example, say you're running a daemon in development mode on your
+        workstation at port 8080, and want to funnel traffic to it from a
+        production or staging environment.
+
+        In most situations this isn't possible as your office/home network
+        probably blocks inbound traffic. But you have SSH access to this
+        server, so you can temporarily make port 8080 on that server act like
+        port 8080 on your workstation::
+
+            from fabric import Connection
+
+            c = Connection('my-remote-server')
+            with c.forward_remote(8080):
+                c.run("remote-data-writer --port 8080")
+                # Assuming remote-data-writer runs until interrupted, this will
+                # stay open until you Ctrl-C...
+
+        This method is analogous to using the ``-R`` option of OpenSSH's
+        ``ssh`` program.
+
+        :param int remote_port: The remote port number on which to listen.
+
+        :param int local_port:
+            The local port number. Defaults to the same value as
+            ``remote_port``.
+
+        :param str local_host:
+            The local hostname/interface the forwarded connection talks to.
+            Default: ``localhost``.
+
+        :param str remote_host:
+            The remote interface address to listen on when forwarding
+            connections. Default: ``127.0.0.1`` (i.e. only listen on the remote
+            localhost).
+
+        :returns:
+            Nothing; this method is only useful as a context manager affecting
+            local operating system state.
+
+        .. versionadded:: 2.0
+        """
+        if not local_port:
+            local_port = remote_port
+        # Callback executes on each connection to the remote port and is given
+        # a Channel hooked up to said port. (We don't actually care about the
+        # source/dest host/port pairs at all; only whether the channel has data
+        # to read and suchlike.)
+        # We then pair that channel with a new 'outbound' socket connection to
+        # the local host/port being forwarded, in a new Tunnel.
+        # That Tunnel is then added to a shared data structure so we can track
+        # & close them during shutdown.
+        #
+        # TODO: this approach is less than ideal because we have to share state
+        # between ourselves & the callback handed into the transport's own
+        # thread handling (which is roughly analogous to our self-controlled
+        # TunnelManager for local forwarding). See if we can use more of
+        # Paramiko's API (or improve it and then do so) so that isn't
+        # necessary.
+        tunnels = []
+
+        def callback(channel, src_addr_tup, dst_addr_tup):
+            sock = socket.socket()
+            # TODO: handle connection failure such that channel, etc get closed
+            sock.connect((local_host, local_port))
+            # TODO: we don't actually need to generate the Events at our level,
+            # do we? Just let Tunnel.__init__ do it; all we do is "press its
+            # button" on shutdown...
+            tunnel = Tunnel(channel=channel, sock=sock, finished=Event())
+            tunnel.start()
+            # Communication between ourselves & the Paramiko handling subthread
+            tunnels.append(tunnel)
+
+        # Ask Paramiko (really, the remote sshd) to call our callback whenever
+        # connections are established on the remote iface/port.
+        # transport.request_port_forward(remote_host, remote_port, callback)
+        try:
+            self.transport.request_port_forward(
+                address=remote_host, port=remote_port, handler=callback
+            )
+            yield
+        finally:
+            # TODO: see above re: lack of a TunnelManager
+            # TODO: and/or also refactor with TunnelManager re: shutdown logic.
+            # E.g. maybe have a non-thread TunnelManager-alike with a method
+            # that acts as the callback? At least then there's a tiny bit more
+            # encapsulation...meh.
+            for tunnel in tunnels:
+                tunnel.finished.set()
+                tunnel.join()
+            self.transport.cancel_port_forward(
+                address=remote_host, port=remote_port
+            )
diff -Nru fabric-1.14.0/fabric/context_managers.py fabric-2.5.0/fabric/context_managers.py
--- fabric-1.14.0/fabric/context_managers.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/fabric/context_managers.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,601 +0,0 @@
-"""
-Context managers for use with the ``with`` statement.
-
-.. note:: When using Python 2.5, you will need to start your fabfile
-    with ``from __future__ import with_statement`` in order to make use of
-    the ``with`` statement (which is a regular, non ``__future__`` feature of
-    Python 2.6+.)
-
-.. note:: If you are using multiple directly nested ``with`` statements, it can
-    be convenient to use multiple context expressions in one single with
-    statement. Instead of writing::
-
-        with cd('/path/to/app'):
-            with prefix('workon myvenv'):
-                run('./manage.py syncdb')
-                run('./manage.py loaddata myfixture')
-
-    you can write::
-
-        with cd('/path/to/app'), prefix('workon myvenv'):
-            run('./manage.py syncdb')
-            run('./manage.py loaddata myfixture')
-
-    Note that you need Python 2.7+ for this to work. On Python 2.5 or 2.6, you
-    can do the following::
-
-        from contextlib import nested
-
-        with nested(cd('/path/to/app'), prefix('workon myvenv')):
-            ...
-
-    Finally, note that `~fabric.context_managers.settings` implements
-    ``nested`` itself -- see its API doc for details.
-"""
-
-from contextlib import contextmanager, nested
-import socket
-import select
-
-from fabric.thread_handling import ThreadHandler
-from fabric.state import output, win32, connections, env
-from fabric import state
-from fabric.utils import isatty
-
-if not win32:
-    import termios
-    import tty
-
-
-def _set_output(groups, which):
-    """
-    Refactored subroutine used by ``hide`` and ``show``.
-    """
-    previous = {}
-    try:
-        # Preserve original values, pull in new given value to use
-        for group in output.expand_aliases(groups):
-            previous[group] = output[group]
-            output[group] = which
-        # Yield control
-        yield
-    finally:
-        # Restore original values
-        output.update(previous)
-
-
-def documented_contextmanager(func):
-    wrapper = contextmanager(func)
-    wrapper.undecorated = func
-    return wrapper
-
-
-@documented_contextmanager
-def show(*groups):
-    """
-    Context manager for setting the given output ``groups`` to True.
-
-    ``groups`` must be one or more strings naming the output groups defined in
-    `~fabric.state.output`. The given groups will be set to True for the
-    duration of the enclosed block, and restored to their previous value
-    afterwards.
-
-    For example, to turn on debug output (which is typically off by default)::
-
-        def my_task():
-            with show('debug'):
-                run('ls /var/www')
-
-    As almost all output groups are displayed by default, `show` is most useful
-    for turning on the normally-hidden ``debug`` group, or when you know or
-    suspect that code calling your own code is trying to hide output with
-    `hide`.
-    """
-    return _set_output(groups, True)
-
-
-@documented_contextmanager
-def hide(*groups):
-    """
-    Context manager for setting the given output ``groups`` to False.
-
-    ``groups`` must be one or more strings naming the output groups defined in
-    `~fabric.state.output`. The given groups will be set to False for the
-    duration of the enclosed block, and restored to their previous value
-    afterwards.
-
-    For example, to hide the "[hostname] run:" status lines, as well as
-    preventing printout of stdout and stderr, one might use `hide` as follows::
-
-        def my_task():
-            with hide('running', 'stdout', 'stderr'):
-                run('ls /var/www')
-    """
-    return _set_output(groups, False)
-
-
-@documented_contextmanager
-def _setenv(variables):
-    """
-    Context manager temporarily overriding ``env`` with given key/value pairs.
-
-    A callable that returns a dict can also be passed. This is necessary when
-    new values are being calculated from current values, in order to ensure that
-    the "current" value is current at the time that the context is entered, not
-    when the context manager is initialized. (See Issue #736.)
-
-    This context manager is used internally by `settings` and is not intended
-    to be used directly.
-    """
-    if callable(variables):
-        variables = variables()
-    clean_revert = variables.pop('clean_revert', False)
-    previous = {}
-    new = []
-    for key, value in variables.iteritems():
-        if key in state.env:
-            previous[key] = state.env[key]
-        else:
-            new.append(key)
-        state.env[key] = value
-    try:
-        yield
-    finally:
-        if clean_revert:
-            for key, value in variables.iteritems():
-                # If the current env value for this key still matches the
-                # value we set it to beforehand, we are OK to revert it to the
-                # pre-block value.
-                if key in state.env and value == state.env[key]:
-                    if key in previous:
-                        state.env[key] = previous[key]
-                    else:
-                        del state.env[key]
-        else:
-            state.env.update(previous)
-            for key in new:
-                del state.env[key]
-
-
-def settings(*args, **kwargs):
-    """
-    Nest context managers and/or override ``env`` variables.
-
-    `settings` serves two purposes:
-
-    * Most usefully, it allows temporary overriding/updating of ``env`` with
-      any provided keyword arguments, e.g. ``with settings(user='foo'):``.
-      Original values, if any, will be restored once the ``with`` block closes.
-
-        * The keyword argument ``clean_revert`` has special meaning for
-          ``settings`` itself (see below) and will be stripped out before
-          execution.
-
-    * In addition, it will use `contextlib.nested`_ to nest any given
-      non-keyword arguments, which should be other context managers, e.g.
-      ``with settings(hide('stderr'), show('stdout')):``.
-
-    .. _contextlib.nested: http://docs.python.org/library/contextlib.html#contextlib.nested
-
-    These behaviors may be specified at the same time if desired. An example
-    will hopefully illustrate why this is considered useful::
-
-        def my_task():
-            with settings(
-                hide('warnings', 'running', 'stdout', 'stderr'),
-                warn_only=True
-            ):
-                if run('ls /etc/lsb-release'):
-                    return 'Ubuntu'
-                elif run('ls /etc/redhat-release'):
-                    return 'RedHat'
-
-    The above task executes a `run` statement, but will warn instead of
-    aborting if the ``ls`` fails, and all output -- including the warning
-    itself -- is prevented from printing to the user. The end result, in this
-    scenario, is a completely silent task that allows the caller to figure out
-    what type of system the remote host is, without incurring the handful of
-    output that would normally occur.
-
-    Thus, `settings` may be used to set any combination of environment
-    variables in tandem with hiding (or showing) specific levels of output, or
-    in tandem with any other piece of Fabric functionality implemented as a
-    context manager.
-
-    If ``clean_revert`` is set to ``True``, ``settings`` will **not** revert
-    keys which are altered within the nested block, instead only reverting keys
-    whose values remain the same as those given. More examples will make this
-    clear; below is how ``settings`` operates normally::
-
-        # Before the block, env.parallel defaults to False, host_string to None
-        with settings(parallel=True, host_string='myhost'):
-            # env.parallel is True
-            # env.host_string is 'myhost'
-            env.host_string = 'otherhost'
-            # env.host_string is now 'otherhost'
-        # Outside the block:
-        # * env.parallel is False again
-        # * env.host_string is None again
-
-    The internal modification of ``env.host_string`` is nullified -- not always
-    desirable. That's where ``clean_revert`` comes in::
-
-        # Before the block, env.parallel defaults to False, host_string to None
-        with settings(parallel=True, host_string='myhost', clean_revert=True):
-            # env.parallel is True
-            # env.host_string is 'myhost'
-            env.host_string = 'otherhost'
-            # env.host_string is now 'otherhost'
-        # Outside the block:
-        # * env.parallel is False again
-        # * env.host_string remains 'otherhost'
-
-    Brand new keys which did not exist in ``env`` prior to using ``settings``
-    are also preserved if ``clean_revert`` is active. When ``False``, such keys
-    are removed when the block exits.
-
-    .. versionadded:: 1.4.1
-        The ``clean_revert`` kwarg.
-    """
-    managers = list(args)
-    if kwargs:
-        managers.append(_setenv(kwargs))
-    return nested(*managers)
-
-
-def cd(path):
-    """
-    Context manager that keeps directory state when calling remote operations.
-
-    Any calls to `run`, `sudo`, `get`, or `put` within the wrapped block will
-    implicitly have a string similar to ``"cd <path> && "`` prefixed in order
-    to give the sense that there is actually statefulness involved.
-
-    .. note::
-        `cd` only affects *remote* paths -- to modify *local* paths, use
-        `~fabric.context_managers.lcd`.
-
-    Because use of `cd` affects all such invocations, any code making use of
-    those operations, such as much of the ``contrib`` section, will also be
-    affected by use of `cd`.
-
-    Like the actual 'cd' shell builtin, `cd` may be called with relative paths
-    (keep in mind that your default starting directory is your remote user's
-    ``$HOME``) and may be nested as well.
-
-    Below is a "normal" attempt at using the shell 'cd', which doesn't work due
-    to how shell-less SSH connections are implemented -- state is **not** kept
-    between invocations of `run` or `sudo`::
-
-        run('cd /var/www')
-        run('ls')
-
-    The above snippet will list the contents of the remote user's ``$HOME``
-    instead of ``/var/www``. With `cd`, however, it will work as expected::
-
-        with cd('/var/www'):
-            run('ls') # Turns into "cd /var/www && ls"
-
-    Finally, a demonstration (see inline comments) of nesting::
-
-        with cd('/var/www'):
-            run('ls') # cd /var/www && ls
-            with cd('website1'):
-                run('ls') # cd /var/www/website1 && ls
-
-    .. note::
-
-        This context manager is currently implemented by appending to (and, as
-        always, restoring afterwards) the current value of an environment
-        variable, ``env.cwd``. However, this implementation may change in the
-        future, so we do not recommend manually altering ``env.cwd`` -- only
-        the *behavior* of `cd` will have any guarantee of backwards
-        compatibility.
-
-    .. note::
-
-        Space characters will be escaped automatically to make dealing with
-        such directory names easier.
-
-    .. versionchanged:: 1.0
-        Applies to `get` and `put` in addition to the command-running
-        operations.
-
-    .. seealso:: `~fabric.context_managers.lcd`
-    """
-    return _change_cwd('cwd', path)
-
-
-def lcd(path):
-    """
-    Context manager for updating local current working directory.
-
-    This context manager is identical to `~fabric.context_managers.cd`, except
-    that it changes a different env var (`lcwd`, instead of `cwd`) and thus
-    only affects the invocation of `~fabric.operations.local` and the local
-    arguments to `~fabric.operations.get`/`~fabric.operations.put`.
-
-    Relative path arguments are relative to the local user's current working
-    directory, which will vary depending on where Fabric (or Fabric-using code)
-    was invoked. You can check what this is with `os.getcwd
-    <http://docs.python.org/release/2.6/library/os.html#os.getcwd>`_. It may be
-    useful to pin things relative to the location of the fabfile in use, which
-    may be found in :ref:`env.real_fabfile <real-fabfile>`
-
-    .. versionadded:: 1.0
-    """
-    return _change_cwd('lcwd', path)
-
-
-def _change_cwd(which, path):
-    path = path.replace(' ', '\ ')
-    if state.env.get(which) and not path.startswith('/') and not path.startswith('~'):
-        new_cwd = state.env.get(which) + '/' + path
-    else:
-        new_cwd = path
-    return _setenv({which: new_cwd})
-
-
-def path(path, behavior='append'):
-    """
-    Append the given ``path`` to the PATH used to execute any wrapped commands.
-
-    Any calls to `run` or `sudo` within the wrapped block will implicitly have
-    a string similar to ``"PATH=$PATH:<path> "`` prepended before the given
-    command.
-
-    You may customize the behavior of `path` by specifying the optional
-    ``behavior`` keyword argument, as follows:
-
-    * ``'append'``: append given path to the current ``$PATH``, e.g.
-      ``PATH=$PATH:<path>``. This is the default behavior.
-    * ``'prepend'``: prepend given path to the current ``$PATH``, e.g.
-      ``PATH=<path>:$PATH``.
-    * ``'replace'``: ignore previous value of ``$PATH`` altogether, e.g.
-      ``PATH=<path>``.
-
-    .. note::
-
-        This context manager is currently implemented by modifying (and, as
-        always, restoring afterwards) the current value of environment
-        variables, ``env.path`` and ``env.path_behavior``. However, this
-        implementation may change in the future, so we do not recommend
-        manually altering them directly.
-
-    .. versionadded:: 1.0
-    """
-    return _setenv({'path': path, 'path_behavior': behavior})
-
-
-def prefix(command):
-    """
-    Prefix all wrapped `run`/`sudo` commands with given command plus ``&&``.
-
-    This is nearly identical to `~fabric.operations.cd`, except that nested
-    invocations append to a list of command strings instead of modifying a
-    single string.
-
-    Most of the time, you'll want to be using this alongside a shell script
-    which alters shell state, such as ones which export or alter shell
-    environment variables.
-
-    For example, one of the most common uses of this tool is with the
-    ``workon`` command from `virtualenvwrapper
-    <http://www.doughellmann.com/projects/virtualenvwrapper/>`_::
-
-        with prefix('workon myvenv'):
-            run('./manage.py syncdb')
-
-    In the above snippet, the actual shell command run would be this::
-
-        $ workon myvenv && ./manage.py syncdb
-
-    This context manager is compatible with `~fabric.context_managers.cd`, so
-    if your virtualenv doesn't ``cd`` in its ``postactivate`` script, you could
-    do the following::
-
-        with cd('/path/to/app'):
-            with prefix('workon myvenv'):
-                run('./manage.py syncdb')
-                run('./manage.py loaddata myfixture')
-
-    Which would result in executions like so::
-
-        $ cd /path/to/app && workon myvenv && ./manage.py syncdb
-        $ cd /path/to/app && workon myvenv && ./manage.py loaddata myfixture
-
-    Finally, as alluded to near the beginning,
-    `~fabric.context_managers.prefix` may be nested if desired, e.g.::
-
-        with prefix('workon myenv'):
-            run('ls')
-            with prefix('source /some/script'):
-                run('touch a_file')
-
-    The result::
-
-        $ workon myenv && ls
-        $ workon myenv && source /some/script && touch a_file
-
-    Contrived, but hopefully illustrative.
-    """
-    return _setenv(lambda: {'command_prefixes': state.env.command_prefixes + [command]})
-
-
-@documented_contextmanager
-def char_buffered(pipe):
-    """
-    Force local terminal ``pipe`` be character, not line, buffered.
-
-    Only applies on Unix-based systems; on Windows this is a no-op.
-    """
-    if win32 or not isatty(pipe):
-        yield
-    else:
-        old_settings = termios.tcgetattr(pipe)
-        tty.setcbreak(pipe)
-        try:
-            yield
-        finally:
-            termios.tcsetattr(pipe, termios.TCSADRAIN, old_settings)
-
-
-def shell_env(**kw):
-    """
-    Set shell environment variables for wrapped commands.
-
-    For example, the below shows how you might set a ZeroMQ related environment
-    variable when installing a Python ZMQ library::
-
-        with shell_env(ZMQ_DIR='/home/user/local'):
-            run('pip install pyzmq')
-
-    As with `~fabric.context_managers.prefix`, this effectively turns the
-    ``run`` command into::
-
-        $ export ZMQ_DIR='/home/user/local' && pip install pyzmq
-
-    Multiple key-value pairs may be given simultaneously.
-
-    .. note::
-        If used to affect the behavior of `~fabric.operations.local` when
-        running from a Windows localhost, ``SET`` commands will be used to
-        implement this feature.
-    """
-    return _setenv({'shell_env': kw})
-
-
-def _forwarder(chan, sock):
-    # Bidirectionally forward data between a socket and a Paramiko channel.
-    while True:
-        r, w, x = select.select([sock, chan], [], [])
-        if sock in r:
-            data = sock.recv(1024)
-            if len(data) == 0:
-                break
-            chan.send(data)
-        if chan in r:
-            data = chan.recv(1024)
-            if len(data) == 0:
-                break
-            sock.send(data)
-    chan.close()
-    sock.close()
-
-
-@documented_contextmanager
-def remote_tunnel(remote_port, local_port=None, local_host="localhost",
-    remote_bind_address="127.0.0.1"):
-    """
-    Create a tunnel forwarding a locally-visible port to the remote target.
-
-    For example, you can let the remote host access a database that is
-    installed on the client host::
-
-        # Map localhost:6379 on the server to localhost:6379 on the client,
-        # so that the remote 'redis-cli' program ends up speaking to the local
-        # redis-server.
-        with remote_tunnel(6379):
-            run("redis-cli -i")
-
-    The database might be installed on a client only reachable from the client
-    host (as opposed to *on* the client itself)::
-
-        # Map localhost:6379 on the server to redis.internal:6379 on the client
-        with remote_tunnel(6379, local_host="redis.internal")
-            run("redis-cli -i")
-
-    ``remote_tunnel`` accepts up to four arguments:
-
-    * ``remote_port`` (mandatory) is the remote port to listen to.
-    * ``local_port`` (optional) is the local port to connect to; the default is
-      the same port as the remote one.
-    * ``local_host`` (optional) is the locally-reachable computer (DNS name or
-      IP address) to connect to; the default is ``localhost`` (that is, the
-      same computer Fabric is running on).
-    * ``remote_bind_address`` (optional) is the remote IP address to bind to
-      for listening, on the current target. It should be an IP address assigned
-      to an interface on the target (or a DNS name that resolves to such IP).
-      You can use "0.0.0.0" to bind to all interfaces.
-
-    .. note::
-        By default, most SSH servers only allow remote tunnels to listen to the
-        localhost interface (127.0.0.1). In these cases, `remote_bind_address`
-        is ignored by the server, and the tunnel will listen only to 127.0.0.1.
-
-    .. versionadded: 1.6
-    """
-    if local_port is None:
-        local_port = remote_port
-
-    sockets = []
-    channels = []
-    threads = []
-
-    def accept(channel, (src_addr, src_port), (dest_addr, dest_port)):
-        channels.append(channel)
-        sock = socket.socket()
-        sockets.append(sock)
-
-        try:
-            sock.connect((local_host, local_port))
-        except Exception:
-            print "[%s] rtunnel: cannot connect to %s:%d (from local)" % (env.host_string, local_host, local_port)
-            channel.close()
-            return
-
-        print "[%s] rtunnel: opened reverse tunnel: %r -> %r -> %r"\
-              % (env.host_string, channel.origin_addr,
-                 channel.getpeername(), (local_host, local_port))
-
-        th = ThreadHandler('fwd', _forwarder, channel, sock)
-        threads.append(th)
-
-    transport = connections[env.host_string].get_transport()
-    transport.request_port_forward(remote_bind_address, remote_port, handler=accept)
-
-    try:
-        yield
-    finally:
-        for sock, chan, th in zip(sockets, channels, threads):
-            sock.close()
-            chan.close()
-            th.thread.join()
-            th.raise_if_needed()
-        transport.cancel_port_forward(remote_bind_address, remote_port)
-
-
-quiet = lambda: settings(hide('everything'), warn_only=True)
-quiet.__doc__ = """
-    Alias to ``settings(hide('everything'), warn_only=True)``.
-
-    Useful for wrapping remote interrogative commands which you expect to fail
-    occasionally, and/or which you want to silence.
-
-    Example::
-
-        with quiet():
-            have_build_dir = run("test -e /tmp/build").succeeded
-
-    When used in a task, the above snippet will not produce any ``run: test -e
-    /tmp/build`` line, nor will any stdout/stderr display, and command failure
-    is ignored.
-
-    .. seealso::
-        :ref:`env.warn_only <warn_only>`,
-        `~fabric.context_managers.settings`,
-        `~fabric.context_managers.hide`
-
-    .. versionadded:: 1.5
-"""
-
-
-warn_only = lambda: settings(warn_only=True)
-warn_only.__doc__ = """
-    Alias to ``settings(warn_only=True)``.
-
-    .. seealso::
-        :ref:`env.warn_only <warn_only>`,
-        `~fabric.context_managers.settings`,
-        `~fabric.context_managers.quiet`
-"""
diff -Nru fabric-1.14.0/fabric/contrib/console.py fabric-2.5.0/fabric/contrib/console.py
--- fabric-1.14.0/fabric/contrib/console.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/fabric/contrib/console.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,38 +0,0 @@
-"""
-Console/terminal user interface functionality.
-"""
-
-from fabric.api import prompt
-
-
-def confirm(question, default=True):
-    """
-    Ask user a yes/no question and return their response as True or False.
-
-    ``question`` should be a simple, grammatically complete question such as
-    "Do you wish to continue?", and will have a string similar to " [Y/n] "
-    appended automatically. This function will *not* append a question mark for
-    you.
-
-    By default, when the user presses Enter without typing anything, "yes" is
-    assumed. This can be changed by specifying ``default=False``.
-    """
-    # Set up suffix
-    if default:
-        suffix = "Y/n"
-    else:
-        suffix = "y/N"
-    # Loop till we get something we like
-    while True:
-        response = prompt("%s [%s] " % (question, suffix)).lower()
-        # Default
-        if not response:
-            return default
-        # Yes
-        if response in ['y', 'yes']:
-            return True
-        # No
-        if response in ['n', 'no']:
-            return False
-        # Didn't get empty, yes or no, so complain and loop
-        print("I didn't understand you. Please specify '(y)es' or '(n)o'.")
diff -Nru fabric-1.14.0/fabric/contrib/django.py fabric-2.5.0/fabric/contrib/django.py
--- fabric-1.14.0/fabric/contrib/django.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/fabric/contrib/django.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,94 +0,0 @@
-"""
-.. versionadded:: 0.9.2
-
-These functions streamline the process of initializing Django's settings module
-environment variable. Once this is done, your fabfile may import from your
-Django project, or Django itself, without requiring the use of ``manage.py``
-plugins or having to set the environment variable yourself every time you use
-your fabfile.
-
-Currently, these functions only allow Fabric to interact with
-local-to-your-fabfile Django installations. This is not as limiting as it
-sounds; for example, you can use Fabric as a remote "build" tool as well as
-using it locally. Imagine the following fabfile::
-
-    from fabric.api import run, local, hosts, cd
-    from fabric.contrib import django
-
-    django.project('myproject')
-    from myproject.myapp.models import MyModel
-
-    def print_instances():
-        for instance in MyModel.objects.all():
-            print(instance)
-
-    @hosts('production-server')
-    def print_production_instances():
-        with cd('/path/to/myproject'):
-            run('fab print_instances')
-
-With Fabric installed on both ends, you could execute
-``print_production_instances`` locally, which would trigger ``print_instances``
-on the production server -- which would then be interacting with your
-production Django database.
-
-As another example, if your local and remote settings are similar, you can use
-it to obtain e.g. your database settings, and then use those when executing a
-remote (non-Fabric) command. This would allow you some degree of freedom even
-if Fabric is only installed locally::
-
-    from fabric.api import run
-    from fabric.contrib import django
-
-    django.settings_module('myproject.settings')
-    from django.conf import settings
-
-    def dump_production_database():
-        run('mysqldump -u %s -p=%s %s > /tmp/prod-db.sql' % (
-            settings.DATABASE_USER,
-            settings.DATABASE_PASSWORD,
-            settings.DATABASE_NAME
-        ))
-
-The above snippet will work if run from a local, development environment, again
-provided your local ``settings.py`` mirrors your remote one in terms of
-database connection info.
-"""
-
-import os
-
-
-def settings_module(module):
-    """
-    Set ``DJANGO_SETTINGS_MODULE`` shell environment variable to ``module``.
-
-    Due to how Django works, imports from Django or a Django project will fail
-    unless the shell environment variable ``DJANGO_SETTINGS_MODULE`` is
-    correctly set (see `the Django settings docs
-    <http://docs.djangoproject.com/en/dev/topics/settings/>`_.)
-
-    This function provides a shortcut for doing so; call it near the top of
-    your fabfile or Fabric-using code, after which point any Django imports
-    should work correctly.
-
-    .. note::
-
-        This function sets a **shell** environment variable (via
-        ``os.environ``) and is unrelated to Fabric's own internal "env"
-        variables.
-    """
-    os.environ['DJANGO_SETTINGS_MODULE'] = module
-
-
-def project(name):
-    """
-    Sets ``DJANGO_SETTINGS_MODULE`` to ``'<name>.settings'``.
-
-    This function provides a handy shortcut for the common case where one is
-    using the Django default naming convention for their settings file and
-    location.
-
-    Uses `settings_module` -- see its documentation for details on why and how
-    to use this functionality.
-    """
-    settings_module('%s.settings' % name)
diff -Nru fabric-1.14.0/fabric/contrib/files.py fabric-2.5.0/fabric/contrib/files.py
--- fabric-1.14.0/fabric/contrib/files.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/fabric/contrib/files.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,473 +0,0 @@
-"""
-Module providing easy API for working with remote files and folders.
-"""
-
-from __future__ import with_statement
-
-import hashlib
-import os
-from StringIO import StringIO
-from functools import partial
-
-from fabric.api import run, sudo, hide, settings, env, put, abort
-from fabric.utils import apply_lcwd
-
-
-def exists(path, use_sudo=False, verbose=False):
-    """
-    Return True if given path exists on the current remote host.
-
-    If ``use_sudo`` is True, will use `sudo` instead of `run`.
-
-    `exists` will, by default, hide all output (including the run line, stdout,
-    stderr and any warning resulting from the file not existing) in order to
-    avoid cluttering output. You may specify ``verbose=True`` to change this
-    behavior.
-
-    .. versionchanged:: 1.13
-        Replaced internal use of ``test -e`` with ``stat`` for improved remote
-        cross-platform (e.g. Windows) compatibility.
-    """
-    func = use_sudo and sudo or run
-    cmd = 'stat %s' % _expand_path(path)
-    # If verbose, run normally
-    if verbose:
-        with settings(warn_only=True):
-            return not func(cmd).failed
-    # Otherwise, be quiet
-    with settings(hide('everything'), warn_only=True):
-        return not func(cmd).failed
-
-
-def is_link(path, use_sudo=False, verbose=False):
-    """
-    Return True if the given path is a symlink on the current remote host.
-
-    If ``use_sudo`` is True, will use `.sudo` instead of `.run`.
-
-    `.is_link` will, by default, hide all output. Give ``verbose=True`` to
-    change this.
-    """
-    func = sudo if use_sudo else run
-    cmd = 'test -L "$(echo %s)"' % path
-    args, kwargs = [], {'warn_only': True}
-    if not verbose:
-        args = [hide('everything')]
-    with settings(*args, **kwargs):
-        return func(cmd).succeeded
-
-
-def first(*args, **kwargs):
-    """
-    Given one or more file paths, returns first one found, or None if none
-    exist. May specify ``use_sudo`` and ``verbose`` which are passed to
-    `exists`.
-    """
-    for directory in args:
-        if exists(directory, **kwargs):
-            return directory
-
-
-def upload_template(filename, destination, context=None, use_jinja=False,
-    template_dir=None, use_sudo=False, backup=True, mirror_local_mode=False,
-    mode=None, pty=None, keep_trailing_newline=False, temp_dir=''):
-    """
-    Render and upload a template text file to a remote host.
-
-    Returns the result of the inner call to `~fabric.operations.put` -- see its
-    documentation for details.
-
-    ``filename`` should be the path to a text file, which may contain `Python
-    string interpolation formatting
-    <http://docs.python.org/library/stdtypes.html#string-formatting>`_ and will
-    be rendered with the given context dictionary ``context`` (if given.)
-
-    Alternately, if ``use_jinja`` is set to True and you have the Jinja2
-    templating library available, Jinja will be used to render the template
-    instead. Templates will be loaded from the invoking user's current working
-    directory by default, or from ``template_dir`` if given.
-
-    The resulting rendered file will be uploaded to the remote file path
-    ``destination``.  If the destination file already exists, it will be
-    renamed with a ``.bak`` extension unless ``backup=False`` is specified.
-
-    By default, the file will be copied to ``destination`` as the logged-in
-    user; specify ``use_sudo=True`` to use `sudo` instead.
-
-    The ``mirror_local_mode``, ``mode``, and ``temp_dir`` kwargs are passed
-    directly to an internal `~fabric.operations.put` call; please see its
-    documentation for details on these two options.
-
-    The ``pty`` kwarg will be passed verbatim to any internal
-    `~fabric.operations.run`/`~fabric.operations.sudo` calls, such as those
-    used for testing directory-ness, making backups, etc.
-
-    The ``keep_trailing_newline`` kwarg will be passed when creating
-    Jinja2 Environment which is False by default, same as Jinja2's
-    behaviour.
-
-    .. versionchanged:: 1.1
-        Added the ``backup``, ``mirror_local_mode`` and ``mode`` kwargs.
-    .. versionchanged:: 1.9
-        Added the ``pty`` kwarg.
-    .. versionchanged:: 1.11
-        Added the ``keep_trailing_newline`` kwarg.
-    .. versionchanged:: 1.11
-        Added the  ``temp_dir`` kwarg.
-    """
-    func = use_sudo and sudo or run
-    if pty is not None:
-        func = partial(func, pty=pty)
-    # Normalize destination to be an actual filename, due to using StringIO
-    with settings(hide('everything'), warn_only=True):
-        if func('test -d %s' % _expand_path(destination)).succeeded:
-            sep = "" if destination.endswith('/') else "/"
-            destination += sep + os.path.basename(filename)
-
-    # Use mode kwarg to implement mirror_local_mode, again due to using
-    # StringIO
-    if mirror_local_mode and mode is None:
-        mode = os.stat(apply_lcwd(filename, env)).st_mode
-        # To prevent put() from trying to do this
-        # logic itself
-        mirror_local_mode = False
-
-    # Process template
-    text = None
-    if use_jinja:
-        try:
-            template_dir = template_dir or os.getcwd()
-            template_dir = apply_lcwd(template_dir, env)
-            from jinja2 import Environment, FileSystemLoader
-            jenv = Environment(loader=FileSystemLoader(template_dir),
-                               keep_trailing_newline=keep_trailing_newline)
-            text = jenv.get_template(filename).render(**context or {})
-            # Force to a byte representation of Unicode, or str()ification
-            # within Paramiko's SFTP machinery may cause decode issues for
-            # truly non-ASCII characters.
-            text = text.encode('utf-8')
-        except ImportError:
-            import traceback
-            tb = traceback.format_exc()
-            abort(tb + "\nUnable to import Jinja2 -- see above.")
-    else:
-        if template_dir:
-            filename = os.path.join(template_dir, filename)
-        filename = apply_lcwd(filename, env)
-        with open(os.path.expanduser(filename)) as inputfile:
-            text = inputfile.read()
-        if context:
-            text = text % context
-
-    # Back up original file
-    if backup and exists(destination):
-        func("cp %s{,.bak}" % _expand_path(destination))
-
-    # Upload the file.
-    return put(
-        local_path=StringIO(text),
-        remote_path=destination,
-        use_sudo=use_sudo,
-        mirror_local_mode=mirror_local_mode,
-        mode=mode,
-        temp_dir=temp_dir
-    )
-
-
-def sed(filename, before, after, limit='', use_sudo=False, backup='.bak',
-    flags='', shell=False):
-    """
-    Run a search-and-replace on ``filename`` with given regex patterns.
-
-    Equivalent to ``sed -i<backup> -r -e "/<limit>/ s/<before>/<after>/<flags>g"
-    <filename>``. Setting ``backup`` to an empty string will, disable backup
-    file creation.
-
-    For convenience, ``before`` and ``after`` will automatically escape forward
-    slashes, single quotes and parentheses for you, so you don't need to
-    specify e.g.  ``http:\/\/foo\.com``, instead just using ``http://foo\.com``
-    is fine.
-
-    If ``use_sudo`` is True, will use `sudo` instead of `run`.
-
-    The ``shell`` argument will be eventually passed to `run`/`sudo`. It
-    defaults to False in order to avoid problems with many nested levels of
-    quotes and backslashes. However, setting it to True may help when using
-    ``~fabric.operations.cd`` to wrap explicit or implicit ``sudo`` calls.
-    (``cd`` by it's nature is a shell built-in, not a standalone command, so it
-    should be called within a shell.)
-
-    Other options may be specified with sed-compatible regex flags -- for
-    example, to make the search and replace case insensitive, specify
-    ``flags="i"``. The ``g`` flag is always specified regardless, so you do not
-    need to remember to include it when overriding this parameter.
-
-    .. versionadded:: 1.1
-        The ``flags`` parameter.
-    .. versionadded:: 1.6
-        Added the ``shell`` keyword argument.
-    """
-    func = use_sudo and sudo or run
-    # Characters to be escaped in both
-    for char in "/'":
-        before = before.replace(char, r'\%s' % char)
-        after = after.replace(char, r'\%s' % char)
-    # Characters to be escaped in replacement only (they're useful in regexen
-    # in the 'before' part)
-    for char in "()":
-        after = after.replace(char, r'\%s' % char)
-    if limit:
-        limit = r'/%s/ ' % limit
-    context = {
-        'script': r"'%ss/%s/%s/%sg'" % (limit, before, after, flags),
-        'filename': _expand_path(filename),
-        'backup': backup
-    }
-    # Test the OS because of differences between sed versions
-
-    with hide('running', 'stdout'):
-        platform = run("uname", shell=False, pty=False)
-    if platform in ('NetBSD', 'OpenBSD', 'QNX'):
-        # Attempt to protect against failures/collisions
-        hasher = hashlib.sha1()
-        hasher.update(env.host_string)
-        hasher.update(filename)
-        context['tmp'] = "/tmp/%s" % hasher.hexdigest()
-        # Use temp file to work around lack of -i
-        expr = r"""cp -p %(filename)s %(tmp)s \
-&& sed -r -e %(script)s %(filename)s > %(tmp)s \
-&& cp -p %(filename)s %(filename)s%(backup)s \
-&& mv %(tmp)s %(filename)s"""
-    else:
-        context['extended_regex'] = '-E' if platform == 'Darwin' else '-r'
-        expr = r"sed -i%(backup)s %(extended_regex)s -e %(script)s %(filename)s"
-    command = expr % context
-    return func(command, shell=shell)
-
-
-def uncomment(filename, regex, use_sudo=False, char='#', backup='.bak',
-    shell=False):
-    """
-    Attempt to uncomment all lines in ``filename`` matching ``regex``.
-
-    The default comment delimiter is `#` and may be overridden by the ``char``
-    argument.
-
-    This function uses the `sed` function, and will accept the same
-    ``use_sudo``, ``shell`` and ``backup`` keyword arguments that `sed` does.
-
-    `uncomment` will remove a single whitespace character following the comment
-    character, if it exists, but will preserve all preceding whitespace.  For
-    example, ``# foo`` would become ``foo`` (the single space is stripped) but
-    ``    # foo`` would become ``    foo`` (the single space is still stripped,
-    but the preceding 4 spaces are not.)
-
-    .. versionchanged:: 1.6
-        Added the ``shell`` keyword argument.
-    """
-    return sed(
-        filename,
-        before=r'^([[:space:]]*)%s[[:space:]]?' % char,
-        after=r'\1',
-        limit=regex,
-        use_sudo=use_sudo,
-        backup=backup,
-        shell=shell
-    )
-
-
-def comment(filename, regex, use_sudo=False, char='#', backup='.bak',
-    shell=False):
-    """
-    Attempt to comment out all lines in ``filename`` matching ``regex``.
-
-    The default commenting character is `#` and may be overridden by the
-    ``char`` argument.
-
-    This function uses the `sed` function, and will accept the same
-    ``use_sudo``, ``shell`` and ``backup`` keyword arguments that `sed` does.
-
-    `comment` will prepend the comment character to the beginning of the line,
-    so that lines end up looking like so::
-
-        this line is uncommented
-        #this line is commented
-        #   this line is indented and commented
-
-    In other words, comment characters will not "follow" indentation as they
-    sometimes do when inserted by hand. Neither will they have a trailing space
-    unless you specify e.g. ``char='# '``.
-
-    .. note::
-
-        In order to preserve the line being commented out, this function will
-        wrap your ``regex`` argument in parentheses, so you don't need to. It
-        will ensure that any preceding/trailing ``^`` or ``$`` characters are
-        correctly moved outside the parentheses. For example, calling
-        ``comment(filename, r'^foo$')`` will result in a `sed` call with the
-        "before" regex of ``r'^(foo)$'`` (and the "after" regex, naturally, of
-        ``r'#\\1'``.)
-
-    .. versionadded:: 1.5
-        Added the ``shell`` keyword argument.
-    """
-    carot, dollar = '', ''
-    if regex.startswith('^'):
-        carot = '^'
-        regex = regex[1:]
-    if regex.endswith('$'):
-        dollar = '$'
-        regex = regex[:-1]
-    regex = "%s(%s)%s" % (carot, regex, dollar)
-    return sed(
-        filename,
-        before=regex,
-        after=r'%s\1' % char,
-        use_sudo=use_sudo,
-        backup=backup,
-        shell=shell
-    )
-
-
-def contains(filename, text, exact=False, use_sudo=False, escape=True,
-    shell=False, case_sensitive=True):
-    """
-    Return True if ``filename`` contains ``text`` (which may be a regex.)
-
-    By default, this function will consider a partial line match (i.e. where
-    ``text`` only makes up part of the line it's on). Specify ``exact=True`` to
-    change this behavior so that only a line containing exactly ``text``
-    results in a True return value.
-
-    This function leverages ``egrep`` on the remote end (so it may not follow
-    Python regular expression syntax perfectly), and skips ``env.shell``
-    wrapper by default.
-
-    If ``use_sudo`` is True, will use `sudo` instead of `run`.
-
-    If ``escape`` is False, no extra regular expression related escaping is
-    performed (this includes overriding ``exact`` so that no ``^``/``$`` is
-    added.)
-
-    The ``shell`` argument will be eventually passed to ``run/sudo``. See
-    description of the same argument in ``~fabric.contrib.sed`` for details.
-
-    If ``case_sensitive`` is False, the `-i` flag will be passed to ``egrep``.
-
-    .. versionchanged:: 1.0
-        Swapped the order of the ``filename`` and ``text`` arguments to be
-        consistent with other functions in this module.
-    .. versionchanged:: 1.4
-        Updated the regular expression related escaping to try and solve
-        various corner cases.
-    .. versionchanged:: 1.4
-        Added ``escape`` keyword argument.
-    .. versionadded:: 1.6
-        Added the ``shell`` keyword argument.
-    .. versionadded:: 1.11
-        Added the ``case_sensitive`` keyword argument.
-    """
-    func = use_sudo and sudo or run
-    if escape:
-        text = _escape_for_regex(text)
-        if exact:
-            text = "^%s$" % text
-    with settings(hide('everything'), warn_only=True):
-        egrep_cmd = 'egrep "%s" %s' % (text, _expand_path(filename))
-        if not case_sensitive:
-            egrep_cmd = egrep_cmd.replace('egrep', 'egrep -i', 1)
-        return func(egrep_cmd, shell=shell).succeeded
-
-
-def append(filename, text, use_sudo=False, partial=False, escape=True,
-    shell=False):
-    """
-    Append string (or list of strings) ``text`` to ``filename``.
-
-    When a list is given, each string inside is handled independently (but in
-    the order given.)
-
-    If ``text`` is already found in ``filename``, the append is not run, and
-    None is returned immediately. Otherwise, the given text is appended to the
-    end of the given ``filename`` via e.g. ``echo '$text' >> $filename``.
-
-    The test for whether ``text`` already exists defaults to a full line match,
-    e.g. ``^<text>$``, as this seems to be the most sensible approach for the
-    "append lines to a file" use case. You may override this and force partial
-    searching (e.g. ``^<text>``) by specifying ``partial=True``.
-
-    Because ``text`` is single-quoted, single quotes will be transparently
-    backslash-escaped. This can be disabled with ``escape=False``.
-
-    If ``use_sudo`` is True, will use `sudo` instead of `run`.
-
-    The ``shell`` argument will be eventually passed to ``run/sudo``. See
-    description of the same argumnet in ``~fabric.contrib.sed`` for details.
-
-    .. versionchanged:: 0.9.1
-        Added the ``partial`` keyword argument.
-    .. versionchanged:: 1.0
-        Swapped the order of the ``filename`` and ``text`` arguments to be
-        consistent with other functions in this module.
-    .. versionchanged:: 1.0
-        Changed default value of ``partial`` kwarg to be ``False``.
-    .. versionchanged:: 1.4
-        Updated the regular expression related escaping to try and solve
-        various corner cases.
-    .. versionadded:: 1.6
-        Added the ``shell`` keyword argument.
-    """
-    func = use_sudo and sudo or run
-    # Normalize non-list input to be a list
-    if isinstance(text, basestring):
-        text = [text]
-    for line in text:
-        regex = '^' + _escape_for_regex(line)  + ('' if partial else '$')
-        if (exists(filename, use_sudo=use_sudo) and line
-            and contains(filename, regex, use_sudo=use_sudo, escape=False,
-                         shell=shell)):
-            continue
-        line = line.replace("'", r"'\\''") if escape else line
-        func("echo '%s' >> %s" % (line, _expand_path(filename)))
-
-def _escape_for_regex(text):
-    """Escape ``text`` to allow literal matching using egrep"""
-    re_specials = '\\^$|(){}[]*+?.'
-    sh_specials = '\\$`"'
-    re_chars = []
-    sh_chars = []
-
-    for c in text:
-        if c in re_specials:
-            re_chars.append('\\')
-        re_chars.append(c)
-
-    for c in re_chars:
-        if c in sh_specials:
-            sh_chars.append('\\')
-        sh_chars.append(c)
-
-    return ''.join(sh_chars)
-
-def is_win():
-    """
-    Return True if remote SSH server is running Windows, False otherwise.
-
-    The idea is based on echoing quoted text: \*NIX systems will echo quoted
-    text only, while Windows echoes quotation marks as well.
-    """
-    with settings(hide('everything'), warn_only=True):
-        return '"' in run('echo "Will you echo quotation marks"')
-
-def _expand_path(path):
-    """
-    Return a path expansion
-
-    E.g.    ~/some/path     ->  /home/myuser/some/path
-            /user/\*/share   ->  /user/local/share
-    More examples can be found here: http://linuxcommand.org/lc3_lts0080.php
-
-    .. versionchanged:: 1.0
-        Avoid breaking remote Windows commands which does not support expansion.
-    """
-    return path if is_win() else '"$(echo %s)"' % path
diff -Nru fabric-1.14.0/fabric/contrib/project.py fabric-2.5.0/fabric/contrib/project.py
--- fabric-1.14.0/fabric/contrib/project.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/fabric/contrib/project.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,213 +0,0 @@
-"""
-Useful non-core functionality, e.g. functions composing multiple operations.
-"""
-from __future__ import with_statement
-
-from os import getcwd, sep
-import os.path
-from tempfile import mkdtemp
-
-from fabric.network import needs_host, key_filenames, normalize
-from fabric.operations import local, run, sudo, put
-from fabric.state import env, output
-from fabric.context_managers import cd
-
-__all__ = ['rsync_project', 'upload_project']
-
-@needs_host
-def rsync_project(
-    remote_dir,
-    local_dir=None,
-    exclude=(),
-    delete=False,
-    extra_opts='',
-    ssh_opts='',
-    capture=False,
-    upload=True,
-    default_opts='-pthrvz'
-):
-    """
-    Synchronize a remote directory with the current project directory via rsync.
-
-    Where ``upload_project()`` makes use of ``scp`` to copy one's entire
-    project every time it is invoked, ``rsync_project()`` uses the ``rsync``
-    command-line utility, which only transfers files newer than those on the
-    remote end.
-
-    ``rsync_project()`` is thus a simple wrapper around ``rsync``; for
-    details on how ``rsync`` works, please see its manpage. ``rsync`` must be
-    installed on both your local and remote systems in order for this operation
-    to work correctly.
-
-    This function makes use of Fabric's ``local()`` operation, and returns the
-    output of that function call; thus it will return the stdout, if any, of
-    the resultant ``rsync`` call.
-
-    ``rsync_project()`` uses the current Fabric connection parameters (user,
-    host, port) by default, adding them to rsync's ssh options (then mixing in
-    ``ssh_opts``, if given -- see below.)
-
-    ``rsync_project()`` takes the following parameters:
-
-    * ``remote_dir``: the only required parameter, this is the path to the
-      directory on the remote server. Due to how ``rsync`` is implemented, the
-      exact behavior depends on the value of ``local_dir``:
-
-        * If ``local_dir`` ends with a trailing slash, the files will be
-          dropped inside of ``remote_dir``. E.g.
-          ``rsync_project("/home/username/project/", "foldername/")`` will drop
-          the contents of ``foldername`` inside of ``/home/username/project``.
-        * If ``local_dir`` does **not** end with a trailing slash (and this
-          includes the default scenario, when ``local_dir`` is not specified),
-          ``remote_dir`` is effectively the "parent" directory, and a new
-          directory named after ``local_dir`` will be created inside of it. So
-          ``rsync_project("/home/username", "foldername")`` would create a new
-          directory ``/home/username/foldername`` (if needed) and place the
-          files there.
-
-    * ``local_dir``: by default, ``rsync_project`` uses your current working
-      directory as the source directory. This may be overridden by specifying
-      ``local_dir``, which is a string passed verbatim to ``rsync``, and thus
-      may be a single directory (``"my_directory"``) or multiple directories
-      (``"dir1 dir2"``). See the ``rsync`` documentation for details.
-    * ``exclude``: optional, may be a single string, or an iterable of strings,
-      and is used to pass one or more ``--exclude`` options to ``rsync``.
-    * ``delete``: a boolean controlling whether ``rsync``'s ``--delete`` option
-      is used. If True, instructs ``rsync`` to remove remote files that no
-      longer exist locally. Defaults to False.
-    * ``extra_opts``: an optional, arbitrary string which you may use to pass
-      custom arguments or options to ``rsync``.
-    * ``ssh_opts``: Like ``extra_opts`` but specifically for the SSH options
-      string (rsync's ``--rsh`` flag.)
-    * ``capture``: Sent directly into an inner `~fabric.operations.local` call.
-    * ``upload``: a boolean controlling whether file synchronization is
-      performed up or downstream. Upstream by default.
-    * ``default_opts``: the default rsync options ``-pthrvz``, override if
-      desired (e.g. to remove verbosity, etc).
-
-    Furthermore, this function transparently honors Fabric's port and SSH key
-    settings. Calling this function when the current host string contains a
-    nonstandard port, or when ``env.key_filename`` is non-empty, will use the
-    specified port and/or SSH key filename(s).
-
-    For reference, the approximate ``rsync`` command-line call that is
-    constructed by this function is the following::
-
-        rsync [--delete] [--exclude exclude[0][, --exclude[1][, ...]]] \\
-            [default_opts] [extra_opts] <local_dir> <host_string>:<remote_dir>
-
-    .. versionadded:: 1.4.0
-        The ``ssh_opts`` keyword argument.
-    .. versionadded:: 1.4.1
-        The ``capture`` keyword argument.
-    .. versionadded:: 1.8.0
-        The ``default_opts`` keyword argument.
-    """
-    # Turn single-string exclude into a one-item list for consistency
-    if not hasattr(exclude, '__iter__'):
-        exclude = (exclude,)
-    # Create --exclude options from exclude list
-    exclude_opts = ' --exclude "%s"' * len(exclude)
-    # Double-backslash-escape
-    exclusions = tuple([str(s).replace('"', '\\\\"') for s in exclude])
-    # Honor SSH key(s)
-    key_string = ""
-    keys = key_filenames()
-    if keys:
-        key_string = "-i " + " -i ".join(keys)
-    # Port
-    user, host, port = normalize(env.host_string)
-    port_string = "-p %s" % port
-    # RSH
-    rsh_string = ""
-    if env.gateway is None:
-        gateway_opts = ""
-    else:
-        gw_user, gw_host, gw_port = normalize(env.gateway)
-        gw_str = "-A -o \"ProxyCommand=ssh %s -p %s %s@%s nc %s %s\""
-        gateway_opts = gw_str % (
-            key_string, gw_port, gw_user, gw_host, host, port
-        )
-
-    rsh_parts = [key_string, port_string, ssh_opts, gateway_opts]
-    if any(rsh_parts):
-        rsh_string = "--rsh='ssh %s'" % " ".join(rsh_parts)
-    # Set up options part of string
-    options_map = {
-        'delete': '--delete' if delete else '',
-        'exclude': exclude_opts % exclusions,
-        'rsh': rsh_string,
-        'default': default_opts,
-        'extra': extra_opts,
-    }
-    options = "%(delete)s%(exclude)s %(default)s %(extra)s %(rsh)s" % options_map
-    # Get local directory
-    if local_dir is None:
-        local_dir = '../' + getcwd().split(sep)[-1]
-    # Create and run final command string
-    if host.count(':') > 1:
-        # Square brackets are mandatory for IPv6 rsync address,
-        # even if port number is not specified
-        remote_prefix = "[%s@%s]" % (user, host)
-    else:
-        remote_prefix = "%s@%s" % (user, host)
-    if upload:
-        cmd = "rsync %s %s %s:%s" % (options, local_dir, remote_prefix, remote_dir)
-    else:
-        cmd = "rsync %s %s:%s %s" % (options, remote_prefix, remote_dir, local_dir)
-
-    if output.running:
-        print("[%s] rsync_project: %s" % (env.host_string, cmd))
-    return local(cmd, capture=capture)
-
-
-def upload_project(local_dir=None, remote_dir="", use_sudo=False):
-    """
-    Upload the current project to a remote system via ``tar``/``gzip``.
-
-    ``local_dir`` specifies the local project directory to upload, and defaults
-    to the current working directory.
-
-    ``remote_dir`` specifies the target directory to upload into (meaning that
-    a copy of ``local_dir`` will appear as a subdirectory of ``remote_dir``)
-    and defaults to the remote user's home directory.
-
-    ``use_sudo`` specifies which method should be used when executing commands
-    remotely. ``sudo`` will be used if use_sudo is True, otherwise ``run`` will
-    be used.
-
-    This function makes use of the ``tar`` and ``gzip`` programs/libraries,
-    thus it will not work too well on Win32 systems unless one is using Cygwin
-    or something similar. It will attempt to clean up the local and remote
-    tarfiles when it finishes executing, even in the event of a failure.
-
-    .. versionchanged:: 1.1
-        Added the ``local_dir`` and ``remote_dir`` kwargs.
-
-    .. versionchanged:: 1.7
-        Added the ``use_sudo`` kwarg.
-    """
-    runner = use_sudo and sudo or run
-
-    local_dir = local_dir or os.getcwd()
-
-    # Remove final '/' in local_dir so that basename() works
-    local_dir = local_dir.rstrip(os.sep)
-
-    local_path, local_name = os.path.split(local_dir)
-    local_path = local_path or '.'
-    tar_file = "%s.tar.gz" % local_name
-    target_tar = os.path.join(remote_dir, tar_file)
-    tmp_folder = mkdtemp()
-
-    try:
-        tar_path = os.path.join(tmp_folder, tar_file)
-        local("tar -czf %s -C %s %s" % (tar_path, local_path, local_name))
-        put(tar_path, target_tar, use_sudo=use_sudo)
-        with cd(remote_dir):
-            try:
-                runner("tar -xzf %s" % tar_file)
-            finally:
-                runner("rm -f %s" % tar_file)
-    finally:
-        local("rm -rf %s" % tmp_folder)
diff -Nru fabric-1.14.0/fabric/decorators.py fabric-2.5.0/fabric/decorators.py
--- fabric-1.14.0/fabric/decorators.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/fabric/decorators.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,222 +0,0 @@
-"""
-Convenience decorators for use in fabfiles.
-"""
-from __future__ import with_statement
-
-import types
-from functools import wraps
-
-try:
-    from Crypto import Random
-except ImportError:
-    Random = None
-
-from fabric import tasks
-from .context_managers import settings
-
-
-def task(*args, **kwargs):
-    """
-    Decorator declaring the wrapped function to be a new-style task.
-
-    May be invoked as a simple, argument-less decorator (i.e. ``@task``) or
-    with arguments customizing its behavior (e.g. ``@task(alias='myalias')``).
-
-    Please see the :ref:`new-style task <task-decorator>` documentation for
-    details on how to use this decorator.
-
-    .. versionchanged:: 1.2
-        Added the ``alias``, ``aliases``, ``task_class`` and ``default``
-        keyword arguments. See :ref:`task-decorator-arguments` for details.
-    .. versionchanged:: 1.5
-        Added the ``name`` keyword argument.
-
-    .. seealso:: `~fabric.docs.unwrap_tasks`, `~fabric.tasks.WrappedCallableTask`
-    """
-    invoked = bool(not args or kwargs)
-    task_class = kwargs.pop("task_class", tasks.WrappedCallableTask)
-    if not invoked:
-        func, args = args[0], ()
-
-    def wrapper(func):
-        return task_class(func, *args, **kwargs)
-
-    return wrapper if invoked else wrapper(func)
-
-def _wrap_as_new(original, new):
-    if isinstance(original, tasks.Task):
-        return tasks.WrappedCallableTask(new)
-    return new
-
-
-def _list_annotating_decorator(attribute, *values):
-    def attach_list(func):
-        @wraps(func)
-        def inner_decorator(*args, **kwargs):
-            return func(*args, **kwargs)
-        _values = values
-        # Allow for single iterable argument as well as *args
-        if len(_values) == 1 and not isinstance(_values[0], basestring):
-            _values = _values[0]
-        setattr(inner_decorator, attribute, list(_values))
-        # Don't replace @task new-style task objects with inner_decorator by
-        # itself -- wrap in a new Task object first.
-        inner_decorator = _wrap_as_new(func, inner_decorator)
-        return inner_decorator
-    return attach_list
-
-
-def hosts(*host_list):
-    """
-    Decorator defining which host or hosts to execute the wrapped function on.
-
-    For example, the following will ensure that, barring an override on the
-    command line, ``my_func`` will be run on ``host1``, ``host2`` and
-    ``host3``, and with specific users on ``host1`` and ``host3``::
-
-        @hosts('user1@host1', 'host2', 'user2@host3')
-        def my_func():
-            pass
-
-    `~fabric.decorators.hosts` may be invoked with either an argument list
-    (``@hosts('host1')``, ``@hosts('host1', 'host2')``) or a single, iterable
-    argument (``@hosts(['host1', 'host2'])``).
-
-    Note that this decorator actually just sets the function's ``.hosts``
-    attribute, which is then read prior to executing the function.
-
-    .. versionchanged:: 0.9.2
-        Allow a single, iterable argument (``@hosts(iterable)``) to be used
-        instead of requiring ``@hosts(*iterable)``.
-    """
-    return _list_annotating_decorator('hosts', *host_list)
-
-
-def roles(*role_list):
-    """
-    Decorator defining a list of role names, used to look up host lists.
-
-    A role is simply defined as a key in `env` whose value is a list of one or
-    more host connection strings. For example, the following will ensure that,
-    barring an override on the command line, ``my_func`` will be executed
-    against the hosts listed in the ``webserver`` and ``dbserver`` roles::
-
-        env.roledefs.update({
-            'webserver': ['www1', 'www2'],
-            'dbserver': ['db1']
-        })
-
-        @roles('webserver', 'dbserver')
-        def my_func():
-            pass
-
-    As with `~fabric.decorators.hosts`, `~fabric.decorators.roles` may be
-    invoked with either an argument list or a single, iterable argument.
-    Similarly, this decorator uses the same mechanism as
-    `~fabric.decorators.hosts` and simply sets ``<function>.roles``.
-
-    .. versionchanged:: 0.9.2
-        Allow a single, iterable argument to be used (same as
-        `~fabric.decorators.hosts`).
-    """
-    return _list_annotating_decorator('roles', *role_list)
-
-
-def runs_once(func):
-    """
-    Decorator preventing wrapped function from running more than once.
-
-    By keeping internal state, this decorator allows you to mark a function
-    such that it will only run once per Python interpreter session, which in
-    typical use means "once per invocation of the ``fab`` program".
-
-    Any function wrapped with this decorator will silently fail to execute the
-    2nd, 3rd, ..., Nth time it is called, and will return the value of the
-    original run.
-    
-    .. note:: ``runs_once`` does not work with parallel task execution.
-    """
-    @wraps(func)
-    def decorated(*args, **kwargs):
-        if not hasattr(decorated, 'return_value'):
-            decorated.return_value = func(*args, **kwargs)
-        return decorated.return_value
-    decorated = _wrap_as_new(func, decorated)
-    # Mark as serial (disables parallelism) and return
-    return serial(decorated)
-
-
-def serial(func):
-    """
-    Forces the wrapped function to always run sequentially, never in parallel.
-
-    This decorator takes precedence over the global value of :ref:`env.parallel
-    <env-parallel>`. However, if a task is decorated with both
-    `~fabric.decorators.serial` *and* `~fabric.decorators.parallel`,
-    `~fabric.decorators.parallel` wins.
-
-    .. versionadded:: 1.3
-    """
-    if not getattr(func, 'parallel', False):
-        func.serial = True
-    return _wrap_as_new(func, func)
-
-
-def parallel(pool_size=None):
-    """
-    Forces the wrapped function to run in parallel, instead of sequentially.
-
-    This decorator takes precedence over the global value of :ref:`env.parallel
-    <env-parallel>`. It also takes precedence over `~fabric.decorators.serial`
-    if a task is decorated with both.
-
-    .. versionadded:: 1.3
-    """
-    called_without_args = type(pool_size) == types.FunctionType
-
-    def real_decorator(func):
-        @wraps(func)
-        def inner(*args, **kwargs):
-            # Required for ssh/PyCrypto to be happy in multiprocessing
-            # (as far as we can tell, this is needed even with the extra such
-            # calls in newer versions of paramiko.)
-            if Random:
-                Random.atfork()
-            return func(*args, **kwargs)
-        inner.parallel = True
-        inner.serial = False
-        inner.pool_size = None if called_without_args else pool_size
-        return _wrap_as_new(func, inner)
-
-    # Allow non-factory-style decorator use (@decorator vs @decorator())
-    if called_without_args:
-        return real_decorator(pool_size)
-
-    return real_decorator
-
-
-def with_settings(*arg_settings, **kw_settings):
-    """
-    Decorator equivalent of ``fabric.context_managers.settings``.
-
-    Allows you to wrap an entire function as if it was called inside a block
-    with the ``settings`` context manager. This may be useful if you know you
-    want a given setting applied to an entire function body, or wish to
-    retrofit old code without indenting everything.
-
-    For example, to turn aborts into warnings for an entire task function::
-
-        @with_settings(warn_only=True)
-        def foo():
-            ...
-
-    .. seealso:: `~fabric.context_managers.settings`
-    .. versionadded:: 1.1
-    """
-    def outer(func):
-        @wraps(func)
-        def inner(*args, **kwargs):
-            with settings(*arg_settings, **kw_settings):
-                return func(*args, **kwargs)
-        return _wrap_as_new(func, inner)
-    return outer
diff -Nru fabric-1.14.0/fabric/docs.py fabric-2.5.0/fabric/docs.py
--- fabric-1.14.0/fabric/docs.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/fabric/docs.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,57 +0,0 @@
-from fabric.tasks import WrappedCallableTask
-
-
-def unwrap_tasks(module, hide_nontasks=False):
-    """
-    Replace task objects on ``module`` with their wrapped functions instead.
-
-    Specifically, look for instances of `~fabric.tasks.WrappedCallableTask` and
-    replace them with their ``.wrapped`` attribute (the original decorated
-    function.)
-
-    This is intended for use with the Sphinx autodoc tool, to be run near the
-    bottom of a project's ``conf.py``. It ensures that the autodoc extension
-    will have full access to the "real" function, in terms of function
-    signature and so forth. Without use of ``unwrap_tasks``, autodoc is unable
-    to access the function signature (though it is able to see e.g.
-    ``__doc__``.)
-
-    For example, at the bottom of your ``conf.py``::
-
-        from fabric.docs import unwrap_tasks
-        import my_package.my_fabfile
-        unwrap_tasks(my_package.my_fabfile)
-
-    You can go above and beyond, and explicitly **hide** all non-task
-    functions, by saying ``hide_nontasks=True``. This renames all objects
-    failing the "is it a task?" check so they appear to be private, which will
-    then cause autodoc to skip over them.
-
-    ``hide_nontasks`` is thus useful when you have a fabfile mixing in
-    subroutines with real tasks and want to document *just* the real tasks.
-    
-    If you run this within an actual Fabric-code-using session (instead of
-    within a Sphinx ``conf.py``), please seek immediate medical attention.
-
-    .. versionadded: 1.5
-
-    .. seealso:: `~fabric.tasks.WrappedCallableTask`, `~fabric.decorators.task`
-    """
-    set_tasks = []
-    for name, obj in vars(module).items():
-        if isinstance(obj, WrappedCallableTask):
-            setattr(module, obj.name, obj.wrapped)
-            # Handle situation where a task's real name shadows a builtin.
-            # If the builtin comes after the task in vars().items(), the object
-            # we just setattr'd above will get re-hidden :(
-            set_tasks.append(obj.name)
-            # In the same vein, "privately" named wrapped functions whose task
-            # name is public, needs to get renamed so autodoc picks it up.
-            obj.wrapped.func_name = obj.name
-        else:
-            if name in set_tasks:
-                continue
-            has_docstring = getattr(obj, '__doc__', False)
-            if hide_nontasks and has_docstring and not name.startswith('_'):
-                setattr(module, '_%s' % name, obj)
-                delattr(module, name)
diff -Nru fabric-1.14.0/fabric/exceptions.py fabric-2.5.0/fabric/exceptions.py
--- fabric-1.14.0/fabric/exceptions.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/fabric/exceptions.py	2019-08-06 23:57:28.000000000 +0100
@@ -1,32 +1,26 @@
-"""
-Custom Fabric exception classes.
+# TODO: this may want to move to Invoke if we can find a use for it there too?
+# Or make it _more_ narrowly focused and stay here?
+class NothingToDo(Exception):
+    pass
 
-Most are simply distinct Exception subclasses for purposes of message-passing
-(though typically still in actual error situations.)
-"""
-
-
-class NetworkError(Exception):
-    # Must allow for calling with zero args/kwargs, since pickle is apparently
-    # stupid with exceptions and tries to call it as such when passed around in
-    # a multiprocessing.Queue.
-    def __init__(self, message=None, wrapped=None):
-        self.message = message
-        self.wrapped = wrapped
-
-    def __str__(self):
-        return self.message or ""
-
-    def __repr__(self):
-        return "%s(%s) => %r" % (
-            self.__class__.__name__, self.message, self.wrapped
-        )
-
-
-class CommandTimeout(Exception):
-    def __init__(self, timeout):
-        self.timeout = timeout
-
-        message = 'Command failed to finish in %s seconds' % (timeout)
-        self.message = message
-        super(CommandTimeout, self).__init__(message)
+
+class GroupException(Exception):
+    """
+    Lightweight exception wrapper for `.GroupResult` when one contains errors.
+
+    .. versionadded:: 2.0
+    """
+
+    def __init__(self, result):
+        #: The `.GroupResult` object which would have been returned, had there
+        #: been no errors. See its docstring (and that of `.Group`) for
+        #: details.
+        self.result = result
+
+
+class InvalidV1Env(Exception):
+    """
+    Raised when attempting to import a Fabric 1 ``env`` which is missing data.
+    """
+
+    pass
diff -Nru fabric-1.14.0/fabric/executor.py fabric-2.5.0/fabric/executor.py
--- fabric-1.14.0/fabric/executor.py	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/fabric/executor.py	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,127 @@
+import invoke
+from invoke import Call, Task
+
+from .tasks import ConnectionCall
+from .exceptions import NothingToDo
+from .util import debug
+
+
+class Executor(invoke.Executor):
+    """
+    `~invoke.executor.Executor` subclass which understands Fabric concepts.
+
+    Designed to work in tandem with Fabric's `@task
+    <fabric.tasks.task>`/`~fabric.tasks.Task`, and is capable of acting on
+    information stored on the resulting objects -- such as default host lists.
+
+    This class is written to be backwards compatible with vanilla Invoke-level
+    tasks, which it simply delegates to its superclass.
+
+    Please see the parent class' `documentation <invoke.executor.Executor>` for
+    details on most public API members and object lifecycle.
+    """
+
+    def normalize_hosts(self, hosts):
+        """
+        Normalize mixed host-strings-or-kwarg-dicts into kwarg dicts only.
+
+        In other words, transforms data taken from the CLI (--hosts, always
+        strings) or decorator arguments (may be strings or kwarg dicts) into
+        kwargs suitable for creating Connection instances.
+
+        Subclasses may wish to override or extend this to perform, for example,
+        database or custom config file lookups (vs this default behavior, which
+        is to simply assume that strings are 'host' kwargs).
+
+        :param hosts:
+            Potentially heterogenous list of host connection values, as per the
+            ``hosts`` param to `.task`.
+
+        :returns: Homogenous list of Connection init kwarg dicts.
+        """
+        dicts = []
+        for value in hosts or []:
+            # Assume first posarg to Connection() if not already a dict.
+            if not isinstance(value, dict):
+                value = dict(host=value)
+            dicts.append(value)
+        return dicts
+
+    def expand_calls(self, calls, apply_hosts=True):
+        # Generate new call list with per-host variants & Connections inserted
+        ret = []
+        cli_hosts = []
+        host_str = self.core[0].args.hosts.value
+        if apply_hosts and host_str:
+            cli_hosts = host_str.split(",")
+        for call in calls:
+            if isinstance(call, Task):
+                call = Call(task=call)
+            # TODO: expand this to allow multiple types of execution plans,
+            # pending outcome of invoke#461 (which, if flexible enough to
+            # handle intersect of dependencies+parameterization, just becomes
+            # 'honor that new feature of Invoke')
+            # TODO: roles, other non-runtime host parameterizations, etc
+            # Pre-tasks get added only once, not once per host.
+            ret.extend(self.expand_calls(call.pre, apply_hosts=False))
+            # Determine final desired host list based on CLI and task values
+            # (with CLI, being closer to runtime, winning) and normalize to
+            # Connection-init kwargs.
+            call_hosts = getattr(call, "hosts", None)
+            cxn_params = self.normalize_hosts(cli_hosts or call_hosts)
+            # Main task, per host/connection
+            for init_kwargs in cxn_params:
+                ret.append(self.parameterize(call, init_kwargs))
+            # Deal with lack of hosts list (acts same as `inv` in that case)
+            # TODO: no tests for this branch?
+            if not cxn_params:
+                ret.append(call)
+            # Post-tasks added once, not once per host.
+            ret.extend(self.expand_calls(call.post, apply_hosts=False))
+        # Add remainder as anonymous task
+        if self.core.remainder:
+            # TODO: this will need to change once there are more options for
+            # setting host lists besides "-H or 100% within-task"
+            if not cli_hosts:
+                raise NothingToDo(
+                    "Was told to run a command, but not given any hosts to run it on!"  # noqa
+                )
+
+            def anonymous(c):
+                c.run(self.core.remainder)
+
+            anon = Call(Task(body=anonymous))
+            # TODO: see above TODOs about non-parameterized setups, roles etc
+            # TODO: will likely need to refactor that logic some more so it can
+            # be used both there and here.
+            for init_kwargs in self.normalize_hosts(cli_hosts):
+                ret.append(self.parameterize(anon, init_kwargs))
+        return ret
+
+    def parameterize(self, call, connection_init_kwargs):
+        """
+        Parameterize a Call with its Context set to a per-host Connection.
+
+        :param call:
+            The generic `.Call` being parameterized.
+        :param connection_init_kwargs:
+            The dict of `.Connection` init params/kwargs to attach to the
+            resulting `.ConnectionCall`.
+
+        :returns:
+            `.ConnectionCall`.
+        """
+        msg = "Parameterizing {!r} with Connection kwargs {!r}"
+        debug(msg.format(call, connection_init_kwargs))
+        # Generate a custom ConnectionCall that has init_kwargs (used for
+        # creating the Connection at runtime) set to the requested params.
+        new_call_kwargs = dict(init_kwargs=connection_init_kwargs)
+        clone = call.clone(into=ConnectionCall, with_=new_call_kwargs)
+        return clone
+
+    def dedupe(self, tasks):
+        # Don't perform deduping, we will often have "duplicate" tasks w/
+        # distinct host values/etc.
+        # TODO: might want some deduplication later on though - falls under
+        # "how to mesh parameterization with pre/post/etc deduping".
+        return tasks
diff -Nru fabric-1.14.0/fabric/group.py fabric-2.5.0/fabric/group.py
--- fabric-1.14.0/fabric/group.py	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/fabric/group.py	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,300 @@
+try:
+    from invoke.vendor.six.moves.queue import Queue
+except ImportError:
+    from six.moves.queue import Queue
+
+from invoke.util import ExceptionHandlingThread
+
+from .connection import Connection
+from .exceptions import GroupException
+
+
+class Group(list):
+    """
+    A collection of `.Connection` objects whose API operates on its contents.
+
+    .. warning::
+        **This is a partially abstract class**; you need to use one of its
+        concrete subclasses (such as `.SerialGroup` or `.ThreadingGroup`) or
+        you'll get ``NotImplementedError`` on most of the methods.
+
+    Most methods in this class mirror those of `.Connection`, taking the same
+    arguments; however their return values and exception-raising behavior
+    differs:
+
+    - Return values are dict-like objects (`.GroupResult`) mapping
+      `.Connection` objects to the return value for the respective connections:
+      `.Group.run` returns a map of `.Connection` to `.runners.Result`,
+      `.Group.get` returns a map of `.Connection` to `.transfer.Result`, etc.
+    - If any connections encountered exceptions, a `.GroupException` is raised,
+      which is a thin wrapper around what would otherwise have been the
+      `.GroupResult` returned; within that wrapped `.GroupResult`, the
+      excepting connections map to the exception that was raised, in place of a
+      ``Result`` (as no ``Result`` was obtained.) Any non-excepting connections
+      will have a ``Result`` value, as normal.
+
+    For example, when no exceptions occur, a session might look like this::
+
+        >>> group = SerialGroup('host1', 'host2')
+        >>> group.run("this is fine")
+        {
+            <Connection host='host1'>: <Result cmd='this is fine' exited=0>,
+            <Connection host='host2'>: <Result cmd='this is fine' exited=0>,
+        }
+
+    With exceptions (anywhere from 1 to "all of them"), it looks like so; note
+    the different exception classes, e.g. `~invoke.exceptions.UnexpectedExit`
+    for a completed session whose command exited poorly, versus
+    `socket.gaierror` for a host that had DNS problems::
+
+        >>> group = SerialGroup('host1', 'host2', 'notahost')
+        >>> group.run("will it blend?")
+        {
+            <Connection host='host1'>: <Result cmd='will it blend?' exited=0>,
+            <Connection host='host2'>: <UnexpectedExit: cmd='...' exited=1>,
+            <Connection host='notahost'>: gaierror(...),
+        }
+
+    As with `.Connection`, `.Group` objects may be used as context managers,
+    which will automatically `.close` the object on block exit.
+
+    .. versionadded:: 2.0
+    .. versionchanged:: 2.4
+        Added context manager behavior.
+    """
+
+    def __init__(self, *hosts, **kwargs):
+        """
+        Create a group of connections from one or more shorthand host strings.
+
+        See `.Connection` for details on the format of these strings - they
+        will be used as the first positional argument of `.Connection`
+        constructors.
+
+        Any keyword arguments given will be forwarded directly to those
+        `.Connection` constructors as well. For example, to get a serially
+        executing group object that connects to ``admin@host1``,
+        ``admin@host2`` and ``admin@host3``, and forwards your SSH agent too::
+
+            group = SerialGroup(
+                "host1", "host2", "host3", user="admin", forward_agent=True,
+            )
+
+        .. versionchanged:: 2.3
+            Added ``**kwargs`` (was previously only ``*hosts``).
+        """
+        # TODO: #563, #388 (could be here or higher up in Program area)
+        self.extend([Connection(host, **kwargs) for host in hosts])
+
+    @classmethod
+    def from_connections(cls, connections):
+        """
+        Alternate constructor accepting `.Connection` objects.
+
+        .. versionadded:: 2.0
+        """
+        # TODO: *args here too; or maybe just fold into __init__ and type
+        # check?
+        group = cls()
+        group.extend(connections)
+        return group
+
+    def run(self, *args, **kwargs):
+        """
+        Executes `.Connection.run` on all member `Connections <.Connection>`.
+
+        :returns: a `.GroupResult`.
+
+        .. versionadded:: 2.0
+        """
+        # TODO: probably best to suck it up & match actual run() sig?
+        # TODO: how to change method of execution across contents? subclass,
+        # kwargs, additional methods, inject an executor? Doing subclass for
+        # now, but not 100% sure it's the best route.
+        # TODO: also need way to deal with duplicate connections (see THOUGHTS)
+        # TODO: and errors - probably FailureSet? How to handle other,
+        # regular, non Failure, exceptions though? Still need an aggregate
+        # exception type either way, whether it is FailureSet or what...
+        # TODO: OTOH, users may well want to be able to operate on the hosts
+        # that did not fail (esp if failure % is low) so we really _do_ want
+        # something like a result object mixing success and failure, or maybe a
+        # golang style two-tuple of successes and failures?
+        # TODO: or keep going w/ a "return or except", but the object is
+        # largely similar (if not identical) in both situations, with the
+        # exception just being the signal that Shit Broke?
+        raise NotImplementedError
+
+    # TODO: how to handle sudo? Probably just an inner worker method that takes
+    # the method name to actually call (run, sudo, etc)?
+
+    # TODO: this all needs to mesh well with similar strategies applied to
+    # entire tasks - so that may still end up factored out into Executors or
+    # something lower level than both those and these?
+
+    # TODO: local? Invoke wants ability to do that on its own though, which
+    # would be distinct from Group. (May want to switch Group to use that,
+    # though, whatever it ends up being?)
+
+    def get(self, *args, **kwargs):
+        """
+        Executes `.Connection.get` on all member `Connections <.Connection>`.
+
+        :returns: a `.GroupResult`.
+
+        .. versionadded:: 2.0
+        """
+        # TODO: probably best to suck it up & match actual get() sig?
+        # TODO: actually implement on subclasses
+        raise NotImplementedError
+
+    def close(self):
+        """
+        Executes `.Connection.close` on all member `Connections <.Connection>`.
+
+        .. versionadded:: 2.4
+        """
+        for cxn in self:
+            cxn.close()
+
+    def __enter__(self):
+        return self
+
+    def __exit__(self, *exc):
+        self.close()
+
+
+class SerialGroup(Group):
+    """
+    Subclass of `.Group` which executes in simple, serial fashion.
+
+    .. versionadded:: 2.0
+    """
+
+    def run(self, *args, **kwargs):
+        results = GroupResult()
+        excepted = False
+        for cxn in self:
+            try:
+                results[cxn] = cxn.run(*args, **kwargs)
+            except Exception as e:
+                results[cxn] = e
+                excepted = True
+        if excepted:
+            raise GroupException(results)
+        return results
+
+
+def thread_worker(cxn, queue, args, kwargs):
+    result = cxn.run(*args, **kwargs)
+    # TODO: namedtuple or attrs object?
+    queue.put((cxn, result))
+
+
+class ThreadingGroup(Group):
+    """
+    Subclass of `.Group` which uses threading to execute concurrently.
+
+    .. versionadded:: 2.0
+    """
+
+    def run(self, *args, **kwargs):
+        results = GroupResult()
+        queue = Queue()
+        threads = []
+        for cxn in self:
+            my_kwargs = dict(cxn=cxn, queue=queue, args=args, kwargs=kwargs)
+            thread = ExceptionHandlingThread(
+                target=thread_worker, kwargs=my_kwargs
+            )
+            threads.append(thread)
+        for thread in threads:
+            thread.start()
+        for thread in threads:
+            # TODO: configurable join timeout
+            # TODO: (in sudo's version) configurability around interactive
+            # prompting resulting in an exception instead, as in v1
+            thread.join()
+        # Get non-exception results from queue
+        while not queue.empty():
+            # TODO: io-sleep? shouldn't matter if all threads are now joined
+            cxn, result = queue.get(block=False)
+            # TODO: outstanding musings about how exactly aggregate results
+            # ought to ideally operate...heterogenous obj like this, multiple
+            # objs, ??
+            results[cxn] = result
+        # Get exceptions from the threads themselves.
+        # TODO: in a non-thread setup, this would differ, e.g.:
+        # - a queue if using multiprocessing
+        # - some other state-passing mechanism if using e.g. coroutines
+        # - ???
+        excepted = False
+        for thread in threads:
+            wrapper = thread.exception()
+            if wrapper is not None:
+                # Outer kwargs is Thread instantiation kwargs, inner is kwargs
+                # passed to thread target/body.
+                cxn = wrapper.kwargs["kwargs"]["cxn"]
+                results[cxn] = wrapper.value
+                excepted = True
+        if excepted:
+            raise GroupException(results)
+        return results
+
+
+class GroupResult(dict):
+    """
+    Collection of results and/or exceptions arising from `.Group` methods.
+
+    Acts like a dict, but adds a couple convenience methods, to wit:
+
+    - Keys are the individual `.Connection` objects from within the `.Group`.
+    - Values are either return values / results from the called method (e.g.
+      `.runners.Result` objects), *or* an exception object, if one prevented
+      the method from returning.
+    - Subclasses `dict`, so has all dict methods.
+    - Has `.succeeded` and `.failed` attributes containing sub-dicts limited to
+      just those key/value pairs that succeeded or encountered exceptions,
+      respectively.
+
+      - Of note, these attributes allow high level logic, e.g. ``if
+        mygroup.run('command').failed`` and so forth.
+
+    .. versionadded:: 2.0
+    """
+
+    def __init__(self, *args, **kwargs):
+        super(dict, self).__init__(*args, **kwargs)
+        self._successes = {}
+        self._failures = {}
+
+    def _bifurcate(self):
+        # Short-circuit to avoid reprocessing every access.
+        if self._successes or self._failures:
+            return
+        # TODO: if we ever expect .succeeded/.failed to be useful before a
+        # GroupResult is fully initialized, this needs to become smarter.
+        for key, value in self.items():
+            if isinstance(value, BaseException):
+                self._failures[key] = value
+            else:
+                self._successes[key] = value
+
+    @property
+    def succeeded(self):
+        """
+        A sub-dict containing only successful results.
+
+        .. versionadded:: 2.0
+        """
+        self._bifurcate()
+        return self._successes
+
+    @property
+    def failed(self):
+        """
+        A sub-dict containing only failed results.
+
+        .. versionadded:: 2.0
+        """
+        self._bifurcate()
+        return self._failures
diff -Nru fabric-1.14.0/fabric/__init__.py fabric-2.5.0/fabric/__init__.py
--- fabric-1.14.0/fabric/__init__.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/fabric/__init__.py	2019-08-06 23:57:28.000000000 +0100
@@ -1,3 +1,7 @@
-"""
-See `fabric.api` for the publically importable API.
-"""
+# flake8: noqa
+from ._version import __version_info__, __version__
+from .connection import Config, Connection
+from .runners import Remote, Result
+from .group import Group, SerialGroup, ThreadingGroup, GroupResult
+from .tasks import task, Task
+from .executor import Executor
diff -Nru fabric-1.14.0/fabric/io.py fabric-2.5.0/fabric/io.py
--- fabric-1.14.0/fabric/io.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/fabric/io.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,239 +0,0 @@
-from __future__ import with_statement
-
-import sys
-import time
-import re
-import socket
-from select import select
-
-from fabric.state import env, output, win32
-from fabric.auth import get_password, set_password
-import fabric.network
-from fabric.network import ssh, normalize
-from fabric.utils import RingBuffer
-from fabric.exceptions import CommandTimeout
-
-
-if win32:
-    import msvcrt
-
-
-def _endswith(char_list, substring):
-    tail = char_list[-1 * len(substring):]
-    substring = list(substring)
-    return tail == substring
-
-
-def _has_newline(bytelist):
-    return '\r' in bytelist or '\n' in bytelist
-
-
-def output_loop(*args, **kwargs):
-    OutputLooper(*args, **kwargs).loop()
-
-
-class OutputLooper(object):
-    def __init__(self, chan, attr, stream, capture, timeout):
-        self.chan = chan
-        self.stream = stream
-        self.capture = capture
-        self.timeout = timeout
-        self.read_func = getattr(chan, attr)
-        self.prefix = "[%s] %s: " % (
-            env.host_string,
-            "out" if attr == 'recv' else "err"
-        )
-        self.printing = getattr(output, 'stdout' if (attr == 'recv') else 'stderr')
-        self.linewise = (env.linewise or env.parallel)
-        self.reprompt = False
-        self.read_size = 4096
-        self.write_buffer = RingBuffer([], maxlen=len(self.prefix))
-
-    def _flush(self, text):
-        self.stream.write(text)
-        # Actually only flush if not in linewise mode.
-        # When linewise is set (e.g. in parallel mode) flushing makes
-        # doubling-up of line prefixes, and other mixed output, more likely.
-        if not env.linewise:
-            self.stream.flush()
-        self.write_buffer.extend(text)
-
-    def loop(self):
-        """
-        Loop, reading from <chan>.<attr>(), writing to <stream> and buffering to <capture>.
-
-        Will raise `~fabric.exceptions.CommandTimeout` if network timeouts
-        continue to be seen past the defined ``self.timeout`` threshold.
-        (Timeouts before then are considered part of normal short-timeout fast
-        network reading; see Fabric issue #733 for background.)
-        """
-        # Initialize loop variables
-        initial_prefix_printed = False
-        seen_cr = False
-        line = []
-
-        # Allow prefix to be turned off.
-        if not env.output_prefix:
-            self.prefix = ""
-
-        start = time.time()
-        while True:
-            # Handle actual read
-            try:
-                bytelist = self.read_func(self.read_size)
-            except socket.timeout:
-                elapsed = time.time() - start
-                if self.timeout is not None and elapsed > self.timeout:
-                    raise CommandTimeout(timeout=self.timeout)
-                continue
-            # Empty byte == EOS
-            if bytelist == '':
-                # If linewise, ensure we flush any leftovers in the buffer.
-                if self.linewise and line:
-                    self._flush(self.prefix)
-                    self._flush("".join(line))
-                break
-            # A None capture variable implies that we're in open_shell()
-            if self.capture is None:
-                # Just print directly -- no prefixes, no capturing, nada
-                # And since we know we're using a pty in this mode, just go
-                # straight to stdout.
-                self._flush(bytelist)
-            # Otherwise, we're in run/sudo and need to handle capturing and
-            # prompts.
-            else:
-                # Print to user
-                if self.printing:
-                    printable_bytes = bytelist
-                    # Small state machine to eat \n after \r
-                    if printable_bytes[-1] == "\r":
-                        seen_cr = True
-                    if printable_bytes[0] == "\n" and seen_cr:
-                        printable_bytes = printable_bytes[1:]
-                        seen_cr = False
-
-                    while _has_newline(printable_bytes) and printable_bytes != "":
-                        # at most 1 split !
-                        cr = re.search("(\r\n|\r|\n)", printable_bytes)
-                        if cr is None:
-                            break
-                        end_of_line = printable_bytes[:cr.start(0)]
-                        printable_bytes = printable_bytes[cr.end(0):]
-
-                        if not initial_prefix_printed:
-                            self._flush(self.prefix)
-
-                        if _has_newline(end_of_line):
-                            end_of_line = ''
-
-                        if self.linewise:
-                            self._flush("".join(line) + end_of_line + "\n")
-                            line = []
-                        else:
-                            self._flush(end_of_line + "\n")
-                        initial_prefix_printed = False
-
-                    if self.linewise:
-                        line += [printable_bytes]
-                    else:
-                        if not initial_prefix_printed:
-                            self._flush(self.prefix)
-                            initial_prefix_printed = True
-                        self._flush(printable_bytes)
-
-                # Now we have handled printing, handle interactivity
-                read_lines = re.split(r"(\r|\n|\r\n)", bytelist)
-                for fragment in read_lines:
-                    # Store in capture buffer
-                    self.capture += fragment
-                    # Handle prompts
-                    expected, response = self._get_prompt_response()
-                    if expected:
-                        del self.capture[-1 * len(expected):]
-                        self.chan.sendall(str(response) + '\n')
-                    else:
-                        prompt = _endswith(self.capture, env.sudo_prompt)
-                        try_again = (_endswith(self.capture, env.again_prompt + '\n')
-                            or _endswith(self.capture, env.again_prompt + '\r\n'))
-                        if prompt:
-                            self.prompt()
-                        elif try_again:
-                            self.try_again()
-
-        # Print trailing new line if the last thing we printed was our line
-        # prefix.
-        if self.prefix and "".join(self.write_buffer) == self.prefix:
-            self._flush('\n')
-
-    def prompt(self):
-        # Obtain cached password, if any
-        password = get_password(*normalize(env.host_string))
-        # Remove the prompt itself from the capture buffer. This is
-        # backwards compatible with Fabric 0.9.x behavior; the user
-        # will still see the prompt on their screen (no way to avoid
-        # this) but at least it won't clutter up the captured text.
-        del self.capture[-1 * len(env.sudo_prompt):]
-        # If the password we just tried was bad, prompt the user again.
-        if (not password) or self.reprompt:
-            # Print the prompt and/or the "try again" notice if
-            # output is being hidden. In other words, since we need
-            # the user's input, they need to see why we're
-            # prompting them.
-            if not self.printing:
-                self._flush(self.prefix)
-                if self.reprompt:
-                    self._flush(env.again_prompt + '\n' + self.prefix)
-                self._flush(env.sudo_prompt)
-            # Prompt for, and store, password. Give empty prompt so the
-            # initial display "hides" just after the actually-displayed
-            # prompt from the remote end.
-            self.chan.input_enabled = False
-            password = fabric.network.prompt_for_password(
-                prompt=" ", no_colon=True, stream=self.stream
-            )
-            self.chan.input_enabled = True
-            # Update env.password, env.passwords if necessary
-            user, host, port = normalize(env.host_string)
-            # TODO: in 2.x, make sure to only update sudo-specific password
-            # config values, not login ones.
-            set_password(user, host, port, password)
-            # Reset reprompt flag
-            self.reprompt = False
-        # Send current password down the pipe
-        self.chan.sendall(password + '\n')
-
-    def try_again(self):
-        # Remove text from capture buffer
-        self.capture = self.capture[:len(env.again_prompt)]
-        # Set state so we re-prompt the user at the next prompt.
-        self.reprompt = True
-
-    def _get_prompt_response(self):
-        """
-        Iterate through the request prompts dict and return the response and
-        original request if we find a match
-        """
-        for tup in env.prompts.iteritems():
-            if _endswith(self.capture, tup[0]):
-                return tup
-        return None, None
-
-
-def input_loop(chan, using_pty):
-    while not chan.exit_status_ready():
-        if win32:
-            have_char = msvcrt.kbhit()
-        else:
-            r, w, x = select([sys.stdin], [], [], 0.0)
-            have_char = (r and r[0] == sys.stdin)
-        if have_char and chan.input_enabled:
-            # Send all local stdin to remote end's stdin
-            byte = msvcrt.getch() if win32 else sys.stdin.read(1)
-            chan.sendall(byte)
-            # Optionally echo locally, if needed.
-            if not using_pty and env.echo_stdin:
-                # Not using fastprint() here -- it prints as 'user'
-                # output level, don't want it to be accidentally hidden
-                sys.stdout.write(byte)
-                sys.stdout.flush()
-        time.sleep(ssh.io_sleep)
diff -Nru fabric-1.14.0/fabric/job_queue.py fabric-2.5.0/fabric/job_queue.py
--- fabric-1.14.0/fabric/job_queue.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/fabric/job_queue.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,236 +0,0 @@
-"""
-Sliding-window-based job/task queue class (& example of use.)
-
-May use ``multiprocessing.Process`` or ``threading.Thread`` objects as queue
-items, though within Fabric itself only ``Process`` objects are used/supported.
-"""
-
-from __future__ import with_statement
-import time
-import Queue
-from multiprocessing import Process
-
-from fabric.network import ssh
-from fabric.context_managers import settings
-
-
-class JobQueue(object):
-    """
-    The goal of this class is to make a queue of processes to run, and go
-    through them running X number at any given time. 
-
-    So if the bubble is 5 start with 5 running and move the bubble of running
-    procs along the queue looking something like this:
-
-        Start
-        ...........................
-        [~~~~~]....................
-        ___[~~~~~].................
-        _________[~~~~~]...........
-        __________________[~~~~~]..
-        ____________________[~~~~~]
-        ___________________________
-                                End 
-    """
-    def __init__(self, max_running, comms_queue):
-        """
-        Setup the class to resonable defaults.
-        """
-        self._queued = []
-        self._running = []
-        self._completed = []
-        self._num_of_jobs = 0
-        self._max = max_running
-        self._comms_queue = comms_queue
-        self._finished = False
-        self._closed = False
-        self._debug = False
-
-    def _all_alive(self):
-        """
-        Simply states if all procs are alive or not. Needed to determine when
-        to stop looping, and pop dead procs off and add live ones.
-        """
-        if self._running:
-            return all([x.is_alive() for x in self._running])
-        else:
-            return False
-
-    def __len__(self):
-        """
-        Just going to use number of jobs as the JobQueue length.
-        """
-        return self._num_of_jobs
-
-    def close(self):
-        """
-        A sanity check, so that the need to care about new jobs being added in
-        the last throws of the job_queue's run are negated.
-        """
-        if self._debug:
-            print("job queue closed.")
-
-        self._closed = True
-
-    def append(self, process):
-        """
-        Add the Process() to the queue, so that later it can be checked up on.
-        That is if the JobQueue is still open.
-
-        If the queue is closed, this will just silently do nothing.
-
-        To get data back out of this process, give ``process`` access to a
-        ``multiprocessing.Queue`` object, and give it here as ``queue``. Then
-        ``JobQueue.run`` will include the queue's contents in its return value.
-        """
-        if not self._closed:
-            self._queued.append(process)
-            self._num_of_jobs += 1
-            if self._debug:
-                print("job queue appended %s." % process.name)
-
-    def run(self):
-        """
-        This is the workhorse. It will take the intial jobs from the _queue,
-        start them, add them to _running, and then go into the main running
-        loop.
-
-        This loop will check for done procs, if found, move them out of
-        _running into _completed. It also checks for a _running queue with open
-        spots, which it will then fill as discovered.
-
-        To end the loop, there have to be no running procs, and no more procs
-        to be run in the queue.
-
-        This function returns an iterable of all its children's exit codes.
-        """
-        def _advance_the_queue():
-            """
-            Helper function to do the job of poping a new proc off the queue
-            start it, then add it to the running queue. This will eventually
-            depleate the _queue, which is a condition of stopping the running
-            while loop.
-
-            It also sets the env.host_string from the job.name, so that fabric
-            knows that this is the host to be making connections on.
-            """
-            job = self._queued.pop()
-            if self._debug:
-                print("Popping '%s' off the queue and starting it" % job.name)
-            with settings(clean_revert=True, host_string=job.name, host=job.name):
-                job.start()
-            self._running.append(job)
-
-        # Prep return value so we can start filling it during main loop
-        results = {}
-        for job in self._queued:
-            results[job.name] = dict.fromkeys(('exit_code', 'results'))
-
-        if not self._closed:
-            raise Exception("Need to close() before starting.")
-
-        if self._debug:
-            print("Job queue starting.")
-
-        while len(self._running) < self._max:
-            _advance_the_queue()
-
-        # Main loop!
-        while not self._finished:
-            while len(self._running) < self._max and self._queued:
-                _advance_the_queue()
-
-            if not self._all_alive():
-                for id, job in enumerate(self._running):
-                    if not job.is_alive():
-                        if self._debug:
-                            print("Job queue found finished proc: %s." %
-                                    job.name)
-                        done = self._running.pop(id)
-                        self._completed.append(done)
-
-                if self._debug:
-                    print("Job queue has %d running." % len(self._running))
-
-            if not (self._queued or self._running):
-                if self._debug:
-                    print("Job queue finished.")
-
-                for job in self._completed:
-                    job.join()
-
-                self._finished = True
-
-            # Each loop pass, try pulling results off the queue to keep its
-            # size down. At this point, we don't actually care if any results
-            # have arrived yet; they will be picked up after the main loop.
-            self._fill_results(results)
-
-            time.sleep(ssh.io_sleep)
-
-        # Consume anything left in the results queue. Note that there is no
-        # need to block here, as the main loop ensures that all workers will
-        # already have finished.
-        self._fill_results(results)
-
-        # Attach exit codes now that we're all done & have joined all jobs
-        for job in self._completed:
-            if isinstance(job, Process):
-                results[job.name]['exit_code'] = job.exitcode
-
-        return results
-
-    def _fill_results(self, results):
-        """
-        Attempt to pull data off self._comms_queue and add to 'results' dict.
-        If no data is available (i.e. the queue is empty), bail immediately.
-        """
-        while True:
-            try:
-                datum = self._comms_queue.get_nowait()
-                results[datum['name']]['results'] = datum['result']
-            except Queue.Empty:
-                break
-
-
-#### Sample
-
-def try_using(parallel_type):
-    """
-    This will run the queue through it's paces, and show a simple way of using
-    the job queue.
-    """
-
-    def print_number(number):
-        """
-        Simple function to give a simple task to execute.
-        """
-        print(number)
-
-    if parallel_type == "multiprocessing":
-        from multiprocessing import Process as Bucket
-
-    elif parallel_type == "threading":
-        from threading import Thread as Bucket
-
-    # Make a job_queue with a bubble of len 5, and have it print verbosely
-    queue = Queue.Queue()
-    jobs = JobQueue(5, queue)
-    jobs._debug = True
-
-    # Add 20 procs onto the stack
-    for x in range(20):
-        jobs.append(Bucket(
-            target=print_number,
-            args=[x],
-            kwargs={},
-            ))
-
-    # Close up the queue and then start it's execution
-    jobs.close()
-    jobs.run()
-
-
-if __name__ == '__main__':
-    try_using("multiprocessing")
-    try_using("threading")
diff -Nru fabric-1.14.0/fabric/__main__.py fabric-2.5.0/fabric/__main__.py
--- fabric-1.14.0/fabric/__main__.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/fabric/__main__.py	2019-08-06 23:57:28.000000000 +0100
@@ -1,2 +1,9 @@
-import fabric.main
-fabric.main.main()
+"""
+This code provides the ability to run fabric
+package as a script
+Usage: python -m fabric
+"""
+
+from .main import program
+
+program.run()
diff -Nru fabric-1.14.0/fabric/main.py fabric-2.5.0/fabric/main.py
--- fabric-1.14.0/fabric/main.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/fabric/main.py	2019-08-06 23:57:28.000000000 +0100
@@ -1,775 +1,159 @@
 """
-This module contains Fab's `main` method plus related subroutines.
+CLI entrypoint & parser configuration.
 
-`main` is executed as the command line ``fab`` program and takes care of
-parsing options and commands, loading the user settings file, loading a
-fabfile, and executing the commands given.
-
-The other callables defined in this module are internal only. Anything useful
-to individuals leveraging Fabric as a library, should be kept elsewhere.
+Builds on top of Invoke's core functionality for same.
 """
-import getpass
-import inspect
-from operator import isMappingType
-from optparse import OptionParser
-import os
-import sys
-import types
-
-# For checking callables against the API, & easy mocking
-from fabric import api, state, colors
-from fabric.contrib import console, files, project
-
-from fabric.network import disconnect_all, ssh
-from fabric.state import env_options
-from fabric.tasks import Task, execute, get_task_details
-from fabric.task_utils import _Dict, crawl
-from fabric.utils import abort, indent, warn, _pty_size
-
-
-# One-time calculation of "all internal callables" to avoid doing this on every
-# check of a given fabfile callable (in is_classic_task()).
-_modules = [api, project, files, console, colors]
-_internals = reduce(lambda x, y: x + filter(callable, vars(y).values()),
-    _modules,
-    []
-)
-
-
-# Module recursion cache
-class _ModuleCache(object):
-    """
-    Set-like object operating on modules and storing __name__s internally.
-    """
-    def __init__(self):
-        self.cache = set()
-
-    def __contains__(self, value):
-        return value.__name__ in self.cache
-
-    def add(self, value):
-        return self.cache.add(value.__name__)
-
-    def clear(self):
-        return self.cache.clear()
-
-_seen = _ModuleCache()
-
-
-def load_settings(path):
-    """
-    Take given file path and return dictionary of any key=value pairs found.
-
-    Usage docs are in sites/docs/usage/fab.rst, in "Settings files."
-    """
-    if os.path.exists(path):
-        comments = lambda s: s and not s.startswith("#")
-        settings = filter(comments, open(path, 'r'))
-        return dict((k.strip(), v.strip()) for k, _, v in
-            [s.partition('=') for s in settings])
-    # Handle nonexistent or empty settings file
-    return {}
-
-
-def _is_package(path):
-    """
-    Is the given path a Python package?
-    """
-    _exists = lambda s: os.path.exists(os.path.join(path, s))
-    return (
-        os.path.isdir(path)
-        and (_exists('__init__.py') or _exists('__init__.pyc'))
-    )
 
+import getpass
 
-def find_fabfile(names=None):
-    """
-    Attempt to locate a fabfile, either explicitly or by searching parent dirs.
-
-    Usage docs are in sites/docs/usage/fabfiles.rst, in "Fabfile discovery."
-    """
-    # Obtain env value if not given specifically
-    if names is None:
-        names = [state.env.fabfile]
-    # Create .py version if necessary
-    if not names[0].endswith('.py'):
-        names += [names[0] + '.py']
-    # Does the name contain path elements?
-    if os.path.dirname(names[0]):
-        # If so, expand home-directory markers and test for existence
-        for name in names:
-            expanded = os.path.expanduser(name)
-            if os.path.exists(expanded):
-                if name.endswith('.py') or _is_package(expanded):
-                    return os.path.abspath(expanded)
-    else:
-        # Otherwise, start in cwd and work downwards towards filesystem root
-        path = '.'
-        # Stop before falling off root of filesystem (should be platform
-        # agnostic)
-        while os.path.split(os.path.abspath(path))[1]:
-            for name in names:
-                joined = os.path.join(path, name)
-                if os.path.exists(joined):
-                    if name.endswith('.py') or _is_package(joined):
-                        return os.path.abspath(joined)
-            path = os.path.join('..', path)
-    # Implicit 'return None' if nothing was found
-
-
-def is_classic_task(tup):
-    """
-    Takes (name, object) tuple, returns True if it's a non-Fab public callable.
-    """
-    name, func = tup
-    try:
-        is_classic = (
-            callable(func)
-            and (func not in _internals)
-            and not name.startswith('_')
-            and not (inspect.isclass(func) and issubclass(func, Exception))
+from invoke import Argument, Collection, Program
+from invoke import __version__ as invoke
+from paramiko import __version__ as paramiko
+
+from . import __version__ as fabric
+from . import Config, Executor
+
+
+class Fab(Program):
+    def print_version(self):
+        super(Fab, self).print_version()
+        print("Paramiko {}".format(paramiko))
+        print("Invoke {}".format(invoke))
+
+    def core_args(self):
+        core_args = super(Fab, self).core_args()
+        my_args = [
+            Argument(
+                names=("H", "hosts"),
+                help="Comma-separated host name(s) to execute tasks against.",
+            ),
+            Argument(
+                names=("i", "identity"),
+                kind=list,  # Same as OpenSSH, can give >1 key
+                # TODO: automatically add hint about iterable-ness to Invoke
+                # help display machinery?
+                help="Path to runtime SSH identity (key) file. May be given multiple times.",  # noqa
+            ),
+            # TODO: worth having short flags for these prompt args?
+            Argument(
+                names=("prompt-for-login-password",),
+                kind=bool,
+                help="Request an upfront SSH-auth password prompt.",
+            ),
+            Argument(
+                names=("prompt-for-passphrase",),
+                kind=bool,
+                help="Request an upfront SSH key passphrase prompt.",
+            ),
+            Argument(
+                names=("S", "ssh-config"),
+                help="Path to runtime SSH config file.",
+            ),
+            Argument(
+                names=("t", "connect-timeout"),
+                kind=int,
+                help="Specifies default connection timeout, in seconds.",
+            ),
+        ]
+        return core_args + my_args
+
+    @property
+    def _remainder_only(self):
+        # No 'unparsed' (i.e. tokens intended for task contexts), and remainder
+        # (text after a double-dash) implies a contextless/taskless remainder
+        # execution of the style 'fab -H host -- command'.
+        # NOTE: must ALSO check to ensure the double dash isn't being used for
+        # tab completion machinery...
+        return (
+            not self.core.unparsed
+            and self.core.remainder
+            and not self.args.complete.value
         )
-    # Handle poorly behaved __eq__ implementations
-    except (ValueError, TypeError):
-        is_classic = False
-    return is_classic
-
-
-def load_fabfile(path, importer=None):
-    """
-    Import given fabfile path and return (docstring, callables).
-
-    Specifically, the fabfile's ``__doc__`` attribute (a string) and a
-    dictionary of ``{'name': callable}`` containing all callables which pass
-    the "is a Fabric task" test.
-    """
-    if importer is None:
-        importer = __import__
-    # Get directory and fabfile name
-    directory, fabfile = os.path.split(path)
-    # If the directory isn't in the PYTHONPATH, add it so our import will work
-    added_to_path = False
-    index = None
-    if directory not in sys.path:
-        sys.path.insert(0, directory)
-        added_to_path = True
-    # If the directory IS in the PYTHONPATH, move it to the front temporarily,
-    # otherwise other fabfiles -- like Fabric's own -- may scoop the intended
-    # one.
-    else:
-        i = sys.path.index(directory)
-        if i != 0:
-            # Store index for later restoration
-            index = i
-            # Add to front, then remove from original position
-            sys.path.insert(0, directory)
-            del sys.path[i + 1]
-    # Perform the import (trimming off the .py)
-    imported = importer(os.path.splitext(fabfile)[0])
-    # Remove directory from path if we added it ourselves (just to be neat)
-    if added_to_path:
-        del sys.path[0]
-    # Put back in original index if we moved it
-    if index is not None:
-        sys.path.insert(index + 1, directory)
-        del sys.path[0]
-
-    # Actually load tasks
-    docstring, new_style, classic, default = load_tasks_from_module(imported)
-    tasks = new_style if state.env.new_style_tasks else classic
-    # Clean up after ourselves
-    _seen.clear()
-    return docstring, tasks, default
-
-
-def load_tasks_from_module(imported):
-    """
-    Handles loading all of the tasks for a given `imported` module
-    """
-    # Obey the use of <module>.__all__ if it is present
-    imported_vars = vars(imported)
-    if "__all__" in imported_vars:
-        imported_vars = [(name, imported_vars[name]) for name in \
-                         imported_vars if name in imported_vars["__all__"]]
-    else:
-        imported_vars = imported_vars.items()
-    # Return a two-tuple value.  First is the documentation, second is a
-    # dictionary of callables only (and don't include Fab operations or
-    # underscored callables)
-    new_style, classic, default = extract_tasks(imported_vars)
-    return imported.__doc__, new_style, classic, default
-
-
-def extract_tasks(imported_vars):
-    """
-    Handle extracting tasks from a given list of variables
-    """
-    new_style_tasks = _Dict()
-    classic_tasks = {}
-    default_task = None
-    if 'new_style_tasks' not in state.env:
-        state.env.new_style_tasks = False
-    for tup in imported_vars:
-        name, obj = tup
-        if is_task_object(obj):
-            state.env.new_style_tasks = True
-            # Use instance.name if defined
-            if obj.name and obj.name != 'undefined':
-                new_style_tasks[obj.name] = obj
-            else:
-                obj.name = name
-                new_style_tasks[name] = obj
-            # Handle aliasing
-            if obj.aliases is not None:
-                for alias in obj.aliases:
-                    new_style_tasks[alias] = obj
-            # Handle defaults
-            if obj.is_default:
-                default_task = obj
-        elif is_classic_task(tup):
-            classic_tasks[name] = obj
-        elif is_task_module(obj):
-            docs, newstyle, classic, default = load_tasks_from_module(obj)
-            for task_name, task in newstyle.items():
-                if name not in new_style_tasks:
-                    new_style_tasks[name] = _Dict()
-                new_style_tasks[name][task_name] = task
-            if default is not None:
-                new_style_tasks[name].default = default
-    return new_style_tasks, classic_tasks, default_task
-
-
-def is_task_module(a):
-    """
-    Determine if the provided value is a task module
-    """
-    #return (type(a) is types.ModuleType and
-    #        any(map(is_task_object, vars(a).values())))
-    if isinstance(a, types.ModuleType) and a not in _seen:
-        # Flag module as seen
-        _seen.add(a)
-        # Signal that we need to check it out
-        return True
-
-
-def is_task_object(a):
-    """
-    Determine if the provided value is a ``Task`` object.
-
-    This returning True signals that all tasks within the fabfile
-    module must be Task objects.
-    """
-    return isinstance(a, Task) and a.use_task_objects
-
-
-def parse_options():
-    """
-    Handle command-line options with optparse.OptionParser.
-
-    Return list of arguments, largely for use in `parse_arguments`.
-    """
-    #
-    # Initialize
-    #
-
-    parser = OptionParser(
-        usage=("fab [options] <command>"
-               "[:arg1,arg2=val2,host=foo,hosts='h1;h2',...] ..."))
-
-    #
-    # Define options that don't become `env` vars (typically ones which cause
-    # Fabric to do something other than its normal execution, such as
-    # --version)
-    #
-
-    # Display info about a specific command
-    parser.add_option('-d', '--display',
-        metavar='NAME',
-        help="print detailed info about command NAME"
-    )
-
-    # Control behavior of --list
-    LIST_FORMAT_OPTIONS = ('short', 'normal', 'nested')
-    parser.add_option('-F', '--list-format',
-        choices=LIST_FORMAT_OPTIONS,
-        default='normal',
-        metavar='FORMAT',
-        help="formats --list, choices: %s" % ", ".join(LIST_FORMAT_OPTIONS)
-    )
-
-    parser.add_option('-I', '--initial-password-prompt',
-        action='store_true',
-        default=False,
-        help="Force password prompt up-front"
-    )
 
-    parser.add_option('--initial-sudo-password-prompt',
-        action='store_true',
-        default=False,
-        help="Force sudo password prompt up-front"
-    )
-
-    # List Fab commands found in loaded fabfiles/source files
-    parser.add_option('-l', '--list',
-        action='store_true',
-        dest='list_commands',
-        default=False,
-        help="print list of possible commands and exit"
-    )
-
-    # Allow setting of arbitrary env vars at runtime.
-    parser.add_option('--set',
-        metavar="KEY=VALUE,...",
-        dest='env_settings',
-        default="",
-        help="comma separated KEY=VALUE pairs to set Fab env vars"
-    )
+    def load_collection(self):
+        # Stick in a dummy Collection if it looks like we were invoked w/o any
+        # tasks, and with a remainder.
+        # This isn't super ideal, but Invoke proper has no obvious "just run my
+        # remainder" use case, so having it be capable of running w/o any task
+        # module, makes no sense. But we want that capability for testing &
+        # things like 'fab -H x,y,z -- mycommand'.
+        if self._remainder_only:
+            # TODO: hm we're probably not honoring project-specific configs in
+            # this branch; is it worth having it assume CWD==project, since
+            # that's often what users expect? Even tho no task collection to
+            # honor the real "lives by task coll"?
+            self.collection = Collection()
+        else:
+            super(Fab, self).load_collection()
 
-    # Like --list, but text processing friendly
-    parser.add_option('--shortlist',
-        action='store_true',
-        dest='shortlist',
-        default=False,
-        help="alias for -F short --list"
+    def no_tasks_given(self):
+        # As above, neuter the usual "hey you didn't give me any tasks, let me
+        # print help for you" behavior, if necessary.
+        if not self._remainder_only:
+            super(Fab, self).no_tasks_given()
+
+    def create_config(self):
+        # Create config, as parent does, but with lazy=True to avoid our own
+        # SSH config autoload. (Otherwise, we can't correctly load _just_ the
+        # runtime file if one's being given later.)
+        self.config = self.config_class(lazy=True)
+        # However, we don't really want the parent class' lazy behavior (which
+        # skips loading system/global invoke-type conf files) so we manually do
+        # that here to match upstream behavior.
+        self.config.load_base_conf_files()
+        # And merge again so that data is available.
+        # TODO: really need to either A) stop giving fucks about calling
+        # merge() "too many times", or B) make merge() itself determine whether
+        # it needs to run and/or just merge stuff that's changed, so log spam
+        # isn't as bad.
+        self.config.merge()
+
+    def update_config(self):
+        # Note runtime SSH path, if given, and load SSH configurations.
+        # NOTE: must do parent before our work, in case users want to disable
+        # SSH config loading within a runtime-level conf file/flag.
+        super(Fab, self).update_config(merge=False)
+        self.config.set_runtime_ssh_path(self.args["ssh-config"].value)
+        self.config.load_ssh_config()
+        # Load -i identity file, if given, into connect_kwargs, at overrides
+        # level.
+        # TODO: this feels a little gross, but since the parent has already
+        # called load_overrides, this is best we can do for now w/o losing
+        # data. Still feels correct; just might be cleaner to have even more
+        # Config API members around this sort of thing. Shrug.
+        connect_kwargs = {}
+        path = self.args["identity"].value
+        if path:
+            connect_kwargs["key_filename"] = path
+        # Ditto for connect timeout
+        timeout = self.args["connect-timeout"].value
+        if timeout:
+            connect_kwargs["timeout"] = timeout
+        # Secrets prompts that want to happen at handoff time instead of
+        # later/at user-time.
+        # TODO: should this become part of Invoke proper in case other
+        # downstreams have need of it? E.g. a prompt Argument 'type'? We're
+        # already doing a similar thing there for sudo password...
+        if self.args["prompt-for-login-password"].value:
+            prompt = "Enter login password for use with SSH auth: "
+            connect_kwargs["password"] = getpass.getpass(prompt)
+        if self.args["prompt-for-passphrase"].value:
+            prompt = "Enter passphrase for use unlocking SSH keys: "
+            connect_kwargs["passphrase"] = getpass.getpass(prompt)
+        self.config._overrides["connect_kwargs"] = connect_kwargs
+        # Since we gave merge=False above, we must do it ourselves here. (Also
+        # allows us to 'compile' our overrides manipulation.)
+        self.config.merge()
+
+
+# Mostly a concession to testing.
+def make_program():
+    return Fab(
+        name="Fabric",
+        version=fabric,
+        executor_class=Executor,
+        config_class=Config,
     )
 
-    # Version number (optparse gives you --version but we have to do it
-    # ourselves to get -V too. sigh)
-    parser.add_option('-V', '--version',
-        action='store_true',
-        dest='show_version',
-        default=False,
-        help="show program's version number and exit"
-    )
 
-    #
-    # Add in options which are also destined to show up as `env` vars.
-    #
-
-    for option in env_options:
-        parser.add_option(option)
-
-    #
-    # Finalize
-    #
-
-    # Return three-tuple of parser + the output from parse_args (opt obj, args)
-    opts, args = parser.parse_args()
-    return parser, opts, args
-
-
-def _is_task(name, value):
-    """
-    Is the object a task as opposed to e.g. a dict or int?
-    """
-    return is_classic_task((name, value)) or is_task_object(value)
-
-
-def _sift_tasks(mapping):
-    tasks, collections = [], []
-    for name, value in mapping.iteritems():
-        if _is_task(name, value):
-            tasks.append(name)
-        elif isMappingType(value):
-            collections.append(name)
-    tasks = sorted(tasks)
-    collections = sorted(collections)
-    return tasks, collections
-
-
-def _task_names(mapping):
-    """
-    Flatten & sort task names in a breadth-first fashion.
-
-    Tasks are always listed before submodules at the same level, but within
-    those two groups, sorting is alphabetical.
-    """
-    tasks, collections = _sift_tasks(mapping)
-    for collection in collections:
-        module = mapping[collection]
-        if hasattr(module, 'default'):
-            tasks.append(collection)
-        join = lambda x: ".".join((collection, x))
-        tasks.extend(map(join, _task_names(module)))
-    return tasks
-
-
-def _print_docstring(docstrings, name):
-    if not docstrings:
-        return False
-    docstring = crawl(name, state.commands).__doc__
-    if isinstance(docstring, basestring):
-        return docstring
-
-
-def _normal_list(docstrings=True):
-    result = []
-    task_names = _task_names(state.commands)
-    # Want separator between name, description to be straight col
-    max_len = reduce(lambda a, b: max(a, len(b)), task_names, 0)
-    sep = '  '
-    trail = '...'
-    max_width = _pty_size()[1] - 1 - len(trail)
-    for name in task_names:
-        output = None
-        docstring = _print_docstring(docstrings, name)
-        if docstring:
-            lines = filter(None, docstring.splitlines())
-            first_line = lines[0].strip()
-            # Truncate it if it's longer than N chars
-            size = max_width - (max_len + len(sep) + len(trail))
-            if len(first_line) > size:
-                first_line = first_line[:size] + trail
-            output = name.ljust(max_len) + sep + first_line
-        # Or nothing (so just the name)
-        else:
-            output = name
-        result.append(indent(output))
-    return result
-
-
-def _nested_list(mapping, level=1):
-    result = []
-    tasks, collections = _sift_tasks(mapping)
-    # Tasks come first
-    result.extend(map(lambda x: indent(x, spaces=level * 4), tasks))
-    for collection in collections:
-        module = mapping[collection]
-        # Section/module "header"
-        result.append(indent(collection + ":", spaces=level * 4))
-        # Recurse
-        result.extend(_nested_list(module, level + 1))
-    return result
-
-COMMANDS_HEADER = "Available commands"
-NESTED_REMINDER = " (remember to call as module.[...].task)"
-
-
-def list_commands(docstring, format_):
-    """
-    Print all found commands/tasks, then exit. Invoked with ``-l/--list.``
-
-    If ``docstring`` is non-empty, it will be printed before the task list.
-
-    ``format_`` should conform to the options specified in
-    ``LIST_FORMAT_OPTIONS``, e.g. ``"short"``, ``"normal"``.
-    """
-    # Short-circuit with simple short output
-    if format_ == "short":
-        return _task_names(state.commands)
-    # Otherwise, handle more verbose modes
-    result = []
-    # Docstring at top, if applicable
-    if docstring:
-        trailer = "\n" if not docstring.endswith("\n") else ""
-        result.append(docstring + trailer)
-    header = COMMANDS_HEADER
-    if format_ == "nested":
-        header += NESTED_REMINDER
-    result.append(header + ":\n")
-    c = _normal_list() if format_ == "normal" else _nested_list(state.commands)
-    result.extend(c)
-    return result
-
-
-def display_command(name):
-    """
-    Print command function's docstring, then exit. Invoked with -d/--display.
-    """
-    # Sanity check
-    command = crawl(name, state.commands)
-    if command is None:
-        msg = "Task '%s' does not appear to exist. Valid task names:\n%s"
-        abort(msg % (name, "\n".join(_normal_list(False))))
-    # Print out nicely presented docstring if found
-    if hasattr(command, '__details__'):
-        task_details = command.__details__()
-    else:
-        task_details = get_task_details(command)
-    if task_details:
-        print("Displaying detailed information for task '%s':" % name)
-        print('')
-        print(indent(task_details, strip=True))
-        print('')
-    # Or print notice if not
-    else:
-        print("No detailed information available for task '%s':" % name)
-    sys.exit(0)
-
-
-def _escape_split(sep, argstr):
-    """
-    Allows for escaping of the separator: e.g. task:arg='foo\, bar'
-
-    It should be noted that the way bash et. al. do command line parsing, those
-    single quotes are required.
-    """
-    escaped_sep = r'\%s' % sep
-
-    if escaped_sep not in argstr:
-        return argstr.split(sep)
-
-    before, _, after = argstr.partition(escaped_sep)
-    startlist = before.split(sep)  # a regular split is fine here
-    unfinished = startlist[-1]
-    startlist = startlist[:-1]
-
-    # recurse because there may be more escaped separators
-    endlist = _escape_split(sep, after)
-
-    # finish building the escaped value. we use endlist[0] becaue the first
-    # part of the string sent in recursion is the rest of the escaped value.
-    unfinished += sep + endlist[0]
-
-    return startlist + [unfinished] + endlist[1:]  # put together all the parts
-
-
-def parse_arguments(arguments):
-    """
-    Parse string list into list of tuples: command, args, kwargs, hosts, roles.
-
-    See sites/docs/usage/fab.rst, section on "per-task arguments" for details.
-    """
-    cmds = []
-    for cmd in arguments:
-        args = []
-        kwargs = {}
-        hosts = []
-        roles = []
-        exclude_hosts = []
-        if ':' in cmd:
-            cmd, argstr = cmd.split(':', 1)
-            for pair in _escape_split(',', argstr):
-                result = _escape_split('=', pair)
-                if len(result) > 1:
-                    k, v = result
-                    # Catch, interpret host/hosts/role/roles/exclude_hosts
-                    # kwargs
-                    if k in ['host', 'hosts', 'role', 'roles', 'exclude_hosts']:
-                        if k == 'host':
-                            hosts = [v.strip()]
-                        elif k == 'hosts':
-                            hosts = [x.strip() for x in v.split(';')]
-                        elif k == 'role':
-                            roles = [v.strip()]
-                        elif k == 'roles':
-                            roles = [x.strip() for x in v.split(';')]
-                        elif k == 'exclude_hosts':
-                            exclude_hosts = [x.strip() for x in v.split(';')]
-                    # Otherwise, record as usual
-                    else:
-                        kwargs[k] = v
-                else:
-                    args.append(result[0])
-        cmds.append((cmd, args, kwargs, hosts, roles, exclude_hosts))
-    return cmds
-
-
-def parse_remainder(arguments):
-    """
-    Merge list of "remainder arguments" into a single command string.
-    """
-    return ' '.join(arguments)
-
-
-def update_output_levels(show, hide):
-    """
-    Update state.output values as per given comma-separated list of key names.
-
-    For example, ``update_output_levels(show='debug,warnings')`` is
-    functionally equivalent to ``state.output['debug'] = True ;
-    state.output['warnings'] = True``. Conversely, anything given to ``hide``
-    sets the values to ``False``.
-    """
-    if show:
-        for key in show.split(','):
-            state.output[key] = True
-    if hide:
-        for key in hide.split(','):
-            state.output[key] = False
-
-
-def show_commands(docstring, format, code=0):
-    print("\n".join(list_commands(docstring, format)))
-    sys.exit(code)
-
-
-def main(fabfile_locations=None):
-    """
-    Main command-line execution loop.
-    """
-    try:
-        # Parse command line options
-        parser, options, arguments = parse_options()
-
-        # Handle regular args vs -- args
-        arguments = parser.largs
-        remainder_arguments = parser.rargs
-
-        # Allow setting of arbitrary env keys.
-        # This comes *before* the "specific" env_options so that those may
-        # override these ones. Specific should override generic, if somebody
-        # was silly enough to specify the same key in both places.
-        # E.g. "fab --set shell=foo --shell=bar" should have env.shell set to
-        # 'bar', not 'foo'.
-        for pair in _escape_split(',', options.env_settings):
-            pair = _escape_split('=', pair)
-            # "--set x" => set env.x to True
-            # "--set x=" => set env.x to ""
-            key = pair[0]
-            value = True
-            if len(pair) == 2:
-                value = pair[1]
-            state.env[key] = value
-
-        # Update env with any overridden option values
-        # NOTE: This needs to remain the first thing that occurs
-        # post-parsing, since so many things hinge on the values in env.
-        for option in env_options:
-            state.env[option.dest] = getattr(options, option.dest)
-
-        # Handle --hosts, --roles, --exclude-hosts (comma separated string =>
-        # list)
-        for key in ['hosts', 'roles', 'exclude_hosts']:
-            if key in state.env and isinstance(state.env[key], basestring):
-                state.env[key] = state.env[key].split(',')
-
-        # Feed the env.tasks : tasks that are asked to be executed.
-        state.env['tasks'] = arguments
-
-        # Handle output control level show/hide
-        update_output_levels(show=options.show, hide=options.hide)
-
-        # Handle version number option
-        if options.show_version:
-            print("Fabric %s" % state.env.version)
-            print("Paramiko %s" % ssh.__version__)
-            sys.exit(0)
-
-        # Load settings from user settings file, into shared env dict.
-        state.env.update(load_settings(state.env.rcfile))
-
-        # Find local fabfile path or abort
-        fabfile = find_fabfile(fabfile_locations)
-        if not fabfile and not remainder_arguments:
-            abort("""Couldn't find any fabfiles!
-
-Remember that -f can be used to specify fabfile path, and use -h for help.""")
-
-        # Store absolute path to fabfile in case anyone needs it
-        state.env.real_fabfile = fabfile
-
-        # Load fabfile (which calls its module-level code, including
-        # tweaks to env values) and put its commands in the shared commands
-        # dict
-        default = None
-        if fabfile:
-            docstring, callables, default = load_fabfile(fabfile)
-            state.commands.update(callables)
-
-        # Handle case where we were called bare, i.e. just "fab", and print
-        # a help message.
-        actions = (options.list_commands, options.shortlist, options.display,
-            arguments, remainder_arguments, default)
-        if not any(actions):
-            parser.print_help()
-            sys.exit(1)
-
-        # Abort if no commands found
-        if not state.commands and not remainder_arguments:
-            abort("Fabfile didn't contain any commands!")
-
-        # Now that we're settled on a fabfile, inform user.
-        if state.output.debug:
-            if fabfile:
-                print("Using fabfile '%s'" % fabfile)
-            else:
-                print("No fabfile loaded -- remainder command only")
-
-        # Shortlist is now just an alias for the "short" list format;
-        # it overrides use of --list-format if somebody were to specify both
-        if options.shortlist:
-            options.list_format = 'short'
-            options.list_commands = True
-
-        # List available commands
-        if options.list_commands:
-            show_commands(docstring, options.list_format)
-
-        # Handle show (command-specific help) option
-        if options.display:
-            display_command(options.display)
-
-        # If user didn't specify any commands to run, show help
-        if not (arguments or remainder_arguments or default):
-            parser.print_help()
-            sys.exit(0)  # Or should it exit with error (1)?
-
-        # Parse arguments into commands to run (plus args/kwargs/hosts)
-        commands_to_run = parse_arguments(arguments)
-
-        # Parse remainders into a faux "command" to execute
-        remainder_command = parse_remainder(remainder_arguments)
-
-        # Figure out if any specified task names are invalid
-        unknown_commands = []
-        for tup in commands_to_run:
-            if crawl(tup[0], state.commands) is None:
-                unknown_commands.append(tup[0])
-
-        # Abort if any unknown commands were specified
-        if unknown_commands and not state.env.get('skip_unknown_tasks', False):
-            warn("Command(s) not found:\n%s" \
-                % indent(unknown_commands))
-            show_commands(None, options.list_format, 1)
-
-        # Generate remainder command and insert into commands, commands_to_run
-        if remainder_command:
-            r = '<remainder>'
-            state.commands[r] = lambda: api.run(remainder_command)
-            commands_to_run.append((r, [], {}, [], [], []))
-
-        # Ditto for a default, if found
-        if not commands_to_run and default:
-            commands_to_run.append((default.name, [], {}, [], [], []))
-
-        # Initial password prompt, if requested
-        if options.initial_password_prompt:
-            prompt = "Initial value for env.password: "
-            state.env.password = getpass.getpass(prompt)
-
-        # Ditto sudo_password
-        if options.initial_sudo_password_prompt:
-            prompt = "Initial value for env.sudo_password: "
-            state.env.sudo_password = getpass.getpass(prompt)
-
-        if state.output.debug:
-            names = ", ".join(x[0] for x in commands_to_run)
-            print("Commands to run: %s" % names)
-
-        # At this point all commands must exist, so execute them in order.
-        for name, args, kwargs, arg_hosts, arg_roles, arg_exclude_hosts in commands_to_run:
-            execute(
-                name,
-                hosts=arg_hosts,
-                roles=arg_roles,
-                exclude_hosts=arg_exclude_hosts,
-                *args, **kwargs
-            )
-        # If we got here, no errors occurred, so print a final note.
-        if state.output.status:
-            print("\nDone.")
-    except SystemExit:
-        # a number of internal functions might raise this one.
-        raise
-    except KeyboardInterrupt:
-        if state.output.status:
-            sys.stderr.write("\nStopped.\n")
-        sys.exit(1)
-    except:
-        sys.excepthook(*sys.exc_info())
-        # we might leave stale threads if we don't explicitly exit()
-        sys.exit(1)
-    finally:
-        disconnect_all()
-    sys.exit(0)
+program = make_program()
diff -Nru fabric-1.14.0/fabric/network.py fabric-2.5.0/fabric/network.py
--- fabric-1.14.0/fabric/network.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/fabric/network.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,709 +0,0 @@
-"""
-Classes and subroutines dealing with network connections and related topics.
-"""
-
-from __future__ import with_statement
-
-from functools import wraps
-import getpass
-import os
-import re
-import time
-import socket
-import sys
-from StringIO import StringIO
-
-
-from fabric.auth import get_password, set_password
-from fabric.utils import handle_prompt_abort, warn
-from fabric.exceptions import NetworkError
-
-try:
-    import warnings
-    warnings.simplefilter('ignore', DeprecationWarning)
-    import paramiko as ssh
-except ImportError, e:
-    import traceback
-    traceback.print_exc()
-    msg = """
-There was a problem importing our SSH library (see traceback above).
-Please make sure all dependencies are installed and importable.
-""".rstrip()
-    sys.stderr.write(msg + '\n')
-    sys.exit(1)
-
-
-ipv6_regex = re.compile(
-    '^\[?(?P<host>[0-9A-Fa-f:]+(?:%[a-z]+\d+)?)\]?(:(?P<port>\d+))?$')
-
-
-def direct_tcpip(client, host, port):
-    return client.get_transport().open_channel(
-        'direct-tcpip',
-        (host, int(port)),
-        ('', 0)
-    )
-
-
-def is_key_load_error(e):
-    return (
-        e.__class__ is ssh.SSHException
-        and 'Unable to parse key file' in str(e)
-    )
-
-
-def _tried_enough(tries):
-    from fabric.state import env
-    return tries >= env.connection_attempts
-
-
-def get_gateway(host, port, cache, replace=False):
-    """
-    Create and return a gateway socket, if one is needed.
-
-    This function checks ``env`` for gateway or proxy-command settings and
-    returns the necessary socket-like object for use by a final host
-    connection.
-
-    :param host:
-        Hostname of target server.
-
-    :param port:
-        Port to connect to on target server.
-
-    :param cache:
-        A ``HostConnectionCache`` object, in which gateway ``SSHClient``
-        objects are to be retrieved/cached.
-
-    :param replace:
-        Whether to forcibly replace a cached gateway client object.
-
-    :returns:
-        A ``socket.socket``-like object, or ``None`` if none was created.
-    """
-    from fabric.state import env, output
-    sock = None
-    proxy_command = ssh_config().get('proxycommand', None)
-    if env.gateway:
-        gateway = normalize_to_string(env.gateway)
-        # ensure initial gateway connection
-        if replace or gateway not in cache:
-            if output.debug:
-                print "Creating new gateway connection to %r" % gateway
-            cache[gateway] = connect(*normalize(gateway) + (cache, False))
-        # now we should have an open gw connection and can ask it for a
-        # direct-tcpip channel to the real target. (bypass cache's own
-        # __getitem__ override to avoid hilarity - this is usually called
-        # within that method.)
-        sock = direct_tcpip(dict.__getitem__(cache, gateway), host, port)
-    elif proxy_command:
-        sock = ssh.ProxyCommand(proxy_command)
-    return sock
-
-
-class HostConnectionCache(dict):
-    """
-    Dict subclass allowing for caching of host connections/clients.
-
-    This subclass will intelligently create new client connections when keys
-    are requested, or return previously created connections instead.
-
-    It also handles creating new socket-like objects when required to implement
-    gateway connections and `ProxyCommand`, and handing them to the inner
-    connection methods.
-
-    Key values are the same as host specifiers throughout Fabric: optional
-    username + ``@``, mandatory hostname, optional ``:`` + port number.
-    Examples:
-
-    * ``example.com`` - typical Internet host address.
-    * ``firewall`` - atypical, but still legal, local host address.
-    * ``user@example.com`` - with specific username attached.
-    * ``bob@smith.org:222`` - with specific nonstandard port attached.
-
-    When the username is not given, ``env.user`` is used. ``env.user``
-    defaults to the currently running user at startup but may be overwritten by
-    user code or by specifying a command-line flag.
-
-    Note that differing explicit usernames for the same hostname will result in
-    multiple client connections being made. For example, specifying
-    ``user1@example.com`` will create a connection to ``example.com``, logged
-    in as ``user1``; later specifying ``user2@example.com`` will create a new,
-    2nd connection as ``user2``.
-
-    The same applies to ports: specifying two different ports will result in
-    two different connections to the same host being made. If no port is given,
-    22 is assumed, so ``example.com`` is equivalent to ``example.com:22``.
-    """
-    def connect(self, key):
-        """
-        Force a new connection to ``key`` host string.
-        """
-        from fabric.state import env
-
-        user, host, port = normalize(key)
-        key = normalize_to_string(key)
-        seek_gateway = True
-        # break the loop when the host is gateway itself
-        if env.gateway:
-            seek_gateway = normalize_to_string(env.gateway) != key
-        self[key] = connect(
-            user, host, port, cache=self, seek_gateway=seek_gateway)
-
-    def __getitem__(self, key):
-        """
-        Autoconnect + return connection object
-        """
-        key = normalize_to_string(key)
-        if key not in self:
-            self.connect(key)
-        return dict.__getitem__(self, key)
-
-    #
-    # Dict overrides that normalize input keys
-    #
-
-    def __setitem__(self, key, value):
-        return dict.__setitem__(self, normalize_to_string(key), value)
-
-    def __delitem__(self, key):
-        return dict.__delitem__(self, normalize_to_string(key))
-
-    def __contains__(self, key):
-        return dict.__contains__(self, normalize_to_string(key))
-
-
-def ssh_config(host_string=None):
-    """
-    Return ssh configuration dict for current env.host_string host value.
-
-    Memoizes the loaded SSH config file, but not the specific per-host results.
-
-    This function performs the necessary "is SSH config enabled?" checks and
-    will simply return an empty dict if not. If SSH config *is* enabled and the
-    value of env.ssh_config_path is not a valid file, it will abort.
-
-    May give an explicit host string as ``host_string``.
-    """
-    from fabric.state import env
-    dummy = {}
-    if not env.use_ssh_config:
-        return dummy
-    if '_ssh_config' not in env:
-        try:
-            conf = ssh.SSHConfig()
-            path = os.path.expanduser(env.ssh_config_path)
-            with open(path) as fd:
-                conf.parse(fd)
-                env._ssh_config = conf
-        except IOError:
-            warn("Unable to load SSH config file '%s'" % path)
-            return dummy
-    host = parse_host_string(host_string or env.host_string)['host']
-    return env._ssh_config.lookup(host)
-
-
-def key_filenames():
-    """
-    Returns list of SSH key filenames for the current env.host_string.
-
-    Takes into account ssh_config and env.key_filename, including normalization
-    to a list. Also performs ``os.path.expanduser`` expansion on any key
-    filenames.
-    """
-    from fabric.state import env
-    keys = env.key_filename
-    # For ease of use, coerce stringish key filename into list
-    if isinstance(env.key_filename, basestring) or env.key_filename is None:
-        keys = [keys]
-    # Strip out any empty strings (such as the default value...meh)
-    keys = filter(bool, keys)
-    # Honor SSH config
-    conf = ssh_config()
-    if 'identityfile' in conf:
-        # Assume a list here as we require Paramiko 1.10+
-        keys.extend(conf['identityfile'])
-    return map(os.path.expanduser, keys)
-
-
-def key_from_env(passphrase=None):
-    """
-    Returns a paramiko-ready key from a text string of a private key
-    """
-    from fabric.state import env, output
-
-    if 'key' in env:
-        if output.debug:
-            # NOTE: this may not be the most secure thing; OTOH anybody running
-            # the process must by definition have access to the key value,
-            # so only serious problem is if they're logging the output.
-            sys.stderr.write("Trying to honor in-memory key %r\n" % env.key)
-        for pkey_class in (ssh.rsakey.RSAKey, ssh.dsskey.DSSKey):
-            if output.debug:
-                sys.stderr.write("Trying to load it as %s\n" % pkey_class)
-            try:
-                return pkey_class.from_private_key(StringIO(env.key), passphrase)
-            except Exception, e:
-                # File is valid key, but is encrypted: raise it, this will
-                # cause cxn loop to prompt for passphrase & retry
-                if 'Private key file is encrypted' in e:
-                    raise
-                # Otherwise, it probably means it wasn't a valid key of this
-                # type, so try the next one.
-                else:
-                    pass
-
-
-def parse_host_string(host_string):
-    # Split host_string to user (optional) and host/port
-    user_hostport = host_string.rsplit('@', 1)
-    hostport = user_hostport.pop()
-    user = user_hostport[0] if user_hostport and user_hostport[0] else None
-
-    # Split host/port string to host and optional port
-    # For IPv6 addresses square brackets are mandatory for host/port separation
-    if hostport.count(':') > 1:
-        # Looks like IPv6 address
-        r = ipv6_regex.match(hostport).groupdict()
-        host = r['host'] or None
-        port = r['port'] or None
-    else:
-        # Hostname or IPv4 address
-        host_port = hostport.rsplit(':', 1)
-        host = host_port.pop(0) or None
-        port = host_port[0] if host_port and host_port[0] else None
-
-    return {'user': user, 'host': host, 'port': port}
-
-
-def normalize(host_string, omit_port=False):
-    """
-    Normalizes a given host string, returning explicit host, user, port.
-
-    If ``omit_port`` is given and is True, only the host and user are returned.
-
-    This function will process SSH config files if Fabric is configured to do
-    so, and will use them to fill in some default values or swap in hostname
-    aliases.
-
-    Regarding SSH port used:
-
-    * Ports explicitly given within host strings always win, no matter what.
-    * When the host string lacks a port, SSH-config driven port configurations
-      are used next.
-    * When the SSH config doesn't specify a port (at all - including a default
-      ``Host *`` block), Fabric's internal setting ``env.port`` is consulted.
-    * If ``env.port`` is empty, ``env.default_port`` is checked (which should
-      always be, as one would expect, port ``22``).
-    """
-    from fabric.state import env
-    # Gracefully handle "empty" input by returning empty output
-    if not host_string:
-        return ('', '') if omit_port else ('', '', '')
-    # Parse host string (need this early on to look up host-specific ssh_config
-    # values)
-    r = parse_host_string(host_string)
-    host = r['host']
-
-    # Env values (using defaults if somehow earlier defaults were replaced with
-    # empty values)
-    user = env.user or env.local_user
-
-    # SSH config data
-    conf = ssh_config(host_string)
-    # Only use ssh_config values if the env value appears unmodified from
-    # the true defaults. If the user has tweaked them, that new value
-    # takes precedence.
-    if user == env.local_user and 'user' in conf:
-        user = conf['user']
-
-    # Also override host if needed
-    if 'hostname' in conf:
-        host = conf['hostname']
-    # Merge explicit user/port values with the env/ssh_config derived ones
-    # (Host is already done at this point.)
-    user = r['user'] or user
-
-    if omit_port:
-        return user, host
-
-    # determine port from ssh config if enabled
-    ssh_config_port = None
-    if env.use_ssh_config:
-        ssh_config_port = conf.get('port', None)
-
-    # port priority order (as in docstring)
-    port = r['port'] or ssh_config_port or env.port or env.default_port
-
-    return user, host, port
-
-
-def to_dict(host_string):
-    user, host, port = normalize(host_string)
-    return {
-        'user': user, 'host': host, 'port': port, 'host_string': host_string
-    }
-
-
-def from_dict(arg):
-    return join_host_strings(arg['user'], arg['host'], arg['port'])
-
-
-def denormalize(host_string):
-    """
-    Strips out default values for the given host string.
-
-    If the user part is the default user, it is removed;
-    if the port is port 22, it also is removed.
-    """
-    from fabric.state import env
-
-    r = parse_host_string(host_string)
-    user = ''
-    if r['user'] is not None and r['user'] != env.user:
-        user = r['user'] + '@'
-    port = ''
-    if r['port'] is not None and r['port'] != '22':
-        port = ':' + r['port']
-    host = r['host']
-    host = '[%s]' % host if port and host.count(':') > 1 else host
-    return user + host + port
-
-
-def join_host_strings(user, host, port=None):
-    """
-    Turns user/host/port strings into ``user@host:port`` combined string.
-
-    This function is not responsible for handling missing user/port strings;
-    for that, see the ``normalize`` function.
-
-    If ``host`` looks like IPv6 address, it will be enclosed in square brackets
-
-    If ``port`` is omitted, the returned string will be of the form
-    ``user@host``.
-    """
-    if port:
-        # Square brackets are necessary for IPv6 host/port separation
-        template = "%s@[%s]:%s" if host.count(':') > 1 else "%s@%s:%s"
-        return template % (user, host, port)
-    else:
-        return "%s@%s" % (user, host)
-
-
-def normalize_to_string(host_string):
-    """
-    normalize() returns a tuple; this returns another valid host string.
-    """
-    return join_host_strings(*normalize(host_string))
-
-
-def connect(user, host, port, cache, seek_gateway=True):
-    """
-    Create and return a new SSHClient instance connected to given host.
-
-    :param user: Username to connect as.
-
-    :param host: Network hostname.
-
-    :param port: SSH daemon port.
-
-    :param cache:
-        A ``HostConnectionCache`` instance used to cache/store gateway hosts
-        when gatewaying is enabled.
-
-    :param seek_gateway:
-        Whether to try setting up a gateway socket for this connection. Used so
-        the actual gateway connection can prevent recursion.
-    """
-    from fabric.state import env, output
-
-    #
-    # Initialization
-    #
-
-    # Init client
-    client = ssh.SSHClient()
-
-    # Load system hosts file (e.g. /etc/ssh/ssh_known_hosts)
-    known_hosts = env.get('system_known_hosts')
-    if known_hosts:
-        client.load_system_host_keys(known_hosts)
-
-    # Load known host keys (e.g. ~/.ssh/known_hosts) unless user says not to.
-    if not env.disable_known_hosts:
-        client.load_system_host_keys()
-    # Unless user specified not to, accept/add new, unknown host keys
-    if not env.reject_unknown_hosts:
-        client.set_missing_host_key_policy(ssh.AutoAddPolicy())
-
-    #
-    # Connection attempt loop
-    #
-
-    # Initialize loop variables
-    connected = False
-    password = get_password(user, host, port, login_only=True)
-    tries = 0
-    sock = None
-
-    # Loop until successful connect (keep prompting for new password)
-    while not connected:
-        # Attempt connection
-        try:
-            tries += 1
-
-            # (Re)connect gateway socket, if needed.
-            # Nuke cached client object if not on initial try.
-            if seek_gateway:
-                sock = get_gateway(host, port, cache, replace=tries > 0)
-
-            # Set up kwargs (this lets us skip GSS-API kwargs unless explicitly
-            # set; otherwise older Paramiko versions will be cranky.)
-            kwargs = dict(
-                hostname=host,
-                port=int(port),
-                username=user,
-                password=password,
-                pkey=key_from_env(password),
-                key_filename=key_filenames(),
-                timeout=env.timeout,
-                allow_agent=not env.no_agent,
-                look_for_keys=not env.no_keys,
-                sock=sock,
-            )
-            for suffix in ('auth', 'deleg_creds', 'kex'):
-                name = "gss_" + suffix
-                val = env.get(name, None)
-                if val is not None:
-                    kwargs[name] = val
-
-            # Ready to connect
-            client.connect(**kwargs)
-            connected = True
-
-            # set a keepalive if desired
-            if env.keepalive:
-                client.get_transport().set_keepalive(env.keepalive)
-
-            return client
-        # BadHostKeyException corresponds to key mismatch, i.e. what on the
-        # command line results in the big banner error about man-in-the-middle
-        # attacks.
-        except ssh.BadHostKeyException, e:
-            raise NetworkError("Host key for %s did not match pre-existing key! Server's key was changed recently, or possible man-in-the-middle attack." % host, e)
-        # Prompt for new password to try on auth failure
-        except (
-            ssh.AuthenticationException,
-            ssh.PasswordRequiredException,
-            ssh.SSHException
-        ), e:
-            msg = str(e)
-            # If we get SSHExceptionError and the exception message indicates
-            # SSH protocol banner read failures, assume it's caused by the
-            # server load and try again.
-            #
-            # If we are using a gateway, we will get a ChannelException if
-            # connection to the downstream host fails. We should retry.
-            if (e.__class__ is ssh.SSHException \
-                and msg == 'Error reading SSH protocol banner') \
-                or e.__class__ is ssh.ChannelException:
-                if _tried_enough(tries):
-                    raise NetworkError(msg, e)
-                continue
-
-            # For whatever reason, empty password + no ssh key or agent
-            # results in an SSHException instead of an
-            # AuthenticationException. Since it's difficult to do
-            # otherwise, we must assume empty password + SSHException ==
-            # auth exception.
-            #
-            # Conversely: if we get SSHException and there
-            # *was* a password -- it is probably something non auth
-            # related, and should be sent upwards. (This is not true if the
-            # exception message does indicate key parse problems.)
-            #
-            # This also holds true for rejected/unknown host keys: we have to
-            # guess based on other heuristics.
-            if (
-                e.__class__ is ssh.SSHException
-                and (
-                    password
-                    or msg.startswith('Unknown server')
-                    or "not found in known_hosts" in msg
-                )
-                and not is_key_load_error(e)
-            ):
-                raise NetworkError(msg, e)
-
-            # Otherwise, assume an auth exception, and prompt for new/better
-            # password.
-
-            # Paramiko doesn't handle prompting for locked private
-            # keys (i.e.  keys with a passphrase and not loaded into an agent)
-            # so we have to detect this and tweak our prompt slightly.
-            # (Otherwise, however, the logic flow is the same, because
-            # ssh's connect() method overrides the password argument to be
-            # either the login password OR the private key passphrase. Meh.)
-            #
-            # NOTE: This will come up if you normally use a
-            # passphrase-protected private key with ssh-agent, and enter an
-            # incorrect remote username, because ssh.connect:
-            # * Tries the agent first, which will fail as you gave the wrong
-            # username, so obviously any loaded keys aren't gonna work for a
-            # nonexistent remote account;
-            # * Then tries the on-disk key file, which is passphrased;
-            # * Realizes there's no password to try unlocking that key with,
-            # because you didn't enter a password, because you're using
-            # ssh-agent;
-            # * In this condition (trying a key file, password is None)
-            # ssh raises PasswordRequiredException.
-            text = None
-            if e.__class__ is ssh.PasswordRequiredException \
-                or is_key_load_error(e):
-                # NOTE: we can't easily say WHICH key's passphrase is needed,
-                # because ssh doesn't provide us with that info, and
-                # env.key_filename may be a list of keys, so we can't know
-                # which one raised the exception. Best not to try.
-                prompt = "[%s] Passphrase for private key"
-                text = prompt % env.host_string
-            password = prompt_for_password(text)
-            # Update env.password, env.passwords if empty
-            set_password(user, host, port, password)
-        # Ctrl-D / Ctrl-C for exit
-        # TODO: this may no longer actually serve its original purpose and may
-        # also hide TypeErrors from paramiko. Double check in v2.
-        except (EOFError, TypeError):
-            # Print a newline (in case user was sitting at prompt)
-            print('')
-            sys.exit(0)
-        # Handle DNS error / name lookup failure
-        except socket.gaierror, e:
-            raise NetworkError('Name lookup failed for %s' % host, e)
-        # Handle timeouts and retries, including generic errors
-        # NOTE: In 2.6, socket.error subclasses IOError
-        except socket.error, e:
-            not_timeout = type(e) is not socket.timeout
-            giving_up = _tried_enough(tries)
-            # Baseline error msg for when debug is off
-            msg = "Timed out trying to connect to %s" % host
-            # Expanded for debug on
-            err = msg + " (attempt %s of %s)" % (tries, env.connection_attempts)
-            if giving_up:
-                err += ", giving up"
-            err += ")"
-            # Debuggin'
-            if output.debug:
-                sys.stderr.write(err + '\n')
-            # Having said our piece, try again
-            if not giving_up:
-                # Sleep if it wasn't a timeout, so we still get timeout-like
-                # behavior
-                if not_timeout:
-                    time.sleep(env.timeout)
-                continue
-            # Override eror msg if we were retrying other errors
-            if not_timeout:
-                msg = "Low level socket error connecting to host %s on port %s: %s" % (
-                    host, port, e[1]
-                )
-            # Here, all attempts failed. Tweak error msg to show # tries.
-            # TODO: find good humanization module, jeez
-            s = "s" if env.connection_attempts > 1 else ""
-            msg += " (tried %s time%s)" % (env.connection_attempts, s)
-            raise NetworkError(msg, e)
-        # Ensure that if we terminated without connecting and we were given an
-        # explicit socket, close it out.
-        finally:
-            if not connected and sock is not None:
-                sock.close()
-
-
-def _password_prompt(prompt, stream):
-    # NOTE: Using encode-to-ascii to prevent (Windows, at least) getpass from
-    # choking if given Unicode.
-    return getpass.getpass(prompt.encode('ascii', 'ignore'), stream)
-
-def prompt_for_password(prompt=None, no_colon=False, stream=None):
-    """
-    Prompts for and returns a new password if required; otherwise, returns
-    None.
-
-    A trailing colon is appended unless ``no_colon`` is True.
-
-    If the user supplies an empty password, the user will be re-prompted until
-    they enter a non-empty password.
-
-    ``prompt_for_password`` autogenerates the user prompt based on the current
-    host being connected to. To override this, specify a string value for
-    ``prompt``.
-
-    ``stream`` is the stream the prompt will be printed to; if not given,
-    defaults to ``sys.stderr``.
-    """
-    from fabric.state import env
-    handle_prompt_abort("a connection or sudo password")
-    stream = stream or sys.stderr
-    # Construct prompt
-    default = "[%s] Login password for '%s'" % (env.host_string, env.user)
-    password_prompt = prompt if (prompt is not None) else default
-    if not no_colon:
-        password_prompt += ": "
-    # Get new password value
-    new_password = _password_prompt(password_prompt, stream)
-    # Otherwise, loop until user gives us a non-empty password (to prevent
-    # returning the empty string, and to avoid unnecessary network overhead.)
-    while not new_password:
-        print("Sorry, you can't enter an empty password. Please try again.")
-        new_password = _password_prompt(password_prompt, stream)
-    return new_password
-
-
-def needs_host(func):
-    """
-    Prompt user for value of ``env.host_string`` when ``env.host_string`` is
-    empty.
-
-    This decorator is basically a safety net for silly users who forgot to
-    specify the host/host list in one way or another. It should be used to wrap
-    operations which require a network connection.
-
-    Due to how we execute commands per-host in ``main()``, it's not possible to
-    specify multiple hosts at this point in time, so only a single host will be
-    prompted for.
-
-    Because this decorator sets ``env.host_string``, it will prompt once (and
-    only once) per command. As ``main()`` clears ``env.host_string`` between
-    commands, this decorator will also end up prompting the user once per
-    command (in the case where multiple commands have no hosts set, of course.)
-    """
-    from fabric.state import env
-    @wraps(func)
-    def host_prompting_wrapper(*args, **kwargs):
-        while not env.get('host_string', False):
-            handle_prompt_abort("the target host connection string")
-            host_string = raw_input("No hosts found. Please specify (single)"
-                                    " host string for connection: ")
-            env.update(to_dict(host_string))
-        return func(*args, **kwargs)
-    host_prompting_wrapper.undecorated = func
-    return host_prompting_wrapper
-
-
-def disconnect_all():
-    """
-    Disconnect from all currently connected servers.
-
-    Used at the end of ``fab``'s main loop, and also intended for use by
-    library users.
-    """
-    from fabric.state import connections, output
-    # Explicitly disconnect from all servers
-    for key in connections.keys():
-        if output.status:
-            # Here we can't use the py3k print(x, end=" ")
-            # because 2.5 backwards compatibility
-            sys.stdout.write("Disconnecting from %s... " % denormalize(key))
-        connections[key].close()
-        del connections[key]
-        if output.status:
-            sys.stdout.write("done.\n")
diff -Nru fabric-1.14.0/fabric/operations.py fabric-2.5.0/fabric/operations.py
--- fabric-1.14.0/fabric/operations.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/fabric/operations.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,1295 +0,0 @@
-"""
-
-Functions to be used in fabfiles and other non-core code, such as run()/sudo().
-"""
-
-from __future__ import with_statement
-
-import errno
-import os
-import os.path
-import posixpath
-import re
-import subprocess
-import sys
-import time
-from glob import glob
-from contextlib import closing, contextmanager
-
-from fabric.context_managers import (settings, char_buffered, hide,
-    quiet as quiet_manager, warn_only as warn_only_manager)
-from fabric.io import output_loop, input_loop
-from fabric.network import needs_host, ssh, ssh_config
-from fabric.sftp import SFTP
-from fabric.state import env, connections, output, win32, default_channel
-from fabric.thread_handling import ThreadHandler
-from fabric.utils import (
-    abort, error, handle_prompt_abort, indent, _pty_size, warn, apply_lcwd,
-    RingBuffer,
-)
-
-
-def _shell_escape(string):
-    """
-    Escape double quotes, backticks and dollar signs in given ``string``.
-
-    For example::
-
-        >>> _shell_escape('abc$')
-        'abc\\\\$'
-        >>> _shell_escape('"')
-        '\\\\"'
-    """
-    for char in ('"', '$', '`'):
-        string = string.replace(char, '\%s' % char)
-    return string
-
-
-class _AttributeString(str):
-    """
-    Simple string subclass to allow arbitrary attribute access.
-    """
-    @property
-    def stdout(self):
-        return str(self)
-
-
-class _AttributeList(list):
-    """
-    Like _AttributeString, but for lists.
-    """
-    pass
-
-
-# Can't wait till Python versions supporting 'def func(*args, foo=bar)' become
-# widespread :(
-def require(*keys, **kwargs):
-    """
-    Check for given keys in the shared environment dict and abort if not found.
-
-    Positional arguments should be strings signifying what env vars should be
-    checked for. If any of the given arguments do not exist, Fabric will abort
-    execution and print the names of the missing keys.
-
-    The optional keyword argument ``used_for`` may be a string, which will be
-    printed in the error output to inform users why this requirement is in
-    place. ``used_for`` is printed as part of a string similar to::
-
-        "Th(is|ese) variable(s) (are|is) used for %s"
-
-    so format it appropriately.
-
-    The optional keyword argument ``provided_by`` may be a list of functions or
-    function names or a single function or function name which the user should
-    be able to execute in order to set the key or keys; it will be included in
-    the error output if requirements are not met.
-
-    Note: it is assumed that the keyword arguments apply to all given keys as a
-    group. If you feel the need to specify more than one ``used_for``, for
-    example, you should break your logic into multiple calls to ``require()``.
-
-    .. versionchanged:: 1.1
-        Allow iterable ``provided_by`` values instead of just single values.
-    """
-    # If all keys exist and are non-empty, we're good, so keep going.
-    missing_keys = filter(lambda x: x not in env or (x in env and
-        isinstance(env[x], (dict, list, tuple, set)) and not env[x]), keys)
-    if not missing_keys:
-        return
-    # Pluralization
-    if len(missing_keys) > 1:
-        variable = "variables were"
-        used = "These variables are"
-    else:
-        variable = "variable was"
-        used = "This variable is"
-    # Regardless of kwargs, print what was missing. (Be graceful if used outside
-    # of a command.)
-    if 'command' in env:
-        prefix = "The command '%s' failed because the " % env.command
-    else:
-        prefix = "The "
-    msg = "%sfollowing required environment %s not defined:\n%s" % (
-        prefix, variable, indent(missing_keys)
-    )
-    # Print used_for if given
-    if 'used_for' in kwargs:
-        msg += "\n\n%s used for %s" % (used, kwargs['used_for'])
-    # And print provided_by if given
-    if 'provided_by' in kwargs:
-        funcs = kwargs['provided_by']
-        # non-iterable is given, treat it as a list of this single item
-        if not hasattr(funcs, '__iter__'):
-            funcs = [funcs]
-        if len(funcs) > 1:
-            command = "one of the following commands"
-        else:
-            command = "the following command"
-        to_s = lambda obj: getattr(obj, '__name__', str(obj))
-        provided_by = [to_s(obj) for obj in funcs]
-        msg += "\n\nTry running %s prior to this one, to fix the problem:\n%s"\
-            % (command, indent(provided_by))
-    abort(msg)
-
-
-def prompt(text, key=None, default='', validate=None):
-    """
-    Prompt user with ``text`` and return the input (like ``raw_input``).
-
-    A single space character will be appended for convenience, but nothing
-    else. Thus, you may want to end your prompt text with a question mark or a
-    colon, e.g. ``prompt("What hostname?")``.
-
-    If ``key`` is given, the user's input will be stored as ``env.<key>`` in
-    addition to being returned by `prompt`. If the key already existed in
-    ``env``, its value will be overwritten and a warning printed to the user.
-
-    If ``default`` is given, it is displayed in square brackets and used if the
-    user enters nothing (i.e. presses Enter without entering any text).
-    ``default`` defaults to the empty string. If non-empty, a space will be
-    appended, so that a call such as ``prompt("What hostname?",
-    default="foo")`` would result in a prompt of ``What hostname? [foo]`` (with
-    a trailing space after the ``[foo]``.)
-
-    The optional keyword argument ``validate`` may be a callable or a string:
-
-    * If a callable, it is called with the user's input, and should return the
-      value to be stored on success. On failure, it should raise an exception
-      with an exception message, which will be printed to the user.
-    * If a string, the value passed to ``validate`` is used as a regular
-      expression. It is thus recommended to use raw strings in this case. Note
-      that the regular expression, if it is not fully matching (bounded by
-      ``^`` and ``$``) it will be made so. In other words, the input must fully
-      match the regex.
-
-    Either way, `prompt` will re-prompt until validation passes (or the user
-    hits ``Ctrl-C``).
-
-    .. note::
-        `~fabric.operations.prompt` honors :ref:`env.abort_on_prompts
-        <abort-on-prompts>` and will call `~fabric.utils.abort` instead of
-        prompting if that flag is set to ``True``. If you want to block on user
-        input regardless, try wrapping with
-        `~fabric.context_managers.settings`.
-
-    Examples::
-
-        # Simplest form:
-        environment = prompt('Please specify target environment: ')
-
-        # With default, and storing as env.dish:
-        prompt('Specify favorite dish: ', 'dish', default='spam & eggs')
-
-        # With validation, i.e. requiring integer input:
-        prompt('Please specify process nice level: ', key='nice', validate=int)
-
-        # With validation against a regular expression:
-        release = prompt('Please supply a release name',
-                validate=r'^\w+-\d+(\.\d+)?$')
-
-        # Prompt regardless of the global abort-on-prompts setting:
-        with settings(abort_on_prompts=False):
-            prompt('I seriously need an answer on this! ')
-
-    """
-    handle_prompt_abort("a user-specified prompt() call")
-    # Store previous env value for later display, if necessary
-    if key:
-        previous_value = env.get(key)
-    # Set up default display
-    default_str = ""
-    if default != '':
-        default_str = " [%s] " % str(default).strip()
-    else:
-        default_str = " "
-    # Construct full prompt string
-    prompt_str = text.strip() + default_str
-    # Loop until we pass validation
-    value = None
-    while value is None:
-        # Get input
-        value = raw_input(prompt_str) or default
-        # Handle validation
-        if validate:
-            # Callable
-            if callable(validate):
-                # Callable validate() must raise an exception if validation
-                # fails.
-                try:
-                    value = validate(value)
-                except Exception, e:
-                    # Reset value so we stay in the loop
-                    value = None
-                    print("Validation failed for the following reason:")
-                    print(indent(e.message) + "\n")
-            # String / regex must match and will be empty if validation fails.
-            else:
-                # Need to transform regex into full-matching one if it's not.
-                if not validate.startswith('^'):
-                    validate = r'^' + validate
-                if not validate.endswith('$'):
-                    validate += r'$'
-                result = re.findall(validate, value)
-                if not result:
-                    print("Regular expression validation failed: '%s' does not match '%s'\n" % (value, validate))
-                    # Reset value so we stay in the loop
-                    value = None
-    # At this point, value must be valid, so update env if necessary
-    if key:
-        env[key] = value
-    # Print warning if we overwrote some other value
-    if key and previous_value is not None and previous_value != value:
-        warn("overwrote previous env variable '%s'; used to be '%s', is now '%s'." % (
-            key, previous_value, value
-        ))
-    # And return the value, too, just in case someone finds that useful.
-    return value
-
-
-@needs_host
-def put(local_path=None, remote_path=None, use_sudo=False,
-    mirror_local_mode=False, mode=None, use_glob=True, temp_dir=""):
-    """
-    Upload one or more files to a remote host.
-
-    As with the OpenSSH ``sftp`` program, `.put` will overwrite pre-existing
-    remote files without requesting confirmation.
-
-    `~fabric.operations.put` returns an iterable containing the absolute file
-    paths of all remote files uploaded. This iterable also exhibits a
-    ``.failed`` attribute containing any local file paths which failed to
-    upload (and may thus be used as a boolean test.) You may also check
-    ``.succeeded`` which is equivalent to ``not .failed``.
-
-    ``local_path`` may be a relative or absolute local file or directory path,
-    and may contain shell-style wildcards, as understood by the Python ``glob``
-    module (give ``use_glob=False`` to disable this behavior).  Tilde expansion
-    (as implemented by ``os.path.expanduser``) is also performed.
-
-    ``local_path`` may alternately be a file-like object, such as the result of
-    ``open('path')`` or a ``StringIO`` instance.
-
-    .. note::
-        In this case, `~fabric.operations.put` will attempt to read the entire
-        contents of the file-like object by rewinding it using ``seek`` (and
-        will use ``tell`` afterwards to preserve the previous file position).
-
-    ``remote_path`` may also be a relative or absolute location, but applied to
-    the remote host. Relative paths are relative to the remote user's home
-    directory, but tilde expansion (e.g. ``~/.ssh/``) will also be performed if
-    necessary.
-
-    An empty string, in either path argument, will be replaced by the
-    appropriate end's current working directory.
-
-    While the SFTP protocol (which `put` uses) has no direct ability to upload
-    files to locations not owned by the connecting user, you may specify
-    ``use_sudo=True`` to work around this. When set, this setting causes `put`
-    to upload the local files to a temporary location on the remote end
-    (defaults to remote user's ``$HOME``; this may be overridden via
-    ``temp_dir``), and then use `sudo` to move them to ``remote_path``.
-
-    In some use cases, it is desirable to force a newly uploaded file to match
-    the mode of its local counterpart (such as when uploading executable
-    scripts). To do this, specify ``mirror_local_mode=True``.
-
-    Alternately, you may use the ``mode`` kwarg to specify an exact mode, in
-    the same vein as ``os.chmod``, such as an exact octal number (``0755``) or
-    a string representing one (``"0755"``).
-
-    `~fabric.operations.put` will honor `~fabric.context_managers.cd`, so
-    relative values in ``remote_path`` will be prepended by the current remote
-    working directory, if applicable. Thus, for example, the below snippet
-    would attempt to upload to ``/tmp/files/test.txt`` instead of
-    ``~/files/test.txt``::
-
-        with cd('/tmp'):
-            put('/path/to/local/test.txt', 'files')
-
-    Use of `~fabric.context_managers.lcd` will affect ``local_path`` in the
-    same manner.
-
-    Examples::
-
-        put('bin/project.zip', '/tmp/project.zip')
-        put('*.py', 'cgi-bin/')
-        put('index.html', 'index.html', mode=0755)
-
-    .. note::
-        If a file-like object such as StringIO has a ``name`` attribute, that
-        will be used in Fabric's printed output instead of the default
-        ``<file obj>``
-    .. versionchanged:: 1.0
-        Now honors the remote working directory as manipulated by
-        `~fabric.context_managers.cd`, and the local working directory as
-        manipulated by `~fabric.context_managers.lcd`.
-    .. versionchanged:: 1.0
-        Now allows file-like objects in the ``local_path`` argument.
-    .. versionchanged:: 1.0
-        Directories may be specified in the ``local_path`` argument and will
-        trigger recursive uploads.
-    .. versionchanged:: 1.0
-        Return value is now an iterable of uploaded remote file paths which
-        also exhibits the ``.failed`` and ``.succeeded`` attributes.
-    .. versionchanged:: 1.5
-        Allow a ``name`` attribute on file-like objects for log output
-    .. versionchanged:: 1.7
-        Added ``use_glob`` option to allow disabling of globbing.
-    """
-    # Handle empty local path
-    local_path = local_path or os.getcwd()
-
-    # Test whether local_path is a path or a file-like object
-    local_is_path = not (hasattr(local_path, 'read') \
-        and callable(local_path.read))
-
-    ftp = SFTP(env.host_string)
-
-    with closing(ftp) as ftp:
-        home = ftp.normalize('.')
-
-        # Empty remote path implies cwd
-        remote_path = remote_path or home
-
-        # Expand tildes
-        if remote_path.startswith('~'):
-            remote_path = remote_path.replace('~', home, 1)
-
-        # Honor cd() (assumes Unix style file paths on remote end)
-        if not os.path.isabs(remote_path) and env.get('cwd'):
-            remote_path = env.cwd.rstrip('/') + '/' + remote_path
-
-        if local_is_path:
-            # Apply lcwd, expand tildes, etc
-            local_path = os.path.expanduser(local_path)
-            local_path = apply_lcwd(local_path, env)
-            if use_glob:
-                # Glob local path
-                names = glob(local_path)
-            else:
-                # Check if file exists first so ValueError gets raised
-                if os.path.exists(local_path):
-                    names = [local_path]
-                else:
-                    names = []
-        else:
-            names = [local_path]
-
-        # Make sure local arg exists
-        if local_is_path and not names:
-            err = "'%s' is not a valid local path or glob." % local_path
-            raise ValueError(err)
-
-        # Sanity check and wierd cases
-        if ftp.exists(remote_path):
-            if local_is_path and len(names) != 1 and not ftp.isdir(remote_path):
-                raise ValueError("'%s' is not a directory" % remote_path)
-
-        # Iterate over all given local files
-        remote_paths = []
-        failed_local_paths = []
-        for lpath in names:
-            try:
-                if local_is_path and os.path.isdir(lpath):
-                    p = ftp.put_dir(lpath, remote_path, use_sudo,
-                        mirror_local_mode, mode, temp_dir)
-                    remote_paths.extend(p)
-                else:
-                    p = ftp.put(lpath, remote_path, use_sudo, mirror_local_mode,
-                        mode, local_is_path, temp_dir)
-                    remote_paths.append(p)
-            except Exception, e:
-                msg = "put() encountered an exception while uploading '%s'"
-                failure = lpath if local_is_path else "<StringIO>"
-                failed_local_paths.append(failure)
-                error(message=msg % lpath, exception=e)
-
-        ret = _AttributeList(remote_paths)
-        ret.failed = failed_local_paths
-        ret.succeeded = not ret.failed
-        return ret
-
-
-@needs_host
-def get(remote_path, local_path=None, use_sudo=False, temp_dir=""):
-    """
-    Download one or more files from a remote host.
-
-    `~fabric.operations.get` returns an iterable containing the absolute paths
-    to all local files downloaded, which will be empty if ``local_path`` was a
-    StringIO object (see below for more on using StringIO). This object will
-    also exhibit a ``.failed`` attribute containing any remote file paths which
-    failed to download, and a ``.succeeded`` attribute equivalent to ``not
-    .failed``.
-
-    ``remote_path`` is the remote file or directory path to download, which may
-    contain shell glob syntax, e.g. ``"/var/log/apache2/*.log"``, and will have
-    tildes replaced by the remote home directory. Relative paths will be
-    considered relative to the remote user's home directory, or the current
-    remote working directory as manipulated by `~fabric.context_managers.cd`.
-    If the remote path points to a directory, that directory will be downloaded
-    recursively.
-
-    ``local_path`` is the local file path where the downloaded file or files
-    will be stored. If relative, it will honor the local current working
-    directory as manipulated by `~fabric.context_managers.lcd`. It may be
-    interpolated, using standard Python dict-based interpolation, with the
-    following variables:
-
-    * ``host``: The value of ``env.host_string``, eg ``myhostname`` or
-      ``user@myhostname-222`` (the colon between hostname and port is turned
-      into a dash to maximize filesystem compatibility)
-    * ``dirname``: The directory part of the remote file path, e.g. the
-      ``src/projectname`` in ``src/projectname/utils.py``.
-    * ``basename``: The filename part of the remote file path, e.g. the
-      ``utils.py`` in ``src/projectname/utils.py``
-    * ``path``: The full remote path, e.g. ``src/projectname/utils.py``.
-
-    While the SFTP protocol (which `get` uses) has no direct ability to download
-    files from locations not owned by the connecting user, you may specify
-    ``use_sudo=True`` to work around this. When set, this setting allows `get`
-    to copy (using sudo) the remote files to a temporary location on the remote end
-    (defaults to remote user's ``$HOME``; this may be overridden via ``temp_dir``),
-    and then download them to ``local_path``.
-
-    .. note::
-        When ``remote_path`` is an absolute directory path, only the inner
-        directories will be recreated locally and passed into the above
-        variables. So for example, ``get('/var/log', '%(path)s')`` would start
-        writing out files like ``apache2/access.log``,
-        ``postgresql/8.4/postgresql.log``, etc, in the local working directory.
-        It would **not** write out e.g.  ``var/log/apache2/access.log``.
-
-        Additionally, when downloading a single file, ``%(dirname)s`` and
-        ``%(path)s`` do not make as much sense and will be empty and equivalent
-        to ``%(basename)s``, respectively. Thus a call like
-        ``get('/var/log/apache2/access.log', '%(path)s')`` will save a local
-        file named ``access.log``, not ``var/log/apache2/access.log``.
-
-        This behavior is intended to be consistent with the command-line
-        ``scp`` program.
-
-    If left blank, ``local_path`` defaults to ``"%(host)s/%(path)s"`` in order
-    to be safe for multi-host invocations.
-
-    .. warning::
-        If your ``local_path`` argument does not contain ``%(host)s`` and your
-        `~fabric.operations.get` call runs against multiple hosts, your local
-        files will be overwritten on each successive run!
-
-    If ``local_path`` does not make use of the above variables (i.e. if it is a
-    simple, explicit file path) it will act similar to ``scp`` or ``cp``,
-    overwriting pre-existing files if necessary, downloading into a directory
-    if given (e.g. ``get('/path/to/remote_file.txt', 'local_directory')`` will
-    create ``local_directory/remote_file.txt``) and so forth.
-
-    ``local_path`` may alternately be a file-like object, such as the result of
-    ``open('path', 'w')`` or a ``StringIO`` instance.
-
-    .. note::
-        Attempting to `get` a directory into a file-like object is not valid
-        and will result in an error.
-
-    .. note::
-        This function will use ``seek`` and ``tell`` to overwrite the entire
-        contents of the file-like object, in order to be consistent with the
-        behavior of `~fabric.operations.put` (which also considers the entire
-        file). However, unlike `~fabric.operations.put`, the file pointer will
-        not be restored to its previous location, as that doesn't make as much
-        sense here and/or may not even be possible.
-
-    .. note::
-        If a file-like object such as StringIO has a ``name`` attribute, that
-        will be used in Fabric's printed output instead of the default
-        ``<file obj>``
-
-    .. versionchanged:: 1.0
-        Now honors the remote working directory as manipulated by
-        `~fabric.context_managers.cd`, and the local working directory as
-        manipulated by `~fabric.context_managers.lcd`.
-    .. versionchanged:: 1.0
-        Now allows file-like objects in the ``local_path`` argument.
-    .. versionchanged:: 1.0
-        ``local_path`` may now contain interpolated path- and host-related
-        variables.
-    .. versionchanged:: 1.0
-        Directories may be specified in the ``remote_path`` argument and will
-        trigger recursive downloads.
-    .. versionchanged:: 1.0
-        Return value is now an iterable of downloaded local file paths, which
-        also exhibits the ``.failed`` and ``.succeeded`` attributes.
-    .. versionchanged:: 1.5
-        Allow a ``name`` attribute on file-like objects for log output
-    """
-    # Handle empty local path / default kwarg value
-    local_path = local_path or "%(host)s/%(path)s"
-
-    # Test whether local_path is a path or a file-like object
-    local_is_path = not (hasattr(local_path, 'write') \
-        and callable(local_path.write))
-
-    # Honor lcd() where it makes sense
-    if local_is_path:
-        local_path = apply_lcwd(local_path, env)
-
-    ftp = SFTP(env.host_string)
-
-    with closing(ftp) as ftp:
-        home = ftp.normalize('.')
-        # Expand home directory markers (tildes, etc)
-        if remote_path.startswith('~'):
-            remote_path = remote_path.replace('~', home, 1)
-        if local_is_path:
-            local_path = os.path.expanduser(local_path)
-
-        # Honor cd() (assumes Unix style file paths on remote end)
-        if not os.path.isabs(remote_path):
-            # Honor cwd if it's set (usually by with cd():)
-            if env.get('cwd'):
-                remote_path_escaped = env.cwd.rstrip('/')
-                remote_path_escaped = remote_path_escaped.replace('\\ ', ' ')
-                remote_path = remote_path_escaped + '/' + remote_path
-            # Otherwise, be relative to remote home directory (SFTP server's
-            # '.')
-            else:
-                remote_path = posixpath.join(home, remote_path)
-
-        # Track final local destination files so we can return a list
-        local_files = []
-        failed_remote_files = []
-
-        try:
-            # Glob remote path if necessary
-            if '*' in remote_path or '?' in remote_path:
-                names = ftp.glob(remote_path)
-                # Handle "file not found" errors (like Paramiko does if we
-                # explicitly try to grab a glob-like filename).
-                if not names:
-                    raise IOError(errno.ENOENT, "No such file")
-            else:
-                names = [remote_path]
-
-            # Handle invalid local-file-object situations
-            if not local_is_path:
-                if len(names) > 1 or ftp.isdir(names[0]):
-                    error("[%s] %s is a glob or directory, but local_path is a file object!" % (env.host_string, remote_path))
-
-            for remote_path in names:
-                if ftp.isdir(remote_path):
-                    result = ftp.get_dir(remote_path, local_path, use_sudo, temp_dir)
-                    local_files.extend(result)
-                else:
-                    # Perform actual get. If getting to real local file path,
-                    # add result (will be true final path value) to
-                    # local_files. File-like objects are omitted.
-                    result = ftp.get(remote_path, local_path, use_sudo, local_is_path, os.path.basename(remote_path), temp_dir)
-                    if local_is_path:
-                        local_files.append(result)
-
-        except Exception, e:
-            failed_remote_files.append(remote_path)
-            msg = "get() encountered an exception while downloading '%s'"
-            error(message=msg % remote_path, exception=e)
-
-        ret = _AttributeList(local_files if local_is_path else [])
-        ret.failed = failed_remote_files
-        ret.succeeded = not ret.failed
-        return ret
-
-
-def _sudo_prefix_argument(argument, value):
-    if value is None:
-        return ""
-    if str(value).isdigit():
-        value = "#%s" % value
-    return ' %s "%s"' % (argument, value)
-
-
-def _sudo_prefix(user, group=None):
-    """
-    Return ``env.sudo_prefix`` with ``user``/``group`` inserted if necessary.
-    """
-    # Insert env.sudo_prompt into env.sudo_prefix
-    prefix = env.sudo_prefix % env
-    if user is not None or group is not None:
-        return "%s%s%s " % (prefix,
-                            _sudo_prefix_argument('-u', user),
-                            _sudo_prefix_argument('-g', group))
-    return prefix
-
-
-def _shell_wrap(command, shell_escape, shell=True, sudo_prefix=None):
-    """
-    Conditionally wrap given command in env.shell (while honoring sudo.)
-    """
-    # Honor env.shell, while allowing the 'shell' kwarg to override it (at
-    # least in terms of turning it off.)
-    if shell and not env.use_shell:
-        shell = False
-    # Sudo plus space, or empty string
-    if sudo_prefix is None:
-        sudo_prefix = ""
-    else:
-        sudo_prefix += " "
-    # If we're shell wrapping, prefix shell and space. Next, escape the command
-    # if requested, and then quote it. Otherwise, empty string.
-    if shell:
-        shell = env.shell + " "
-        if shell_escape:
-            command = _shell_escape(command)
-        command = '"%s"' % command
-    else:
-        shell = ""
-    # Resulting string should now have correct formatting
-    return sudo_prefix + shell + command
-
-
-def _prefix_commands(command, which):
-    """
-    Prefixes ``command`` with all prefixes found in ``env.command_prefixes``.
-
-    ``env.command_prefixes`` is a list of strings which is modified by the
-    `~fabric.context_managers.prefix` context manager.
-
-    This function also handles a special-case prefix, ``cwd``, used by
-    `~fabric.context_managers.cd`. The ``which`` kwarg should be a string,
-    ``"local"`` or ``"remote"``, which will determine whether ``cwd`` or
-    ``lcwd`` is used.
-    """
-    # Local prefix list (to hold env.command_prefixes + any special cases)
-    prefixes = list(env.command_prefixes)
-    # Handle current working directory, which gets its own special case due to
-    # being a path string that gets grown/shrunk, instead of just a single
-    # string or lack thereof.
-    # Also place it at the front of the list, in case user is expecting another
-    # prefixed command to be "in" the current working directory.
-    cwd = env.cwd if which == 'remote' else env.lcwd
-    redirect = " >/dev/null" if not win32 else ''
-    if cwd:
-        prefixes.insert(0, 'cd %s%s' % (cwd, redirect))
-    glue = " && "
-    prefix = (glue.join(prefixes) + glue) if prefixes else ""
-    return prefix + command
-
-
-def _prefix_env_vars(command, local=False):
-    """
-    Prefixes ``command`` with any shell environment vars, e.g. ``PATH=foo ``.
-
-    Currently, this only applies the PATH updating implemented in
-    `~fabric.context_managers.path` and environment variables from
-    `~fabric.context_managers.shell_env`.
-
-    Will switch to using Windows style 'SET' commands when invoked by
-    ``local()`` and on a Windows localhost.
-    """
-    env_vars = {}
-
-    # path(): local shell env var update, appending/prepending/replacing $PATH
-    path = env.path
-    if path:
-        if env.path_behavior == 'append':
-            path = '$PATH:\"%s\"' % path
-        elif env.path_behavior == 'prepend':
-            path = '\"%s\":$PATH' % path
-        elif env.path_behavior == 'replace':
-            path = '\"%s\"' % path
-
-        env_vars['PATH'] = path
-
-    # shell_env()
-    env_vars.update(env.shell_env)
-
-    if env_vars:
-        set_cmd, exp_cmd = '', ''
-        if win32 and local:
-            set_cmd = 'SET '
-        else:
-            exp_cmd = 'export '
-
-        exports = ' '.join(
-            '%s%s="%s"' % (set_cmd, k, v if k == 'PATH' else _shell_escape(v))
-            for k, v in env_vars.iteritems()
-        )
-        shell_env_str = '%s%s && ' % (exp_cmd, exports)
-    else:
-        shell_env_str = ''
-
-    return shell_env_str + command
-
-
-def _execute(channel, command, pty=True, combine_stderr=None,
-    invoke_shell=False, stdout=None, stderr=None, timeout=None,
-    capture_buffer_size=None):
-    """
-    Execute ``command`` over ``channel``.
-
-    ``pty`` controls whether a pseudo-terminal is created.
-
-    ``combine_stderr`` controls whether we call ``channel.set_combine_stderr``.
-    By default, the global setting for this behavior (:ref:`env.combine_stderr
-    <combine-stderr>`) is consulted, but you may specify ``True`` or ``False``
-    here to override it.
-
-    ``invoke_shell`` controls whether we use ``exec_command`` or
-    ``invoke_shell`` (plus a handful of other things, such as always forcing a
-    pty.)
-
-    ``capture_buffer_size`` controls the length of the ring-buffers used to
-    capture stdout/stderr. (This is ignored if ``invoke_shell=True``, since
-    that completely disables capturing overall.)
-
-    Returns a three-tuple of (``stdout``, ``stderr``, ``status``), where
-    ``stdout``/``stderr`` are captured output strings and ``status`` is the
-    program's return code, if applicable.
-    """
-    # stdout/stderr redirection
-    stdout = stdout or sys.stdout
-    stderr = stderr or sys.stderr
-
-    # Timeout setting control
-    timeout = env.command_timeout if (timeout is None) else timeout
-
-    # What to do with CTRl-C?
-    remote_interrupt = env.remote_interrupt
-
-    with char_buffered(sys.stdin):
-        # Combine stdout and stderr to get around oddball mixing issues
-        if combine_stderr is None:
-            combine_stderr = env.combine_stderr
-        channel.set_combine_stderr(combine_stderr)
-
-        # Assume pty use, and allow overriding of this either via kwarg or env
-        # var.  (invoke_shell always wants a pty no matter what.)
-        using_pty = True
-        if not invoke_shell and (not pty or not env.always_use_pty):
-            using_pty = False
-        # Request pty with size params (default to 80x24, obtain real
-        # parameters if on POSIX platform)
-        if using_pty:
-            rows, cols = _pty_size()
-            channel.get_pty(width=cols, height=rows)
-
-        # Use SSH agent forwarding from 'ssh' if enabled by user
-        config_agent = ssh_config().get('forwardagent', 'no').lower() == 'yes'
-        forward = None
-        if env.forward_agent or config_agent:
-            forward = ssh.agent.AgentRequestHandler(channel)
-
-        # Kick off remote command
-        if invoke_shell:
-            channel.invoke_shell()
-            if command:
-                channel.sendall(command + "\n")
-        else:
-            channel.exec_command(command=command)
-
-        # Init stdout, stderr capturing. Must use lists instead of strings as
-        # strings are immutable and we're using these as pass-by-reference
-        stdout_buf = RingBuffer(value=[], maxlen=capture_buffer_size)
-        stderr_buf = RingBuffer(value=[], maxlen=capture_buffer_size)
-        if invoke_shell:
-            stdout_buf = stderr_buf = None
-
-        workers = (
-            ThreadHandler('out', output_loop, channel, "recv",
-                capture=stdout_buf, stream=stdout, timeout=timeout),
-            ThreadHandler('err', output_loop, channel, "recv_stderr",
-                capture=stderr_buf, stream=stderr, timeout=timeout),
-            ThreadHandler('in', input_loop, channel, using_pty)
-        )
-
-        if remote_interrupt is None:
-            remote_interrupt = invoke_shell
-        if remote_interrupt and not using_pty:
-            remote_interrupt = False
-
-        while True:
-            if channel.exit_status_ready():
-                break
-            else:
-                # Check for thread exceptions here so we can raise ASAP
-                # (without chance of getting blocked by, or hidden by an
-                # exception within, recv_exit_status())
-                for worker in workers:
-                    worker.raise_if_needed()
-            try:
-                time.sleep(ssh.io_sleep)
-            except KeyboardInterrupt:
-                if not remote_interrupt:
-                    raise
-                channel.send('\x03')
-
-        # Obtain exit code of remote program now that we're done.
-        status = channel.recv_exit_status()
-
-        # Wait for threads to exit so we aren't left with stale threads
-        for worker in workers:
-            worker.thread.join()
-            worker.raise_if_needed()
-
-        # Close channel
-        channel.close()
-        # Close any agent forward proxies
-        if forward is not None:
-            forward.close()
-
-        # Update stdout/stderr with captured values if applicable
-        if not invoke_shell:
-            stdout_buf = ''.join(stdout_buf).strip()
-            stderr_buf = ''.join(stderr_buf).strip()
-
-        # Tie off "loose" output by printing a newline. Helps to ensure any
-        # following print()s aren't on the same line as a trailing line prefix
-        # or similar. However, don't add an extra newline if we've already
-        # ended up with one, as that adds a entire blank line instead.
-        if output.running \
-            and (output.stdout and stdout_buf and not stdout_buf.endswith("\n")) \
-            or (output.stderr and stderr_buf and not stderr_buf.endswith("\n")):
-            print("")
-
-        return stdout_buf, stderr_buf, status
-
-
-@needs_host
-def open_shell(command=None):
-    """
-    Invoke a fully interactive shell on the remote end.
-
-    If ``command`` is given, it will be sent down the pipe before handing
-    control over to the invoking user.
-
-    This function is most useful for when you need to interact with a heavily
-    shell-based command or series of commands, such as when debugging or when
-    fully interactive recovery is required upon remote program failure.
-
-    It should be considered an easy way to work an interactive shell session
-    into the middle of a Fabric script and is *not* a drop-in replacement for
-    `~fabric.operations.run`, which is also capable of interacting with the
-    remote end (albeit only while its given command is executing) and has much
-    stronger programmatic abilities such as error handling and stdout/stderr
-    capture.
-
-    Specifically, `~fabric.operations.open_shell` provides a better interactive
-    experience than `~fabric.operations.run`, but use of a full remote shell
-    prevents Fabric from determining whether programs run within the shell have
-    failed, and pollutes the stdout/stderr stream with shell output such as
-    login banners, prompts and echoed stdin.
-
-    Thus, this function does not have a return value and will not trigger
-    Fabric's failure handling if any remote programs result in errors.
-
-    .. versionadded:: 1.0
-    """
-    _execute(channel=default_channel(), command=command, pty=True,
-        combine_stderr=True, invoke_shell=True)
-
-
-@contextmanager
-def _noop():
-    yield
-
-
-def _run_command(command, shell=True, pty=True, combine_stderr=True,
-    sudo=False, user=None, quiet=False, warn_only=False, stdout=None,
-    stderr=None, group=None, timeout=None, shell_escape=None,
-    capture_buffer_size=None):
-    """
-    Underpinnings of `run` and `sudo`. See their docstrings for more info.
-    """
-    manager = _noop
-    if warn_only:
-        manager = warn_only_manager
-    # Quiet's behavior is a superset of warn_only's, so it wins.
-    if quiet:
-        manager = quiet_manager
-    with manager():
-        # Set up new var so original argument can be displayed verbatim later.
-        given_command = command
-
-        # Check if shell_escape has been overridden in env
-        if shell_escape is None:
-            shell_escape = env.get('shell_escape', True)
-
-        # Handle context manager modifications, and shell wrapping
-        wrapped_command = _shell_wrap(
-            _prefix_env_vars(_prefix_commands(command, 'remote')),
-            shell_escape,
-            shell,
-            _sudo_prefix(user, group) if sudo else None
-        )
-        # Execute info line
-        which = 'sudo' if sudo else 'run'
-        if output.debug:
-            print("[%s] %s: %s" % (env.host_string, which, wrapped_command))
-        elif output.running:
-            print("[%s] %s: %s" % (env.host_string, which, given_command))
-
-        # Actual execution, stdin/stdout/stderr handling, and termination
-        result_stdout, result_stderr, status = _execute(
-            channel=default_channel(), command=wrapped_command, pty=pty,
-            combine_stderr=combine_stderr, invoke_shell=False, stdout=stdout,
-            stderr=stderr, timeout=timeout,
-            capture_buffer_size=capture_buffer_size)
-
-        # Assemble output string
-        out = _AttributeString(result_stdout)
-        err = _AttributeString(result_stderr)
-
-        # Error handling
-        out.failed = False
-        out.command = given_command
-        out.real_command = wrapped_command
-        if status not in env.ok_ret_codes:
-            out.failed = True
-            msg = "%s() received nonzero return code %s while executing" % (
-                which, status
-            )
-            if env.warn_only:
-                msg += " '%s'!" % given_command
-            else:
-                msg += "!\n\nRequested: %s\nExecuted: %s" % (
-                    given_command, wrapped_command
-                )
-            error(message=msg, stdout=out, stderr=err)
-
-        # Attach return code to output string so users who have set things to
-        # warn only, can inspect the error code.
-        out.return_code = status
-
-        # Convenience mirror of .failed
-        out.succeeded = not out.failed
-
-        # Attach stderr for anyone interested in that.
-        out.stderr = err
-
-        return out
-
-
-@needs_host
-def run(command, shell=True, pty=True, combine_stderr=None, quiet=False,
-    warn_only=False, stdout=None, stderr=None, timeout=None, shell_escape=None,
-    capture_buffer_size=None):
-    """
-    Run a shell command on a remote host.
-
-    If ``shell`` is True (the default), `run` will execute the given command
-    string via a shell interpreter, the value of which may be controlled by
-    setting ``env.shell`` (defaulting to something similar to ``/bin/bash -l -c
-    "<command>"``.) Any double-quote (``"``) or dollar-sign (``$``) characters
-    in ``command`` will be automatically escaped when ``shell`` is True (unless
-    disabled by setting ``shell_escape=False``).
-
-    When ``shell=False``, no shell wrapping or escaping will occur. (It's
-    possible to specify ``shell=False, shell_escape=True`` if desired, which
-    will still trigger escaping of dollar signs, etc but will not wrap with a
-    shell program invocation).
-
-    `run` will return the result of the remote program's stdout as a single
-    (likely multiline) string. This string will exhibit ``failed`` and
-    ``succeeded`` boolean attributes specifying whether the command failed or
-    succeeded, and will also include the return code as the ``return_code``
-    attribute. Furthermore, it includes a copy of the requested & actual
-    command strings executed, as ``.command`` and ``.real_command``,
-    respectively.
-
-    To lessen memory use when running extremely verbose programs (and,
-    naturally, when having access to their full output afterwards is not
-    necessary!) you may limit how much of the program's stdout/err is stored by
-    setting ``capture_buffer_size`` to an integer value.
-
-    .. warning::
-        Do not set ``capture_buffer_size`` to any value smaller than the length
-        of ``env.sudo_prompt`` or you will likely break the functionality of
-        `sudo`! Ditto any user prompts stored in ``env.prompts``.
-
-    .. note::
-        This value is used for each buffer independently, so e.g. ``1024`` may
-        result in storing a total of ``2048`` bytes if there's data in both
-        streams.)
-
-    Any text entered in your local terminal will be forwarded to the remote
-    program as it runs, thus allowing you to interact with password or other
-    prompts naturally. For more on how this works, see
-    :doc:`/usage/interactivity`.
-
-    You may pass ``pty=False`` to forego creation of a pseudo-terminal on the
-    remote end in case the presence of one causes problems for the command in
-    question. However, this will force Fabric itself to echo any  and all input
-    you type while the command is running, including sensitive passwords. (With
-    ``pty=True``, the remote pseudo-terminal will echo for you, and will
-    intelligently handle password-style prompts.) See :ref:`pseudottys` for
-    details.
-
-    Similarly, if you need to programmatically examine the stderr stream of the
-    remote program (exhibited as the ``stderr`` attribute on this function's
-    return value), you may set ``combine_stderr=False``. Doing so has a high
-    chance of causing garbled output to appear on your terminal (though the
-    resulting strings returned by `~fabric.operations.run` will be properly
-    separated). For more info, please read :ref:`combine_streams`.
-
-    To ignore non-zero return codes, specify ``warn_only=True``. To both ignore
-    non-zero return codes *and* force a command to run silently, specify
-    ``quiet=True``.
-
-    To override which local streams are used to display remote stdout and/or
-    stderr, specify ``stdout`` or ``stderr``. (By default, the regular
-    ``sys.stdout`` and ``sys.stderr`` Python stream objects are used.)
-
-    For example, ``run("command", stderr=sys.stdout)`` would print the remote
-    standard error to the local standard out, while preserving it as its own
-    distinct attribute on the return value (as per above.) Alternately, you
-    could even provide your own stream objects or loggers, e.g. ``myout =
-    StringIO(); run("command", stdout=myout)``.
-
-    If you want an exception raised when the remote program takes too long to
-    run, specify ``timeout=N`` where ``N`` is an integer number of seconds,
-    after which to time out. This will cause ``run`` to raise a
-    `~fabric.exceptions.CommandTimeout` exception.
-
-    If you want to disable Fabric's automatic attempts at escaping quotes,
-    dollar signs etc., specify ``shell_escape=False``.
-
-    Examples::
-
-        run("ls /var/www/")
-        run("ls /home/myuser", shell=False)
-        output = run('ls /var/www/site1')
-        run("take_a_long_time", timeout=5)
-
-    .. versionadded:: 1.0
-        The ``succeeded`` and ``stderr`` return value attributes, the
-        ``combine_stderr`` kwarg, and interactive behavior.
-
-    .. versionchanged:: 1.0
-        The default value of ``pty`` is now ``True``.
-
-    .. versionchanged:: 1.0.2
-        The default value of ``combine_stderr`` is now ``None`` instead of
-        ``True``. However, the default *behavior* is unchanged, as the global
-        setting is still ``True``.
-
-    .. versionadded:: 1.5
-        The ``quiet``, ``warn_only``, ``stdout`` and ``stderr`` kwargs.
-
-    .. versionadded:: 1.5
-        The return value attributes ``.command`` and ``.real_command``.
-
-    .. versionadded:: 1.6
-        The ``timeout`` argument.
-
-    .. versionadded:: 1.7
-        The ``shell_escape`` argument.
-
-    .. versionadded:: 1.11
-        The ``capture_buffer_size`` argument.
-    """
-    return _run_command(
-        command, shell, pty, combine_stderr, quiet=quiet,
-        warn_only=warn_only, stdout=stdout, stderr=stderr, timeout=timeout,
-        shell_escape=shell_escape, capture_buffer_size=capture_buffer_size,
-    )
-
-
-@needs_host
-def sudo(command, shell=True, pty=True, combine_stderr=None, user=None,
-    quiet=False, warn_only=False, stdout=None, stderr=None, group=None,
-    timeout=None, shell_escape=None, capture_buffer_size=None):
-    """
-    Run a shell command on a remote host, with superuser privileges.
-
-    `sudo` is identical in every way to `run`, except that it will always wrap
-    the given ``command`` in a call to the ``sudo`` program to provide
-    superuser privileges.
-
-    `sudo` accepts additional ``user`` and ``group`` arguments, which are
-    passed to ``sudo`` and allow you to run as some user and/or group other
-    than root.  On most systems, the ``sudo`` program can take a string
-    username/group or an integer userid/groupid (uid/gid); ``user`` and
-    ``group`` may likewise be strings or integers.
-
-    You may set :ref:`env.sudo_user <sudo_user>` at module level or via
-    `~fabric.context_managers.settings` if you want multiple ``sudo`` calls to
-    have the same ``user`` value. An explicit ``user`` argument will, of
-    course, override this global setting.
-
-    Examples::
-
-        sudo("~/install_script.py")
-        sudo("mkdir /var/www/new_docroot", user="www-data")
-        sudo("ls /home/jdoe", user=1001)
-        result = sudo("ls /tmp/")
-        with settings(sudo_user='mysql'):
-            sudo("whoami") # prints 'mysql'
-
-    .. versionchanged:: 1.0
-        See the changed and added notes for `~fabric.operations.run`.
-
-    .. versionchanged:: 1.5
-        Now honors :ref:`env.sudo_user <sudo_user>`.
-
-    .. versionadded:: 1.5
-        The ``quiet``, ``warn_only``, ``stdout`` and ``stderr`` kwargs.
-
-    .. versionadded:: 1.5
-        The return value attributes ``.command`` and ``.real_command``.
-
-    .. versionadded:: 1.7
-        The ``shell_escape`` argument.
-
-    .. versionadded:: 1.11
-        The ``capture_buffer_size`` argument.
-    """
-    return _run_command(
-        command, shell, pty, combine_stderr, sudo=True,
-        user=user if user else env.sudo_user,
-        group=group, quiet=quiet, warn_only=warn_only, stdout=stdout,
-        stderr=stderr, timeout=timeout, shell_escape=shell_escape,
-        capture_buffer_size=capture_buffer_size,
-    )
-
-
-def local(command, capture=False, shell=None):
-    """
-    Run a command on the local system.
-
-    `local` is simply a convenience wrapper around the use of the builtin
-    Python ``subprocess`` module with ``shell=True`` activated. If you need to
-    do anything special, consider using the ``subprocess`` module directly.
-
-    ``shell`` is passed directly to `subprocess.Popen
-    <http://docs.python.org/library/subprocess.html#subprocess.Popen>`_'s
-    ``execute`` argument (which determines the local shell to use.)  As per the
-    linked documentation, on Unix the default behavior is to use ``/bin/sh``,
-    so this option is useful for setting that value to e.g.  ``/bin/bash``.
-
-    `local` is not currently capable of simultaneously printing and
-    capturing output, as `~fabric.operations.run`/`~fabric.operations.sudo`
-    do. The ``capture`` kwarg allows you to switch between printing and
-    capturing as necessary, and defaults to ``False``.
-
-    When ``capture=False``, the local subprocess' stdout and stderr streams are
-    hooked up directly to your terminal, though you may use the global
-    :doc:`output controls </usage/output_controls>` ``output.stdout`` and
-    ``output.stderr`` to hide one or both if desired. In this mode, the return
-    value's stdout/stderr values are always empty.
-
-    When ``capture=True``, you will not see any output from the subprocess in
-    your terminal, but the return value will contain the captured
-    stdout/stderr.
-
-    In either case, as with `~fabric.operations.run` and
-    `~fabric.operations.sudo`, this return value exhibits the ``return_code``,
-    ``stderr``, ``failed``, ``succeeded``, ``command`` and ``real_command``
-    attributes. See `run` for details.
-
-    `~fabric.operations.local` will honor the `~fabric.context_managers.lcd`
-    context manager, allowing you to control its current working directory
-    independently of the remote end (which honors
-    `~fabric.context_managers.cd`).
-
-    .. versionchanged:: 1.0
-        Added the ``succeeded`` and ``stderr`` attributes.
-    .. versionchanged:: 1.0
-        Now honors the `~fabric.context_managers.lcd` context manager.
-    .. versionchanged:: 1.0
-        Changed the default value of ``capture`` from ``True`` to ``False``.
-    .. versionadded:: 1.9
-        The return value attributes ``.command`` and ``.real_command``.
-    """
-    given_command = command
-    # Apply cd(), path() etc
-    with_env = _prefix_env_vars(command, local=True)
-    wrapped_command = _prefix_commands(with_env, 'local')
-    if output.debug:
-        print("[localhost] local: %s" % (wrapped_command))
-    elif output.running:
-        print("[localhost] local: " + given_command)
-    # Tie in to global output controls as best we can; our capture argument
-    # takes precedence over the output settings.
-    dev_null = None
-    if capture:
-        out_stream = subprocess.PIPE
-        err_stream = subprocess.PIPE
-    else:
-        dev_null = open(os.devnull, 'w+')
-        # Non-captured, hidden streams are discarded.
-        out_stream = None if output.stdout else dev_null
-        err_stream = None if output.stderr else dev_null
-    try:
-        cmd_arg = wrapped_command if win32 else [wrapped_command]
-        p = subprocess.Popen(cmd_arg, shell=True, stdout=out_stream,
-                             stderr=err_stream, executable=shell,
-                             close_fds=(not win32))
-        (stdout, stderr) = p.communicate()
-    finally:
-        if dev_null is not None:
-            dev_null.close()
-    # Handle error condition (deal with stdout being None, too)
-    out = _AttributeString(stdout.strip() if stdout else "")
-    err = _AttributeString(stderr.strip() if stderr else "")
-    out.command = given_command
-    out.real_command = wrapped_command
-    out.failed = False
-    out.return_code = p.returncode
-    out.stderr = err
-    if p.returncode not in env.ok_ret_codes:
-        out.failed = True
-        msg = "local() encountered an error (return code %s) while executing '%s'" % (p.returncode, command)
-        error(message=msg, stdout=out, stderr=err)
-    out.succeeded = not out.failed
-    # If we were capturing, this will be a string; otherwise it will be None.
-    return out
-
-
-@needs_host
-def reboot(wait=120, command='reboot', use_sudo=True):
-    """
-    Reboot the remote system.
-
-    Will temporarily tweak Fabric's reconnection settings (:ref:`timeout` and
-    :ref:`connection-attempts`) to ensure that reconnection does not give up
-    for at least ``wait`` seconds.
-
-    .. note::
-        As of Fabric 1.4, the ability to reconnect partway through a session no
-        longer requires use of internal APIs.  While we are not officially
-        deprecating this function, adding more features to it will not be a
-        priority.
-
-        Users who want greater control
-        are encouraged to check out this function's (6 lines long, well
-        commented) source code and write their own adaptation using different
-        timeout/attempt values or additional logic.
-
-    .. versionadded:: 0.9.2
-    .. versionchanged:: 1.4
-        Changed the ``wait`` kwarg to be optional, and refactored to leverage
-        the new reconnection functionality; it may not actually have to wait
-        for ``wait`` seconds before reconnecting.
-    .. versionchanged:: 1.11
-        Added ``use_sudo`` as a kwarg. Maintained old functionality by setting
-        the default value to True.
-    """
-    # Shorter timeout for a more granular cycle than the default.
-    timeout = 5
-    # Use 'wait' as max total wait time
-    attempts = int(round(float(wait) / float(timeout)))
-    # Don't bleed settings, since this is supposed to be self-contained.
-    # User adaptations will probably want to drop the "with settings()" and
-    # just have globally set timeout/attempts values.
-    with settings(
-        hide('running'),
-        timeout=timeout,
-        connection_attempts=attempts
-    ):
-        (sudo if use_sudo else run)(command)
-        # Try to make sure we don't slip in before pre-reboot lockdown
-        time.sleep(5)
-        # This is actually an internal-ish API call, but users can simply drop
-        # it in real fabfile use -- the next run/sudo/put/get/etc call will
-        # automatically trigger a reconnect.
-        # We use it here to force the reconnect while this function is still in
-        # control and has the above timeout settings enabled.
-        connections.connect(env.host_string)
-    # At this point we should be reconnected to the newly rebooted server.
diff -Nru fabric-1.14.0/fabric/runners.py fabric-2.5.0/fabric/runners.py
--- fabric-1.14.0/fabric/runners.py	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/fabric/runners.py	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,148 @@
+from invoke import Runner, pty_size, Result as InvokeResult
+
+
+class Remote(Runner):
+    """
+    Run a shell command over an SSH connection.
+
+    This class subclasses `invoke.runners.Runner`; please see its documentation
+    for most public API details.
+
+    .. note::
+        `.Remote`'s ``__init__`` method expects a `.Connection` (or subclass)
+        instance for its ``context`` argument.
+
+    .. versionadded:: 2.0
+    """
+
+    def __init__(self, *args, **kwargs):
+        """
+        Thin wrapper for superclass' ``__init__``; please see it for details.
+
+        Additional keyword arguments defined here are listed below.
+
+        :param bool inline_env:
+            Whether to 'inline' shell env vars as prefixed parameters, instead
+            of trying to submit them via `.Channel.update_environment`.
+            Default:: ``False``.
+
+        .. versionchanged:: 2.3
+            Added the ``inline_env`` parameter.
+        """
+        self.inline_env = kwargs.pop("inline_env", None)
+        super(Remote, self).__init__(*args, **kwargs)
+
+    def start(self, command, shell, env, timeout=None):
+        self.channel = self.context.create_session()
+        if self.using_pty:
+            rows, cols = pty_size()
+            self.channel.get_pty(width=rows, height=cols)
+        if env:
+            # TODO: honor SendEnv from ssh_config (but if we do, _should_ we
+            # honor it even when prefixing? That would depart from OpenSSH
+            # somewhat (albeit as a "what we can do that it cannot" feature...)
+            if self.inline_env:
+                # TODO: escaping, if we can find a FOOLPROOF THIRD PARTY METHOD
+                # for doing so!
+                # TODO: switch to using a higher-level generic command
+                # prefixing functionality, when implemented.
+                parameters = " ".join(
+                    ["{}={}".format(k, v) for k, v in sorted(env.items())]
+                )
+                # NOTE: we can assume 'export' and '&&' relatively safely, as
+                # sshd always brings some shell into play, even if it's just
+                # /bin/sh.
+                command = "export {} && {}".format(parameters, command)
+            else:
+                self.channel.update_environment(env)
+        self.channel.exec_command(command)
+
+    def read_proc_stdout(self, num_bytes):
+        return self.channel.recv(num_bytes)
+
+    def read_proc_stderr(self, num_bytes):
+        return self.channel.recv_stderr(num_bytes)
+
+    def _write_proc_stdin(self, data):
+        return self.channel.sendall(data)
+
+    def close_proc_stdin(self):
+        return self.channel.shutdown_write()
+
+    @property
+    def process_is_finished(self):
+        return self.channel.exit_status_ready()
+
+    def send_interrupt(self, interrupt):
+        # NOTE: in v1, we just reraised the KeyboardInterrupt unless a PTY was
+        # present; this seems to have been because without a PTY, the
+        # below escape sequence is ignored, so all we can do is immediately
+        # terminate on our end.
+        # NOTE: also in v1, the raising of the KeyboardInterrupt completely
+        # skipped all thread joining & cleanup; presumably regular interpreter
+        # shutdown suffices to tie everything off well enough.
+        if self.using_pty:
+            # Submit hex ASCII character 3, aka ETX, which most Unix PTYs
+            # interpret as a foreground SIGINT.
+            # TODO: is there anything else we can do here to be more portable?
+            self.channel.send(u"\x03")
+        else:
+            raise interrupt
+
+    def returncode(self):
+        return self.channel.recv_exit_status()
+
+    def generate_result(self, **kwargs):
+        kwargs["connection"] = self.context
+        return Result(**kwargs)
+
+    def stop(self):
+        if hasattr(self, "channel"):
+            self.channel.close()
+
+    def kill(self):
+        # Just close the channel immediately, which is about as close as we can
+        # get to a local SIGKILL unfortunately.
+        # TODO: consider _also_ calling .send_interrupt() and only doing this
+        # after another few seconds; but A) kinda fragile/complex and B) would
+        # belong in invoke.Runner anyways?
+        self.channel.close()
+
+    # TODO: shit that is in fab 1 run() but could apply to invoke.Local too:
+    # * see rest of stuff in _run_command/_execute in operations.py...there is
+    # a bunch that applies generally like optional exit codes, etc
+
+    # TODO: general shit not done yet
+    # * stdin; Local relies on local process management to ensure stdin is
+    # hooked up; we cannot do that.
+    # * output prefixing
+    # * agent forwarding
+    # * reading at 4096 bytes/time instead of whatever inv defaults to (also,
+    # document why we are doing that, iirc it changed recentlyish via ticket)
+    # * TODO: oh god so much more, go look it up
+
+    # TODO: shit that has no Local equivalent that we probs need to backfill
+    # into Runner, probably just as a "finish()" or "stop()" (to mirror
+    # start()):
+    # * channel close()
+    # * agent-forward close()
+
+
+class Result(InvokeResult):
+    """
+    An `invoke.runners.Result` exposing which `.Connection` was run against.
+
+    Exposes all attributes from its superclass, then adds a ``.connection``,
+    which is simply a reference to the `.Connection` whose method yielded this
+    result.
+
+    .. versionadded:: 2.0
+    """
+
+    def __init__(self, **kwargs):
+        connection = kwargs.pop("connection")
+        super(Result, self).__init__(**kwargs)
+        self.connection = connection
+
+    # TODO: have useful str/repr differentiation from invoke.Result,
+    # transfer.Result etc.
diff -Nru fabric-1.14.0/fabric/sftp.py fabric-2.5.0/fabric/sftp.py
--- fabric-1.14.0/fabric/sftp.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/fabric/sftp.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,318 +0,0 @@
-from __future__ import with_statement
-
-import os
-import posixpath
-import stat
-import re
-import uuid
-from fnmatch import filter as fnfilter
-
-from fabric.state import output, connections, env
-from fabric.utils import warn
-from fabric.context_managers import settings
-
-
-# TODO: use self.sftp.listdir_iter on Paramiko 1.15+
-
-
-def _format_local(local_path, local_is_path):
-    """Format a path for log output"""
-    if local_is_path:
-        return local_path
-    else:
-        # This allows users to set a name attr on their StringIO objects
-        # just like an open file object would have
-        return getattr(local_path, 'name', '<file obj>')
-
-
-class SFTP(object):
-    """
-    SFTP helper class, which is also a facade for ssh.SFTPClient.
-    """
-    def __init__(self, host_string):
-        self.ftp = connections[host_string].open_sftp()
-
-    # Recall that __getattr__ is the "fallback" attribute getter, and is thus
-    # pretty safe to use for facade-like behavior as we're doing here.
-    def __getattr__(self, attr):
-        return getattr(self.ftp, attr)
-
-    def isdir(self, path):
-        try:
-            return stat.S_ISDIR(self.ftp.stat(path).st_mode)
-        except IOError:
-            return False
-
-    def islink(self, path):
-        try:
-            return stat.S_ISLNK(self.ftp.lstat(path).st_mode)
-        except IOError:
-            return False
-
-    def exists(self, path):
-        try:
-            self.ftp.lstat(path).st_mode
-        except IOError:
-            return False
-        return True
-
-    def glob(self, path):
-        from fabric.state import win32
-        dirpart, pattern = os.path.split(path)
-        rlist = self.ftp.listdir(dirpart)
-
-        names = fnfilter([f for f in rlist if not f[0] == '.'], pattern)
-        ret = []
-        if len(names):
-            s = '/'
-            ret = [dirpart.rstrip(s) + s + name.lstrip(s) for name in names]
-            if not win32:
-                ret = [posixpath.join(dirpart, name) for name in names]
-        return ret
-
-    def walk(self, top, topdown=True, onerror=None, followlinks=False):
-        from os.path import join
-
-        # We may not have read permission for top, in which case we can't get a
-        # list of the files the directory contains. os.path.walk always
-        # suppressed the exception then, rather than blow up for a minor reason
-        # when (say) a thousand readable directories are still left to visit.
-        # That logic is copied here.
-        try:
-            # Note that listdir and error are globals in this module due to
-            # earlier import-*.
-            names = self.ftp.listdir(top)
-        except Exception, err:
-            if onerror is not None:
-                onerror(err)
-            return
-
-        dirs, nondirs = [], []
-        for name in names:
-            if self.isdir(join(top, name)):
-                dirs.append(name)
-            else:
-                nondirs.append(name)
-
-        if topdown:
-            yield top, dirs, nondirs
-
-        for name in dirs:
-            path = join(top, name)
-            if followlinks or not self.islink(path):
-                for x in self.walk(path, topdown, onerror, followlinks):
-                    yield x
-        if not topdown:
-            yield top, dirs, nondirs
-
-    def mkdir(self, path, use_sudo):
-        from fabric.api import sudo, hide
-        if use_sudo:
-            with hide('everything'):
-                sudo('mkdir "%s"' % path)
-        else:
-            self.ftp.mkdir(path)
-
-    def get(self, remote_path, local_path, use_sudo, local_is_path, rremote=None, temp_dir=""):
-        from fabric.api import sudo, hide
-
-        # rremote => relative remote path, so get(/var/log) would result in
-        # this function being called with
-        # remote_path=/var/log/apache2/access.log and
-        # rremote=apache2/access.log
-        rremote = rremote if rremote is not None else remote_path
-        # Handle format string interpolation (e.g. %(dirname)s)
-        path_vars = {
-            'host': env.host_string.replace(':', '-'),
-            'basename': os.path.basename(rremote),
-            'dirname': os.path.dirname(rremote),
-            'path': rremote
-        }
-
-        if local_is_path:
-            # Fix for issue #711 and #1348 - escape %'s as well as possible.
-            format_re = r'(%%(?!\((?:%s)\)\w))' % '|'.join(path_vars.keys())
-            escaped_path = re.sub(format_re, r'%\1', local_path)
-            local_path = os.path.abspath(escaped_path % path_vars)
-
-            # Ensure we give ssh.SFTPCLient a file by prepending and/or
-            # creating local directories as appropriate.
-            dirpath, filepath = os.path.split(local_path)
-            if dirpath and not os.path.exists(dirpath):
-                os.makedirs(dirpath)
-            if os.path.isdir(local_path):
-                local_path = os.path.join(local_path, path_vars['basename'])
-
-        if output.running:
-            print("[%s] download: %s <- %s" % (
-                env.host_string,
-                _format_local(local_path, local_is_path),
-                remote_path
-            ))
-        # Warn about overwrites, but keep going
-        if local_is_path and os.path.exists(local_path):
-            msg = "Local file %s already exists and is being overwritten."
-            warn(msg % local_path)
-
-        # When using sudo, "bounce" the file through a guaranteed-unique file
-        # path in the default remote CWD (which, typically, the login user will
-        # have write permissions on) in order to sudo(cp) it.
-        if use_sudo:
-            target_path = posixpath.join(temp_dir, uuid.uuid4().hex)
-            # Temporarily nuke 'cwd' so sudo() doesn't "cd" its mv command.
-            # (The target path has already been cwd-ified elsewhere.)
-            with settings(hide('everything'), cwd=""):
-                sudo('cp -p "%s" "%s"' % (remote_path, target_path))
-                # The user should always own the copied file.
-                sudo('chown %s "%s"' % (env.user, target_path))
-                # Only root and the user has the right to read the file
-                sudo('chmod %o "%s"' % (0400, target_path))
-                remote_path = target_path
-
-        try:
-            # File-like objects: reset to file seek 0 (to ensure full overwrite)
-            # and then use Paramiko's getfo() directly
-            getter = self.ftp.get
-            if not local_is_path:
-                local_path.seek(0)
-                getter = self.ftp.getfo
-            getter(remote_path, local_path)
-        finally:
-            # try to remove the temporary file after the download
-            if use_sudo:
-                with settings(hide('everything'), cwd=""):
-                    sudo('rm -f "%s"' % remote_path)
-
-        # Return local_path object for posterity. (If mutated, caller will want
-        # to know.)
-        return local_path
-
-    def get_dir(self, remote_path, local_path, use_sudo, temp_dir):
-        # Decide what needs to be stripped from remote paths so they're all
-        # relative to the given remote_path
-        if os.path.basename(remote_path):
-            strip = os.path.dirname(remote_path)
-        else:
-            strip = os.path.dirname(os.path.dirname(remote_path))
-
-        # Store all paths gotten so we can return them when done
-        result = []
-        # Use our facsimile of os.walk to find all files within remote_path
-        for context, dirs, files in self.walk(remote_path):
-            # Normalize current directory to be relative
-            # E.g. remote_path of /var/log and current dir of /var/log/apache2
-            # would be turned into just 'apache2'
-            lcontext = rcontext = context.replace(strip, '', 1).lstrip('/')
-            # Prepend local path to that to arrive at the local mirrored
-            # version of this directory. So if local_path was 'mylogs', we'd
-            # end up with 'mylogs/apache2'
-            lcontext = os.path.join(local_path, lcontext)
-
-            # Download any files in current directory
-            for f in files:
-                # Construct full and relative remote paths to this file
-                rpath = posixpath.join(context, f)
-                rremote = posixpath.join(rcontext, f)
-                # If local_path isn't using a format string that expands to
-                # include its remote path, we need to add it here.
-                if "%(path)s" not in local_path \
-                    and "%(dirname)s" not in local_path:
-                    lpath = os.path.join(lcontext, f)
-                # Otherwise, just passthrough local_path to self.get()
-                else:
-                    lpath = local_path
-                # Now we can make a call to self.get() with specific file paths
-                # on both ends.
-                result.append(self.get(rpath, lpath, use_sudo, True, rremote, temp_dir))
-        return result
-
-    def put(self, local_path, remote_path, use_sudo, mirror_local_mode, mode,
-        local_is_path, temp_dir):
-
-        from fabric.api import sudo, hide
-        pre = self.ftp.getcwd()
-        pre = pre if pre else ''
-        if local_is_path and self.isdir(remote_path):
-            basename = os.path.basename(local_path)
-            remote_path = posixpath.join(remote_path, basename)
-        if output.running:
-            print("[%s] put: %s -> %s" % (
-                env.host_string,
-                _format_local(local_path, local_is_path),
-                posixpath.join(pre, remote_path)
-            ))
-        # When using sudo, "bounce" the file through a guaranteed-unique file
-        # path in the default remote CWD (which, typically, the login user will
-        # have write permissions on) in order to sudo(mv) it later.
-        if use_sudo:
-            target_path = remote_path
-            remote_path = posixpath.join(temp_dir, uuid.uuid4().hex)
-        # Read, ensuring we handle file-like objects correct re: seek pointer
-        putter = self.ftp.put
-        if not local_is_path:
-            old_pointer = local_path.tell()
-            local_path.seek(0)
-            putter = self.ftp.putfo
-        rattrs = putter(local_path, remote_path)
-        if not local_is_path:
-            local_path.seek(old_pointer)
-        # Handle modes if necessary
-        if (local_is_path and mirror_local_mode) or (mode is not None):
-            lmode = os.stat(local_path).st_mode if mirror_local_mode else mode
-            # Cast to octal integer in case of string
-            if isinstance(lmode, basestring):
-                lmode = int(lmode, 8)
-            lmode = lmode & 07777
-            rmode = rattrs.st_mode
-            # Only bitshift if we actually got an rmode
-            if rmode is not None:
-                rmode = (rmode & 07777)
-            if lmode != rmode:
-                if use_sudo:
-                    # Temporarily nuke 'cwd' so sudo() doesn't "cd" its mv
-                    # command. (The target path has already been cwd-ified
-                    # elsewhere.)
-                    with settings(hide('everything'), cwd=""):
-                        sudo('chmod %o \"%s\"' % (lmode, remote_path))
-                else:
-                    self.ftp.chmod(remote_path, lmode)
-        if use_sudo:
-            # Temporarily nuke 'cwd' so sudo() doesn't "cd" its mv command.
-            # (The target path has already been cwd-ified elsewhere.)
-            with settings(hide('everything'), cwd=""):
-                sudo("mv \"%s\" \"%s\"" % (remote_path, target_path))
-            # Revert to original remote_path for return value's sake
-            remote_path = target_path
-        return remote_path
-
-    def put_dir(self, local_path, remote_path, use_sudo, mirror_local_mode,
-        mode, temp_dir):
-        if os.path.basename(local_path):
-            strip = os.path.dirname(local_path)
-        else:
-            strip = os.path.dirname(os.path.dirname(local_path))
-
-        remote_paths = []
-
-        for context, dirs, files in os.walk(local_path):
-            rcontext = context.replace(strip, '', 1)
-            # normalize pathname separators with POSIX separator
-            rcontext = rcontext.replace(os.sep, '/')
-            rcontext = rcontext.lstrip('/')
-            rcontext = posixpath.join(remote_path, rcontext)
-
-            if not self.exists(rcontext):
-                self.mkdir(rcontext, use_sudo)
-
-            for d in dirs:
-                n = posixpath.join(rcontext, d)
-                if not self.exists(n):
-                    self.mkdir(n, use_sudo)
-
-            for f in files:
-                local_path = os.path.join(context, f)
-                n = posixpath.join(rcontext, f)
-                p = self.put(local_path, n, use_sudo, mirror_local_mode, mode,
-                    True, temp_dir)
-                remote_paths.append(p)
-        return remote_paths
diff -Nru fabric-1.14.0/fabric/state.py fabric-2.5.0/fabric/state.py
--- fabric-1.14.0/fabric/state.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/fabric/state.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,474 +0,0 @@
-"""
-Internal shared-state variables such as config settings and host lists.
-"""
-
-import os
-import sys
-from optparse import make_option
-
-from fabric.network import HostConnectionCache, ssh
-from fabric.version import get_version
-from fabric.utils import _AliasDict, _AttributeDict
-
-
-#
-# Win32 flag
-#
-
-# Impacts a handful of platform specific behaviors. Note that Cygwin's Python
-# is actually close enough to "real" UNIXes that it doesn't need (or want!) to
-# use PyWin32 -- so we only test for literal Win32 setups (vanilla Python,
-# ActiveState etc) here.
-win32 = (sys.platform == 'win32')
-
-
-#
-# Environment dictionary - support structures
-#
-
-# By default, if the user (including code using Fabric as a library) doesn't
-# set the username, we obtain the currently running username and use that.
-def _get_system_username():
-    """
-    Obtain name of current system user, which will be default connection user.
-    """
-    import getpass
-    username = None
-    try:
-        username = getpass.getuser()
-    # getpass.getuser supported on both Unix and Windows systems.
-    # getpass.getuser may call pwd.getpwuid which in turns may raise KeyError
-    # if it cannot find a username for the given UID, e.g. on ep.io
-    # and similar "non VPS" style services. Rather than error out, just keep
-    # the 'default' username to None. Can check for this value later if needed.
-    except KeyError:
-        pass
-    except ImportError:
-        if win32:
-            import win32api
-            import win32security # noqa
-            import win32profile # noqa
-            username = win32api.GetUserName()
-    return username
-
-def _rc_path():
-    """
-    Return platform-specific default file path for $HOME/.fabricrc.
-    """
-    rc_file = '.fabricrc'
-    rc_path = '~/' + rc_file
-    expanded_rc_path = os.path.expanduser(rc_path)
-    if expanded_rc_path == rc_path and win32:
-            from win32com.shell.shell import SHGetSpecialFolderPath
-            from win32com.shell.shellcon import CSIDL_PROFILE
-            expanded_rc_path = "%s/%s" % (
-                SHGetSpecialFolderPath(0, CSIDL_PROFILE),
-                rc_file
-                )
-    return expanded_rc_path
-
-default_port = '22'  # hurr durr
-default_ssh_config_path = os.path.join(os.path.expanduser('~'), '.ssh', 'config')
-
-# Options/settings which exist both as environment keys and which can be set on
-# the command line, are defined here. When used via `fab` they will be added to
-# the optparse parser, and either way they are added to `env` below (i.e.  the
-# 'dest' value becomes the environment key and the value, the env value).
-#
-# Keep in mind that optparse changes hyphens to underscores when automatically
-# deriving the `dest` name, e.g. `--reject-unknown-hosts` becomes
-# `reject_unknown_hosts`.
-#
-# Furthermore, *always* specify some sort of default to avoid ending up with
-# optparse.NO_DEFAULT (currently a two-tuple)! In general, None is a better
-# default than ''.
-#
-# User-facing documentation for these are kept in sites/docs/env.rst.
-env_options = [
-
-    make_option('-a', '--no_agent',
-        action='store_true',
-        default=False,
-        help="don't use the running SSH agent"
-    ),
-
-    make_option('-A', '--forward-agent',
-        action='store_true',
-        default=False,
-        help="forward local agent to remote end"
-    ),
-
-    make_option('--abort-on-prompts',
-        action='store_true',
-        default=False,
-        help="abort instead of prompting (for password, host, etc)"
-    ),
-
-    make_option('-c', '--config',
-        dest='rcfile',
-        default=_rc_path(),
-        metavar='PATH',
-        help="specify location of config file to use"
-    ),
-
-    make_option('--colorize-errors',
-        action='store_true',
-        default=False,
-        help="Color error output",
-    ),
-
-    make_option('-D', '--disable-known-hosts',
-        action='store_true',
-        default=False,
-        help="do not load user known_hosts file"
-    ),
-
-    make_option('-e', '--eagerly-disconnect',
-        action='store_true',
-        default=False,
-        help="disconnect from hosts as soon as possible"
-    ),
-
-    make_option('-f', '--fabfile',
-        default='fabfile',
-        metavar='PATH',
-        help="python module file to import, e.g. '../other.py'"
-    ),
-
-    make_option('-g', '--gateway',
-        default=None,
-        metavar='HOST',
-        help="gateway host to connect through"
-    ),
-
-    make_option('--gss-auth',
-        action='store_true',
-        default=None,
-        help="Use GSS-API authentication"
-    ),
-
-    make_option('--gss-deleg',
-        action='store_true',
-        default=None,
-        help="Delegate GSS-API client credentials or not"
-    ),
-
-    make_option('--gss-kex',
-        action='store_true',
-        default=None,
-        help="Perform GSS-API Key Exchange and user authentication"
-    ),
-
-    make_option('--hide',
-        metavar='LEVELS',
-        help="comma-separated list of output levels to hide"
-    ),
-
-    make_option('-H', '--hosts',
-        default=[],
-        help="comma-separated list of hosts to operate on"
-    ),
-
-    make_option('-i',
-        action='append',
-        dest='key_filename',
-        metavar='PATH',
-        default=None,
-        help="path to SSH private key file. May be repeated."
-    ),
-
-    make_option('-k', '--no-keys',
-        action='store_true',
-        default=False,
-        help="don't load private key files from ~/.ssh/"
-    ),
-
-    make_option('--keepalive',
-        dest='keepalive',
-        type=int,
-        default=0,
-        metavar="N",
-        help="enables a keepalive every N seconds"
-    ),
-
-    make_option('--linewise',
-        action='store_true',
-        default=False,
-        help="print line-by-line instead of byte-by-byte"
-    ),
-
-    make_option('-n', '--connection-attempts',
-        type='int',
-        metavar='M',
-        dest='connection_attempts',
-        default=1,
-        help="make M attempts to connect before giving up"
-    ),
-
-    make_option('--no-pty',
-        dest='always_use_pty',
-        action='store_false',
-        default=True,
-        help="do not use pseudo-terminal in run/sudo"
-    ),
-
-    make_option('-p', '--password',
-        default=None,
-        help="password for use with authentication and/or sudo"
-    ),
-
-    make_option('-P', '--parallel',
-        dest='parallel',
-        action='store_true',
-        default=False,
-        help="default to parallel execution method"
-    ),
-
-    make_option('--port',
-        default=default_port,
-        help="SSH connection port"
-    ),
-
-    make_option('-r', '--reject-unknown-hosts',
-        action='store_true',
-        default=False,
-        help="reject unknown hosts"
-    ),
-
-    make_option('--sudo-password',
-        default=None,
-        help="password for use with sudo only",
-    ),
-
-    make_option('--system-known-hosts',
-        default=None,
-        help="load system known_hosts file before reading user known_hosts"
-    ),
-
-    make_option('-R', '--roles',
-        default=[],
-        help="comma-separated list of roles to operate on"
-    ),
-
-    make_option('-s', '--shell',
-        default='/bin/bash -l -c',
-        help="specify a new shell, defaults to '/bin/bash -l -c'"
-    ),
-
-    make_option('--show',
-        metavar='LEVELS',
-        help="comma-separated list of output levels to show"
-    ),
-
-    make_option('--skip-bad-hosts',
-        action="store_true",
-        default=False,
-        help="skip over hosts that can't be reached"
-    ),
-
-    make_option('--skip-unknown-tasks',
-        action="store_true",
-        default=False,
-        help="skip over unknown tasks"
-    ),
-
-    make_option('--ssh-config-path',
-        default=default_ssh_config_path,
-        metavar='PATH',
-        help="Path to SSH config file"
-    ),
-
-    make_option('-t', '--timeout',
-        type='int',
-        default=10,
-        metavar="N",
-        help="set connection timeout to N seconds"
-    ),
-
-    make_option('-T', '--command-timeout',
-        dest='command_timeout',
-        type='int',
-        default=None,
-        metavar="N",
-        help="set remote command timeout to N seconds"
-    ),
-
-    make_option('-u', '--user',
-        default=_get_system_username(),
-        help="username to use when connecting to remote hosts"
-    ),
-
-    make_option('-w', '--warn-only',
-        action='store_true',
-        default=False,
-        help="warn, instead of abort, when commands fail"
-    ),
-
-    make_option('-x', '--exclude-hosts',
-        default=[],
-        metavar='HOSTS',
-        help="comma-separated list of hosts to exclude"
-    ),
-
-    make_option('-z', '--pool-size',
-            dest='pool_size',
-            type='int',
-            metavar='INT',
-            default=0,
-            help="number of concurrent processes to use in parallel mode",
-    ),
-
-]
-
-
-#
-# Environment dictionary - actual dictionary object
-#
-
-
-# Global environment dict. Currently a catchall for everything: config settings
-# such as global deep/broad mode, host lists, username etc.
-# Most default values are specified in `env_options` above, in the interests of
-# preserving DRY: anything in here is generally not settable via the command
-# line.
-env = _AttributeDict({
-    'abort_exception': None,
-    'again_prompt': 'Sorry, try again.',
-    'all_hosts': [],
-    'combine_stderr': True,
-    'colorize_errors': False,
-    'command': None,
-    'command_prefixes': [],
-    'cwd': '',  # Must be empty string, not None, for concatenation purposes
-    'dedupe_hosts': True,
-    'default_port': default_port,
-    'eagerly_disconnect': False,
-    'echo_stdin': True,
-    'effective_roles': [],
-    'exclude_hosts': [],
-    'gateway': None,
-    'gss_auth': None,
-    'gss_deleg': None,
-    'gss_kex': None,
-    'host': None,
-    'host_string': None,
-    'lcwd': '',  # Must be empty string, not None, for concatenation purposes
-    'local_user': _get_system_username(),
-    'output_prefix': True,
-    'passwords': {},
-    'path': '',
-    'path_behavior': 'append',
-    'port': default_port,
-    'real_fabfile': None,
-    'remote_interrupt': None,
-    'roles': [],
-    'roledefs': {},
-    'shell_env': {},
-    'skip_bad_hosts': False,
-    'skip_unknown_tasks': False,
-    'ssh_config_path': default_ssh_config_path,
-    'sudo_passwords': {},
-    'ok_ret_codes': [0],     # a list of return codes that indicate success
-    # -S so sudo accepts passwd via stdin, -p with our known-value prompt for
-    # later detection (thus %s -- gets filled with env.sudo_prompt at runtime)
-    'sudo_prefix': "sudo -S -p '%(sudo_prompt)s' ",
-    'sudo_prompt': 'sudo password:',
-    'sudo_user': None,
-    'tasks': [],
-    'prompts': {},
-    'use_exceptions_for': {'network': False},
-    'use_shell': True,
-    'use_ssh_config': False,
-    'user': None,
-    'version': get_version('short')
-})
-
-# Fill in exceptions settings
-exceptions = ['network']
-exception_dict = {}
-for e in exceptions:
-    exception_dict[e] = False
-env.use_exceptions_for = _AliasDict(exception_dict,
-    aliases={'everything': exceptions})
-
-
-# Add in option defaults
-for option in env_options:
-    env[option.dest] = option.default
-
-#
-# Command dictionary
-#
-
-# Keys are the command/function names, values are the callables themselves.
-# This is filled in when main() runs.
-commands = {}
-
-
-#
-# Host connection dict/cache
-#
-
-connections = HostConnectionCache()
-
-
-def _open_session():
-    transport = connections[env.host_string].get_transport()
-    # Try passing session-open timeout for Paramiko versions which support it
-    # (1.14.3+)
-    try:
-        session = transport.open_session(timeout=env.timeout)
-    # Revert to old call behavior if we seem to have hit arity error.
-    # TODO: consider introspecting the exception to avoid masking other
-    # TypeErrors; but this is highly fragile, especially when taking i18n into
-    # account.
-    except TypeError: # Assume arity error
-        session = transport.open_session()
-    return session
-
-
-def default_channel():
-    """
-    Return a channel object based on ``env.host_string``.
-    """
-    try:
-        chan = _open_session()
-    except ssh.SSHException, err:
-        if str(err) == 'SSH session not active':
-            connections[env.host_string].close()
-            del connections[env.host_string]
-            chan = _open_session()
-        else:
-            raise
-    chan.settimeout(0.1)
-    chan.input_enabled = True
-    return chan
-
-
-#
-# Output controls
-#
-
-# Keys are "levels" or "groups" of output, values are always boolean,
-# determining whether output falling into the given group is printed or not
-# printed.
-#
-# By default, everything except 'debug' is printed, as this is what the average
-# user, and new users, are most likely to expect.
-#
-# See docs/usage.rst for details on what these levels mean.
-output = _AliasDict({
-    'status': True,
-    'aborts': True,
-    'warnings': True,
-    'running': True,
-    'stdout': True,
-    'stderr': True,
-    'exceptions': False,
-    'debug': False,
-    'user': True
-}, aliases={
-    'everything': ['warnings', 'running', 'user', 'output', 'exceptions'],
-    'output': ['stdout', 'stderr'],
-    'commands': ['stdout', 'running']
-})
diff -Nru fabric-1.14.0/fabric/tasks.py fabric-2.5.0/fabric/tasks.py
--- fabric-1.14.0/fabric/tasks.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/fabric/tasks.py	2019-08-06 23:57:28.000000000 +0100
@@ -1,429 +1,116 @@
-from __future__ import with_statement
+import invoke
 
-import inspect
-import sys
-import textwrap
-
-from fabric import state
-from fabric.utils import abort, warn, error
-from fabric.network import to_dict, disconnect_all
-from fabric.context_managers import settings
-from fabric.job_queue import JobQueue
-from fabric.task_utils import crawl, merge, parse_kwargs
-from fabric.exceptions import NetworkError
-
-if sys.version_info[:2] == (2, 5):
-    # Python 2.5 inspect.getargspec returns a tuple
-    # instead of ArgSpec namedtuple.
-    class ArgSpec(object):
-        def __init__(self, args, varargs, keywords, defaults):
-            self.args = args
-            self.varargs = varargs
-            self.keywords = keywords
-            self.defaults = defaults
-            self._tuple = (args, varargs, keywords, defaults)
-
-        def __getitem__(self, idx):
-            return self._tuple[idx]
-
-    def patched_get_argspec(func):
-        return ArgSpec(*inspect._getargspec(func))
-
-    inspect._getargspec = inspect.getargspec
-    inspect.getargspec = patched_get_argspec
-
-
-def get_task_details(task):
-    details = [
-        textwrap.dedent(task.__doc__)
-        if task.__doc__
-        else 'No docstring provided']
-    argspec = inspect.getargspec(task)
-
-    default_args = [] if not argspec.defaults else argspec.defaults
-    num_default_args = len(default_args)
-    args_without_defaults = argspec.args[:len(argspec.args) - num_default_args]
-    args_with_defaults = argspec.args[-1 * num_default_args:]
-
-    details.append('Arguments: %s' % (
-        ', '.join(
-            args_without_defaults + [
-                '%s=%r' % (arg, default)
-                for arg, default in zip(args_with_defaults, default_args)
-            ])
-    ))
-
-    return '\n'.join(details)
-
-
-def _get_list(env):
-    def inner(key):
-        return env.get(key, [])
-    return inner
+from .connection import Connection
 
 
-class Task(object):
+class Task(invoke.Task):
     """
-    Abstract base class for objects wishing to be picked up as Fabric tasks.
+    Extends `invoke.tasks.Task` with knowledge of target hosts and similar.
 
-    Instances of subclasses will be treated as valid tasks when present in
-    fabfiles loaded by the :doc:`fab </usage/fab>` tool.
+    As `invoke.tasks.Task` relegates documentation responsibility to its `@task
+    <invoke.tasks.task>` expression, so we relegate most details to our version
+    of `@task <fabric.tasks.task>` - please see its docs for details.
 
-    For details on how to implement and use `~fabric.tasks.Task` subclasses,
-    please see the usage documentation on :ref:`new-style tasks
-    <new-style-tasks>`.
-
-    .. versionadded:: 1.1
+    .. versionadded:: 2.1
     """
-    name = 'undefined'
-    use_task_objects = True
-    aliases = None
-    is_default = False
-
-    # TODO: make it so that this wraps other decorators as expected
-    def __init__(self, alias=None, aliases=None, default=False, name=None,
-        *args, **kwargs):
-        if alias is not None:
-            self.aliases = [alias, ]
-        if aliases is not None:
-            self.aliases = aliases
-        if name is not None:
-            self.name = name
-        self.is_default = default
-
-    def __details__(self):
-        return get_task_details(self.run)
-
-    def run(self):
-        raise NotImplementedError
-
-    def get_hosts_and_effective_roles(self, arg_hosts, arg_roles, arg_exclude_hosts, env=None):
-        """
-        Return a tuple containing the host list the given task should be using
-        and the roles being used.
 
-        See :ref:`host-lists` for detailed documentation on how host lists are
-        set.
-
-        .. versionchanged:: 1.9
-        """
-        env = env or {'hosts': [], 'roles': [], 'exclude_hosts': []}
-        roledefs = env.get('roledefs', {})
-        # Command line per-task takes precedence over anything else.
-        if arg_hosts or arg_roles:
-            return merge(arg_hosts, arg_roles, arg_exclude_hosts, roledefs), arg_roles
-        # Decorator-specific hosts/roles go next
-        func_hosts = getattr(self, 'hosts', [])
-        func_roles = getattr(self, 'roles', [])
-        if func_hosts or func_roles:
-            return merge(func_hosts, func_roles, arg_exclude_hosts, roledefs), func_roles
-        # Finally, the env is checked (which might contain globally set lists
-        # from the CLI or from module-level code). This will be the empty list
-        # if these have not been set -- which is fine, this method should
-        # return an empty list if no hosts have been set anywhere.
-        env_vars = map(_get_list(env), "hosts roles exclude_hosts".split())
-        env_vars.append(roledefs)
-        return merge(*env_vars), env.get('roles', [])
-
-    def get_pool_size(self, hosts, default):
-        # Default parallel pool size (calculate per-task in case variables
-        # change)
-        default_pool_size = default or len(hosts)
-        # Allow per-task override
-        # Also cast to int in case somebody gave a string
-        from_task = getattr(self, 'pool_size', None)
-        pool_size = int(from_task or default_pool_size)
-        # But ensure it's never larger than the number of hosts
-        pool_size = min((pool_size, len(hosts)))
-        # Inform user of final pool size for this task
-        if state.output.debug:
-            print("Parallel tasks now using pool size of %d" % pool_size)
-        return pool_size
+    def __init__(self, *args, **kwargs):
+        # Pull out our own kwargs before hitting super, which will TypeError on
+        # anything it doesn't know about.
+        self.hosts = kwargs.pop("hosts", None)
+        super(Task, self).__init__(*args, **kwargs)
 
 
-class WrappedCallableTask(Task):
+def task(*args, **kwargs):
     """
-    Wraps a given callable transparently, while marking it as a valid Task.
+    Wraps/extends Invoke's `@task <invoke.tasks.task>` with extra kwargs.
 
-    Generally used via `~fabric.decorators.task` and not directly.
+    See `the Invoke-level API docs <invoke.tasks.task>` for most details; this
+    Fabric-specific implementation adds the following additional keyword
+    arguments:
 
-    .. versionadded:: 1.1
+    :param hosts:
+        An iterable of host-connection specifiers appropriate for eventually
+        instantiating a `.Connection`. The existence of this argument will
+        trigger automatic parameterization of the task when invoked from the
+        CLI, similar to the behavior of :option:`--hosts`.
 
-    .. seealso:: `~fabric.docs.unwrap_tasks`, `~fabric.decorators.task`
-    """
-    def __init__(self, callable, *args, **kwargs):
-        super(WrappedCallableTask, self).__init__(*args, **kwargs)
-        self.wrapped = callable
-        # Don't use getattr() here -- we want to avoid touching self.name
-        # entirely so the superclass' value remains default.
-        if hasattr(callable, '__name__'):
-            if self.name == 'undefined':
-                self.__name__ = self.name = callable.__name__
-            else:
-                self.__name__ = self.name
-        if hasattr(callable, '__doc__'):
-            self.__doc__ = callable.__doc__
-        if hasattr(callable, '__module__'):
-            self.__module__ = callable.__module__
-
-    def __call__(self, *args, **kwargs):
-        return self.run(*args, **kwargs)
-
-    def run(self, *args, **kwargs):
-        return self.wrapped(*args, **kwargs)
-
-    def __getattr__(self, k):
-        return getattr(self.wrapped, k)
-
-    def __details__(self):
-        orig = self
-        while 'wrapped' in orig.__dict__:
-            orig = orig.__dict__.get('wrapped')
-        return get_task_details(orig)
+        .. note::
+            This parameterization is "lower-level" than that driven by
+            :option:`--hosts`: if a task decorated with this parameter is
+            executed in a session where :option:`--hosts` was given, the
+            CLI-driven value will win out.
 
+        List members may be one of:
 
-def requires_parallel(task):
-    """
-    Returns True if given ``task`` should be run in parallel mode.
+        - A string appropriate for being the first positional argument to
+          `.Connection` - see its docs for details, but these are typically
+          shorthand-only convenience strings like ``hostname.example.com`` or
+          ``user@host:port``.
+        - A dictionary appropriate for use as keyword arguments when
+          instantiating a `.Connection`. Useful for values that don't mesh well
+          with simple strings (e.g. statically defined IPv6 addresses) or to
+          bake in more complex info (eg ``connect_timeout``, ``connect_kwargs``
+          params like auth info, etc).
 
-    Specifically:
+        These two value types *may* be mixed together in the same list, though
+        we recommend that you keep things homogenous when possible, to avoid
+        confusion when debugging.
 
-    * It's been explicitly marked with ``@parallel``, or:
-    * It's *not* been explicitly marked with ``@serial`` *and* the global
-      parallel option (``env.parallel``) is set to ``True``.
-    """
-    return (
-        (state.env.parallel and not getattr(task, 'serial', False))
-        or getattr(task, 'parallel', False)
-    )
+        .. note::
+            No automatic deduplication of values is performed; if you pass in
+            multiple references to the same effective target host, the wrapped
+            task will execute on that host multiple times (including making
+            separate connections).
 
-
-def _parallel_tasks(commands_to_run):
-    return any(map(
-        lambda x: requires_parallel(crawl(x[0], state.commands)),
-        commands_to_run
-    ))
-
-
-def _is_network_error_ignored():
-    return not state.env.use_exceptions_for['network'] and state.env.skip_bad_hosts
+    .. versionadded:: 2.1
+    """
+    # Override klass to be our own Task, not Invoke's, unless somebody gave it
+    # explicitly.
+    kwargs.setdefault("klass", Task)
+    return invoke.task(*args, **kwargs)
 
 
-def _execute(task, host, my_env, args, kwargs, jobs, queue, multiprocessing):
+class ConnectionCall(invoke.Call):
     """
-    Primary single-host work body of execute()
+    Subclass of `invoke.tasks.Call` that generates `Connections <.Connection>`.
     """
-    # Log to stdout
-    if state.output.running and not hasattr(task, 'return_value'):
-        print("[%s] Executing task '%s'" % (host, my_env['command']))
-    # Create per-run env with connection settings
-    local_env = to_dict(host)
-    local_env.update(my_env)
-    # Set a few more env flags for parallelism
-    if queue is not None:
-        local_env.update({'parallel': True, 'linewise': True})
-    # Handle parallel execution
-    if queue is not None: # Since queue is only set for parallel
-        name = local_env['host_string']
-        # Wrap in another callable that:
-        # * expands the env it's given to ensure parallel, linewise, etc are
-        #   all set correctly and explicitly. Such changes are naturally
-        #   insulted from the parent process.
-        # * nukes the connection cache to prevent shared-access problems
-        # * knows how to send the tasks' return value back over a Queue
-        # * captures exceptions raised by the task
-        def inner(args, kwargs, queue, name, env):
-            state.env.update(env)
-            def submit(result):
-                queue.put({'name': name, 'result': result})
-            try:
-                state.connections.clear()
-                submit(task.run(*args, **kwargs))
-            except BaseException, e: # We really do want to capture everything
-                # SystemExit implies use of abort(), which prints its own
-                # traceback, host info etc -- so we don't want to double up
-                # on that. For everything else, though, we need to make
-                # clear what host encountered the exception that will
-                # print.
-                if e.__class__ is not SystemExit:
-                    if not (isinstance(e, NetworkError) and
-                            _is_network_error_ignored()):
-                        sys.stderr.write("!!! Parallel execution exception under host %r:\n" % name)
-                    submit(e)
-                # Here, anything -- unexpected exceptions, or abort()
-                # driven SystemExits -- will bubble up and terminate the
-                # child process.
-                if not (isinstance(e, NetworkError) and
-                        _is_network_error_ignored()):
-                    raise
-
-        # Stuff into Process wrapper
-        kwarg_dict = {
-            'args': args,
-            'kwargs': kwargs,
-            'queue': queue,
-            'name': name,
-            'env': local_env,
-        }
-        p = multiprocessing.Process(target=inner, kwargs=kwarg_dict)
-        # Name/id is host string
-        p.name = name
-        # Add to queue
-        jobs.append(p)
-    # Handle serial execution
-    else:
-        with settings(**local_env):
-            return task.run(*args, **kwargs)
-
-def _is_task(task):
-    return isinstance(task, Task)
 
-def execute(task, *args, **kwargs):
-    """
-    Execute ``task`` (callable or name), honoring host/role decorators, etc.
+    def __init__(self, *args, **kwargs):
+        """
+        Creates a new `.ConnectionCall`.
 
-    ``task`` may be an actual callable object, or it may be a registered task
-    name, which is used to look up a callable just as if the name had been
-    given on the command line (including :ref:`namespaced tasks <namespaces>`,
-    e.g. ``"deploy.migrate"``.
-
-    The task will then be executed once per host in its host list, which is
-    (again) assembled in the same manner as CLI-specified tasks: drawing from
-    :option:`-H`, :ref:`env.hosts <hosts>`, the `~fabric.decorators.hosts` or
-    `~fabric.decorators.roles` decorators, and so forth.
-
-    ``host``, ``hosts``, ``role``, ``roles`` and ``exclude_hosts`` kwargs will
-    be stripped out of the final call, and used to set the task's host list, as
-    if they had been specified on the command line like e.g. ``fab
-    taskname:host=hostname``.
-
-    Any other arguments or keyword arguments will be passed verbatim into
-    ``task`` (the function itself -- not the ``@task`` decorator wrapping your
-    function!) when it is called, so ``execute(mytask, 'arg1',
-    kwarg1='value')`` will (once per host) invoke ``mytask('arg1',
-    kwarg1='value')``.
-
-    :returns:
-        a dictionary mapping host strings to the given task's return value for
-        that host's execution run. For example, ``execute(foo, hosts=['a',
-        'b'])`` might return ``{'a': None, 'b': 'bar'}`` if ``foo`` returned
-        nothing on host `a` but returned ``'bar'`` on host `b`.
-
-        In situations where a task execution fails for a given host but overall
-        progress does not abort (such as when :ref:`env.skip_bad_hosts
-        <skip-bad-hosts>` is True) the return value for that host will be the
-        error object or message.
-
-    .. seealso::
-        :ref:`The execute usage docs <execute>`, for an expanded explanation
-        and some examples.
-
-    .. versionadded:: 1.3
-    .. versionchanged:: 1.4
-        Added the return value mapping; previously this function had no defined
-        return value.
-    """
-    my_env = {'clean_revert': True}
-    results = {}
-    # Obtain task
-    is_callable = callable(task)
-    if not (is_callable or _is_task(task)):
-        # Assume string, set env.command to it
-        my_env['command'] = task
-        task = crawl(task, state.commands)
-        if task is None:
-            msg = "%r is not callable or a valid task name" % (my_env['command'],)
-            if state.env.get('skip_unknown_tasks', False):
-                warn(msg)
-                return
-            else:
-                abort(msg)
-    # Set env.command if we were given a real function or callable task obj
-    else:
-        dunder_name = getattr(task, '__name__', None)
-        my_env['command'] = getattr(task, 'name', dunder_name)
-    # Normalize to Task instance if we ended up with a regular callable
-    if not _is_task(task):
-        task = WrappedCallableTask(task)
-    # Filter out hosts/roles kwargs
-    new_kwargs, hosts, roles, exclude_hosts = parse_kwargs(kwargs)
-    # Set up host list
-    my_env['all_hosts'], my_env['effective_roles'] = task.get_hosts_and_effective_roles(hosts, roles,
-                                                                                        exclude_hosts, state.env)
-
-    parallel = requires_parallel(task)
-    if parallel:
-        # Import multiprocessing if needed, erroring out usefully
-        # if it can't.
-        try:
-            import multiprocessing
-        except ImportError:
-            import traceback
-            tb = traceback.format_exc()
-            abort(tb + """
-    At least one task needs to be run in parallel, but the
-    multiprocessing module cannot be imported (see above
-    traceback.) Please make sure the module is installed
-    or that the above ImportError is fixed.""")
-    else:
-        multiprocessing = None
-
-    # Get pool size for this task
-    pool_size = task.get_pool_size(my_env['all_hosts'], state.env.pool_size)
-    # Set up job queue in case parallel is needed
-    queue = multiprocessing.Queue() if parallel else None
-    jobs = JobQueue(pool_size, queue)
-    if state.output.debug:
-        jobs._debug = True
-
-    # Call on host list
-    if my_env['all_hosts']:
-        # Attempt to cycle on hosts, skipping if needed
-        for host in my_env['all_hosts']:
-            try:
-                results[host] = _execute(
-                    task, host, my_env, args, new_kwargs, jobs, queue,
-                    multiprocessing
-                )
-            except NetworkError, e:
-                results[host] = e
-                # Backwards compat test re: whether to use an exception or
-                # abort
-                if not state.env.use_exceptions_for['network']:
-                    func = warn if state.env.skip_bad_hosts else abort
-                    error(e.message, func=func, exception=e.wrapped)
-                else:
-                    raise
-
-            # If requested, clear out connections here and not just at the end.
-            if state.env.eagerly_disconnect:
-                disconnect_all()
-
-        # If running in parallel, block until job queue is emptied
-        if jobs:
-            err = "One or more hosts failed while executing task '%s'" % (
-                my_env['command']
-            )
-            jobs.close()
-            # Abort if any children did not exit cleanly (fail-fast).
-            # This prevents Fabric from continuing on to any other tasks.
-            # Otherwise, pull in results from the child run.
-            ran_jobs = jobs.run()
-            for name, d in ran_jobs.iteritems():
-                if d['exit_code'] != 0:
-                    if isinstance(d['results'], NetworkError) and \
-                            _is_network_error_ignored():
-                        error(d['results'].message, func=warn, exception=d['results'].wrapped)
-                    elif isinstance(d['results'], BaseException):
-                        error(err, exception=d['results'])
-                    else:
-                        error(err)
-                results[name] = d['results']
-
-    # Or just run once for local-only
-    else:
-        with settings(**my_env):
-            results['<local-only>'] = task.run(*args, **new_kwargs)
-    # Return what we can from the inner task executions
+        Performs minor extensions to `~invoke.tasks.Call` -- see its docstring
+        for most details. Only specific-to-subclass params are documented here.
 
-    return results
+        :param dict init_kwargs:
+            Keyword arguments used to create a new `.Connection` when the
+            wrapped task is executed. Default: ``None``.
+        """
+        init_kwargs = kwargs.pop("init_kwargs")  # , None)
+        super(ConnectionCall, self).__init__(*args, **kwargs)
+        self.init_kwargs = init_kwargs
+
+    def clone_kwargs(self):
+        # Extend superclass clone_kwargs to work in init_kwargs.
+        # TODO: this pattern comes up a lot; is there a better way to handle it
+        # without getting too crazy on the metaprogramming/over-engineering?
+        # Maybe something attrs library can help with (re: declaring "These are
+        # my bag-of-attributes attributes I want common stuff done to/with")
+        kwargs = super(ConnectionCall, self).clone_kwargs()
+        kwargs["init_kwargs"] = self.init_kwargs
+        return kwargs
+
+    def make_context(self, config):
+        kwargs = self.init_kwargs
+        # TODO: what about corner case of a decorator giving config in a hosts
+        # kwarg member?! For now let's stomp on it, and then if somebody runs
+        # into it, we can identify the use case & decide how best to deal.
+        kwargs["config"] = config
+        return Connection(**kwargs)
+
+    def __repr__(self):
+        ret = super(ConnectionCall, self).__repr__()
+        if self.init_kwargs:
+            ret = ret[:-1] + ", host='{}'>".format(self.init_kwargs["host"])
+        return ret
diff -Nru fabric-1.14.0/fabric/task_utils.py fabric-2.5.0/fabric/task_utils.py
--- fabric-1.14.0/fabric/task_utils.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/fabric/task_utils.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,97 +0,0 @@
-from fabric.utils import abort, indent
-from fabric import state
-
-
-# For attribute tomfoolery
-class _Dict(dict):
-    pass
-
-
-def _crawl(name, mapping):
-    """
-    ``name`` of ``'a.b.c'`` => ``mapping['a']['b']['c']``
-    """
-    key, _, rest = name.partition('.')
-    value = mapping[key]
-    if not rest:
-        return value
-    return _crawl(rest, value)
-
-
-def crawl(name, mapping):
-    try:
-        result = _crawl(name, mapping)
-        # Handle default tasks
-        if isinstance(result, _Dict):
-            if getattr(result, 'default', False):
-                result = result.default
-            # Ensure task modules w/ no default are treated as bad targets
-            else:
-                result = None
-        return result
-    except (KeyError, TypeError):
-        return None
-
-
-def merge(hosts, roles, exclude, roledefs):
-    """
-    Merge given host and role lists into one list of deduped hosts.
-    """
-    # Abort if any roles don't exist
-    bad_roles = [x for x in roles if x not in roledefs]
-    if bad_roles:
-        abort("The following specified roles do not exist:\n%s" % (
-            indent(bad_roles)
-        ))
-
-    # Coerce strings to one-item lists
-    if isinstance(hosts, basestring):
-        hosts = [hosts]
-
-    # Look up roles, turn into flat list of hosts
-    role_hosts = []
-    for role in roles:
-        value = roledefs[role]
-        # Handle dict style roledefs
-        if isinstance(value, dict):
-            value = value['hosts']
-        # Handle "lazy" roles (callables)
-        if callable(value):
-            value = value()
-        role_hosts += value
-
-    # Strip whitespace from host strings.
-    cleaned_hosts = [x.strip() for x in list(hosts) + list(role_hosts)]
-    # Return deduped combo of hosts and role_hosts, preserving order within
-    # them (vs using set(), which may lose ordering) and skipping hosts to be
-    # excluded.
-    # But only if the user hasn't indicated they want this behavior disabled.
-    all_hosts = cleaned_hosts
-    if state.env.dedupe_hosts:
-        deduped_hosts = []
-        for host in cleaned_hosts:
-            if host not in deduped_hosts and host not in exclude:
-                deduped_hosts.append(host)
-        all_hosts = deduped_hosts
-    return all_hosts
-
-
-def parse_kwargs(kwargs):
-    new_kwargs = {}
-    hosts = []
-    roles = []
-    exclude_hosts = []
-    for key, value in kwargs.iteritems():
-        if key == 'host':
-            hosts = [value]
-        elif key == 'hosts':
-            hosts = value
-        elif key == 'role':
-            roles = [value]
-        elif key == 'roles':
-            roles = value
-        elif key == 'exclude_hosts':
-            exclude_hosts = value
-        else:
-            new_kwargs[key] = value
-    return new_kwargs, hosts, roles, exclude_hosts
diff -Nru fabric-1.14.0/fabric/testing/base.py fabric-2.5.0/fabric/testing/base.py
--- fabric-1.14.0/fabric/testing/base.py	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/fabric/testing/base.py	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,402 @@
+"""
+This module contains helpers/fixtures to assist in testing Fabric-driven code.
+
+It is not intended for production use, and pulls in some test-oriented
+dependencies such as `mock <https://pypi.org/project/mock/>`_. You can install
+an 'extra' variant of Fabric to get these dependencies if you aren't already
+using them for your own testing purposes: ``pip install fabric[testing]``.
+
+.. note::
+    If you're using pytest for your test suite, you may be interested in
+    grabbing ``fabric[pytest]`` instead, which encompasses the dependencies of
+    both this module and the `fabric.testing.fixtures` module, which contains
+    pytest fixtures.
+
+.. versionadded:: 2.1
+"""
+
+from itertools import chain, repeat
+from io import BytesIO
+import os
+
+try:
+    from mock import Mock, PropertyMock, call, patch, ANY
+except ImportError:
+    import warnings
+
+    warning = (
+        "You appear to be missing some optional test-related dependencies;"
+        "please 'pip install fabric[testing]'."
+    )
+    warnings.warn(warning, ImportWarning)
+    raise
+
+
+class Command(object):
+    """
+    Data record specifying params of a command execution to mock/expect.
+
+    :param str cmd:
+        Command string to expect. If not given, no expectations about the
+        command executed will be set up. Default: ``None``.
+
+    :param bytes out: Data yielded as remote stdout. Default: ``b""``.
+
+    :param bytes err: Data yielded as remote stderr. Default: ``b""``.
+
+    :param int exit: Remote exit code. Default: ``0``.
+
+    :param int waits:
+        Number of calls to the channel's ``exit_status_ready`` that should
+        return ``False`` before it then returns ``True``. Default: ``0``
+        (``exit_status_ready`` will return ``True`` immediately).
+
+    .. versionadded:: 2.1
+    """
+
+    def __init__(self, cmd=None, out=b"", err=b"", in_=None, exit=0, waits=0):
+        self.cmd = cmd
+        self.out = out
+        self.err = err
+        self.in_ = in_
+        self.exit = exit
+        self.waits = waits
+
+    def __repr__(self):
+        # TODO: just leverage attrs, maybe vendored into Invoke so we don't
+        # grow more dependencies? Ehhh
+        return "<{} cmd={!r}>".format(self.__class__.__name__, self.cmd)
+
+
+class MockChannel(Mock):
+    """
+    Mock subclass that tracks state for its ``recv(_stderr)?`` methods.
+
+    Turns out abusing function closures inside MockRemote to track this state
+    only worked for 1 command per session!
+
+    .. versionadded:: 2.1
+    """
+
+    def __init__(self, *args, **kwargs):
+        # TODO: worth accepting strings and doing the BytesIO setup ourselves?
+        # Stored privately to avoid any possible collisions ever. shrug.
+        object.__setattr__(self, "__stdout", kwargs.pop("stdout"))
+        object.__setattr__(self, "__stderr", kwargs.pop("stderr"))
+        # Stdin less private so it can be asserted about
+        object.__setattr__(self, "_stdin", BytesIO())
+        super(MockChannel, self).__init__(*args, **kwargs)
+
+    def _get_child_mock(self, **kwargs):
+        # Don't return our own class on sub-mocks.
+        return Mock(**kwargs)
+
+    def recv(self, count):
+        return object.__getattribute__(self, "__stdout").read(count)
+
+    def recv_stderr(self, count):
+        return object.__getattribute__(self, "__stderr").read(count)
+
+    def sendall(self, data):
+        return object.__getattribute__(self, "_stdin").write(data)
+
+
+class Session(object):
+    """
+    A mock remote session of a single connection and 1 or more command execs.
+
+    Allows quick configuration of expected remote state, and also helps
+    generate the necessary test mocks used by `MockRemote` itself. Only useful
+    when handed into `MockRemote`.
+
+    The parameters ``cmd``, ``out``, ``err``, ``exit`` and ``waits`` are all
+    shorthand for the same constructor arguments for a single anonymous
+    `.Command`; see `.Command` for details.
+
+    To give fully explicit `.Command` objects, use the ``commands`` parameter.
+
+    :param str user:
+    :param str host:
+    :param int port:
+        Sets up expectations that a connection will be generated to the given
+        user, host and/or port. If ``None`` (default), no expectations are
+        generated / any value is accepted.
+
+    :param commands:
+        Iterable of `.Command` objects, used when mocking nontrivial sessions
+        involving >1 command execution per host. Default: ``None``.
+
+        .. note::
+            Giving ``cmd``, ``out`` etc alongside explicit ``commands`` is not
+            allowed and will result in an error.
+
+    .. versionadded:: 2.1
+    """
+
+    def __init__(
+        self,
+        host=None,
+        user=None,
+        port=None,
+        commands=None,
+        cmd=None,
+        out=None,
+        in_=None,
+        err=None,
+        exit=None,
+        waits=None,
+    ):
+        # Sanity check
+        params = cmd or out or err or exit or waits
+        if commands and params:
+            raise ValueError(
+                "You can't give both 'commands' and individual "
+                "Command parameters!"
+            )  # noqa
+        # Fill in values
+        self.host = host
+        self.user = user
+        self.port = port
+        self.commands = commands
+        if params:
+            # Honestly dunno which is dumber, this or duplicating Command's
+            # default kwarg values in this method's signature...sigh
+            kwargs = {}
+            if cmd is not None:
+                kwargs["cmd"] = cmd
+            if out is not None:
+                kwargs["out"] = out
+            if err is not None:
+                kwargs["err"] = err
+            if in_ is not None:
+                kwargs["in_"] = in_
+            if exit is not None:
+                kwargs["exit"] = exit
+            if waits is not None:
+                kwargs["waits"] = waits
+            self.commands = [Command(**kwargs)]
+        if not self.commands:
+            self.commands = [Command()]
+
+    def generate_mocks(self):
+        """
+        Mocks `~paramiko.client.SSHClient` and `~paramiko.channel.Channel`.
+
+        Specifically, the client will expect itself to be connected to
+        ``self.host`` (if given), the channels will be associated with the
+        client's `~paramiko.transport.Transport`, and the channels will
+        expect/provide command-execution behavior as specified on the
+        `.Command` objects supplied to this `.Session`.
+
+        The client is then attached as ``self.client`` and the channels as
+        ``self.channels``.
+
+        :returns:
+            ``None`` - this is mostly a "deferred setup" method and callers
+            will just reference the above attributes (and call more methods) as
+            needed.
+
+        .. versionadded:: 2.1
+        """
+        client = Mock()
+        transport = client.get_transport.return_value  # another Mock
+
+        # NOTE: this originally did chain([False], repeat(True)) so that
+        # get_transport().active was False initially, then True. However,
+        # because we also have to consider when get_transport() comes back None
+        # (which it does initially), the case where we get back a non-None
+        # transport _and_ it's not active yet, isn't useful to test, and
+        # complicates text expectations. So we don't, for now.
+        actives = repeat(True)
+        # NOTE: setting PropertyMocks on a mock's type() is apparently
+        # How It Must Be Done, otherwise it sets the real attr value.
+        type(transport).active = PropertyMock(side_effect=actives)
+
+        channels = []
+        for command in self.commands:
+            # Mock of a Channel instance, not e.g. Channel-the-class.
+            # Specifically, one that can track individual state for recv*().
+            channel = MockChannel(
+                stdout=BytesIO(command.out), stderr=BytesIO(command.err)
+            )
+            channel.recv_exit_status.return_value = command.exit
+
+            # If requested, make exit_status_ready return False the first N
+            # times it is called in the wait() loop.
+            readies = chain(repeat(False, command.waits), repeat(True))
+            channel.exit_status_ready.side_effect = readies
+
+            channels.append(channel)
+
+        # Have our transport yield those channel mocks in order when
+        # open_session() is called.
+        transport.open_session.side_effect = channels
+
+        self.client = client
+        self.channels = channels
+
+    def sanity_check(self):
+        # Per-session we expect a single transport get
+        transport = self.client.get_transport
+        transport.assert_called_once_with()
+        # And a single connect to our target host.
+        self.client.connect.assert_called_once_with(
+            username=self.user or ANY,
+            hostname=self.host or ANY,
+            port=self.port or ANY,
+        )
+
+        # Calls to open_session will be 1-per-command but are on transport, not
+        # channel, so we can only really inspect how many happened in
+        # aggregate. Save a list for later comparison to call_args.
+        session_opens = []
+
+        for channel, command in zip(self.channels, self.commands):
+            # Expect an open_session for each command exec
+            session_opens.append(call())
+            # Expect that the channel gets an exec_command
+            channel.exec_command.assert_called_with(command.cmd or ANY)
+            # Expect written stdin, if given
+            if command.in_:
+                assert channel._stdin.getvalue() == command.in_
+
+        # Make sure open_session was called expected number of times.
+        calls = transport.return_value.open_session.call_args_list
+        assert calls == session_opens
+
+
+class MockRemote(object):
+    """
+    Class representing mocked remote state.
+
+    By default this class is set up for start/stop style patching as opposed to
+    the more common context-manager or decorator approach; this is so it can be
+    used in situations requiring setup/teardown semantics.
+
+    Defaults to setting up a single anonymous `Session`, so it can be used as a
+    "request & forget" pytest fixture. Users requiring detailed remote session
+    expectations can call methods like `expect`, which wipe that anonymous
+    Session & set up a new one instead.
+
+    .. versionadded:: 2.1
+    """
+
+    def __init__(self):
+        self.expect_sessions(Session())
+
+    # TODO: make it easier to assume single session w/ >1 command?
+
+    def expect(self, *args, **kwargs):
+        """
+        Convenience method for creating & 'expect'ing a single `Session`.
+
+        Returns the single `MockChannel` yielded by that Session.
+
+        .. versionadded:: 2.1
+        """
+        return self.expect_sessions(Session(*args, **kwargs))[0]
+
+    def expect_sessions(self, *sessions):
+        """
+        Sets the mocked remote environment to expect the given ``sessions``.
+
+        Returns a list of `MockChannel` objects, one per input `Session`.
+
+        .. versionadded:: 2.1
+        """
+        # First, stop the default session to clean up its state, if it seems to
+        # be running.
+        self.stop()
+        # Update sessions list with new session(s)
+        self.sessions = sessions
+        # And start patching again, returning mocked channels
+        return self.start()
+
+    def start(self):
+        """
+        Start patching SSHClient with the stored sessions, returning channels.
+
+        .. versionadded:: 2.1
+        """
+        # Patch SSHClient so the sessions' generated mocks can be set as its
+        # return values
+        self.patcher = patcher = patch("fabric.connection.SSHClient")
+        SSHClient = patcher.start()
+        # Mock clients, to be inspected afterwards during sanity-checks
+        clients = []
+        for session in self.sessions:
+            session.generate_mocks()
+            clients.append(session.client)
+        # Each time the mocked SSHClient class is instantiated, it will
+        # yield one of our mocked clients (w/ mocked transport & channel)
+        # generated above.
+        SSHClient.side_effect = clients
+        return list(chain.from_iterable(x.channels for x in self.sessions))
+
+    def stop(self):
+        """
+        Stop patching SSHClient.
+
+        .. versionadded:: 2.1
+        """
+        # Short circuit if we don't seem to have start()ed yet.
+        if not hasattr(self, "patcher"):
+            return
+        # Stop patching SSHClient
+        self.patcher.stop()
+
+    def sanity(self):
+        """
+        Run post-execution sanity checks (usually 'was X called' tests.)
+
+        .. versionadded:: 2.1
+        """
+        for session in self.sessions:
+            # Basic sanity tests about transport, channel etc
+            session.sanity_check()
+
+
+# TODO: unify with the stuff in paramiko itself (now in its tests/conftest.py),
+# they're quite distinct and really shouldn't be.
+class MockSFTP(object):
+    """
+    Class managing mocked SFTP remote state.
+
+    Used in start/stop fashion in eg doctests; wrapped in the SFTP fixtures in
+    conftest.py for main use.
+
+    .. versionadded:: 2.1
+    """
+
+    def __init__(self, autostart=True):
+        if autostart:
+            self.start()
+
+    def start(self):
+        # Set up mocks
+        self.os_patcher = patch("fabric.transfer.os")
+        self.client_patcher = patch("fabric.connection.SSHClient")
+        mock_os = self.os_patcher.start()
+        Client = self.client_patcher.start()
+        sftp = Client.return_value.open_sftp.return_value
+
+        # Handle common filepath massage actions; tests will assume these.
+        def fake_abspath(path):
+            return "/local/{}".format(path)
+
+        mock_os.path.abspath.side_effect = fake_abspath
+        sftp.getcwd.return_value = "/remote"
+        # Ensure stat st_mode is a real number; Python 2 stat.S_IMODE doesn't
+        # appear to care if it's handed a MagicMock, but Python 3's does (?!)
+        fake_mode = 0o644  # arbitrary real-ish mode
+        sftp.stat.return_value.st_mode = fake_mode
+        mock_os.stat.return_value.st_mode = fake_mode
+        # Not super clear to me why the 'wraps' functionality in mock isn't
+        # working for this :(
+        mock_os.path.basename.side_effect = os.path.basename
+        # Return the sftp and OS mocks for use by decorator use case.
+        return sftp, mock_os
+
+    def stop(self):
+        self.os_patcher.stop()
+        self.client_patcher.stop()
diff -Nru fabric-1.14.0/fabric/testing/fixtures.py fabric-2.5.0/fabric/testing/fixtures.py
--- fabric-1.14.0/fabric/testing/fixtures.py	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/fabric/testing/fixtures.py	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,175 @@
+"""
+`pytest <https://pytest.org>`_ fixtures for easy use of Fabric test helpers.
+
+To get Fabric plus this module's dependencies (as well as those of the main
+`fabric.testing.base` module which these fixtures wrap), ``pip install
+fabric[pytest]``.
+
+The simplest way to get these fixtures loaded into your test suite so Pytest
+notices them is to import them into a ``conftest.py`` (`docs
+<http://pytest.readthedocs.io/en/latest/fixture.html#conftest-py-sharing-fixture-functions>`_).
+For example, if you intend to use the `remote` and `client` fixtures::
+
+    from fabric.testing.fixtures import client, remote
+
+.. versionadded:: 2.1
+"""
+
+try:
+    from pytest import fixture
+    from mock import patch, Mock
+except ImportError:
+    import warnings
+
+    warning = (
+        "You appear to be missing some optional test-related dependencies;"
+        "please 'pip install fabric[pytest]'."
+    )
+    warnings.warn(warning, ImportWarning)
+    raise
+
+from .. import Connection
+from ..transfer import Transfer
+
+# TODO: if we find a lot of people somehow ending up _with_ pytest but
+# _without_ mock and other deps from testing.base, consider doing the
+# try/except here too. But, really?
+
+from .base import MockRemote, MockSFTP
+
+
+@fixture
+def connection():
+    """
+    Yields a `.Connection` object with mocked methods.
+
+    Specifically:
+
+    - the hostname is set to ``"host"`` and the username to ``"user"``;
+    - the primary API members (`.Connection.run`, `.Connection.local`, etc) are
+      replaced with ``mock.Mock`` instances;
+    - the ``run.in_stream`` config option is set to ``False`` to avoid attempts
+      to read from stdin (which typically plays poorly with pytest and other
+      capturing test runners);
+
+    .. versionadded:: 2.1
+    """
+    c = Connection(host="host", user="user")
+    c.config.run.in_stream = False
+    c.run = Mock()
+    c.local = Mock()
+    # TODO: rest of API should get mocked too
+    # TODO: is there a nice way to mesh with MockRemote et al? Is that ever
+    # really that useful for code that just wants to assert about how run() and
+    # friends were called?
+    yield c
+
+
+#: A convenience rebinding of `connection`.
+#:
+#: .. versionadded:: 2.1
+cxn = connection
+
+
+@fixture
+def remote():
+    """
+    Fixture allowing setup of a mocked remote session & access to sub-mocks.
+
+    Yields a `.MockRemote` object (which may need to be updated via
+    `.MockRemote.expect`, `.MockRemote.expect_sessions`, etc; otherwise a
+    default session will be used) & calls `.MockRemote.sanity` and
+    `.MockRemote.stop` on teardown.
+
+    .. versionadded:: 2.1
+    """
+    remote = MockRemote()
+    yield remote
+    remote.sanity()
+    remote.stop()
+
+
+@fixture
+def sftp():
+    """
+    Fixture allowing setup of a mocked remote SFTP session.
+
+    Yields a 3-tuple of: Transfer() object, SFTPClient object, and mocked OS
+    module.
+
+    For many/most tests which only want the Transfer and/or SFTPClient objects,
+    see `sftp_objs` and `transfer` which wrap this fixture.
+
+    .. versionadded:: 2.1
+    """
+    mock = MockSFTP(autostart=False)
+    client, mock_os = mock.start()
+    transfer = Transfer(Connection("host"))
+    yield transfer, client, mock_os
+    # TODO: old mock_sftp() lacked any 'stop'...why? feels bad man
+
+
+@fixture
+def sftp_objs(sftp):
+    """
+    Wrapper for `sftp` which only yields the Transfer and SFTPClient.
+
+    .. versionadded:: 2.1
+    """
+    yield sftp[:2]
+
+
+@fixture
+def transfer(sftp):
+    """
+    Wrapper for `sftp` which only yields the Transfer object.
+
+    .. versionadded:: 2.1
+    """
+    yield sftp[0]
+
+
+@fixture
+def client():
+    """
+    Mocks `~paramiko.client.SSHClient` for testing calls to ``connect()``.
+
+    Yields a mocked ``SSHClient`` instance.
+
+    This fixture updates `~paramiko.client.SSHClient.get_transport` to return a
+    mock that appears active on first check, then inactive after, matching most
+    tests' needs by default:
+
+    - `.Connection` instantiates, with a None ``.transport``.
+    - Calls to ``.open()`` test ``.is_connected``, which returns ``False`` when
+      ``.transport`` is falsey, and so the first open will call
+      ``SSHClient.connect`` regardless.
+    - ``.open()`` then sets ``.transport`` to ``SSHClient.get_transport()``, so
+      ``Connection.transport`` is effectively
+      ``client.get_transport.return_value``.
+    - Subsequent activity will want to think the mocked SSHClient is
+      "connected", meaning we want the mocked transport's ``.active`` to be
+      ``True``.
+    - This includes `.Connection.close`, which short-circuits if
+      ``.is_connected``; having a statically ``True`` active flag means a full
+      open -> close cycle will run without error. (Only tests that double-close
+      or double-open should have issues here.)
+
+    End result is that:
+
+    - ``.is_connected`` behaves False after instantiation and before ``.open``,
+      then True after ``.open``
+    - ``.close`` will work normally on 1st call
+    - ``.close`` will behave "incorrectly" on subsequent calls (since it'll
+      think connection is still live.) Tests that check the idempotency of
+      ``.close`` will need to tweak their mock mid-test.
+
+    For 'full' fake remote session interaction (i.e. stdout/err
+    reading/writing, channel opens, etc) see `remote`.
+
+    .. versionadded:: 2.1
+    """
+    with patch("fabric.connection.SSHClient") as SSHClient:
+        client = SSHClient.return_value
+        client.get_transport.return_value = Mock(active=True)
+        yield client
diff -Nru fabric-1.14.0/fabric/thread_handling.py fabric-2.5.0/fabric/thread_handling.py
--- fabric-1.14.0/fabric/thread_handling.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/fabric/thread_handling.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,25 +0,0 @@
-import threading
-import sys
-
-
-class ThreadHandler(object):
-    def __init__(self, name, callable, *args, **kwargs):
-        # Set up exception handling
-        self.exception = None
-
-        def wrapper(*args, **kwargs):
-            try:
-                callable(*args, **kwargs)
-            except BaseException:
-                self.exception = sys.exc_info()
-        # Kick off thread
-        thread = threading.Thread(None, wrapper, name, args, kwargs)
-        thread.setDaemon(True)
-        thread.start()
-        # Make thread available to instantiator
-        self.thread = thread
-
-    def raise_if_needed(self):
-        if self.exception:
-            e = self.exception
-            raise e[0], e[1], e[2]
diff -Nru fabric-1.14.0/fabric/transfer.py fabric-2.5.0/fabric/transfer.py
--- fabric-1.14.0/fabric/transfer.py	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/fabric/transfer.py	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,324 @@
+"""
+File transfer via SFTP and/or SCP.
+"""
+
+import os
+import posixpath
+import stat
+
+from .util import debug  # TODO: actual logging! LOL
+
+# TODO: figure out best way to direct folks seeking rsync, to patchwork's rsync
+# call (which needs updating to use invoke.run() & fab 2 connection methods,
+# but is otherwise suitable).
+# UNLESS we want to try and shoehorn it into this module after all? Delegate
+# any recursive get/put to it? Requires users to have rsync available of
+# course.
+
+
+class Transfer(object):
+    """
+    `.Connection`-wrapping class responsible for managing file upload/download.
+
+    .. versionadded:: 2.0
+    """
+
+    # TODO: SFTP clear default, but how to do SCP? subclass? init kwarg?
+
+    def __init__(self, connection):
+        self.connection = connection
+
+    @property
+    def sftp(self):
+        return self.connection.sftp()
+
+    def is_remote_dir(self, path):
+        try:
+            return stat.S_ISDIR(self.sftp.stat(path).st_mode)
+        except IOError:
+            return False
+
+    def get(self, remote, local=None, preserve_mode=True):
+        """
+        Download a file from the current connection to the local filesystem.
+
+        :param str remote:
+            Remote file to download.
+
+            May be absolute, or relative to the remote working directory.
+
+            .. note::
+                Most SFTP servers set the remote working directory to the
+                connecting user's home directory, and (unlike most shells) do
+                *not* expand tildes (``~``).
+
+                For example, instead of saying ``get("~/tmp/archive.tgz")``,
+                say ``get("tmp/archive.tgz")``.
+
+        :param local:
+            Local path to store downloaded file in, or a file-like object.
+
+            **If None or another 'falsey'/empty value is given** (the default),
+            the remote file is downloaded to the current working directory (as
+            seen by `os.getcwd`) using its remote filename.
+
+            **If a string is given**, it should be a path to a local directory
+            or file and is subject to similar behavior as that seen by common
+            Unix utilities or OpenSSH's ``sftp`` or ``scp`` tools.
+
+            For example, if the local path is a directory, the remote path's
+            base filename will be added onto it (so ``get('foo/bar/file.txt',
+            '/tmp/')`` would result in creation or overwriting of
+            ``/tmp/file.txt``).
+
+            .. note::
+                When dealing with nonexistent file paths, normal Python file
+                handling concerns come into play - for example, a ``local``
+                path containing non-leaf directories which do not exist, will
+                typically result in an `OSError`.
+
+            **If a file-like object is given**, the contents of the remote file
+            are simply written into it.
+
+        :param bool preserve_mode:
+            Whether to `os.chmod` the local file so it matches the remote
+            file's mode (default: ``True``).
+
+        :returns: A `.Result` object.
+
+        .. versionadded:: 2.0
+        """
+        # TODO: how does this API change if we want to implement
+        # remote-to-remote file transfer? (Is that even realistic?)
+        # TODO: handle v1's string interpolation bits, especially the default
+        # one, or at least think about how that would work re: split between
+        # single and multiple server targets.
+        # TODO: callback support
+        # TODO: how best to allow changing the behavior/semantics of
+        # remote/local (e.g. users might want 'safer' behavior that complains
+        # instead of overwriting existing files) - this likely ties into the
+        # "how to handle recursive/rsync" and "how to handle scp" questions
+
+        # Massage remote path
+        if not remote:
+            raise ValueError("Remote path must not be empty!")
+        orig_remote = remote
+        remote = posixpath.join(
+            self.sftp.getcwd() or self.sftp.normalize("."), remote
+        )
+
+        # Massage local path:
+        # - handle file-ness
+        # - if path, fill with remote name if empty, & make absolute
+        orig_local = local
+        is_file_like = hasattr(local, "write") and callable(local.write)
+        if not local:
+            local = posixpath.basename(remote)
+        if not is_file_like:
+            local = os.path.abspath(local)
+
+        # Run Paramiko-level .get() (side-effects only. womp.)
+        # TODO: push some of the path handling into Paramiko; it should be
+        # responsible for dealing with path cleaning etc.
+        # TODO: probably preserve warning message from v1 when overwriting
+        # existing files. Use logging for that obviously.
+        #
+        # If local appears to be a file-like object, use sftp.getfo, not get
+        if is_file_like:
+            self.sftp.getfo(remotepath=remote, fl=local)
+        else:
+            self.sftp.get(remotepath=remote, localpath=local)
+            # Set mode to same as remote end
+            # TODO: Push this down into SFTPClient sometime (requires backwards
+            # incompat release.)
+            if preserve_mode:
+                remote_mode = self.sftp.stat(remote).st_mode
+                mode = stat.S_IMODE(remote_mode)
+                os.chmod(local, mode)
+        # Return something useful
+        return Result(
+            orig_remote=orig_remote,
+            remote=remote,
+            orig_local=orig_local,
+            local=local,
+            connection=self.connection,
+        )
+
+    def put(self, local, remote=None, preserve_mode=True):
+        """
+        Upload a file from the local filesystem to the current connection.
+
+        :param local:
+            Local path of file to upload, or a file-like object.
+
+            **If a string is given**, it should be a path to a local (regular)
+            file (not a directory).
+
+            .. note::
+                When dealing with nonexistent file paths, normal Python file
+                handling concerns come into play - for example, trying to
+                upload a nonexistent ``local`` path will typically result in an
+                `OSError`.
+
+            **If a file-like object is given**, its contents are written to the
+            remote file path.
+
+        :param str remote:
+            Remote path to which the local file will be written.
+
+            .. note::
+                Most SFTP servers set the remote working directory to the
+                connecting user's home directory, and (unlike most shells) do
+                *not* expand tildes (``~``).
+
+                For example, instead of saying ``put("archive.tgz",
+                "~/tmp/")``, say ``put("archive.tgz", "tmp/")``.
+
+                In addition, this means that 'falsey'/empty values (such as the
+                default value, ``None``) are allowed and result in uploading to
+                the remote home directory.
+
+            .. note::
+                When ``local`` is a file-like object, ``remote`` is required
+                and must refer to a valid file path (not a directory).
+
+        :param bool preserve_mode:
+            Whether to ``chmod`` the remote file so it matches the local file's
+            mode (default: ``True``).
+
+        :returns: A `.Result` object.
+
+        .. versionadded:: 2.0
+        """
+        if not local:
+            raise ValueError("Local path must not be empty!")
+
+        is_file_like = hasattr(local, "write") and callable(local.write)
+
+        # Massage remote path
+        orig_remote = remote
+        if is_file_like:
+            local_base = getattr(local, "name", None)
+        else:
+            local_base = os.path.basename(local)
+        if not remote:
+            if is_file_like:
+                raise ValueError(
+                    "Must give non-empty remote path when local is a file-like object!"  # noqa
+                )
+            else:
+                remote = local_base
+                debug("Massaged empty remote path into {!r}".format(remote))
+        elif self.is_remote_dir(remote):
+            # non-empty local_base implies a) text file path or b) FLO which
+            # had a non-empty .name attribute. huzzah!
+            if local_base:
+                remote = posixpath.join(remote, local_base)
+            else:
+                if is_file_like:
+                    raise ValueError(
+                        "Can't put a file-like-object into a directory unless it has a non-empty .name attribute!"  # noqa
+                    )
+                else:
+                    # TODO: can we ever really end up here? implies we want to
+                    # reorganize all this logic so it has fewer potential holes
+                    raise ValueError(
+                        "Somehow got an empty local file basename ({!r}) when uploading to a directory ({!r})!".format(  # noqa
+                            local_base, remote
+                        )
+                    )
+
+        prejoined_remote = remote
+        remote = posixpath.join(
+            self.sftp.getcwd() or self.sftp.normalize("."), remote
+        )
+        if remote != prejoined_remote:
+            msg = "Massaged relative remote path {!r} into {!r}"
+            debug(msg.format(prejoined_remote, remote))
+
+        # Massage local path
+        orig_local = local
+        if not is_file_like:
+            local = os.path.abspath(local)
+            if local != orig_local:
+                debug(
+                    "Massaged relative local path {!r} into {!r}".format(
+                        orig_local, local
+                    )
+                )  # noqa
+
+        # Run Paramiko-level .put() (side-effects only. womp.)
+        # TODO: push some of the path handling into Paramiko; it should be
+        # responsible for dealing with path cleaning etc.
+        # TODO: probably preserve warning message from v1 when overwriting
+        # existing files. Use logging for that obviously.
+        #
+        # If local appears to be a file-like object, use sftp.putfo, not put
+        if is_file_like:
+            msg = "Uploading file-like object {!r} to {!r}"
+            debug(msg.format(local, remote))
+            pointer = local.tell()
+            try:
+                local.seek(0)
+                self.sftp.putfo(fl=local, remotepath=remote)
+            finally:
+                local.seek(pointer)
+        else:
+            debug("Uploading {!r} to {!r}".format(local, remote))
+            self.sftp.put(localpath=local, remotepath=remote)
+            # Set mode to same as local end
+            # TODO: Push this down into SFTPClient sometime (requires backwards
+            # incompat release.)
+            if preserve_mode:
+                local_mode = os.stat(local).st_mode
+                mode = stat.S_IMODE(local_mode)
+                self.sftp.chmod(remote, mode)
+        # Return something useful
+        return Result(
+            orig_remote=orig_remote,
+            remote=remote,
+            orig_local=orig_local,
+            local=local,
+            connection=self.connection,
+        )
+
+
+class Result(object):
+    """
+    A container for information about the result of a file transfer.
+
+    See individual attribute/method documentation below for details.
+
+    .. note::
+        Unlike similar classes such as `invoke.runners.Result` or
+        `fabric.runners.Result` (which have a concept of "warn and return
+        anyways on failure") this class has no useful truthiness behavior. If a
+        file transfer fails, some exception will be raised, either an `OSError`
+        or an error from within Paramiko.
+
+    .. versionadded:: 2.0
+    """
+
+    # TODO: how does this differ from put vs get? field stating which? (feels
+    # meh) distinct classes differing, for now, solely by name? (also meh)
+    def __init__(self, local, orig_local, remote, orig_remote, connection):
+        #: The local path the file was saved as, or the object it was saved
+        #: into if a file-like object was given instead.
+        #:
+        #: If a string path, this value is massaged to be absolute; see
+        #: `.orig_local` for the original argument value.
+        self.local = local
+        #: The original value given as the returning method's ``local``
+        #: argument.
+        self.orig_local = orig_local
+        #: The remote path downloaded from. Massaged to be absolute; see
+        #: `.orig_remote` for the original argument value.
+        self.remote = remote
+        #: The original argument value given as the returning method's
+        #: ``remote`` argument.
+        self.orig_remote = orig_remote
+        #: The `.Connection` object this result was obtained from.
+        self.connection = connection
+
+    # TODO: ensure str/repr makes it easily differentiable from run() or
+    # local() result objects (and vice versa).
diff -Nru fabric-1.14.0/fabric/tunnels.py fabric-2.5.0/fabric/tunnels.py
--- fabric-1.14.0/fabric/tunnels.py	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/fabric/tunnels.py	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,157 @@
+"""
+Tunnel and connection forwarding internals.
+
+If you're looking for simple, end-user-focused connection forwarding, please
+see `.Connection`, e.g. `.Connection.forward_local`.
+"""
+
+import errno
+import select
+import socket
+import time
+from threading import Event
+
+from invoke.exceptions import ThreadException
+from invoke.util import ExceptionHandlingThread
+
+
+class TunnelManager(ExceptionHandlingThread):
+    """
+    Thread subclass for tunnelling connections over SSH between two endpoints.
+
+    Specifically, one instance of this class is sufficient to sit around
+    forwarding any number of individual connections made to one end of the
+    tunnel or the other. If you need to forward connections between more than
+    one set of ports, you'll end up instantiating multiple TunnelManagers.
+
+    Wraps a `~paramiko.transport.Transport`, which should already be connected
+    to the remote server.
+
+    .. versionadded:: 2.0
+    """
+
+    def __init__(
+        self,
+        local_host,
+        local_port,
+        remote_host,
+        remote_port,
+        transport,
+        finished,
+    ):
+        super(TunnelManager, self).__init__()
+        self.local_address = (local_host, local_port)
+        self.remote_address = (remote_host, remote_port)
+        self.transport = transport
+        self.finished = finished
+
+    def _run(self):
+        # Track each tunnel that gets opened during our lifetime
+        tunnels = []
+
+        # Set up OS-level listener socket on forwarded port
+        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
+        # TODO: why do we want REUSEADDR exactly? and is it portable?
+        sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
+        # NOTE: choosing to deal with nonblocking semantics and a fast loop,
+        # versus an older approach which blocks & expects outer scope to cause
+        # a socket exception by close()ing the socket.
+        sock.setblocking(0)
+        sock.bind(self.local_address)
+        sock.listen(1)
+
+        while not self.finished.is_set():
+            # Main loop-wait: accept connections on the local listener
+            # NOTE: EAGAIN means "you're nonblocking and nobody happened to
+            # connect at this point in time"
+            try:
+                tun_sock, local_addr = sock.accept()
+                # Set TCP_NODELAY to match OpenSSH's forwarding socket behavior
+                tun_sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)
+            except socket.error as e:
+                if e.errno is errno.EAGAIN:
+                    # TODO: make configurable
+                    time.sleep(0.01)
+                    continue
+                raise
+
+            # Set up direct-tcpip channel on server end
+            # TODO: refactor w/ what's used for gateways
+            channel = self.transport.open_channel(
+                "direct-tcpip", self.remote_address, local_addr
+            )
+
+            # Set up 'worker' thread for this specific connection to our
+            # tunnel, plus its dedicated signal event (which will appear as a
+            # public attr, no need to track both independently).
+            finished = Event()
+            tunnel = Tunnel(channel=channel, sock=tun_sock, finished=finished)
+            tunnel.start()
+            tunnels.append(tunnel)
+
+        exceptions = []
+        # Propogate shutdown signal to all tunnels & wait for closure
+        # TODO: would be nice to have some output or at least logging here,
+        # especially for "sets up a handful of tunnels" use cases like
+        # forwarding nontrivial HTTP traffic.
+        for tunnel in tunnels:
+            tunnel.finished.set()
+            tunnel.join()
+            wrapper = tunnel.exception()
+            if wrapper:
+                exceptions.append(wrapper)
+        # Handle exceptions
+        if exceptions:
+            raise ThreadException(exceptions)
+
+        # All we have left to close is our own sock.
+        # TODO: use try/finally?
+        sock.close()
+
+
+class Tunnel(ExceptionHandlingThread):
+    """
+    Bidirectionally forward data between an SSH channel and local socket.
+
+    .. versionadded:: 2.0
+    """
+
+    def __init__(self, channel, sock, finished):
+        self.channel = channel
+        self.sock = sock
+        self.finished = finished
+        self.socket_chunk_size = 1024
+        self.channel_chunk_size = 1024
+        super(Tunnel, self).__init__()
+
+    def _run(self):
+        try:
+            empty_sock, empty_chan = None, None
+            while not self.finished.is_set():
+                r, w, x = select.select([self.sock, self.channel], [], [], 1)
+                if self.sock in r:
+                    empty_sock = self.read_and_write(
+                        self.sock, self.channel, self.socket_chunk_size
+                    )
+                if self.channel in r:
+                    empty_chan = self.read_and_write(
+                        self.channel, self.sock, self.channel_chunk_size
+                    )
+                if empty_sock or empty_chan:
+                    break
+        finally:
+            self.channel.close()
+            self.sock.close()
+
+    def read_and_write(self, reader, writer, chunk_size):
+        """
+        Read ``chunk_size`` from ``reader``, writing result to ``writer``.
+
+        Returns ``None`` if successful, or ``True`` if the read was empty.
+
+        .. versionadded:: 2.0
+        """
+        data = reader.recv(chunk_size)
+        if len(data) == 0:
+            return True
+        writer.sendall(data)
diff -Nru fabric-1.14.0/fabric/util.py fabric-2.5.0/fabric/util.py
--- fabric-1.14.0/fabric/util.py	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/fabric/util.py	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,45 @@
+import logging
+import sys
+
+
+# Ape the half-assed logging junk from Invoke, but ensuring the logger reflects
+# our name, not theirs. (Assume most contexts will rely on Invoke itself to
+# literally enable/disable logging, for now.)
+log = logging.getLogger("fabric")
+for x in ("debug",):
+    globals()[x] = getattr(log, x)
+
+
+win32 = sys.platform == "win32"
+
+
+def get_local_user():
+    """
+    Return the local executing username, or ``None`` if one can't be found.
+
+    .. versionadded:: 2.0
+    """
+    # TODO: I don't understand why these lines were added outside the
+    # try/except, since presumably it means the attempt at catching ImportError
+    # wouldn't work. However, that's how the contributing user committed it.
+    # Need an older Windows box to test it out, most likely.
+    import getpass
+
+    username = None
+    # All Unix and most Windows systems support the getpass module.
+    try:
+        username = getpass.getuser()
+    # Some SaaS platforms raise KeyError, implying there is no real user
+    # involved. They get the default value of None.
+    except KeyError:
+        pass
+    # Older (?) Windows systems don't support getpass well; they should
+    # have the `win32` module instead.
+    except ImportError:  # pragma: nocover
+        if win32:
+            import win32api
+            import win32security  # noqa
+            import win32profile  # noqa
+
+            username = win32api.GetUserName()
+    return username
diff -Nru fabric-1.14.0/fabric/utils.py fabric-2.5.0/fabric/utils.py
--- fabric-1.14.0/fabric/utils.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/fabric/utils.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,416 +0,0 @@
-"""
-Internal subroutines for e.g. aborting execution with an error message,
-or performing indenting on multiline output.
-"""
-import os
-import sys
-import textwrap
-from traceback import format_exc
-
-
-def _encode(msg, stream):
-    if isinstance(msg, unicode) and hasattr(stream, 'encoding') and not stream.encoding is None:
-        return msg.encode(stream.encoding)
-    else:
-        return str(msg)
-
-
-def isatty(stream):
-    """Check if a stream is a tty.
-
-    Not all file-like objects implement the `isatty` method.
-    """
-    fn = getattr(stream, 'isatty', None)
-    if fn is None:
-        return False
-    return fn()
-
-
-def abort(msg):
-    """
-    Abort execution, print ``msg`` to stderr and exit with error status (1.)
-
-    This function currently makes use of `SystemExit`_ in a manner that is
-    similar to `sys.exit`_ (but which skips the automatic printing to stderr,
-    allowing us to more tightly control it via settings).
-
-    Therefore, it's possible to detect and recover from inner calls to `abort`
-    by using ``except SystemExit`` or similar.
-
-    .. _sys.exit: http://docs.python.org/library/sys.html#sys.exit
-    .. _SystemExit: http://docs.python.org/library/exceptions.html#exceptions.SystemExit
-    """
-    from fabric.state import output, env
-    if not env.colorize_errors:
-        red  = lambda x: x
-    else:
-        from colors import red
-
-    if output.aborts:
-        sys.stderr.write(red("\nFatal error: %s\n" % _encode(msg, sys.stderr)))
-        sys.stderr.write(red("\nAborting.\n"))
-
-    if env.abort_exception:
-        raise env.abort_exception(msg)
-    else:
-        # See issue #1318 for details on the below; it lets us construct a
-        # valid, useful SystemExit while sidestepping the automatic stderr
-        # print (which would otherwise duplicate with the above in a
-        # non-controllable fashion).
-        e = SystemExit(1)
-        e.message = msg
-        raise e
-
-
-def warn(msg):
-    """
-    Print warning message, but do not abort execution.
-
-    This function honors Fabric's :doc:`output controls
-    <../../usage/output_controls>` and will print the given ``msg`` to stderr,
-    provided that the ``warnings`` output level (which is active by default) is
-    turned on.
-    """
-    from fabric.state import output, env
-
-    if not env.colorize_errors:
-        magenta = lambda x: x
-    else:
-        from colors import magenta
-
-    if output.warnings:
-        msg = _encode(msg, sys.stderr)
-        sys.stderr.write(magenta("\nWarning: %s\n\n" % msg))
-
-
-def indent(text, spaces=4, strip=False):
-    """
-    Return ``text`` indented by the given number of spaces.
-
-    If text is not a string, it is assumed to be a list of lines and will be
-    joined by ``\\n`` prior to indenting.
-
-    When ``strip`` is ``True``, a minimum amount of whitespace is removed from
-    the left-hand side of the given string (so that relative indents are
-    preserved, but otherwise things are left-stripped). This allows you to
-    effectively "normalize" any previous indentation for some inputs.
-    """
-    # Normalize list of strings into a string for dedenting. "list" here means
-    # "not a string" meaning "doesn't have splitlines". Meh.
-    if not hasattr(text, 'splitlines'):
-        text = '\n'.join(text)
-    # Dedent if requested
-    if strip:
-        text = textwrap.dedent(text)
-    prefix = ' ' * spaces
-    output = '\n'.join(prefix + line for line in text.splitlines())
-    # Strip out empty lines before/aft
-    output = output.strip()
-    # Reintroduce first indent (which just got stripped out)
-    output = prefix + output
-    return output
-
-
-def puts(text, show_prefix=None, end="\n", flush=False):
-    """
-    An alias for ``print`` whose output is managed by Fabric's output controls.
-
-    In other words, this function simply prints to ``sys.stdout``, but will
-    hide its output if the ``user`` :doc:`output level
-    </usage/output_controls>` is set to ``False``.
-
-    If ``show_prefix=False``, `puts` will omit the leading ``[hostname]``
-    which it tacks on by default. (It will also omit this prefix if
-    ``env.host_string`` is empty.)
-
-    Newlines may be disabled by setting ``end`` to the empty string (``''``).
-    (This intentionally mirrors Python 3's ``print`` syntax.)
-
-    You may force output flushing (e.g. to bypass output buffering) by setting
-    ``flush=True``.
-
-    .. versionadded:: 0.9.2
-    .. seealso:: `~fabric.utils.fastprint`
-    """
-    from fabric.state import output, env
-    if show_prefix is None:
-        show_prefix = env.output_prefix
-    if output.user:
-        prefix = ""
-        if env.host_string and show_prefix:
-            prefix = "[%s] " % env.host_string
-        sys.stdout.write(prefix + _encode(text, sys.stdout) + end)
-        if flush:
-            sys.stdout.flush()
-
-
-def fastprint(text, show_prefix=False, end="", flush=True):
-    """
-    Print ``text`` immediately, without any prefix or line ending.
-
-    This function is simply an alias of `~fabric.utils.puts` with different
-    default argument values, such that the ``text`` is printed without any
-    embellishment and immediately flushed.
-
-    It is useful for any situation where you wish to print text which might
-    otherwise get buffered by Python's output buffering (such as within a
-    processor intensive ``for`` loop). Since such use cases typically also
-    require a lack of line endings (such as printing a series of dots to
-    signify progress) it also omits the traditional newline by default.
-
-    .. note::
-
-        Since `~fabric.utils.fastprint` calls `~fabric.utils.puts`, it is
-        likewise subject to the ``user`` :doc:`output level
-        </usage/output_controls>`.
-
-    .. versionadded:: 0.9.2
-    .. seealso:: `~fabric.utils.puts`
-    """
-    return puts(text=text, show_prefix=show_prefix, end=end, flush=flush)
-
-
-def handle_prompt_abort(prompt_for):
-    import fabric.state
-    reason = "Needed to prompt for %s (host: %s), but %%s" % (
-        prompt_for, fabric.state.env.host_string
-    )
-    # Explicit "don't prompt me bro"
-    if fabric.state.env.abort_on_prompts:
-        abort(reason % "abort-on-prompts was set to True")
-    # Implicit "parallel == stdin/prompts have ambiguous target"
-    if fabric.state.env.parallel:
-        abort(reason % "input would be ambiguous in parallel mode")
-
-
-class _AttributeDict(dict):
-    """
-    Dictionary subclass enabling attribute lookup/assignment of keys/values.
-
-    For example::
-
-        >>> m = _AttributeDict({'foo': 'bar'})
-        >>> m.foo
-        'bar'
-        >>> m.foo = 'not bar'
-        >>> m['foo']
-        'not bar'
-
-    ``_AttributeDict`` objects also provide ``.first()`` which acts like
-    ``.get()`` but accepts multiple keys as arguments, and returns the value of
-    the first hit, e.g.::
-
-        >>> m = _AttributeDict({'foo': 'bar', 'biz': 'baz'})
-        >>> m.first('wrong', 'incorrect', 'foo', 'biz')
-        'bar'
-
-    """
-    def __getattr__(self, key):
-        try:
-            return self[key]
-        except KeyError:
-            # to conform with __getattr__ spec
-            raise AttributeError(key)
-
-    def __setattr__(self, key, value):
-        self[key] = value
-
-    def first(self, *names):
-        for name in names:
-            value = self.get(name)
-            if value:
-                return value
-
-
-class _AliasDict(_AttributeDict):
-    """
-    `_AttributeDict` subclass that allows for "aliasing" of keys to other keys.
-
-    Upon creation, takes an ``aliases`` mapping, which should map alias names
-    to lists of key names. Aliases do not store their own value, but instead
-    set (override) all mapped keys' values. For example, in the following
-    `_AliasDict`, calling ``mydict['foo'] = True`` will set the values of
-    ``mydict['bar']``, ``mydict['biz']`` and ``mydict['baz']`` all to True::
-
-        mydict = _AliasDict(
-            {'biz': True, 'baz': False},
-            aliases={'foo': ['bar', 'biz', 'baz']}
-        )
-
-    Because it is possible for the aliased values to be in a heterogenous
-    state, reading aliases is not supported -- only writing to them is allowed.
-    This also means they will not show up in e.g. ``dict.keys()``.
-
-    ..note::
-
-        Aliases are recursive, so you may refer to an alias within the key list
-        of another alias. Naturally, this means that you can end up with
-        infinite loops if you're not careful.
-
-    `_AliasDict` provides a special function, `expand_aliases`, which will take
-    a list of keys as an argument and will return that list of keys with any
-    aliases expanded. This function will **not** dedupe, so any aliases which
-    overlap will result in duplicate keys in the resulting list.
-    """
-    def __init__(self, arg=None, aliases=None):
-        init = super(_AliasDict, self).__init__
-        if arg is not None:
-            init(arg)
-        else:
-            init()
-        # Can't use super() here because of _AttributeDict's setattr override
-        dict.__setattr__(self, 'aliases', aliases)
-
-    def __setitem__(self, key, value):
-        # Attr test required to not blow up when deepcopy'd
-        if hasattr(self, 'aliases') and key in self.aliases:
-            for aliased in self.aliases[key]:
-                self[aliased] = value
-        else:
-            return super(_AliasDict, self).__setitem__(key, value)
-
-    def expand_aliases(self, keys):
-        ret = []
-        for key in keys:
-            if key in self.aliases:
-                ret.extend(self.expand_aliases(self.aliases[key]))
-            else:
-                ret.append(key)
-        return ret
-
-
-def _pty_size():
-    """
-    Obtain (rows, cols) tuple for sizing a pty on the remote end.
-
-    Defaults to 80x24 (which is also the 'ssh' lib's default) but will detect
-    local (stdout-based) terminal window size on non-Windows platforms.
-    """
-    from fabric.state import win32
-    if not win32:
-        import fcntl
-        import termios
-        import struct
-
-    default_rows, default_cols = 24, 80
-    rows, cols = default_rows, default_cols
-    if not win32 and isatty(sys.stdout):
-        # We want two short unsigned integers (rows, cols)
-        fmt = 'HH'
-        # Create an empty (zeroed) buffer for ioctl to map onto. Yay for C!
-        buffer = struct.pack(fmt, 0, 0)
-        # Call TIOCGWINSZ to get window size of stdout, returns our filled
-        # buffer
-        try:
-            result = fcntl.ioctl(sys.stdout.fileno(), termios.TIOCGWINSZ,
-                buffer)
-            # Unpack buffer back into Python data types
-            rows, cols = struct.unpack(fmt, result)
-            # Fall back to defaults if TIOCGWINSZ returns unreasonable values
-            if rows == 0:
-                rows = default_rows
-            if cols == 0:
-                cols = default_cols
-        # Deal with e.g. sys.stdout being monkeypatched, such as in testing.
-        # Or termios not having a TIOCGWINSZ.
-        except AttributeError:
-            pass
-    return rows, cols
-
-
-def error(message, func=None, exception=None, stdout=None, stderr=None):
-    """
-    Call ``func`` with given error ``message``.
-
-    If ``func`` is None (the default), the value of ``env.warn_only``
-    determines whether to call ``abort`` or ``warn``.
-
-    If ``exception`` is given, it is inspected to get a string message, which
-    is printed alongside the user-generated ``message``.
-
-    If ``stdout`` and/or ``stderr`` are given, they are assumed to be strings
-    to be printed.
-    """
-    import fabric.state
-    if func is None:
-        func = fabric.state.env.warn_only and warn or abort
-    # If exception printing is on, append a traceback to the message
-    if fabric.state.output.exceptions or fabric.state.output.debug:
-        exception_message = format_exc()
-        if exception_message:
-            message += "\n\n" + exception_message
-    # Otherwise, if we were given an exception, append its contents.
-    elif exception is not None:
-        # Figure out how to get a string out of the exception; EnvironmentError
-        # subclasses, for example, "are" integers and .strerror is the string.
-        # Others "are" strings themselves. May have to expand this further for
-        # other error types.
-        if hasattr(exception, 'strerror') and exception.strerror is not None:
-            underlying = exception.strerror
-        else:
-            underlying = exception
-        message += "\n\nUnderlying exception:\n" + indent(str(underlying))
-    if func is abort:
-        if stdout and not fabric.state.output.stdout:
-            message += _format_error_output("Standard output", stdout)
-        if stderr and not fabric.state.output.stderr:
-            message += _format_error_output("Standard error", stderr)
-    return func(message)
-
-
-def _format_error_output(header, body):
-    term_width = _pty_size()[1]
-    header_side_length = (term_width - (len(header) + 2)) / 2
-    mark = "="
-    side = mark * header_side_length
-    return "\n\n%s %s %s\n\n%s\n\n%s" % (
-        side, header, side, body, mark * term_width
-    )
-
-
-# TODO: replace with collections.deque(maxlen=xxx) in Python 2.6
-class RingBuffer(list):
-    def __init__(self, value, maxlen):
-        # Because it's annoying typing this multiple times.
-        self._super = super(RingBuffer, self)
-        # Python 2.6 deque compatible option name!
-        self._maxlen = maxlen
-        return self._super.__init__(value)
-
-    def _trim(self):
-        if self._maxlen is None:
-            return
-        overage = max(len(self) - self._maxlen, 0)
-        del self[0:overage]
-
-    def append(self, value):
-        self._super.append(value)
-        self._trim()
-
-    def extend(self, values):
-        self._super.extend(values)
-        self._trim()
-
-    def __iadd__(self, other):
-        self.extend(other)
-        return self
-
-    # Paranoia from here on out.
-    def insert(self, index, value):
-        raise ValueError("Can't insert into the middle of a ring buffer!")
-
-    def __setslice__(self, i, j, sequence):
-        raise ValueError("Can't set a slice of a ring buffer!")
-
-    def __setitem__(self, key, value):
-        if isinstance(key, slice):
-            raise ValueError("Can't set a slice of a ring buffer!")
-        else:
-            return self._super.__setitem__(key, value)
-
-
-def apply_lcwd(path, env):
-    # Apply CWD if a relative path
-    if not os.path.isabs(path) and env.lcwd:
-        path = os.path.join(env.lcwd, path)
-    return path
diff -Nru fabric-1.14.0/fabric/_version.py fabric-2.5.0/fabric/_version.py
--- fabric-1.14.0/fabric/_version.py	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/fabric/_version.py	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,2 @@
+__version_info__ = (2, 5, 0)
+__version__ = ".".join(map(str, __version_info__))
diff -Nru fabric-1.14.0/fabric/version.py fabric-2.5.0/fabric/version.py
--- fabric-1.14.0/fabric/version.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/fabric/version.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,104 +0,0 @@
-"""
-Current Fabric version constant plus version pretty-print method.
-
-This functionality is contained in its own module to prevent circular import
-problems with ``__init__.py`` (which is loaded by setup.py during installation,
-which in turn needs access to this version information.)
-"""
-from subprocess import Popen, PIPE
-from os.path import abspath, dirname
-
-
-VERSION = (1, 14, 0, 'final', 0)
-
-
-def git_sha():
-    loc = abspath(dirname(__file__))
-    try:
-        p = Popen(
-            "cd \"%s\" && git log -1 --format=format:%%h" % loc,
-            shell=True,
-            stdout=PIPE,
-            stderr=PIPE
-        )
-        return p.communicate()[0]
-    # OSError occurs on Unix-derived platforms lacking Popen's configured shell
-    # default, /bin/sh. E.g. Android.
-    except OSError:
-        return None
-
-
-def get_version(form='short'):
-    """
-    Return a version string for this package, based on `VERSION`.
-
-    Takes a single argument, ``form``, which should be one of the following
-    strings:
-
-    * ``branch``: just the major + minor, e.g. "0.9", "1.0".
-    * ``short`` (default): compact, e.g. "0.9rc1", "0.9.0". For package
-      filenames or SCM tag identifiers.
-    * ``normal``: human readable, e.g. "0.9", "0.9.1", "0.9 beta 1". For e.g.
-      documentation site headers.
-    * ``verbose``: like ``normal`` but fully explicit, e.g. "0.9 final". For
-      tag commit messages, or anywhere that it's important to remove ambiguity
-      between a branch and the first final release within that branch.
-    * ``all``: Returns all of the above, as a dict.
-    """
-    # Setup
-    versions = {}
-    branch = "%s.%s" % (VERSION[0], VERSION[1])
-    tertiary = VERSION[2]
-    type_ = VERSION[3]
-    final = (type_ == "final")
-    type_num = VERSION[4]
-    firsts = "".join([x[0] for x in type_.split()])
-
-    # Branch
-    versions['branch'] = branch
-
-    # Short
-    v = branch
-    if (tertiary or final):
-        v += "." + str(tertiary)
-    if not final:
-        v += firsts
-        if type_num:
-            v += str(type_num)
-    versions['short'] = v
-
-    # Normal
-    v = branch
-    if tertiary:
-        v += "." + str(tertiary)
-    if not final:
-        if type_num:
-            v += " " + type_ + " " + str(type_num)
-        else:
-            v += " pre-" + type_
-    versions['normal'] = v
-
-    # Verbose
-    v = branch
-    if tertiary:
-        v += "." + str(tertiary)
-    if not final:
-        if type_num:
-            v += " " + type_ + " " + str(type_num)
-        else:
-            v += " pre-" + type_
-    else:
-        v += " final"
-    versions['verbose'] = v
-
-    try:
-        return versions[form]
-    except KeyError:
-        if form == 'all':
-            return versions
-        raise TypeError('"%s" is not a valid form specifier.' % form)
-
-__version__ = get_version('short')
-
-if __name__ == "__main__":
-    print(get_version('all'))
diff -Nru fabric-1.14.0/fabric2/config.py fabric-2.5.0/fabric2/config.py
--- fabric-1.14.0/fabric2/config.py	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/fabric2/config.py	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,321 @@
+import copy
+import errno
+import os
+
+from invoke.config import Config as InvokeConfig, merge_dicts
+from paramiko.config import SSHConfig
+
+from .runners import Remote
+from .util import get_local_user, debug
+
+
+class Config(InvokeConfig):
+    """
+    An `invoke.config.Config` subclass with extra Fabric-related behavior.
+
+    This class behaves like `invoke.config.Config` in every way, with the
+    following exceptions:
+
+    - its `global_defaults` staticmethod has been extended to add/modify some
+      default settings (see its documentation, below, for details);
+    - it triggers loading of Fabric-specific env vars (e.g.
+      ``FABRIC_RUN_HIDE=true`` instead of ``INVOKE_RUN_HIDE=true``) and
+      filenames (e.g. ``/etc/fabric.yaml`` instead of ``/etc/invoke.yaml``).
+    - it extends the API to account for loading ``ssh_config`` files (which are
+      stored as additional attributes and have no direct relation to the
+      regular config data/hierarchy.)
+    - it adds a new optional constructor, `from_v1`, which :ref:`generates
+      configuration data from Fabric 1 <from-v1>`.
+
+    Intended for use with `.Connection`, as using vanilla
+    `invoke.config.Config` objects would require users to manually define
+    ``port``, ``user`` and so forth.
+
+    .. seealso:: :doc:`/concepts/configuration`, :ref:`ssh-config`
+
+    .. versionadded:: 2.0
+    """
+
+    prefix = "fabric"
+
+    @classmethod
+    def from_v1(cls, env, **kwargs):
+        """
+        Alternate constructor which uses Fabric 1's ``env`` dict for settings.
+
+        All keyword arguments besides ``env`` are passed unmolested into the
+        primary constructor, with the exception of ``overrides``, which is used
+        internally & will end up resembling the data from ``env`` with the
+        user-supplied overrides on top.
+
+        .. warning::
+            Because your own config overrides will win over data from ``env``,
+            make sure you only set values you *intend* to change from your v1
+            environment!
+
+        For details on exactly which ``env`` vars are imported and what they
+        become in the new API, please see :ref:`v1-env-var-imports`.
+
+        :param env:
+            An explicit Fabric 1 ``env`` dict (technically, any
+            ``fabric.utils._AttributeDict`` instance should work) to pull
+            configuration from.
+
+        .. versionadded:: 2.4
+        """
+        # TODO: automagic import, if we can find a way to test that
+        # Use overrides level (and preserve whatever the user may have given)
+        # TODO: we really do want arbitrary number of config levels, don't we?
+        # TODO: most of these need more care re: only filling in when they
+        # differ from the v1 default. As-is these won't overwrite runtime
+        # overrides (due to .setdefault) but they may still be filling in empty
+        # values to stomp on lower level config levels...
+        data = kwargs.pop("overrides", {})
+        # TODO: just use a dataproxy or defaultdict??
+        for subdict in ("connect_kwargs", "run", "sudo", "timeouts"):
+            data.setdefault(subdict, {})
+        # PTY use
+        data["run"].setdefault("pty", env.always_use_pty)
+        # Gateway
+        data.setdefault("gateway", env.gateway)
+        # Agent forwarding
+        data.setdefault("forward_agent", env.forward_agent)
+        # Key filename(s)
+        if env.key_filename is not None:
+            data["connect_kwargs"].setdefault("key_filename", env.key_filename)
+        # Load keys from agent?
+        data["connect_kwargs"].setdefault("allow_agent", not env.no_agent)
+        data.setdefault("ssh_config_path", env.ssh_config_path)
+        # Sudo password
+        data["sudo"].setdefault("password", env.sudo_password)
+        # Vanilla password (may be used for regular and/or sudo, depending)
+        passwd = env.password
+        data["connect_kwargs"].setdefault("password", passwd)
+        if not data["sudo"]["password"]:
+            data["sudo"]["password"] = passwd
+        data["sudo"].setdefault("prompt", env.sudo_prompt)
+        data["timeouts"].setdefault("connect", env.timeout)
+        data.setdefault("load_ssh_configs", env.use_ssh_config)
+        data["run"].setdefault("warn", env.warn_only)
+        # Put overrides back for real constructor and go
+        kwargs["overrides"] = data
+        return cls(**kwargs)
+
+    def __init__(self, *args, **kwargs):
+        """
+        Creates a new Fabric-specific config object.
+
+        For most API details, see `invoke.config.Config.__init__`. Parameters
+        new to this subclass are listed below.
+
+        :param ssh_config:
+            Custom/explicit `paramiko.config.SSHConfig` object. If given,
+            prevents loading of any SSH config files. Default: ``None``.
+
+        :param str runtime_ssh_path:
+            Runtime SSH config path to load. Prevents loading of system/user
+            files if given. Default: ``None``.
+
+        :param str system_ssh_path:
+            Location of the system-level SSH config file. Default:
+            ``/etc/ssh/ssh_config``.
+
+        :param str user_ssh_path:
+            Location of the user-level SSH config file. Default:
+            ``~/.ssh/config``.
+
+        :param bool lazy:
+            Has the same meaning as the parent class' ``lazy``, but
+            additionally controls whether SSH config file loading is deferred
+            (requires manually calling `load_ssh_config` sometime.) For
+            example, one may need to wait for user input before calling
+            `set_runtime_ssh_path`, which will inform exactly what
+            `load_ssh_config` does.
+        """
+        # Tease out our own kwargs.
+        # TODO: consider moving more stuff out of __init__ and into methods so
+        # there's less of this sort of splat-args + pop thing? Eh.
+        ssh_config = kwargs.pop("ssh_config", None)
+        lazy = kwargs.get("lazy", False)
+        self.set_runtime_ssh_path(kwargs.pop("runtime_ssh_path", None))
+        system_path = kwargs.pop("system_ssh_path", "/etc/ssh/ssh_config")
+        self._set(_system_ssh_path=system_path)
+        self._set(_user_ssh_path=kwargs.pop("user_ssh_path", "~/.ssh/config"))
+
+        # Record whether we were given an explicit object (so other steps know
+        # whether to bother loading from disk or not)
+        # This needs doing before super __init__ as that calls our post_init
+        explicit = ssh_config is not None
+        self._set(_given_explicit_object=explicit)
+
+        # Arrive at some non-None SSHConfig object (upon which to run .parse()
+        # later, in _load_ssh_file())
+        if ssh_config is None:
+            ssh_config = SSHConfig()
+        self._set(base_ssh_config=ssh_config)
+
+        # Now that our own attributes have been prepared & kwargs yanked, we
+        # can fall up into parent __init__()
+        super(Config, self).__init__(*args, **kwargs)
+
+        # And finally perform convenience non-lazy bits if needed
+        if not lazy:
+            self.load_ssh_config()
+
+    def set_runtime_ssh_path(self, path):
+        """
+        Configure a runtime-level SSH config file path.
+
+        If set, this will cause `load_ssh_config` to skip system and user
+        files, as OpenSSH does.
+
+        .. versionadded:: 2.0
+        """
+        self._set(_runtime_ssh_path=path)
+
+    def load_ssh_config(self):
+        """
+        Load SSH config file(s) from disk.
+
+        Also (beforehand) ensures that Invoke-level config re: runtime SSH
+        config file paths, is accounted for.
+
+        .. versionadded:: 2.0
+        """
+        # Update the runtime SSH config path (assumes enough regular config
+        # levels have been loaded that anyone wanting to transmit this info
+        # from a 'vanilla' Invoke config, has gotten it set.)
+        if self.ssh_config_path:
+            self._runtime_ssh_path = self.ssh_config_path
+        # Load files from disk if we weren't given an explicit SSHConfig in
+        # __init__
+        if not self._given_explicit_object:
+            self._load_ssh_files()
+
+    def clone(self, *args, **kwargs):
+        # TODO: clone() at this point kinda-sorta feels like it's retreading
+        # __reduce__ and the related (un)pickling stuff...
+        # Get cloned obj.
+        # NOTE: Because we also extend .init_kwargs, the actual core SSHConfig
+        # data is passed in at init time (ensuring no files get loaded a 2nd,
+        # etc time) and will already be present, so we don't need to set
+        # .base_ssh_config ourselves. Similarly, there's no need to worry about
+        # how the SSH config paths may be inaccurate until below; nothing will
+        # be referencing them.
+        new = super(Config, self).clone(*args, **kwargs)
+        # Copy over our custom attributes, so that the clone still resembles us
+        # re: recording where the data originally came from (in case anything
+        # re-runs ._load_ssh_files(), for example).
+        for attr in (
+            "_runtime_ssh_path",
+            "_system_ssh_path",
+            "_user_ssh_path",
+        ):
+            setattr(new, attr, getattr(self, attr))
+        # Load SSH configs, in case they weren't prior to now (e.g. a vanilla
+        # Invoke clone(into), instead of a us-to-us clone.)
+        self.load_ssh_config()
+        # All done
+        return new
+
+    def _clone_init_kwargs(self, *args, **kw):
+        # Parent kwargs
+        kwargs = super(Config, self)._clone_init_kwargs(*args, **kw)
+        # Transmit our internal SSHConfig via explicit-obj kwarg, thus
+        # bypassing any file loading. (Our extension of clone() above copies
+        # over other attributes as well so that the end result looks consistent
+        # with reality.)
+        new_config = SSHConfig()
+        # TODO: as with other spots, this implies SSHConfig needs a cleaner
+        # public API re: creating and updating its core data.
+        new_config._config = copy.deepcopy(self.base_ssh_config._config)
+        return dict(kwargs, ssh_config=new_config)
+
+    def _load_ssh_files(self):
+        """
+        Trigger loading of configured SSH config file paths.
+
+        Expects that ``base_ssh_config`` has already been set to an
+        `~paramiko.config.SSHConfig` object.
+
+        :returns: ``None``.
+        """
+        # TODO: does this want to more closely ape the behavior of
+        # InvokeConfig.load_files? re: having a _found attribute for each that
+        # determines whether to load or skip
+        if self._runtime_ssh_path is not None:
+            path = self._runtime_ssh_path
+            # Manually blow up like open() (_load_ssh_file normally doesn't)
+            if not os.path.exists(path):
+                msg = "No such file or directory: {!r}".format(path)
+                raise IOError(errno.ENOENT, msg)
+            self._load_ssh_file(os.path.expanduser(path))
+        elif self.load_ssh_configs:
+            for path in (self._user_ssh_path, self._system_ssh_path):
+                self._load_ssh_file(os.path.expanduser(path))
+
+    def _load_ssh_file(self, path):
+        """
+        Attempt to open and parse an SSH config file at ``path``.
+
+        Does nothing if ``path`` is not a path to a valid file.
+
+        :returns: ``None``.
+        """
+        if os.path.isfile(path):
+            old_rules = len(self.base_ssh_config._config)
+            with open(path) as fd:
+                self.base_ssh_config.parse(fd)
+            new_rules = len(self.base_ssh_config._config)
+            msg = "Loaded {} new ssh_config rules from {!r}"
+            debug(msg.format(new_rules - old_rules, path))
+        else:
+            debug("File not found, skipping")
+
+    @staticmethod
+    def global_defaults():
+        """
+        Default configuration values and behavior toggles.
+
+        Fabric only extends this method in order to make minor adjustments and
+        additions to Invoke's `~invoke.config.Config.global_defaults`; see its
+        documentation for the base values, such as the config subtrees
+        controlling behavior of ``run`` or how ``tasks`` behave.
+
+        For Fabric-specific modifications and additions to the Invoke-level
+        defaults, see our own config docs at :ref:`default-values`.
+
+        .. versionadded:: 2.0
+        """
+        # TODO: hrm should the run-related things actually be derived from the
+        # runner_class? E.g. Local defines local stuff, Remote defines remote
+        # stuff? Doesn't help with the final config tree tho...
+        # TODO: as to that, this is a core problem, Fabric wants split
+        # local/remote stuff, eg replace_env wants to be False for local and
+        # True remotely; shell wants to differ depending on target (and either
+        # way, does not want to use local interrogation for remote)
+        # TODO: is it worth moving all of our 'new' settings to a discrete
+        # namespace for cleanliness' sake? e.g. ssh.port, ssh.user etc.
+        # It wouldn't actually simplify this code any, but it would make it
+        # easier for users to determine what came from which library/repo.
+        defaults = InvokeConfig.global_defaults()
+        ours = {
+            # New settings
+            "connect_kwargs": {},
+            "forward_agent": False,
+            "gateway": None,
+            # TODO 3.0: change to True and update all docs accordingly.
+            "inline_ssh_env": False,
+            "load_ssh_configs": True,
+            "port": 22,
+            "run": {"replace_env": True},
+            "runners": {"remote": Remote},
+            "ssh_config_path": None,
+            "tasks": {"collection_name": "fabfile"},
+            # TODO: this becomes an override/extend once Invoke grows execution
+            # timeouts (which should be timeouts.execute)
+            "timeouts": {"connect": None},
+            "user": get_local_user(),
+        }
+        merge_dicts(defaults, ours)
+        return defaults
diff -Nru fabric-1.14.0/fabric2/connection.py fabric-2.5.0/fabric2/connection.py
--- fabric-1.14.0/fabric2/connection.py	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/fabric2/connection.py	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,993 @@
+from contextlib import contextmanager
+from threading import Event
+
+try:
+    from invoke.vendor.six import StringIO
+    from invoke.vendor.decorator import decorator
+    from invoke.vendor.six import string_types
+except ImportError:
+    from six import StringIO
+    from decorator import decorator
+    from six import string_types
+import socket
+
+from invoke import Context
+from invoke.exceptions import ThreadException
+from paramiko.agent import AgentRequestHandler
+from paramiko.client import SSHClient, AutoAddPolicy
+from paramiko.config import SSHConfig
+from paramiko.proxy import ProxyCommand
+
+from .config import Config
+from .exceptions import InvalidV1Env
+from .transfer import Transfer
+from .tunnels import TunnelManager, Tunnel
+
+
+@decorator
+def opens(method, self, *args, **kwargs):
+    self.open()
+    return method(self, *args, **kwargs)
+
+
+def derive_shorthand(host_string):
+    user_hostport = host_string.rsplit("@", 1)
+    hostport = user_hostport.pop()
+    user = user_hostport[0] if user_hostport and user_hostport[0] else None
+
+    # IPv6: can't reliably tell where addr ends and port begins, so don't
+    # try (and don't bother adding special syntax either, user should avoid
+    # this situation by using port=).
+    if hostport.count(":") > 1:
+        host = hostport
+        port = None
+    # IPv4: can split on ':' reliably.
+    else:
+        host_port = hostport.rsplit(":", 1)
+        host = host_port.pop(0) or None
+        port = host_port[0] if host_port and host_port[0] else None
+
+    if port is not None:
+        port = int(port)
+
+    return {"user": user, "host": host, "port": port}
+
+
+class Connection(Context):
+    """
+    A connection to an SSH daemon, with methods for commands and file transfer.
+
+    **Basics**
+
+    This class inherits from Invoke's `~invoke.context.Context`, as it is a
+    context within which commands, tasks etc can operate. It also encapsulates
+    a Paramiko `~paramiko.client.SSHClient` instance, performing useful high
+    level operations with that `~paramiko.client.SSHClient` and
+    `~paramiko.channel.Channel` instances generated from it.
+
+    .. _connect_kwargs:
+
+    .. note::
+        Many SSH specific options -- such as specifying private keys and
+        passphrases, timeouts, disabling SSH agents, etc -- are handled
+        directly by Paramiko and should be specified via the
+        :ref:`connect_kwargs argument <connect_kwargs-arg>` of the constructor.
+
+    **Lifecycle**
+
+    `.Connection` has a basic "`create <__init__>`, `connect/open <open>`, `do
+    work <run>`, `disconnect/close <close>`" lifecycle:
+
+    - `Instantiation <__init__>` imprints the object with its connection
+      parameters (but does **not** actually initiate the network connection).
+
+        - An alternate constructor exists for users :ref:`upgrading piecemeal
+          from Fabric 1 <from-v1>`: `from_v1`
+
+    - Methods like `run`, `get` etc automatically trigger a call to
+      `open` if the connection is not active; users may of course call `open`
+      manually if desired.
+    - Connections do not always need to be explicitly closed; much of the
+      time, Paramiko's garbage collection hooks or Python's own shutdown
+      sequence will take care of things. **However**, should you encounter edge
+      cases (for example, sessions hanging on exit) it's helpful to explicitly
+      close connections when you're done with them.
+
+      This can be accomplished by manually calling `close`, or by using the
+      object as a contextmanager::
+
+        with Connection('host') as c:
+            c.run('command')
+            c.put('file')
+
+    .. note::
+        This class rebinds `invoke.context.Context.run` to `.local` so both
+        remote and local command execution can coexist.
+
+    **Configuration**
+
+    Most `.Connection` parameters honor :doc:`Invoke-style configuration
+    </concepts/configuration>` as well as any applicable :ref:`SSH config file
+    directives <connection-ssh-config>`. For example, to end up with a
+    connection to ``admin@myhost``, one could:
+
+    - Use any built-in config mechanism, such as ``/etc/fabric.yml``,
+      ``~/.fabric.json``, collection-driven configuration, env vars, etc,
+      stating ``user: admin`` (or ``{"user": "admin"}``, depending on config
+      format.) Then ``Connection('myhost')`` would implicitly have a ``user``
+      of ``admin``.
+    - Use an SSH config file containing ``User admin`` within any applicable
+      ``Host`` header (``Host myhost``, ``Host *``, etc.) Again,
+      ``Connection('myhost')`` will default to an ``admin`` user.
+    - Leverage host-parameter shorthand (described in `.Config.__init__`), i.e.
+      ``Connection('admin@myhost')``.
+    - Give the parameter directly: ``Connection('myhost', user='admin')``.
+
+    The same applies to agent forwarding, gateways, and so forth.
+
+    .. versionadded:: 2.0
+    """
+
+    # NOTE: these are initialized here to hint to invoke.Config.__setattr__
+    # that they should be treated as real attributes instead of config proxies.
+    # (Additionally, we're doing this instead of using invoke.Config._set() so
+    # we can take advantage of Sphinx's attribute-doc-comment static analysis.)
+    # Once an instance is created, these values will usually be non-None
+    # because they default to the default config values.
+    host = None
+    original_host = None
+    user = None
+    port = None
+    ssh_config = None
+    gateway = None
+    forward_agent = None
+    connect_timeout = None
+    connect_kwargs = None
+    client = None
+    transport = None
+    _sftp = None
+    _agent_handler = None
+
+    @classmethod
+    def from_v1(cls, env, **kwargs):
+        """
+        Alternate constructor which uses Fabric 1's ``env`` dict for settings.
+
+        All keyword arguments besides ``env`` are passed unmolested into the
+        primary constructor.
+
+        .. warning::
+            Because your own config overrides will win over data from ``env``,
+            make sure you only set values you *intend* to change from your v1
+            environment!
+
+        For details on exactly which ``env`` vars are imported and what they
+        become in the new API, please see :ref:`v1-env-var-imports`.
+
+        :param env:
+            An explicit Fabric 1 ``env`` dict (technically, any
+            ``fabric.utils._AttributeDict`` instance should work) to pull
+            configuration from.
+
+        .. versionadded:: 2.4
+        """
+        # TODO: import fabric.state.env (need good way to test it first...)
+        # TODO: how to handle somebody accidentally calling this in a process
+        # where 'fabric' is fabric 2, and there's no fabric 1? Probably just a
+        # re-raise of ImportError??
+        # Our only requirement is a non-empty host_string
+        if not env.host_string:
+            raise InvalidV1Env(
+                "Supplied v1 env has an empty `host_string` value! Please make sure you're calling Connection.from_v1 within a connected Fabric 1 session."  # noqa
+            )
+        # TODO: detect collisions with kwargs & except instead of overwriting?
+        # (More Zen of Python compliant, but also, effort, and also, makes it
+        # harder for users to intentionally overwrite!)
+        connect_kwargs = kwargs.setdefault("connect_kwargs", {})
+        kwargs.setdefault("host", env.host_string)
+        shorthand = derive_shorthand(env.host_string)
+        # TODO: don't we need to do the below skipping for user too?
+        kwargs.setdefault("user", env.user)
+        # Skip port if host string seemed to have it; otherwise we hit our own
+        # ambiguity clause in __init__. v1 would also have been doing this
+        # anyways (host string wins over other settings).
+        if not shorthand["port"]:
+            # Run port through int(); v1 inexplicably has a string default...
+            kwargs.setdefault("port", int(env.port))
+        # key_filename defaults to None in v1, but in v2, we expect it to be
+        # either unset, or set to a list. Thus, we only pull it over if it is
+        # not None.
+        if env.key_filename is not None:
+            connect_kwargs.setdefault("key_filename", env.key_filename)
+        # Obtain config values, if not given, from its own from_v1
+        # NOTE: not using setdefault as we truly only want to call
+        # Config.from_v1 when necessary.
+        if "config" not in kwargs:
+            kwargs["config"] = Config.from_v1(env)
+        return cls(**kwargs)
+
+    # TODO: should "reopening" an existing Connection object that has been
+    # closed, be allowed? (See e.g. how v1 detects closed/semi-closed
+    # connections & nukes them before creating a new client to the same host.)
+    # TODO: push some of this into paramiko.client.Client? e.g. expand what
+    # Client.exec_command does, it already allows configuring a subset of what
+    # we do / will eventually do / did in 1.x. It's silly to have to do
+    # .get_transport().open_session().
+    def __init__(
+        self,
+        host,
+        user=None,
+        port=None,
+        config=None,
+        gateway=None,
+        forward_agent=None,
+        connect_timeout=None,
+        connect_kwargs=None,
+        inline_ssh_env=None,
+    ):
+        """
+        Set up a new object representing a server connection.
+
+        :param str host:
+            the hostname (or IP address) of this connection.
+
+            May include shorthand for the ``user`` and/or ``port`` parameters,
+            of the form ``user@host``, ``host:port``, or ``user@host:port``.
+
+            .. note::
+                Due to ambiguity, IPv6 host addresses are incompatible with the
+                ``host:port`` shorthand (though ``user@host`` will still work
+                OK). In other words, the presence of >1 ``:`` character will
+                prevent any attempt to derive a shorthand port number; use the
+                explicit ``port`` parameter instead.
+
+            .. note::
+                If ``host`` matches a ``Host`` clause in loaded SSH config
+                data, and that ``Host`` clause contains a ``Hostname``
+                directive, the resulting `.Connection` object will behave as if
+                ``host`` is equal to that ``Hostname`` value.
+
+                In all cases, the original value of ``host`` is preserved as
+                the ``original_host`` attribute.
+
+                Thus, given SSH config like so::
+
+                    Host myalias
+                        Hostname realhostname
+
+                a call like ``Connection(host='myalias')`` will result in an
+                object whose ``host`` attribute is ``realhostname``, and whose
+                ``original_host`` attribute is ``myalias``.
+
+        :param str user:
+            the login user for the remote connection. Defaults to
+            ``config.user``.
+
+        :param int port:
+            the remote port. Defaults to ``config.port``.
+
+        :param config:
+            configuration settings to use when executing methods on this
+            `.Connection` (e.g. default SSH port and so forth).
+
+            Should be a `.Config` or an `invoke.config.Config`
+            (which will be turned into a `.Config`).
+
+            Default is an anonymous `.Config` object.
+
+        :param gateway:
+            An object to use as a proxy or gateway for this connection.
+
+            This parameter accepts one of the following:
+
+            - another `.Connection` (for a ``ProxyJump`` style gateway);
+            - a shell command string (for a ``ProxyCommand`` style style
+              gateway).
+
+            Default: ``None``, meaning no gatewaying will occur (unless
+            otherwise configured; if one wants to override a configured gateway
+            at runtime, specify ``gateway=False``.)
+
+            .. seealso:: :ref:`ssh-gateways`
+
+        :param bool forward_agent:
+            Whether to enable SSH agent forwarding.
+
+            Default: ``config.forward_agent``.
+
+        :param int connect_timeout:
+            Connection timeout, in seconds.
+
+            Default: ``config.timeouts.connect``.
+
+        .. _connect_kwargs-arg:
+
+        :param dict connect_kwargs:
+            Keyword arguments handed verbatim to
+            `SSHClient.connect <paramiko.client.SSHClient.connect>` (when
+            `.open` is called).
+
+            `.Connection` tries not to grow additional settings/kwargs of its
+            own unless it is adding value of some kind; thus,
+            ``connect_kwargs`` is currently the right place to hand in paramiko
+            connection parameters such as ``pkey`` or ``key_filename``. For
+            example::
+
+                c = Connection(
+                    host="hostname",
+                    user="admin",
+                    connect_kwargs={
+                        "key_filename": "/home/myuser/.ssh/private.key",
+                    },
+                )
+
+            Default: ``config.connect_kwargs``.
+
+        :param bool inline_ssh_env:
+            Whether to send environment variables "inline" as prefixes in front
+            of command strings (``export VARNAME=value && mycommand here``),
+            instead of trying to submit them through the SSH protocol itself
+            (which is the default behavior). This is necessary if the remote
+            server has a restricted ``AcceptEnv`` setting (which is the common
+            default).
+
+            The default value is the value of the ``inline_ssh_env``
+            :ref:`configuration value <default-values>` (which itself defaults
+            to ``False``).
+
+            .. warning::
+                This functionality does **not** currently perform any shell
+                escaping on your behalf! Be careful when using nontrivial
+                values, and note that you can put in your own quoting,
+                backslashing etc if desired.
+
+                Consider using a different approach (such as actual
+                remote shell scripts) if you run into too many issues here.
+
+            .. note::
+                When serializing into prefixed ``FOO=bar`` format, we apply the
+                builtin `sorted` function to the env dictionary's keys, to
+                remove what would otherwise be ambiguous/arbitrary ordering.
+
+            .. note::
+                This setting has no bearing on *local* shell commands; it only
+                affects remote commands, and thus, methods like `.run` and
+                `.sudo`.
+
+        :raises ValueError:
+            if user or port values are given via both ``host`` shorthand *and*
+            their own arguments. (We `refuse the temptation to guess`_).
+
+        .. _refuse the temptation to guess:
+            http://zen-of-python.info/
+            in-the-face-of-ambiguity-refuse-the-temptation-to-guess.html#12
+
+        .. versionchanged:: 2.3
+            Added the ``inline_ssh_env`` parameter.
+        """
+        # NOTE: parent __init__ sets self._config; for now we simply overwrite
+        # that below. If it's somehow problematic we would want to break parent
+        # __init__ up in a manner that is more cleanly overrideable.
+        super(Connection, self).__init__(config=config)
+
+        #: The .Config object referenced when handling default values (for e.g.
+        #: user or port, when not explicitly given) or deciding how to behave.
+        if config is None:
+            config = Config()
+        # Handle 'vanilla' Invoke config objects, which need cloning 'into' one
+        # of our own Configs (which grants the new defaults, etc, while not
+        # squashing them if the Invoke-level config already accounted for them)
+        elif not isinstance(config, Config):
+            config = config.clone(into=Config)
+        self._set(_config=config)
+        # TODO: when/how to run load_files, merge, load_shell_env, etc?
+        # TODO: i.e. what is the lib use case here (and honestly in invoke too)
+
+        shorthand = self.derive_shorthand(host)
+        host = shorthand["host"]
+        err = "You supplied the {} via both shorthand and kwarg! Please pick one."  # noqa
+        if shorthand["user"] is not None:
+            if user is not None:
+                raise ValueError(err.format("user"))
+            user = shorthand["user"]
+        if shorthand["port"] is not None:
+            if port is not None:
+                raise ValueError(err.format("port"))
+            port = shorthand["port"]
+
+        # NOTE: we load SSH config data as early as possible as it has
+        # potential to affect nearly every other attribute.
+        #: The per-host SSH config data, if any. (See :ref:`ssh-config`.)
+        self.ssh_config = self.config.base_ssh_config.lookup(host)
+
+        self.original_host = host
+        #: The hostname of the target server.
+        self.host = host
+        if "hostname" in self.ssh_config:
+            # TODO: log that this occurred?
+            self.host = self.ssh_config["hostname"]
+
+        #: The username this connection will use to connect to the remote end.
+        self.user = user or self.ssh_config.get("user", self.config.user)
+        # TODO: is it _ever_ possible to give an empty user value (e.g.
+        # user='')? E.g. do some SSH server specs allow for that?
+
+        #: The network port to connect on.
+        self.port = port or int(self.ssh_config.get("port", self.config.port))
+
+        # Gateway/proxy/bastion/jump setting: non-None values - string,
+        # Connection, even eg False - get set directly; None triggers seek in
+        # config/ssh_config
+        #: The gateway `.Connection` or ``ProxyCommand`` string to be used,
+        #: if any.
+        self.gateway = gateway if gateway is not None else self.get_gateway()
+        # NOTE: we use string above, vs ProxyCommand obj, to avoid spinning up
+        # the ProxyCommand subprocess at init time, vs open() time.
+        # TODO: make paramiko.proxy.ProxyCommand lazy instead?
+
+        if forward_agent is None:
+            # Default to config...
+            forward_agent = self.config.forward_agent
+            # But if ssh_config is present, it wins
+            if "forwardagent" in self.ssh_config:
+                # TODO: SSHConfig really, seriously needs some love here, god
+                map_ = {"yes": True, "no": False}
+                forward_agent = map_[self.ssh_config["forwardagent"]]
+        #: Whether agent forwarding is enabled.
+        self.forward_agent = forward_agent
+
+        if connect_timeout is None:
+            connect_timeout = self.ssh_config.get(
+                "connecttimeout", self.config.timeouts.connect
+            )
+        if connect_timeout is not None:
+            connect_timeout = int(connect_timeout)
+        #: Connection timeout
+        self.connect_timeout = connect_timeout
+
+        #: Keyword arguments given to `paramiko.client.SSHClient.connect` when
+        #: `open` is called.
+        self.connect_kwargs = self.resolve_connect_kwargs(connect_kwargs)
+
+        #: The `paramiko.client.SSHClient` instance this connection wraps.
+        client = SSHClient()
+        client.set_missing_host_key_policy(AutoAddPolicy())
+        self.client = client
+
+        #: A convenience handle onto the return value of
+        #: ``self.client.get_transport()``.
+        self.transport = None
+
+        if inline_ssh_env is None:
+            inline_ssh_env = self.config.inline_ssh_env
+        #: Whether to construct remote command lines with env vars prefixed
+        #: inline.
+        self.inline_ssh_env = inline_ssh_env
+
+    def resolve_connect_kwargs(self, connect_kwargs):
+        # Grab connect_kwargs from config if not explicitly given.
+        if connect_kwargs is None:
+            # TODO: is it better to pre-empt conflicts w/ manually-handled
+            # connect() kwargs (hostname, username, etc) here or in open()?
+            # We're doing open() for now in case e.g. someone manually modifies
+            # .connect_kwargs attributewise, but otherwise it feels better to
+            # do it early instead of late.
+            connect_kwargs = self.config.connect_kwargs
+        # Special case: key_filename gets merged instead of overridden.
+        # TODO: probably want some sorta smart merging generally, special cases
+        # are bad.
+        elif "key_filename" in self.config.connect_kwargs:
+            kwarg_val = connect_kwargs.get("key_filename", [])
+            conf_val = self.config.connect_kwargs["key_filename"]
+            # Config value comes before kwarg value (because it may contain
+            # CLI flag value.)
+            connect_kwargs["key_filename"] = conf_val + kwarg_val
+
+        # SSH config identityfile values come last in the key_filename
+        # 'hierarchy'.
+        if "identityfile" in self.ssh_config:
+            connect_kwargs.setdefault("key_filename", [])
+            connect_kwargs["key_filename"].extend(
+                self.ssh_config["identityfile"]
+            )
+
+        return connect_kwargs
+
+    def get_gateway(self):
+        # SSH config wins over Invoke-style config
+        if "proxyjump" in self.ssh_config:
+            # Reverse hop1,hop2,hop3 style ProxyJump directive so we start
+            # with the final (itself non-gatewayed) hop and work up to
+            # the front (actual, supplied as our own gateway) hop
+            hops = reversed(self.ssh_config["proxyjump"].split(","))
+            prev_gw = None
+            for hop in hops:
+                # Short-circuit if we appear to be our own proxy, which would
+                # be a RecursionError. Implies SSH config wildcards.
+                # TODO: in an ideal world we'd check user/port too in case they
+                # differ, but...seriously? They can file a PR with those extra
+                # half dozen test cases in play, E_NOTIME
+                if self.derive_shorthand(hop)["host"] == self.host:
+                    return None
+                # Happily, ProxyJump uses identical format to our host
+                # shorthand...
+                kwargs = dict(config=self.config.clone())
+                if prev_gw is not None:
+                    kwargs["gateway"] = prev_gw
+                cxn = Connection(hop, **kwargs)
+                prev_gw = cxn
+            return prev_gw
+        elif "proxycommand" in self.ssh_config:
+            # Just a string, which we interpret as a proxy command..
+            return self.ssh_config["proxycommand"]
+        # Fallback: config value (may be None).
+        return self.config.gateway
+
+    def __repr__(self):
+        # Host comes first as it's the most common differentiator by far
+        bits = [("host", self.host)]
+        # TODO: maybe always show user regardless? Explicit is good...
+        if self.user != self.config.user:
+            bits.append(("user", self.user))
+        # TODO: harder to make case for 'always show port'; maybe if it's
+        # non-22 (even if config has overridden the local default)?
+        if self.port != self.config.port:
+            bits.append(("port", self.port))
+        # NOTE: sometimes self.gateway may be eg False if someone wants to
+        # explicitly override a configured non-None value (as otherwise it's
+        # impossible for __init__ to tell if a None means "nothing given" or
+        # "seriously please no gatewaying". So, this must always be a vanilla
+        # truth test and not eg "is not None".
+        if self.gateway:
+            # Displaying type because gw params would probs be too verbose
+            val = "proxyjump"
+            if isinstance(self.gateway, string_types):
+                val = "proxycommand"
+            bits.append(("gw", val))
+        return "<Connection {}>".format(
+            " ".join("{}={}".format(*x) for x in bits)
+        )
+
+    def _identity(self):
+        # TODO: consider including gateway and maybe even other init kwargs?
+        # Whether two cxns w/ same user/host/port but different
+        # gateway/keys/etc, should be considered "the same", is unclear.
+        return (self.host, self.user, self.port)
+
+    def __eq__(self, other):
+        if not isinstance(other, Connection):
+            return False
+        return self._identity() == other._identity()
+
+    def __lt__(self, other):
+        return self._identity() < other._identity()
+
+    def __hash__(self):
+        # NOTE: this departs from Context/DataProxy, which is not usefully
+        # hashable.
+        return hash(self._identity())
+
+    def derive_shorthand(self, host_string):
+        # NOTE: used to be defined inline; preserving API call for both
+        # backwards compatibility and because it seems plausible we may want to
+        # modify behavior later, using eg config or other attributes.
+        return derive_shorthand(host_string)
+
+    @property
+    def is_connected(self):
+        """
+        Whether or not this connection is actually open.
+
+        .. versionadded:: 2.0
+        """
+        return self.transport.active if self.transport else False
+
+    def open(self):
+        """
+        Initiate an SSH connection to the host/port this object is bound to.
+
+        This may include activating the configured gateway connection, if one
+        is set.
+
+        Also saves a handle to the now-set Transport object for easier access.
+
+        Various connect-time settings (and/or their corresponding :ref:`SSH
+        config options <ssh-config>`) are utilized here in the call to
+        `SSHClient.connect <paramiko.client.SSHClient.connect>`. (For details,
+        see :doc:`the configuration docs </concepts/configuration>`.)
+
+        .. versionadded:: 2.0
+        """
+        # Short-circuit
+        if self.is_connected:
+            return
+        err = "Refusing to be ambiguous: connect() kwarg '{}' was given both via regular arg and via connect_kwargs!"  # noqa
+        # These may not be given, period
+        for key in """
+            hostname
+            port
+            username
+        """.split():
+            if key in self.connect_kwargs:
+                raise ValueError(err.format(key))
+        # These may be given one way or the other, but not both
+        if (
+            "timeout" in self.connect_kwargs
+            and self.connect_timeout is not None
+        ):
+            raise ValueError(err.format("timeout"))
+        # No conflicts -> merge 'em together
+        kwargs = dict(
+            self.connect_kwargs,
+            username=self.user,
+            hostname=self.host,
+            port=self.port,
+        )
+        if self.gateway:
+            kwargs["sock"] = self.open_gateway()
+        if self.connect_timeout:
+            kwargs["timeout"] = self.connect_timeout
+        # Strip out empty defaults for less noisy debugging
+        if "key_filename" in kwargs and not kwargs["key_filename"]:
+            del kwargs["key_filename"]
+        # Actually connect!
+        self.client.connect(**kwargs)
+        self.transport = self.client.get_transport()
+
+    def open_gateway(self):
+        """
+        Obtain a socket-like object from `gateway`.
+
+        :returns:
+            A ``direct-tcpip`` `paramiko.channel.Channel`, if `gateway` was a
+            `.Connection`; or a `~paramiko.proxy.ProxyCommand`, if `gateway`
+            was a string.
+
+        .. versionadded:: 2.0
+        """
+        # ProxyCommand is faster to set up, so do it first.
+        if isinstance(self.gateway, string_types):
+            # Leverage a dummy SSHConfig to ensure %h/%p/etc are parsed.
+            # TODO: use real SSH config once loading one properly is
+            # implemented.
+            ssh_conf = SSHConfig()
+            dummy = "Host {}\n    ProxyCommand {}"
+            ssh_conf.parse(StringIO(dummy.format(self.host, self.gateway)))
+            return ProxyCommand(ssh_conf.lookup(self.host)["proxycommand"])
+        # Handle inner-Connection gateway type here.
+        # TODO: logging
+        self.gateway.open()
+        # TODO: expose the opened channel itself as an attribute? (another
+        # possible argument for separating the two gateway types...) e.g. if
+        # someone wanted to piggyback on it for other same-interpreter socket
+        # needs...
+        # TODO: and the inverse? allow users to supply their own socket/like
+        # object they got via $WHEREEVER?
+        # TODO: how best to expose timeout param? reuse general connection
+        # timeout from config?
+        return self.gateway.transport.open_channel(
+            kind="direct-tcpip",
+            dest_addr=(self.host, int(self.port)),
+            # NOTE: src_addr needs to be 'empty but not None' values to
+            # correctly encode into a network message. Theoretically Paramiko
+            # could auto-interpret None sometime & save us the trouble.
+            src_addr=("", 0),
+        )
+
+    def close(self):
+        """
+        Terminate the network connection to the remote end, if open.
+
+        If no connection is open, this method does nothing.
+
+        .. versionadded:: 2.0
+        """
+        if self.is_connected:
+            self.client.close()
+            if self.forward_agent and self._agent_handler is not None:
+                self._agent_handler.close()
+
+    def __enter__(self):
+        return self
+
+    def __exit__(self, *exc):
+        self.close()
+
+    @opens
+    def create_session(self):
+        channel = self.transport.open_session()
+        if self.forward_agent:
+            self._agent_handler = AgentRequestHandler(channel)
+        return channel
+
+    def _remote_runner(self):
+        return self.config.runners.remote(self, inline_env=self.inline_ssh_env)
+
+    @opens
+    def run(self, command, **kwargs):
+        """
+        Execute a shell command on the remote end of this connection.
+
+        This method wraps an SSH-capable implementation of
+        `invoke.runners.Runner.run`; see its documentation for details.
+
+        .. warning::
+            There are a few spots where Fabric departs from Invoke's default
+            settings/behaviors; they are documented under
+            `.Config.global_defaults`.
+
+        .. versionadded:: 2.0
+        """
+        return self._run(self._remote_runner(), command, **kwargs)
+
+    @opens
+    def sudo(self, command, **kwargs):
+        """
+        Execute a shell command, via ``sudo``, on the remote end.
+
+        This method is identical to `invoke.context.Context.sudo` in every way,
+        except in that -- like `run` -- it honors per-host/per-connection
+        configuration overrides in addition to the generic/global ones. Thus,
+        for example, per-host sudo passwords may be configured.
+
+        .. versionadded:: 2.0
+        """
+        return self._sudo(self._remote_runner(), command, **kwargs)
+
+    def local(self, *args, **kwargs):
+        """
+        Execute a shell command on the local system.
+
+        This method is effectively a wrapper of `invoke.run`; see its docs for
+        details and call signature.
+
+        .. versionadded:: 2.0
+        """
+        # Superclass run() uses runners.local, so we can literally just call it
+        # straight.
+        return super(Connection, self).run(*args, **kwargs)
+
+    @opens
+    def sftp(self):
+        """
+        Return a `~paramiko.sftp_client.SFTPClient` object.
+
+        If called more than one time, memoizes the first result; thus, any
+        given `.Connection` instance will only ever have a single SFTP client,
+        and state (such as that managed by
+        `~paramiko.sftp_client.SFTPClient.chdir`) will be preserved.
+
+        .. versionadded:: 2.0
+        """
+        if self._sftp is None:
+            self._sftp = self.client.open_sftp()
+        return self._sftp
+
+    def get(self, *args, **kwargs):
+        """
+        Get a remote file to the local filesystem or file-like object.
+
+        Simply a wrapper for `.Transfer.get`. Please see its documentation for
+        all details.
+
+        .. versionadded:: 2.0
+        """
+        return Transfer(self).get(*args, **kwargs)
+
+    def put(self, *args, **kwargs):
+        """
+        Put a remote file (or file-like object) to the remote filesystem.
+
+        Simply a wrapper for `.Transfer.put`. Please see its documentation for
+        all details.
+
+        .. versionadded:: 2.0
+        """
+        return Transfer(self).put(*args, **kwargs)
+
+    # TODO: yield the socket for advanced users? Other advanced use cases
+    # (perhaps factor out socket creation itself)?
+    # TODO: probably push some of this down into Paramiko
+    @contextmanager
+    @opens
+    def forward_local(
+        self,
+        local_port,
+        remote_port=None,
+        remote_host="localhost",
+        local_host="localhost",
+    ):
+        """
+        Open a tunnel connecting ``local_port`` to the server's environment.
+
+        For example, say you want to connect to a remote PostgreSQL database
+        which is locked down and only accessible via the system it's running
+        on. You have SSH access to this server, so you can temporarily make
+        port 5432 on your local system act like port 5432 on the server::
+
+            import psycopg2
+            from fabric import Connection
+
+            with Connection('my-db-server').forward_local(5432):
+                db = psycopg2.connect(
+                    host='localhost', port=5432, database='mydb'
+                )
+                # Do things with 'db' here
+
+        This method is analogous to using the ``-L`` option of OpenSSH's
+        ``ssh`` program.
+
+        :param int local_port: The local port number on which to listen.
+
+        :param int remote_port:
+            The remote port number. Defaults to the same value as
+            ``local_port``.
+
+        :param str local_host:
+            The local hostname/interface on which to listen. Default:
+            ``localhost``.
+
+        :param str remote_host:
+            The remote hostname serving the forwarded remote port. Default:
+            ``localhost`` (i.e., the host this `.Connection` is connected to.)
+
+        :returns:
+            Nothing; this method is only useful as a context manager affecting
+            local operating system state.
+
+        .. versionadded:: 2.0
+        """
+        if not remote_port:
+            remote_port = local_port
+
+        # TunnelManager does all of the work, sitting in the background (so we
+        # can yield) and spawning threads every time somebody connects to our
+        # local port.
+        finished = Event()
+        manager = TunnelManager(
+            local_port=local_port,
+            local_host=local_host,
+            remote_port=remote_port,
+            remote_host=remote_host,
+            # TODO: not a huge fan of handing in our transport, but...?
+            transport=self.transport,
+            finished=finished,
+        )
+        manager.start()
+
+        # Return control to caller now that things ought to be operational
+        try:
+            yield
+        # Teardown once user exits block
+        finally:
+            # Signal to manager that it should close all open tunnels
+            finished.set()
+            # Then wait for it to do so
+            manager.join()
+            # Raise threading errors from within the manager, which would be
+            # one of:
+            # - an inner ThreadException, which was created by the manager on
+            # behalf of its Tunnels; this gets directly raised.
+            # - some other exception, which would thus have occurred in the
+            # manager itself; we wrap this in a new ThreadException.
+            # NOTE: in these cases, some of the metadata tracking in
+            # ExceptionHandlingThread/ExceptionWrapper/ThreadException (which
+            # is useful when dealing with multiple nearly-identical sibling IO
+            # threads) is superfluous, but it doesn't feel worth breaking
+            # things up further; we just ignore it for now.
+            wrapper = manager.exception()
+            if wrapper is not None:
+                if wrapper.type is ThreadException:
+                    raise wrapper.value
+                else:
+                    raise ThreadException([wrapper])
+
+            # TODO: cancel port forward on transport? Does that even make sense
+            # here (where we used direct-tcpip) vs the opposite method (which
+            # is what uses forward-tcpip)?
+
+    # TODO: probably push some of this down into Paramiko
+    @contextmanager
+    @opens
+    def forward_remote(
+        self,
+        remote_port,
+        local_port=None,
+        remote_host="127.0.0.1",
+        local_host="localhost",
+    ):
+        """
+        Open a tunnel connecting ``remote_port`` to the local environment.
+
+        For example, say you're running a daemon in development mode on your
+        workstation at port 8080, and want to funnel traffic to it from a
+        production or staging environment.
+
+        In most situations this isn't possible as your office/home network
+        probably blocks inbound traffic. But you have SSH access to this
+        server, so you can temporarily make port 8080 on that server act like
+        port 8080 on your workstation::
+
+            from fabric import Connection
+
+            c = Connection('my-remote-server')
+            with c.forward_remote(8080):
+                c.run("remote-data-writer --port 8080")
+                # Assuming remote-data-writer runs until interrupted, this will
+                # stay open until you Ctrl-C...
+
+        This method is analogous to using the ``-R`` option of OpenSSH's
+        ``ssh`` program.
+
+        :param int remote_port: The remote port number on which to listen.
+
+        :param int local_port:
+            The local port number. Defaults to the same value as
+            ``remote_port``.
+
+        :param str local_host:
+            The local hostname/interface the forwarded connection talks to.
+            Default: ``localhost``.
+
+        :param str remote_host:
+            The remote interface address to listen on when forwarding
+            connections. Default: ``127.0.0.1`` (i.e. only listen on the remote
+            localhost).
+
+        :returns:
+            Nothing; this method is only useful as a context manager affecting
+            local operating system state.
+
+        .. versionadded:: 2.0
+        """
+        if not local_port:
+            local_port = remote_port
+        # Callback executes on each connection to the remote port and is given
+        # a Channel hooked up to said port. (We don't actually care about the
+        # source/dest host/port pairs at all; only whether the channel has data
+        # to read and suchlike.)
+        # We then pair that channel with a new 'outbound' socket connection to
+        # the local host/port being forwarded, in a new Tunnel.
+        # That Tunnel is then added to a shared data structure so we can track
+        # & close them during shutdown.
+        #
+        # TODO: this approach is less than ideal because we have to share state
+        # between ourselves & the callback handed into the transport's own
+        # thread handling (which is roughly analogous to our self-controlled
+        # TunnelManager for local forwarding). See if we can use more of
+        # Paramiko's API (or improve it and then do so) so that isn't
+        # necessary.
+        tunnels = []
+
+        def callback(channel, src_addr_tup, dst_addr_tup):
+            sock = socket.socket()
+            # TODO: handle connection failure such that channel, etc get closed
+            sock.connect((local_host, local_port))
+            # TODO: we don't actually need to generate the Events at our level,
+            # do we? Just let Tunnel.__init__ do it; all we do is "press its
+            # button" on shutdown...
+            tunnel = Tunnel(channel=channel, sock=sock, finished=Event())
+            tunnel.start()
+            # Communication between ourselves & the Paramiko handling subthread
+            tunnels.append(tunnel)
+
+        # Ask Paramiko (really, the remote sshd) to call our callback whenever
+        # connections are established on the remote iface/port.
+        # transport.request_port_forward(remote_host, remote_port, callback)
+        try:
+            self.transport.request_port_forward(
+                address=remote_host, port=remote_port, handler=callback
+            )
+            yield
+        finally:
+            # TODO: see above re: lack of a TunnelManager
+            # TODO: and/or also refactor with TunnelManager re: shutdown logic.
+            # E.g. maybe have a non-thread TunnelManager-alike with a method
+            # that acts as the callback? At least then there's a tiny bit more
+            # encapsulation...meh.
+            for tunnel in tunnels:
+                tunnel.finished.set()
+                tunnel.join()
+            self.transport.cancel_port_forward(
+                address=remote_host, port=remote_port
+            )
diff -Nru fabric-1.14.0/fabric2/exceptions.py fabric-2.5.0/fabric2/exceptions.py
--- fabric-1.14.0/fabric2/exceptions.py	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/fabric2/exceptions.py	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,26 @@
+# TODO: this may want to move to Invoke if we can find a use for it there too?
+# Or make it _more_ narrowly focused and stay here?
+class NothingToDo(Exception):
+    pass
+
+
+class GroupException(Exception):
+    """
+    Lightweight exception wrapper for `.GroupResult` when one contains errors.
+
+    .. versionadded:: 2.0
+    """
+
+    def __init__(self, result):
+        #: The `.GroupResult` object which would have been returned, had there
+        #: been no errors. See its docstring (and that of `.Group`) for
+        #: details.
+        self.result = result
+
+
+class InvalidV1Env(Exception):
+    """
+    Raised when attempting to import a Fabric 1 ``env`` which is missing data.
+    """
+
+    pass
diff -Nru fabric-1.14.0/fabric2/executor.py fabric-2.5.0/fabric2/executor.py
--- fabric-1.14.0/fabric2/executor.py	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/fabric2/executor.py	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,127 @@
+import invoke
+from invoke import Call, Task
+
+from .tasks import ConnectionCall
+from .exceptions import NothingToDo
+from .util import debug
+
+
+class Executor(invoke.Executor):
+    """
+    `~invoke.executor.Executor` subclass which understands Fabric concepts.
+
+    Designed to work in tandem with Fabric's `@task
+    <fabric.tasks.task>`/`~fabric.tasks.Task`, and is capable of acting on
+    information stored on the resulting objects -- such as default host lists.
+
+    This class is written to be backwards compatible with vanilla Invoke-level
+    tasks, which it simply delegates to its superclass.
+
+    Please see the parent class' `documentation <invoke.executor.Executor>` for
+    details on most public API members and object lifecycle.
+    """
+
+    def normalize_hosts(self, hosts):
+        """
+        Normalize mixed host-strings-or-kwarg-dicts into kwarg dicts only.
+
+        In other words, transforms data taken from the CLI (--hosts, always
+        strings) or decorator arguments (may be strings or kwarg dicts) into
+        kwargs suitable for creating Connection instances.
+
+        Subclasses may wish to override or extend this to perform, for example,
+        database or custom config file lookups (vs this default behavior, which
+        is to simply assume that strings are 'host' kwargs).
+
+        :param hosts:
+            Potentially heterogenous list of host connection values, as per the
+            ``hosts`` param to `.task`.
+
+        :returns: Homogenous list of Connection init kwarg dicts.
+        """
+        dicts = []
+        for value in hosts or []:
+            # Assume first posarg to Connection() if not already a dict.
+            if not isinstance(value, dict):
+                value = dict(host=value)
+            dicts.append(value)
+        return dicts
+
+    def expand_calls(self, calls, apply_hosts=True):
+        # Generate new call list with per-host variants & Connections inserted
+        ret = []
+        cli_hosts = []
+        host_str = self.core[0].args.hosts.value
+        if apply_hosts and host_str:
+            cli_hosts = host_str.split(",")
+        for call in calls:
+            if isinstance(call, Task):
+                call = Call(task=call)
+            # TODO: expand this to allow multiple types of execution plans,
+            # pending outcome of invoke#461 (which, if flexible enough to
+            # handle intersect of dependencies+parameterization, just becomes
+            # 'honor that new feature of Invoke')
+            # TODO: roles, other non-runtime host parameterizations, etc
+            # Pre-tasks get added only once, not once per host.
+            ret.extend(self.expand_calls(call.pre, apply_hosts=False))
+            # Determine final desired host list based on CLI and task values
+            # (with CLI, being closer to runtime, winning) and normalize to
+            # Connection-init kwargs.
+            call_hosts = getattr(call, "hosts", None)
+            cxn_params = self.normalize_hosts(cli_hosts or call_hosts)
+            # Main task, per host/connection
+            for init_kwargs in cxn_params:
+                ret.append(self.parameterize(call, init_kwargs))
+            # Deal with lack of hosts list (acts same as `inv` in that case)
+            # TODO: no tests for this branch?
+            if not cxn_params:
+                ret.append(call)
+            # Post-tasks added once, not once per host.
+            ret.extend(self.expand_calls(call.post, apply_hosts=False))
+        # Add remainder as anonymous task
+        if self.core.remainder:
+            # TODO: this will need to change once there are more options for
+            # setting host lists besides "-H or 100% within-task"
+            if not cli_hosts:
+                raise NothingToDo(
+                    "Was told to run a command, but not given any hosts to run it on!"  # noqa
+                )
+
+            def anonymous(c):
+                c.run(self.core.remainder)
+
+            anon = Call(Task(body=anonymous))
+            # TODO: see above TODOs about non-parameterized setups, roles etc
+            # TODO: will likely need to refactor that logic some more so it can
+            # be used both there and here.
+            for init_kwargs in self.normalize_hosts(cli_hosts):
+                ret.append(self.parameterize(anon, init_kwargs))
+        return ret
+
+    def parameterize(self, call, connection_init_kwargs):
+        """
+        Parameterize a Call with its Context set to a per-host Connection.
+
+        :param call:
+            The generic `.Call` being parameterized.
+        :param connection_init_kwargs:
+            The dict of `.Connection` init params/kwargs to attach to the
+            resulting `.ConnectionCall`.
+
+        :returns:
+            `.ConnectionCall`.
+        """
+        msg = "Parameterizing {!r} with Connection kwargs {!r}"
+        debug(msg.format(call, connection_init_kwargs))
+        # Generate a custom ConnectionCall that has init_kwargs (used for
+        # creating the Connection at runtime) set to the requested params.
+        new_call_kwargs = dict(init_kwargs=connection_init_kwargs)
+        clone = call.clone(into=ConnectionCall, with_=new_call_kwargs)
+        return clone
+
+    def dedupe(self, tasks):
+        # Don't perform deduping, we will often have "duplicate" tasks w/
+        # distinct host values/etc.
+        # TODO: might want some deduplication later on though - falls under
+        # "how to mesh parameterization with pre/post/etc deduping".
+        return tasks
diff -Nru fabric-1.14.0/fabric2/group.py fabric-2.5.0/fabric2/group.py
--- fabric-1.14.0/fabric2/group.py	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/fabric2/group.py	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,300 @@
+try:
+    from invoke.vendor.six.moves.queue import Queue
+except ImportError:
+    from six.moves.queue import Queue
+
+from invoke.util import ExceptionHandlingThread
+
+from .connection import Connection
+from .exceptions import GroupException
+
+
+class Group(list):
+    """
+    A collection of `.Connection` objects whose API operates on its contents.
+
+    .. warning::
+        **This is a partially abstract class**; you need to use one of its
+        concrete subclasses (such as `.SerialGroup` or `.ThreadingGroup`) or
+        you'll get ``NotImplementedError`` on most of the methods.
+
+    Most methods in this class mirror those of `.Connection`, taking the same
+    arguments; however their return values and exception-raising behavior
+    differs:
+
+    - Return values are dict-like objects (`.GroupResult`) mapping
+      `.Connection` objects to the return value for the respective connections:
+      `.Group.run` returns a map of `.Connection` to `.runners.Result`,
+      `.Group.get` returns a map of `.Connection` to `.transfer.Result`, etc.
+    - If any connections encountered exceptions, a `.GroupException` is raised,
+      which is a thin wrapper around what would otherwise have been the
+      `.GroupResult` returned; within that wrapped `.GroupResult`, the
+      excepting connections map to the exception that was raised, in place of a
+      ``Result`` (as no ``Result`` was obtained.) Any non-excepting connections
+      will have a ``Result`` value, as normal.
+
+    For example, when no exceptions occur, a session might look like this::
+
+        >>> group = SerialGroup('host1', 'host2')
+        >>> group.run("this is fine")
+        {
+            <Connection host='host1'>: <Result cmd='this is fine' exited=0>,
+            <Connection host='host2'>: <Result cmd='this is fine' exited=0>,
+        }
+
+    With exceptions (anywhere from 1 to "all of them"), it looks like so; note
+    the different exception classes, e.g. `~invoke.exceptions.UnexpectedExit`
+    for a completed session whose command exited poorly, versus
+    `socket.gaierror` for a host that had DNS problems::
+
+        >>> group = SerialGroup('host1', 'host2', 'notahost')
+        >>> group.run("will it blend?")
+        {
+            <Connection host='host1'>: <Result cmd='will it blend?' exited=0>,
+            <Connection host='host2'>: <UnexpectedExit: cmd='...' exited=1>,
+            <Connection host='notahost'>: gaierror(...),
+        }
+
+    As with `.Connection`, `.Group` objects may be used as context managers,
+    which will automatically `.close` the object on block exit.
+
+    .. versionadded:: 2.0
+    .. versionchanged:: 2.4
+        Added context manager behavior.
+    """
+
+    def __init__(self, *hosts, **kwargs):
+        """
+        Create a group of connections from one or more shorthand host strings.
+
+        See `.Connection` for details on the format of these strings - they
+        will be used as the first positional argument of `.Connection`
+        constructors.
+
+        Any keyword arguments given will be forwarded directly to those
+        `.Connection` constructors as well. For example, to get a serially
+        executing group object that connects to ``admin@host1``,
+        ``admin@host2`` and ``admin@host3``, and forwards your SSH agent too::
+
+            group = SerialGroup(
+                "host1", "host2", "host3", user="admin", forward_agent=True,
+            )
+
+        .. versionchanged:: 2.3
+            Added ``**kwargs`` (was previously only ``*hosts``).
+        """
+        # TODO: #563, #388 (could be here or higher up in Program area)
+        self.extend([Connection(host, **kwargs) for host in hosts])
+
+    @classmethod
+    def from_connections(cls, connections):
+        """
+        Alternate constructor accepting `.Connection` objects.
+
+        .. versionadded:: 2.0
+        """
+        # TODO: *args here too; or maybe just fold into __init__ and type
+        # check?
+        group = cls()
+        group.extend(connections)
+        return group
+
+    def run(self, *args, **kwargs):
+        """
+        Executes `.Connection.run` on all member `Connections <.Connection>`.
+
+        :returns: a `.GroupResult`.
+
+        .. versionadded:: 2.0
+        """
+        # TODO: probably best to suck it up & match actual run() sig?
+        # TODO: how to change method of execution across contents? subclass,
+        # kwargs, additional methods, inject an executor? Doing subclass for
+        # now, but not 100% sure it's the best route.
+        # TODO: also need way to deal with duplicate connections (see THOUGHTS)
+        # TODO: and errors - probably FailureSet? How to handle other,
+        # regular, non Failure, exceptions though? Still need an aggregate
+        # exception type either way, whether it is FailureSet or what...
+        # TODO: OTOH, users may well want to be able to operate on the hosts
+        # that did not fail (esp if failure % is low) so we really _do_ want
+        # something like a result object mixing success and failure, or maybe a
+        # golang style two-tuple of successes and failures?
+        # TODO: or keep going w/ a "return or except", but the object is
+        # largely similar (if not identical) in both situations, with the
+        # exception just being the signal that Shit Broke?
+        raise NotImplementedError
+
+    # TODO: how to handle sudo? Probably just an inner worker method that takes
+    # the method name to actually call (run, sudo, etc)?
+
+    # TODO: this all needs to mesh well with similar strategies applied to
+    # entire tasks - so that may still end up factored out into Executors or
+    # something lower level than both those and these?
+
+    # TODO: local? Invoke wants ability to do that on its own though, which
+    # would be distinct from Group. (May want to switch Group to use that,
+    # though, whatever it ends up being?)
+
+    def get(self, *args, **kwargs):
+        """
+        Executes `.Connection.get` on all member `Connections <.Connection>`.
+
+        :returns: a `.GroupResult`.
+
+        .. versionadded:: 2.0
+        """
+        # TODO: probably best to suck it up & match actual get() sig?
+        # TODO: actually implement on subclasses
+        raise NotImplementedError
+
+    def close(self):
+        """
+        Executes `.Connection.close` on all member `Connections <.Connection>`.
+
+        .. versionadded:: 2.4
+        """
+        for cxn in self:
+            cxn.close()
+
+    def __enter__(self):
+        return self
+
+    def __exit__(self, *exc):
+        self.close()
+
+
+class SerialGroup(Group):
+    """
+    Subclass of `.Group` which executes in simple, serial fashion.
+
+    .. versionadded:: 2.0
+    """
+
+    def run(self, *args, **kwargs):
+        results = GroupResult()
+        excepted = False
+        for cxn in self:
+            try:
+                results[cxn] = cxn.run(*args, **kwargs)
+            except Exception as e:
+                results[cxn] = e
+                excepted = True
+        if excepted:
+            raise GroupException(results)
+        return results
+
+
+def thread_worker(cxn, queue, args, kwargs):
+    result = cxn.run(*args, **kwargs)
+    # TODO: namedtuple or attrs object?
+    queue.put((cxn, result))
+
+
+class ThreadingGroup(Group):
+    """
+    Subclass of `.Group` which uses threading to execute concurrently.
+
+    .. versionadded:: 2.0
+    """
+
+    def run(self, *args, **kwargs):
+        results = GroupResult()
+        queue = Queue()
+        threads = []
+        for cxn in self:
+            my_kwargs = dict(cxn=cxn, queue=queue, args=args, kwargs=kwargs)
+            thread = ExceptionHandlingThread(
+                target=thread_worker, kwargs=my_kwargs
+            )
+            threads.append(thread)
+        for thread in threads:
+            thread.start()
+        for thread in threads:
+            # TODO: configurable join timeout
+            # TODO: (in sudo's version) configurability around interactive
+            # prompting resulting in an exception instead, as in v1
+            thread.join()
+        # Get non-exception results from queue
+        while not queue.empty():
+            # TODO: io-sleep? shouldn't matter if all threads are now joined
+            cxn, result = queue.get(block=False)
+            # TODO: outstanding musings about how exactly aggregate results
+            # ought to ideally operate...heterogenous obj like this, multiple
+            # objs, ??
+            results[cxn] = result
+        # Get exceptions from the threads themselves.
+        # TODO: in a non-thread setup, this would differ, e.g.:
+        # - a queue if using multiprocessing
+        # - some other state-passing mechanism if using e.g. coroutines
+        # - ???
+        excepted = False
+        for thread in threads:
+            wrapper = thread.exception()
+            if wrapper is not None:
+                # Outer kwargs is Thread instantiation kwargs, inner is kwargs
+                # passed to thread target/body.
+                cxn = wrapper.kwargs["kwargs"]["cxn"]
+                results[cxn] = wrapper.value
+                excepted = True
+        if excepted:
+            raise GroupException(results)
+        return results
+
+
+class GroupResult(dict):
+    """
+    Collection of results and/or exceptions arising from `.Group` methods.
+
+    Acts like a dict, but adds a couple convenience methods, to wit:
+
+    - Keys are the individual `.Connection` objects from within the `.Group`.
+    - Values are either return values / results from the called method (e.g.
+      `.runners.Result` objects), *or* an exception object, if one prevented
+      the method from returning.
+    - Subclasses `dict`, so has all dict methods.
+    - Has `.succeeded` and `.failed` attributes containing sub-dicts limited to
+      just those key/value pairs that succeeded or encountered exceptions,
+      respectively.
+
+      - Of note, these attributes allow high level logic, e.g. ``if
+        mygroup.run('command').failed`` and so forth.
+
+    .. versionadded:: 2.0
+    """
+
+    def __init__(self, *args, **kwargs):
+        super(dict, self).__init__(*args, **kwargs)
+        self._successes = {}
+        self._failures = {}
+
+    def _bifurcate(self):
+        # Short-circuit to avoid reprocessing every access.
+        if self._successes or self._failures:
+            return
+        # TODO: if we ever expect .succeeded/.failed to be useful before a
+        # GroupResult is fully initialized, this needs to become smarter.
+        for key, value in self.items():
+            if isinstance(value, BaseException):
+                self._failures[key] = value
+            else:
+                self._successes[key] = value
+
+    @property
+    def succeeded(self):
+        """
+        A sub-dict containing only successful results.
+
+        .. versionadded:: 2.0
+        """
+        self._bifurcate()
+        return self._successes
+
+    @property
+    def failed(self):
+        """
+        A sub-dict containing only failed results.
+
+        .. versionadded:: 2.0
+        """
+        self._bifurcate()
+        return self._failures
diff -Nru fabric-1.14.0/fabric2/__init__.py fabric-2.5.0/fabric2/__init__.py
--- fabric-1.14.0/fabric2/__init__.py	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/fabric2/__init__.py	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,7 @@
+# flake8: noqa
+from ._version import __version_info__, __version__
+from .connection import Config, Connection
+from .runners import Remote, Result
+from .group import Group, SerialGroup, ThreadingGroup, GroupResult
+from .tasks import task, Task
+from .executor import Executor
diff -Nru fabric-1.14.0/fabric2/__main__.py fabric-2.5.0/fabric2/__main__.py
--- fabric-1.14.0/fabric2/__main__.py	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/fabric2/__main__.py	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,9 @@
+"""
+This code provides the ability to run fabric
+package as a script
+Usage: python -m fabric
+"""
+
+from .main import program
+
+program.run()
diff -Nru fabric-1.14.0/fabric2/main.py fabric-2.5.0/fabric2/main.py
--- fabric-1.14.0/fabric2/main.py	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/fabric2/main.py	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,159 @@
+"""
+CLI entrypoint & parser configuration.
+
+Builds on top of Invoke's core functionality for same.
+"""
+
+import getpass
+
+from invoke import Argument, Collection, Program
+from invoke import __version__ as invoke
+from paramiko import __version__ as paramiko
+
+from . import __version__ as fabric
+from . import Config, Executor
+
+
+class Fab(Program):
+    def print_version(self):
+        super(Fab, self).print_version()
+        print("Paramiko {}".format(paramiko))
+        print("Invoke {}".format(invoke))
+
+    def core_args(self):
+        core_args = super(Fab, self).core_args()
+        my_args = [
+            Argument(
+                names=("H", "hosts"),
+                help="Comma-separated host name(s) to execute tasks against.",
+            ),
+            Argument(
+                names=("i", "identity"),
+                kind=list,  # Same as OpenSSH, can give >1 key
+                # TODO: automatically add hint about iterable-ness to Invoke
+                # help display machinery?
+                help="Path to runtime SSH identity (key) file. May be given multiple times.",  # noqa
+            ),
+            # TODO: worth having short flags for these prompt args?
+            Argument(
+                names=("prompt-for-login-password",),
+                kind=bool,
+                help="Request an upfront SSH-auth password prompt.",
+            ),
+            Argument(
+                names=("prompt-for-passphrase",),
+                kind=bool,
+                help="Request an upfront SSH key passphrase prompt.",
+            ),
+            Argument(
+                names=("S", "ssh-config"),
+                help="Path to runtime SSH config file.",
+            ),
+            Argument(
+                names=("t", "connect-timeout"),
+                kind=int,
+                help="Specifies default connection timeout, in seconds.",
+            ),
+        ]
+        return core_args + my_args
+
+    @property
+    def _remainder_only(self):
+        # No 'unparsed' (i.e. tokens intended for task contexts), and remainder
+        # (text after a double-dash) implies a contextless/taskless remainder
+        # execution of the style 'fab -H host -- command'.
+        # NOTE: must ALSO check to ensure the double dash isn't being used for
+        # tab completion machinery...
+        return (
+            not self.core.unparsed
+            and self.core.remainder
+            and not self.args.complete.value
+        )
+
+    def load_collection(self):
+        # Stick in a dummy Collection if it looks like we were invoked w/o any
+        # tasks, and with a remainder.
+        # This isn't super ideal, but Invoke proper has no obvious "just run my
+        # remainder" use case, so having it be capable of running w/o any task
+        # module, makes no sense. But we want that capability for testing &
+        # things like 'fab -H x,y,z -- mycommand'.
+        if self._remainder_only:
+            # TODO: hm we're probably not honoring project-specific configs in
+            # this branch; is it worth having it assume CWD==project, since
+            # that's often what users expect? Even tho no task collection to
+            # honor the real "lives by task coll"?
+            self.collection = Collection()
+        else:
+            super(Fab, self).load_collection()
+
+    def no_tasks_given(self):
+        # As above, neuter the usual "hey you didn't give me any tasks, let me
+        # print help for you" behavior, if necessary.
+        if not self._remainder_only:
+            super(Fab, self).no_tasks_given()
+
+    def create_config(self):
+        # Create config, as parent does, but with lazy=True to avoid our own
+        # SSH config autoload. (Otherwise, we can't correctly load _just_ the
+        # runtime file if one's being given later.)
+        self.config = self.config_class(lazy=True)
+        # However, we don't really want the parent class' lazy behavior (which
+        # skips loading system/global invoke-type conf files) so we manually do
+        # that here to match upstream behavior.
+        self.config.load_base_conf_files()
+        # And merge again so that data is available.
+        # TODO: really need to either A) stop giving fucks about calling
+        # merge() "too many times", or B) make merge() itself determine whether
+        # it needs to run and/or just merge stuff that's changed, so log spam
+        # isn't as bad.
+        self.config.merge()
+
+    def update_config(self):
+        # Note runtime SSH path, if given, and load SSH configurations.
+        # NOTE: must do parent before our work, in case users want to disable
+        # SSH config loading within a runtime-level conf file/flag.
+        super(Fab, self).update_config(merge=False)
+        self.config.set_runtime_ssh_path(self.args["ssh-config"].value)
+        self.config.load_ssh_config()
+        # Load -i identity file, if given, into connect_kwargs, at overrides
+        # level.
+        # TODO: this feels a little gross, but since the parent has already
+        # called load_overrides, this is best we can do for now w/o losing
+        # data. Still feels correct; just might be cleaner to have even more
+        # Config API members around this sort of thing. Shrug.
+        connect_kwargs = {}
+        path = self.args["identity"].value
+        if path:
+            connect_kwargs["key_filename"] = path
+        # Ditto for connect timeout
+        timeout = self.args["connect-timeout"].value
+        if timeout:
+            connect_kwargs["timeout"] = timeout
+        # Secrets prompts that want to happen at handoff time instead of
+        # later/at user-time.
+        # TODO: should this become part of Invoke proper in case other
+        # downstreams have need of it? E.g. a prompt Argument 'type'? We're
+        # already doing a similar thing there for sudo password...
+        if self.args["prompt-for-login-password"].value:
+            prompt = "Enter login password for use with SSH auth: "
+            connect_kwargs["password"] = getpass.getpass(prompt)
+        if self.args["prompt-for-passphrase"].value:
+            prompt = "Enter passphrase for use unlocking SSH keys: "
+            connect_kwargs["passphrase"] = getpass.getpass(prompt)
+        self.config._overrides["connect_kwargs"] = connect_kwargs
+        # Since we gave merge=False above, we must do it ourselves here. (Also
+        # allows us to 'compile' our overrides manipulation.)
+        self.config.merge()
+
+
+# Mostly a concession to testing.
+def make_program():
+    return Fab(
+        name="Fabric",
+        version=fabric,
+        executor_class=Executor,
+        config_class=Config,
+    )
+
+
+program = make_program()
diff -Nru fabric-1.14.0/fabric2/runners.py fabric-2.5.0/fabric2/runners.py
--- fabric-1.14.0/fabric2/runners.py	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/fabric2/runners.py	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,148 @@
+from invoke import Runner, pty_size, Result as InvokeResult
+
+
+class Remote(Runner):
+    """
+    Run a shell command over an SSH connection.
+
+    This class subclasses `invoke.runners.Runner`; please see its documentation
+    for most public API details.
+
+    .. note::
+        `.Remote`'s ``__init__`` method expects a `.Connection` (or subclass)
+        instance for its ``context`` argument.
+
+    .. versionadded:: 2.0
+    """
+
+    def __init__(self, *args, **kwargs):
+        """
+        Thin wrapper for superclass' ``__init__``; please see it for details.
+
+        Additional keyword arguments defined here are listed below.
+
+        :param bool inline_env:
+            Whether to 'inline' shell env vars as prefixed parameters, instead
+            of trying to submit them via `.Channel.update_environment`.
+            Default:: ``False``.
+
+        .. versionchanged:: 2.3
+            Added the ``inline_env`` parameter.
+        """
+        self.inline_env = kwargs.pop("inline_env", None)
+        super(Remote, self).__init__(*args, **kwargs)
+
+    def start(self, command, shell, env, timeout=None):
+        self.channel = self.context.create_session()
+        if self.using_pty:
+            rows, cols = pty_size()
+            self.channel.get_pty(width=rows, height=cols)
+        if env:
+            # TODO: honor SendEnv from ssh_config (but if we do, _should_ we
+            # honor it even when prefixing? That would depart from OpenSSH
+            # somewhat (albeit as a "what we can do that it cannot" feature...)
+            if self.inline_env:
+                # TODO: escaping, if we can find a FOOLPROOF THIRD PARTY METHOD
+                # for doing so!
+                # TODO: switch to using a higher-level generic command
+                # prefixing functionality, when implemented.
+                parameters = " ".join(
+                    ["{}={}".format(k, v) for k, v in sorted(env.items())]
+                )
+                # NOTE: we can assume 'export' and '&&' relatively safely, as
+                # sshd always brings some shell into play, even if it's just
+                # /bin/sh.
+                command = "export {} && {}".format(parameters, command)
+            else:
+                self.channel.update_environment(env)
+        self.channel.exec_command(command)
+
+    def read_proc_stdout(self, num_bytes):
+        return self.channel.recv(num_bytes)
+
+    def read_proc_stderr(self, num_bytes):
+        return self.channel.recv_stderr(num_bytes)
+
+    def _write_proc_stdin(self, data):
+        return self.channel.sendall(data)
+
+    def close_proc_stdin(self):
+        return self.channel.shutdown_write()
+
+    @property
+    def process_is_finished(self):
+        return self.channel.exit_status_ready()
+
+    def send_interrupt(self, interrupt):
+        # NOTE: in v1, we just reraised the KeyboardInterrupt unless a PTY was
+        # present; this seems to have been because without a PTY, the
+        # below escape sequence is ignored, so all we can do is immediately
+        # terminate on our end.
+        # NOTE: also in v1, the raising of the KeyboardInterrupt completely
+        # skipped all thread joining & cleanup; presumably regular interpreter
+        # shutdown suffices to tie everything off well enough.
+        if self.using_pty:
+            # Submit hex ASCII character 3, aka ETX, which most Unix PTYs
+            # interpret as a foreground SIGINT.
+            # TODO: is there anything else we can do here to be more portable?
+            self.channel.send(u"\x03")
+        else:
+            raise interrupt
+
+    def returncode(self):
+        return self.channel.recv_exit_status()
+
+    def generate_result(self, **kwargs):
+        kwargs["connection"] = self.context
+        return Result(**kwargs)
+
+    def stop(self):
+        if hasattr(self, "channel"):
+            self.channel.close()
+
+    def kill(self):
+        # Just close the channel immediately, which is about as close as we can
+        # get to a local SIGKILL unfortunately.
+        # TODO: consider _also_ calling .send_interrupt() and only doing this
+        # after another few seconds; but A) kinda fragile/complex and B) would
+        # belong in invoke.Runner anyways?
+        self.channel.close()
+
+    # TODO: shit that is in fab 1 run() but could apply to invoke.Local too:
+    # * see rest of stuff in _run_command/_execute in operations.py...there is
+    # a bunch that applies generally like optional exit codes, etc
+
+    # TODO: general shit not done yet
+    # * stdin; Local relies on local process management to ensure stdin is
+    # hooked up; we cannot do that.
+    # * output prefixing
+    # * agent forwarding
+    # * reading at 4096 bytes/time instead of whatever inv defaults to (also,
+    # document why we are doing that, iirc it changed recentlyish via ticket)
+    # * TODO: oh god so much more, go look it up
+
+    # TODO: shit that has no Local equivalent that we probs need to backfill
+    # into Runner, probably just as a "finish()" or "stop()" (to mirror
+    # start()):
+    # * channel close()
+    # * agent-forward close()
+
+
+class Result(InvokeResult):
+    """
+    An `invoke.runners.Result` exposing which `.Connection` was run against.
+
+    Exposes all attributes from its superclass, then adds a ``.connection``,
+    which is simply a reference to the `.Connection` whose method yielded this
+    result.
+
+    .. versionadded:: 2.0
+    """
+
+    def __init__(self, **kwargs):
+        connection = kwargs.pop("connection")
+        super(Result, self).__init__(**kwargs)
+        self.connection = connection
+
+    # TODO: have useful str/repr differentiation from invoke.Result,
+    # transfer.Result etc.
diff -Nru fabric-1.14.0/fabric2/tasks.py fabric-2.5.0/fabric2/tasks.py
--- fabric-1.14.0/fabric2/tasks.py	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/fabric2/tasks.py	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,116 @@
+import invoke
+
+from .connection import Connection
+
+
+class Task(invoke.Task):
+    """
+    Extends `invoke.tasks.Task` with knowledge of target hosts and similar.
+
+    As `invoke.tasks.Task` relegates documentation responsibility to its `@task
+    <invoke.tasks.task>` expression, so we relegate most details to our version
+    of `@task <fabric.tasks.task>` - please see its docs for details.
+
+    .. versionadded:: 2.1
+    """
+
+    def __init__(self, *args, **kwargs):
+        # Pull out our own kwargs before hitting super, which will TypeError on
+        # anything it doesn't know about.
+        self.hosts = kwargs.pop("hosts", None)
+        super(Task, self).__init__(*args, **kwargs)
+
+
+def task(*args, **kwargs):
+    """
+    Wraps/extends Invoke's `@task <invoke.tasks.task>` with extra kwargs.
+
+    See `the Invoke-level API docs <invoke.tasks.task>` for most details; this
+    Fabric-specific implementation adds the following additional keyword
+    arguments:
+
+    :param hosts:
+        An iterable of host-connection specifiers appropriate for eventually
+        instantiating a `.Connection`. The existence of this argument will
+        trigger automatic parameterization of the task when invoked from the
+        CLI, similar to the behavior of :option:`--hosts`.
+
+        .. note::
+            This parameterization is "lower-level" than that driven by
+            :option:`--hosts`: if a task decorated with this parameter is
+            executed in a session where :option:`--hosts` was given, the
+            CLI-driven value will win out.
+
+        List members may be one of:
+
+        - A string appropriate for being the first positional argument to
+          `.Connection` - see its docs for details, but these are typically
+          shorthand-only convenience strings like ``hostname.example.com`` or
+          ``user@host:port``.
+        - A dictionary appropriate for use as keyword arguments when
+          instantiating a `.Connection`. Useful for values that don't mesh well
+          with simple strings (e.g. statically defined IPv6 addresses) or to
+          bake in more complex info (eg ``connect_timeout``, ``connect_kwargs``
+          params like auth info, etc).
+
+        These two value types *may* be mixed together in the same list, though
+        we recommend that you keep things homogenous when possible, to avoid
+        confusion when debugging.
+
+        .. note::
+            No automatic deduplication of values is performed; if you pass in
+            multiple references to the same effective target host, the wrapped
+            task will execute on that host multiple times (including making
+            separate connections).
+
+    .. versionadded:: 2.1
+    """
+    # Override klass to be our own Task, not Invoke's, unless somebody gave it
+    # explicitly.
+    kwargs.setdefault("klass", Task)
+    return invoke.task(*args, **kwargs)
+
+
+class ConnectionCall(invoke.Call):
+    """
+    Subclass of `invoke.tasks.Call` that generates `Connections <.Connection>`.
+    """
+
+    def __init__(self, *args, **kwargs):
+        """
+        Creates a new `.ConnectionCall`.
+
+        Performs minor extensions to `~invoke.tasks.Call` -- see its docstring
+        for most details. Only specific-to-subclass params are documented here.
+
+        :param dict init_kwargs:
+            Keyword arguments used to create a new `.Connection` when the
+            wrapped task is executed. Default: ``None``.
+        """
+        init_kwargs = kwargs.pop("init_kwargs")  # , None)
+        super(ConnectionCall, self).__init__(*args, **kwargs)
+        self.init_kwargs = init_kwargs
+
+    def clone_kwargs(self):
+        # Extend superclass clone_kwargs to work in init_kwargs.
+        # TODO: this pattern comes up a lot; is there a better way to handle it
+        # without getting too crazy on the metaprogramming/over-engineering?
+        # Maybe something attrs library can help with (re: declaring "These are
+        # my bag-of-attributes attributes I want common stuff done to/with")
+        kwargs = super(ConnectionCall, self).clone_kwargs()
+        kwargs["init_kwargs"] = self.init_kwargs
+        return kwargs
+
+    def make_context(self, config):
+        kwargs = self.init_kwargs
+        # TODO: what about corner case of a decorator giving config in a hosts
+        # kwarg member?! For now let's stomp on it, and then if somebody runs
+        # into it, we can identify the use case & decide how best to deal.
+        kwargs["config"] = config
+        return Connection(**kwargs)
+
+    def __repr__(self):
+        ret = super(ConnectionCall, self).__repr__()
+        if self.init_kwargs:
+            ret = ret[:-1] + ", host='{}'>".format(self.init_kwargs["host"])
+        return ret
diff -Nru fabric-1.14.0/fabric2/testing/base.py fabric-2.5.0/fabric2/testing/base.py
--- fabric-1.14.0/fabric2/testing/base.py	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/fabric2/testing/base.py	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,402 @@
+"""
+This module contains helpers/fixtures to assist in testing Fabric-driven code.
+
+It is not intended for production use, and pulls in some test-oriented
+dependencies such as `mock <https://pypi.org/project/mock/>`_. You can install
+an 'extra' variant of Fabric to get these dependencies if you aren't already
+using them for your own testing purposes: ``pip install fabric[testing]``.
+
+.. note::
+    If you're using pytest for your test suite, you may be interested in
+    grabbing ``fabric[pytest]`` instead, which encompasses the dependencies of
+    both this module and the `fabric.testing.fixtures` module, which contains
+    pytest fixtures.
+
+.. versionadded:: 2.1
+"""
+
+from itertools import chain, repeat
+from io import BytesIO
+import os
+
+try:
+    from mock import Mock, PropertyMock, call, patch, ANY
+except ImportError:
+    import warnings
+
+    warning = (
+        "You appear to be missing some optional test-related dependencies;"
+        "please 'pip install fabric[testing]'."
+    )
+    warnings.warn(warning, ImportWarning)
+    raise
+
+
+class Command(object):
+    """
+    Data record specifying params of a command execution to mock/expect.
+
+    :param str cmd:
+        Command string to expect. If not given, no expectations about the
+        command executed will be set up. Default: ``None``.
+
+    :param bytes out: Data yielded as remote stdout. Default: ``b""``.
+
+    :param bytes err: Data yielded as remote stderr. Default: ``b""``.
+
+    :param int exit: Remote exit code. Default: ``0``.
+
+    :param int waits:
+        Number of calls to the channel's ``exit_status_ready`` that should
+        return ``False`` before it then returns ``True``. Default: ``0``
+        (``exit_status_ready`` will return ``True`` immediately).
+
+    .. versionadded:: 2.1
+    """
+
+    def __init__(self, cmd=None, out=b"", err=b"", in_=None, exit=0, waits=0):
+        self.cmd = cmd
+        self.out = out
+        self.err = err
+        self.in_ = in_
+        self.exit = exit
+        self.waits = waits
+
+    def __repr__(self):
+        # TODO: just leverage attrs, maybe vendored into Invoke so we don't
+        # grow more dependencies? Ehhh
+        return "<{} cmd={!r}>".format(self.__class__.__name__, self.cmd)
+
+
+class MockChannel(Mock):
+    """
+    Mock subclass that tracks state for its ``recv(_stderr)?`` methods.
+
+    Turns out abusing function closures inside MockRemote to track this state
+    only worked for 1 command per session!
+
+    .. versionadded:: 2.1
+    """
+
+    def __init__(self, *args, **kwargs):
+        # TODO: worth accepting strings and doing the BytesIO setup ourselves?
+        # Stored privately to avoid any possible collisions ever. shrug.
+        object.__setattr__(self, "__stdout", kwargs.pop("stdout"))
+        object.__setattr__(self, "__stderr", kwargs.pop("stderr"))
+        # Stdin less private so it can be asserted about
+        object.__setattr__(self, "_stdin", BytesIO())
+        super(MockChannel, self).__init__(*args, **kwargs)
+
+    def _get_child_mock(self, **kwargs):
+        # Don't return our own class on sub-mocks.
+        return Mock(**kwargs)
+
+    def recv(self, count):
+        return object.__getattribute__(self, "__stdout").read(count)
+
+    def recv_stderr(self, count):
+        return object.__getattribute__(self, "__stderr").read(count)
+
+    def sendall(self, data):
+        return object.__getattribute__(self, "_stdin").write(data)
+
+
+class Session(object):
+    """
+    A mock remote session of a single connection and 1 or more command execs.
+
+    Allows quick configuration of expected remote state, and also helps
+    generate the necessary test mocks used by `MockRemote` itself. Only useful
+    when handed into `MockRemote`.
+
+    The parameters ``cmd``, ``out``, ``err``, ``exit`` and ``waits`` are all
+    shorthand for the same constructor arguments for a single anonymous
+    `.Command`; see `.Command` for details.
+
+    To give fully explicit `.Command` objects, use the ``commands`` parameter.
+
+    :param str user:
+    :param str host:
+    :param int port:
+        Sets up expectations that a connection will be generated to the given
+        user, host and/or port. If ``None`` (default), no expectations are
+        generated / any value is accepted.
+
+    :param commands:
+        Iterable of `.Command` objects, used when mocking nontrivial sessions
+        involving >1 command execution per host. Default: ``None``.
+
+        .. note::
+            Giving ``cmd``, ``out`` etc alongside explicit ``commands`` is not
+            allowed and will result in an error.
+
+    .. versionadded:: 2.1
+    """
+
+    def __init__(
+        self,
+        host=None,
+        user=None,
+        port=None,
+        commands=None,
+        cmd=None,
+        out=None,
+        in_=None,
+        err=None,
+        exit=None,
+        waits=None,
+    ):
+        # Sanity check
+        params = cmd or out or err or exit or waits
+        if commands and params:
+            raise ValueError(
+                "You can't give both 'commands' and individual "
+                "Command parameters!"
+            )  # noqa
+        # Fill in values
+        self.host = host
+        self.user = user
+        self.port = port
+        self.commands = commands
+        if params:
+            # Honestly dunno which is dumber, this or duplicating Command's
+            # default kwarg values in this method's signature...sigh
+            kwargs = {}
+            if cmd is not None:
+                kwargs["cmd"] = cmd
+            if out is not None:
+                kwargs["out"] = out
+            if err is not None:
+                kwargs["err"] = err
+            if in_ is not None:
+                kwargs["in_"] = in_
+            if exit is not None:
+                kwargs["exit"] = exit
+            if waits is not None:
+                kwargs["waits"] = waits
+            self.commands = [Command(**kwargs)]
+        if not self.commands:
+            self.commands = [Command()]
+
+    def generate_mocks(self):
+        """
+        Mocks `~paramiko.client.SSHClient` and `~paramiko.channel.Channel`.
+
+        Specifically, the client will expect itself to be connected to
+        ``self.host`` (if given), the channels will be associated with the
+        client's `~paramiko.transport.Transport`, and the channels will
+        expect/provide command-execution behavior as specified on the
+        `.Command` objects supplied to this `.Session`.
+
+        The client is then attached as ``self.client`` and the channels as
+        ``self.channels``.
+
+        :returns:
+            ``None`` - this is mostly a "deferred setup" method and callers
+            will just reference the above attributes (and call more methods) as
+            needed.
+
+        .. versionadded:: 2.1
+        """
+        client = Mock()
+        transport = client.get_transport.return_value  # another Mock
+
+        # NOTE: this originally did chain([False], repeat(True)) so that
+        # get_transport().active was False initially, then True. However,
+        # because we also have to consider when get_transport() comes back None
+        # (which it does initially), the case where we get back a non-None
+        # transport _and_ it's not active yet, isn't useful to test, and
+        # complicates text expectations. So we don't, for now.
+        actives = repeat(True)
+        # NOTE: setting PropertyMocks on a mock's type() is apparently
+        # How It Must Be Done, otherwise it sets the real attr value.
+        type(transport).active = PropertyMock(side_effect=actives)
+
+        channels = []
+        for command in self.commands:
+            # Mock of a Channel instance, not e.g. Channel-the-class.
+            # Specifically, one that can track individual state for recv*().
+            channel = MockChannel(
+                stdout=BytesIO(command.out), stderr=BytesIO(command.err)
+            )
+            channel.recv_exit_status.return_value = command.exit
+
+            # If requested, make exit_status_ready return False the first N
+            # times it is called in the wait() loop.
+            readies = chain(repeat(False, command.waits), repeat(True))
+            channel.exit_status_ready.side_effect = readies
+
+            channels.append(channel)
+
+        # Have our transport yield those channel mocks in order when
+        # open_session() is called.
+        transport.open_session.side_effect = channels
+
+        self.client = client
+        self.channels = channels
+
+    def sanity_check(self):
+        # Per-session we expect a single transport get
+        transport = self.client.get_transport
+        transport.assert_called_once_with()
+        # And a single connect to our target host.
+        self.client.connect.assert_called_once_with(
+            username=self.user or ANY,
+            hostname=self.host or ANY,
+            port=self.port or ANY,
+        )
+
+        # Calls to open_session will be 1-per-command but are on transport, not
+        # channel, so we can only really inspect how many happened in
+        # aggregate. Save a list for later comparison to call_args.
+        session_opens = []
+
+        for channel, command in zip(self.channels, self.commands):
+            # Expect an open_session for each command exec
+            session_opens.append(call())
+            # Expect that the channel gets an exec_command
+            channel.exec_command.assert_called_with(command.cmd or ANY)
+            # Expect written stdin, if given
+            if command.in_:
+                assert channel._stdin.getvalue() == command.in_
+
+        # Make sure open_session was called expected number of times.
+        calls = transport.return_value.open_session.call_args_list
+        assert calls == session_opens
+
+
+class MockRemote(object):
+    """
+    Class representing mocked remote state.
+
+    By default this class is set up for start/stop style patching as opposed to
+    the more common context-manager or decorator approach; this is so it can be
+    used in situations requiring setup/teardown semantics.
+
+    Defaults to setting up a single anonymous `Session`, so it can be used as a
+    "request & forget" pytest fixture. Users requiring detailed remote session
+    expectations can call methods like `expect`, which wipe that anonymous
+    Session & set up a new one instead.
+
+    .. versionadded:: 2.1
+    """
+
+    def __init__(self):
+        self.expect_sessions(Session())
+
+    # TODO: make it easier to assume single session w/ >1 command?
+
+    def expect(self, *args, **kwargs):
+        """
+        Convenience method for creating & 'expect'ing a single `Session`.
+
+        Returns the single `MockChannel` yielded by that Session.
+
+        .. versionadded:: 2.1
+        """
+        return self.expect_sessions(Session(*args, **kwargs))[0]
+
+    def expect_sessions(self, *sessions):
+        """
+        Sets the mocked remote environment to expect the given ``sessions``.
+
+        Returns a list of `MockChannel` objects, one per input `Session`.
+
+        .. versionadded:: 2.1
+        """
+        # First, stop the default session to clean up its state, if it seems to
+        # be running.
+        self.stop()
+        # Update sessions list with new session(s)
+        self.sessions = sessions
+        # And start patching again, returning mocked channels
+        return self.start()
+
+    def start(self):
+        """
+        Start patching SSHClient with the stored sessions, returning channels.
+
+        .. versionadded:: 2.1
+        """
+        # Patch SSHClient so the sessions' generated mocks can be set as its
+        # return values
+        self.patcher = patcher = patch("fabric.connection.SSHClient")
+        SSHClient = patcher.start()
+        # Mock clients, to be inspected afterwards during sanity-checks
+        clients = []
+        for session in self.sessions:
+            session.generate_mocks()
+            clients.append(session.client)
+        # Each time the mocked SSHClient class is instantiated, it will
+        # yield one of our mocked clients (w/ mocked transport & channel)
+        # generated above.
+        SSHClient.side_effect = clients
+        return list(chain.from_iterable(x.channels for x in self.sessions))
+
+    def stop(self):
+        """
+        Stop patching SSHClient.
+
+        .. versionadded:: 2.1
+        """
+        # Short circuit if we don't seem to have start()ed yet.
+        if not hasattr(self, "patcher"):
+            return
+        # Stop patching SSHClient
+        self.patcher.stop()
+
+    def sanity(self):
+        """
+        Run post-execution sanity checks (usually 'was X called' tests.)
+
+        .. versionadded:: 2.1
+        """
+        for session in self.sessions:
+            # Basic sanity tests about transport, channel etc
+            session.sanity_check()
+
+
+# TODO: unify with the stuff in paramiko itself (now in its tests/conftest.py),
+# they're quite distinct and really shouldn't be.
+class MockSFTP(object):
+    """
+    Class managing mocked SFTP remote state.
+
+    Used in start/stop fashion in eg doctests; wrapped in the SFTP fixtures in
+    conftest.py for main use.
+
+    .. versionadded:: 2.1
+    """
+
+    def __init__(self, autostart=True):
+        if autostart:
+            self.start()
+
+    def start(self):
+        # Set up mocks
+        self.os_patcher = patch("fabric.transfer.os")
+        self.client_patcher = patch("fabric.connection.SSHClient")
+        mock_os = self.os_patcher.start()
+        Client = self.client_patcher.start()
+        sftp = Client.return_value.open_sftp.return_value
+
+        # Handle common filepath massage actions; tests will assume these.
+        def fake_abspath(path):
+            return "/local/{}".format(path)
+
+        mock_os.path.abspath.side_effect = fake_abspath
+        sftp.getcwd.return_value = "/remote"
+        # Ensure stat st_mode is a real number; Python 2 stat.S_IMODE doesn't
+        # appear to care if it's handed a MagicMock, but Python 3's does (?!)
+        fake_mode = 0o644  # arbitrary real-ish mode
+        sftp.stat.return_value.st_mode = fake_mode
+        mock_os.stat.return_value.st_mode = fake_mode
+        # Not super clear to me why the 'wraps' functionality in mock isn't
+        # working for this :(
+        mock_os.path.basename.side_effect = os.path.basename
+        # Return the sftp and OS mocks for use by decorator use case.
+        return sftp, mock_os
+
+    def stop(self):
+        self.os_patcher.stop()
+        self.client_patcher.stop()
diff -Nru fabric-1.14.0/fabric2/testing/fixtures.py fabric-2.5.0/fabric2/testing/fixtures.py
--- fabric-1.14.0/fabric2/testing/fixtures.py	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/fabric2/testing/fixtures.py	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,175 @@
+"""
+`pytest <https://pytest.org>`_ fixtures for easy use of Fabric test helpers.
+
+To get Fabric plus this module's dependencies (as well as those of the main
+`fabric.testing.base` module which these fixtures wrap), ``pip install
+fabric[pytest]``.
+
+The simplest way to get these fixtures loaded into your test suite so Pytest
+notices them is to import them into a ``conftest.py`` (`docs
+<http://pytest.readthedocs.io/en/latest/fixture.html#conftest-py-sharing-fixture-functions>`_).
+For example, if you intend to use the `remote` and `client` fixtures::
+
+    from fabric.testing.fixtures import client, remote
+
+.. versionadded:: 2.1
+"""
+
+try:
+    from pytest import fixture
+    from mock import patch, Mock
+except ImportError:
+    import warnings
+
+    warning = (
+        "You appear to be missing some optional test-related dependencies;"
+        "please 'pip install fabric[pytest]'."
+    )
+    warnings.warn(warning, ImportWarning)
+    raise
+
+from .. import Connection
+from ..transfer import Transfer
+
+# TODO: if we find a lot of people somehow ending up _with_ pytest but
+# _without_ mock and other deps from testing.base, consider doing the
+# try/except here too. But, really?
+
+from .base import MockRemote, MockSFTP
+
+
+@fixture
+def connection():
+    """
+    Yields a `.Connection` object with mocked methods.
+
+    Specifically:
+
+    - the hostname is set to ``"host"`` and the username to ``"user"``;
+    - the primary API members (`.Connection.run`, `.Connection.local`, etc) are
+      replaced with ``mock.Mock`` instances;
+    - the ``run.in_stream`` config option is set to ``False`` to avoid attempts
+      to read from stdin (which typically plays poorly with pytest and other
+      capturing test runners);
+
+    .. versionadded:: 2.1
+    """
+    c = Connection(host="host", user="user")
+    c.config.run.in_stream = False
+    c.run = Mock()
+    c.local = Mock()
+    # TODO: rest of API should get mocked too
+    # TODO: is there a nice way to mesh with MockRemote et al? Is that ever
+    # really that useful for code that just wants to assert about how run() and
+    # friends were called?
+    yield c
+
+
+#: A convenience rebinding of `connection`.
+#:
+#: .. versionadded:: 2.1
+cxn = connection
+
+
+@fixture
+def remote():
+    """
+    Fixture allowing setup of a mocked remote session & access to sub-mocks.
+
+    Yields a `.MockRemote` object (which may need to be updated via
+    `.MockRemote.expect`, `.MockRemote.expect_sessions`, etc; otherwise a
+    default session will be used) & calls `.MockRemote.sanity` and
+    `.MockRemote.stop` on teardown.
+
+    .. versionadded:: 2.1
+    """
+    remote = MockRemote()
+    yield remote
+    remote.sanity()
+    remote.stop()
+
+
+@fixture
+def sftp():
+    """
+    Fixture allowing setup of a mocked remote SFTP session.
+
+    Yields a 3-tuple of: Transfer() object, SFTPClient object, and mocked OS
+    module.
+
+    For many/most tests which only want the Transfer and/or SFTPClient objects,
+    see `sftp_objs` and `transfer` which wrap this fixture.
+
+    .. versionadded:: 2.1
+    """
+    mock = MockSFTP(autostart=False)
+    client, mock_os = mock.start()
+    transfer = Transfer(Connection("host"))
+    yield transfer, client, mock_os
+    # TODO: old mock_sftp() lacked any 'stop'...why? feels bad man
+
+
+@fixture
+def sftp_objs(sftp):
+    """
+    Wrapper for `sftp` which only yields the Transfer and SFTPClient.
+
+    .. versionadded:: 2.1
+    """
+    yield sftp[:2]
+
+
+@fixture
+def transfer(sftp):
+    """
+    Wrapper for `sftp` which only yields the Transfer object.
+
+    .. versionadded:: 2.1
+    """
+    yield sftp[0]
+
+
+@fixture
+def client():
+    """
+    Mocks `~paramiko.client.SSHClient` for testing calls to ``connect()``.
+
+    Yields a mocked ``SSHClient`` instance.
+
+    This fixture updates `~paramiko.client.SSHClient.get_transport` to return a
+    mock that appears active on first check, then inactive after, matching most
+    tests' needs by default:
+
+    - `.Connection` instantiates, with a None ``.transport``.
+    - Calls to ``.open()`` test ``.is_connected``, which returns ``False`` when
+      ``.transport`` is falsey, and so the first open will call
+      ``SSHClient.connect`` regardless.
+    - ``.open()`` then sets ``.transport`` to ``SSHClient.get_transport()``, so
+      ``Connection.transport`` is effectively
+      ``client.get_transport.return_value``.
+    - Subsequent activity will want to think the mocked SSHClient is
+      "connected", meaning we want the mocked transport's ``.active`` to be
+      ``True``.
+    - This includes `.Connection.close`, which short-circuits if
+      ``.is_connected``; having a statically ``True`` active flag means a full
+      open -> close cycle will run without error. (Only tests that double-close
+      or double-open should have issues here.)
+
+    End result is that:
+
+    - ``.is_connected`` behaves False after instantiation and before ``.open``,
+      then True after ``.open``
+    - ``.close`` will work normally on 1st call
+    - ``.close`` will behave "incorrectly" on subsequent calls (since it'll
+      think connection is still live.) Tests that check the idempotency of
+      ``.close`` will need to tweak their mock mid-test.
+
+    For 'full' fake remote session interaction (i.e. stdout/err
+    reading/writing, channel opens, etc) see `remote`.
+
+    .. versionadded:: 2.1
+    """
+    with patch("fabric.connection.SSHClient") as SSHClient:
+        client = SSHClient.return_value
+        client.get_transport.return_value = Mock(active=True)
+        yield client
diff -Nru fabric-1.14.0/fabric2/transfer.py fabric-2.5.0/fabric2/transfer.py
--- fabric-1.14.0/fabric2/transfer.py	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/fabric2/transfer.py	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,324 @@
+"""
+File transfer via SFTP and/or SCP.
+"""
+
+import os
+import posixpath
+import stat
+
+from .util import debug  # TODO: actual logging! LOL
+
+# TODO: figure out best way to direct folks seeking rsync, to patchwork's rsync
+# call (which needs updating to use invoke.run() & fab 2 connection methods,
+# but is otherwise suitable).
+# UNLESS we want to try and shoehorn it into this module after all? Delegate
+# any recursive get/put to it? Requires users to have rsync available of
+# course.
+
+
+class Transfer(object):
+    """
+    `.Connection`-wrapping class responsible for managing file upload/download.
+
+    .. versionadded:: 2.0
+    """
+
+    # TODO: SFTP clear default, but how to do SCP? subclass? init kwarg?
+
+    def __init__(self, connection):
+        self.connection = connection
+
+    @property
+    def sftp(self):
+        return self.connection.sftp()
+
+    def is_remote_dir(self, path):
+        try:
+            return stat.S_ISDIR(self.sftp.stat(path).st_mode)
+        except IOError:
+            return False
+
+    def get(self, remote, local=None, preserve_mode=True):
+        """
+        Download a file from the current connection to the local filesystem.
+
+        :param str remote:
+            Remote file to download.
+
+            May be absolute, or relative to the remote working directory.
+
+            .. note::
+                Most SFTP servers set the remote working directory to the
+                connecting user's home directory, and (unlike most shells) do
+                *not* expand tildes (``~``).
+
+                For example, instead of saying ``get("~/tmp/archive.tgz")``,
+                say ``get("tmp/archive.tgz")``.
+
+        :param local:
+            Local path to store downloaded file in, or a file-like object.
+
+            **If None or another 'falsey'/empty value is given** (the default),
+            the remote file is downloaded to the current working directory (as
+            seen by `os.getcwd`) using its remote filename.
+
+            **If a string is given**, it should be a path to a local directory
+            or file and is subject to similar behavior as that seen by common
+            Unix utilities or OpenSSH's ``sftp`` or ``scp`` tools.
+
+            For example, if the local path is a directory, the remote path's
+            base filename will be added onto it (so ``get('foo/bar/file.txt',
+            '/tmp/')`` would result in creation or overwriting of
+            ``/tmp/file.txt``).
+
+            .. note::
+                When dealing with nonexistent file paths, normal Python file
+                handling concerns come into play - for example, a ``local``
+                path containing non-leaf directories which do not exist, will
+                typically result in an `OSError`.
+
+            **If a file-like object is given**, the contents of the remote file
+            are simply written into it.
+
+        :param bool preserve_mode:
+            Whether to `os.chmod` the local file so it matches the remote
+            file's mode (default: ``True``).
+
+        :returns: A `.Result` object.
+
+        .. versionadded:: 2.0
+        """
+        # TODO: how does this API change if we want to implement
+        # remote-to-remote file transfer? (Is that even realistic?)
+        # TODO: handle v1's string interpolation bits, especially the default
+        # one, or at least think about how that would work re: split between
+        # single and multiple server targets.
+        # TODO: callback support
+        # TODO: how best to allow changing the behavior/semantics of
+        # remote/local (e.g. users might want 'safer' behavior that complains
+        # instead of overwriting existing files) - this likely ties into the
+        # "how to handle recursive/rsync" and "how to handle scp" questions
+
+        # Massage remote path
+        if not remote:
+            raise ValueError("Remote path must not be empty!")
+        orig_remote = remote
+        remote = posixpath.join(
+            self.sftp.getcwd() or self.sftp.normalize("."), remote
+        )
+
+        # Massage local path:
+        # - handle file-ness
+        # - if path, fill with remote name if empty, & make absolute
+        orig_local = local
+        is_file_like = hasattr(local, "write") and callable(local.write)
+        if not local:
+            local = posixpath.basename(remote)
+        if not is_file_like:
+            local = os.path.abspath(local)
+
+        # Run Paramiko-level .get() (side-effects only. womp.)
+        # TODO: push some of the path handling into Paramiko; it should be
+        # responsible for dealing with path cleaning etc.
+        # TODO: probably preserve warning message from v1 when overwriting
+        # existing files. Use logging for that obviously.
+        #
+        # If local appears to be a file-like object, use sftp.getfo, not get
+        if is_file_like:
+            self.sftp.getfo(remotepath=remote, fl=local)
+        else:
+            self.sftp.get(remotepath=remote, localpath=local)
+            # Set mode to same as remote end
+            # TODO: Push this down into SFTPClient sometime (requires backwards
+            # incompat release.)
+            if preserve_mode:
+                remote_mode = self.sftp.stat(remote).st_mode
+                mode = stat.S_IMODE(remote_mode)
+                os.chmod(local, mode)
+        # Return something useful
+        return Result(
+            orig_remote=orig_remote,
+            remote=remote,
+            orig_local=orig_local,
+            local=local,
+            connection=self.connection,
+        )
+
+    def put(self, local, remote=None, preserve_mode=True):
+        """
+        Upload a file from the local filesystem to the current connection.
+
+        :param local:
+            Local path of file to upload, or a file-like object.
+
+            **If a string is given**, it should be a path to a local (regular)
+            file (not a directory).
+
+            .. note::
+                When dealing with nonexistent file paths, normal Python file
+                handling concerns come into play - for example, trying to
+                upload a nonexistent ``local`` path will typically result in an
+                `OSError`.
+
+            **If a file-like object is given**, its contents are written to the
+            remote file path.
+
+        :param str remote:
+            Remote path to which the local file will be written.
+
+            .. note::
+                Most SFTP servers set the remote working directory to the
+                connecting user's home directory, and (unlike most shells) do
+                *not* expand tildes (``~``).
+
+                For example, instead of saying ``put("archive.tgz",
+                "~/tmp/")``, say ``put("archive.tgz", "tmp/")``.
+
+                In addition, this means that 'falsey'/empty values (such as the
+                default value, ``None``) are allowed and result in uploading to
+                the remote home directory.
+
+            .. note::
+                When ``local`` is a file-like object, ``remote`` is required
+                and must refer to a valid file path (not a directory).
+
+        :param bool preserve_mode:
+            Whether to ``chmod`` the remote file so it matches the local file's
+            mode (default: ``True``).
+
+        :returns: A `.Result` object.
+
+        .. versionadded:: 2.0
+        """
+        if not local:
+            raise ValueError("Local path must not be empty!")
+
+        is_file_like = hasattr(local, "write") and callable(local.write)
+
+        # Massage remote path
+        orig_remote = remote
+        if is_file_like:
+            local_base = getattr(local, "name", None)
+        else:
+            local_base = os.path.basename(local)
+        if not remote:
+            if is_file_like:
+                raise ValueError(
+                    "Must give non-empty remote path when local is a file-like object!"  # noqa
+                )
+            else:
+                remote = local_base
+                debug("Massaged empty remote path into {!r}".format(remote))
+        elif self.is_remote_dir(remote):
+            # non-empty local_base implies a) text file path or b) FLO which
+            # had a non-empty .name attribute. huzzah!
+            if local_base:
+                remote = posixpath.join(remote, local_base)
+            else:
+                if is_file_like:
+                    raise ValueError(
+                        "Can't put a file-like-object into a directory unless it has a non-empty .name attribute!"  # noqa
+                    )
+                else:
+                    # TODO: can we ever really end up here? implies we want to
+                    # reorganize all this logic so it has fewer potential holes
+                    raise ValueError(
+                        "Somehow got an empty local file basename ({!r}) when uploading to a directory ({!r})!".format(  # noqa
+                            local_base, remote
+                        )
+                    )
+
+        prejoined_remote = remote
+        remote = posixpath.join(
+            self.sftp.getcwd() or self.sftp.normalize("."), remote
+        )
+        if remote != prejoined_remote:
+            msg = "Massaged relative remote path {!r} into {!r}"
+            debug(msg.format(prejoined_remote, remote))
+
+        # Massage local path
+        orig_local = local
+        if not is_file_like:
+            local = os.path.abspath(local)
+            if local != orig_local:
+                debug(
+                    "Massaged relative local path {!r} into {!r}".format(
+                        orig_local, local
+                    )
+                )  # noqa
+
+        # Run Paramiko-level .put() (side-effects only. womp.)
+        # TODO: push some of the path handling into Paramiko; it should be
+        # responsible for dealing with path cleaning etc.
+        # TODO: probably preserve warning message from v1 when overwriting
+        # existing files. Use logging for that obviously.
+        #
+        # If local appears to be a file-like object, use sftp.putfo, not put
+        if is_file_like:
+            msg = "Uploading file-like object {!r} to {!r}"
+            debug(msg.format(local, remote))
+            pointer = local.tell()
+            try:
+                local.seek(0)
+                self.sftp.putfo(fl=local, remotepath=remote)
+            finally:
+                local.seek(pointer)
+        else:
+            debug("Uploading {!r} to {!r}".format(local, remote))
+            self.sftp.put(localpath=local, remotepath=remote)
+            # Set mode to same as local end
+            # TODO: Push this down into SFTPClient sometime (requires backwards
+            # incompat release.)
+            if preserve_mode:
+                local_mode = os.stat(local).st_mode
+                mode = stat.S_IMODE(local_mode)
+                self.sftp.chmod(remote, mode)
+        # Return something useful
+        return Result(
+            orig_remote=orig_remote,
+            remote=remote,
+            orig_local=orig_local,
+            local=local,
+            connection=self.connection,
+        )
+
+
+class Result(object):
+    """
+    A container for information about the result of a file transfer.
+
+    See individual attribute/method documentation below for details.
+
+    .. note::
+        Unlike similar classes such as `invoke.runners.Result` or
+        `fabric.runners.Result` (which have a concept of "warn and return
+        anyways on failure") this class has no useful truthiness behavior. If a
+        file transfer fails, some exception will be raised, either an `OSError`
+        or an error from within Paramiko.
+
+    .. versionadded:: 2.0
+    """
+
+    # TODO: how does this differ from put vs get? field stating which? (feels
+    # meh) distinct classes differing, for now, solely by name? (also meh)
+    def __init__(self, local, orig_local, remote, orig_remote, connection):
+        #: The local path the file was saved as, or the object it was saved
+        #: into if a file-like object was given instead.
+        #:
+        #: If a string path, this value is massaged to be absolute; see
+        #: `.orig_local` for the original argument value.
+        self.local = local
+        #: The original value given as the returning method's ``local``
+        #: argument.
+        self.orig_local = orig_local
+        #: The remote path downloaded from. Massaged to be absolute; see
+        #: `.orig_remote` for the original argument value.
+        self.remote = remote
+        #: The original argument value given as the returning method's
+        #: ``remote`` argument.
+        self.orig_remote = orig_remote
+        #: The `.Connection` object this result was obtained from.
+        self.connection = connection
+
+    # TODO: ensure str/repr makes it easily differentiable from run() or
+    # local() result objects (and vice versa).
diff -Nru fabric-1.14.0/fabric2/tunnels.py fabric-2.5.0/fabric2/tunnels.py
--- fabric-1.14.0/fabric2/tunnels.py	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/fabric2/tunnels.py	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,157 @@
+"""
+Tunnel and connection forwarding internals.
+
+If you're looking for simple, end-user-focused connection forwarding, please
+see `.Connection`, e.g. `.Connection.forward_local`.
+"""
+
+import errno
+import select
+import socket
+import time
+from threading import Event
+
+from invoke.exceptions import ThreadException
+from invoke.util import ExceptionHandlingThread
+
+
+class TunnelManager(ExceptionHandlingThread):
+    """
+    Thread subclass for tunnelling connections over SSH between two endpoints.
+
+    Specifically, one instance of this class is sufficient to sit around
+    forwarding any number of individual connections made to one end of the
+    tunnel or the other. If you need to forward connections between more than
+    one set of ports, you'll end up instantiating multiple TunnelManagers.
+
+    Wraps a `~paramiko.transport.Transport`, which should already be connected
+    to the remote server.
+
+    .. versionadded:: 2.0
+    """
+
+    def __init__(
+        self,
+        local_host,
+        local_port,
+        remote_host,
+        remote_port,
+        transport,
+        finished,
+    ):
+        super(TunnelManager, self).__init__()
+        self.local_address = (local_host, local_port)
+        self.remote_address = (remote_host, remote_port)
+        self.transport = transport
+        self.finished = finished
+
+    def _run(self):
+        # Track each tunnel that gets opened during our lifetime
+        tunnels = []
+
+        # Set up OS-level listener socket on forwarded port
+        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
+        # TODO: why do we want REUSEADDR exactly? and is it portable?
+        sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
+        # NOTE: choosing to deal with nonblocking semantics and a fast loop,
+        # versus an older approach which blocks & expects outer scope to cause
+        # a socket exception by close()ing the socket.
+        sock.setblocking(0)
+        sock.bind(self.local_address)
+        sock.listen(1)
+
+        while not self.finished.is_set():
+            # Main loop-wait: accept connections on the local listener
+            # NOTE: EAGAIN means "you're nonblocking and nobody happened to
+            # connect at this point in time"
+            try:
+                tun_sock, local_addr = sock.accept()
+                # Set TCP_NODELAY to match OpenSSH's forwarding socket behavior
+                tun_sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)
+            except socket.error as e:
+                if e.errno is errno.EAGAIN:
+                    # TODO: make configurable
+                    time.sleep(0.01)
+                    continue
+                raise
+
+            # Set up direct-tcpip channel on server end
+            # TODO: refactor w/ what's used for gateways
+            channel = self.transport.open_channel(
+                "direct-tcpip", self.remote_address, local_addr
+            )
+
+            # Set up 'worker' thread for this specific connection to our
+            # tunnel, plus its dedicated signal event (which will appear as a
+            # public attr, no need to track both independently).
+            finished = Event()
+            tunnel = Tunnel(channel=channel, sock=tun_sock, finished=finished)
+            tunnel.start()
+            tunnels.append(tunnel)
+
+        exceptions = []
+        # Propogate shutdown signal to all tunnels & wait for closure
+        # TODO: would be nice to have some output or at least logging here,
+        # especially for "sets up a handful of tunnels" use cases like
+        # forwarding nontrivial HTTP traffic.
+        for tunnel in tunnels:
+            tunnel.finished.set()
+            tunnel.join()
+            wrapper = tunnel.exception()
+            if wrapper:
+                exceptions.append(wrapper)
+        # Handle exceptions
+        if exceptions:
+            raise ThreadException(exceptions)
+
+        # All we have left to close is our own sock.
+        # TODO: use try/finally?
+        sock.close()
+
+
+class Tunnel(ExceptionHandlingThread):
+    """
+    Bidirectionally forward data between an SSH channel and local socket.
+
+    .. versionadded:: 2.0
+    """
+
+    def __init__(self, channel, sock, finished):
+        self.channel = channel
+        self.sock = sock
+        self.finished = finished
+        self.socket_chunk_size = 1024
+        self.channel_chunk_size = 1024
+        super(Tunnel, self).__init__()
+
+    def _run(self):
+        try:
+            empty_sock, empty_chan = None, None
+            while not self.finished.is_set():
+                r, w, x = select.select([self.sock, self.channel], [], [], 1)
+                if self.sock in r:
+                    empty_sock = self.read_and_write(
+                        self.sock, self.channel, self.socket_chunk_size
+                    )
+                if self.channel in r:
+                    empty_chan = self.read_and_write(
+                        self.channel, self.sock, self.channel_chunk_size
+                    )
+                if empty_sock or empty_chan:
+                    break
+        finally:
+            self.channel.close()
+            self.sock.close()
+
+    def read_and_write(self, reader, writer, chunk_size):
+        """
+        Read ``chunk_size`` from ``reader``, writing result to ``writer``.
+
+        Returns ``None`` if successful, or ``True`` if the read was empty.
+
+        .. versionadded:: 2.0
+        """
+        data = reader.recv(chunk_size)
+        if len(data) == 0:
+            return True
+        writer.sendall(data)
diff -Nru fabric-1.14.0/fabric2/util.py fabric-2.5.0/fabric2/util.py
--- fabric-1.14.0/fabric2/util.py	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/fabric2/util.py	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,45 @@
+import logging
+import sys
+
+
+# Ape the half-assed logging junk from Invoke, but ensuring the logger reflects
+# our name, not theirs. (Assume most contexts will rely on Invoke itself to
+# literally enable/disable logging, for now.)
+log = logging.getLogger("fabric")
+for x in ("debug",):
+    globals()[x] = getattr(log, x)
+
+
+win32 = sys.platform == "win32"
+
+
+def get_local_user():
+    """
+    Return the local executing username, or ``None`` if one can't be found.
+
+    .. versionadded:: 2.0
+    """
+    # TODO: I don't understand why these lines were added outside the
+    # try/except, since presumably it means the attempt at catching ImportError
+    # wouldn't work. However, that's how the contributing user committed it.
+    # Need an older Windows box to test it out, most likely.
+    import getpass
+
+    username = None
+    # All Unix and most Windows systems support the getpass module.
+    try:
+        username = getpass.getuser()
+    # Some SaaS platforms raise KeyError, implying there is no real user
+    # involved. They get the default value of None.
+    except KeyError:
+        pass
+    # Older (?) Windows systems don't support getpass well; they should
+    # have the `win32` module instead.
+    except ImportError:  # pragma: nocover
+        if win32:
+            import win32api
+            import win32security  # noqa
+            import win32profile  # noqa
+
+            username = win32api.GetUserName()
+    return username
diff -Nru fabric-1.14.0/fabric2/_version.py fabric-2.5.0/fabric2/_version.py
--- fabric-1.14.0/fabric2/_version.py	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/fabric2/_version.py	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,2 @@
+__version_info__ = (2, 5, 0)
+__version__ = ".".join(map(str, __version_info__))
diff -Nru fabric-1.14.0/.gitignore fabric-2.5.0/.gitignore
--- fabric-1.14.0/.gitignore	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/.gitignore	2019-08-06 23:57:28.000000000 +0100
@@ -6,9 +6,9 @@
 *.egg
 .DS_Store
 .*.swp
-Fabric.egg-info
+*.egg-info
 .coverage
-docs/_build
+sites/*/_build
 dist
 build/
 tags
@@ -16,4 +16,5 @@
 .tox
 tox.ini
 .idea/
-sites/*/_build
+htmlcov
+.cache
diff -Nru fabric-1.14.0/INSTALL fabric-2.5.0/INSTALL
--- fabric-1.14.0/INSTALL	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/INSTALL	1970-01-01 01:00:00.000000000 +0100
@@ -1,2 +0,0 @@
-For installation help, please see http://fabfile.org/ or (if using a source
-checkout) sites/www/installing.rst.
diff -Nru fabric-1.14.0/integration/concurrency.py fabric-2.5.0/integration/concurrency.py
--- fabric-1.14.0/integration/concurrency.py	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/integration/concurrency.py	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,91 @@
+import codecs
+
+from invoke.vendor.six.moves.queue import Queue
+from invoke.vendor.six.moves import zip_longest
+
+from invoke.util import ExceptionHandlingThread
+from pytest import skip
+
+from fabric import Connection
+
+
+_words = "/usr/share/dict/words"
+
+
+def _worker(queue, cxn, start, num_words, count, expected):
+    tail = num_words - start
+    cmd = "tail -n {} {} | head -n {}".format(tail, _words, count)
+    stdout = cxn.run(cmd, hide=True).stdout
+    result = [x.strip() for x in stdout.splitlines()]
+    queue.put((cxn, result, expected))
+
+
+class concurrency:
+    # TODO: still useful to use Group API here? Where does this responsibility
+    # fall between Group and Executor (e.g. phrasing this specifically as a
+    # generic subcase of Invoke level task parameterization)?
+
+    # TODO: spin up multiple temp SSHDs / Paramiko servers / ???
+
+    def setup(self):
+        cxn1 = Connection("localhost")
+        cxn2 = Connection("localhost")
+        cxn3 = Connection("localhost")
+        self.cxns = (cxn1, cxn2, cxn3)
+
+    def connections_objects_do_not_share_connection_state(self):
+        cxn1, cxn2, cxn3 = self.cxns
+        [x.open() for x in self.cxns]
+        # Prove no exterior connection caching, socket reuse, etc
+        # NOTE: would phrase these as chained 'is not' but pep8 linter is being
+        # stupid :(
+        assert cxn1 is not cxn2
+        assert cxn2 is not cxn3
+        assert cxn1.client is not cxn2.client
+        assert cxn2.client is not cxn3.client
+        ports = [x.transport.sock.getsockname()[1] for x in self.cxns]
+        assert ports[0] is not ports[1] is not ports[2]
+
+    def manual_threading_works_okay(self):
+        # TODO: needs https://github.com/pyinvoke/invoke/issues/438 fixed
+        # before it will reliably pass
+        skip()
+        # Kind of silly but a nice base case for "how would someone thread this
+        # stuff; and are there any bizarre gotchas lurking in default
+        # config/context/connection state?"
+        # Specifically, cut up the local (usually 100k's long) words dict into
+        # per-thread chunks, then read those chunks via shell command, as a
+        # crummy "make sure each thread isn't polluting things like stored
+        # stdout" sanity test
+        queue = Queue()
+        # TODO: skip test on Windows or find suitable alternative file
+        with codecs.open(_words, encoding="utf-8") as fd:
+            data = [x.strip() for x in fd.readlines()]
+        threads = []
+        num_words = len(data)
+        chunksize = len(data) / len(self.cxns)  # will be an int, which is fine
+        for i, cxn in enumerate(self.cxns):
+            start = i * chunksize
+            end = max([start + chunksize, num_words])
+            chunk = data[start:end]
+            kwargs = dict(
+                queue=queue,
+                cxn=cxn,
+                start=start,
+                num_words=num_words,
+                count=len(chunk),
+                expected=chunk,
+            )
+            thread = ExceptionHandlingThread(target=_worker, kwargs=kwargs)
+            threads.append(thread)
+        for t in threads:
+            t.start()
+        for t in threads:
+            t.join(5)  # Kinda slow, but hey, maybe the test runner is hot
+        while not queue.empty():
+            cxn, result, expected = queue.get(block=False)
+            for resultword, expectedword in zip_longest(result, expected):
+                err = u"({2!r}, {3!r}->{4!r}) {0!r} != {1!r}".format(
+                    resultword, expectedword, cxn, expected[0], expected[-1]
+                )
+                assert resultword == expectedword, err
diff -Nru fabric-1.14.0/integration/connection.py fabric-2.5.0/integration/connection.py
--- fabric-1.14.0/integration/connection.py	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/integration/connection.py	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,136 @@
+import os
+import time
+
+from invoke import pty_size, CommandTimedOut
+from pytest import skip, raises
+
+from fabric import Connection, Config
+
+
+# TODO: use pytest markers
+def skip_outside_travis():
+    if not os.environ.get("TRAVIS", False):
+        skip()
+
+
+class Connection_:
+    class ssh_connections:
+        def open_method_generates_real_connection(self):
+            c = Connection("localhost")
+            c.open()
+            assert c.client.get_transport().active is True
+            assert c.is_connected is True
+            return c
+
+        def close_method_closes_connection(self):
+            # Handy shortcut - open things up, then return Connection for us to
+            # close
+            c = self.open_method_generates_real_connection()
+            c.close()
+            assert c.client.get_transport() is None
+            assert c.is_connected is False
+
+    class run:
+        def simple_command_on_host(self):
+            """
+            Run command on localhost
+            """
+            result = Connection("localhost").run("echo foo", hide=True)
+            assert result.stdout == "foo\n"
+            assert result.exited == 0
+            assert result.ok is True
+
+        def simple_command_with_pty(self):
+            """
+            Run command under PTY on localhost
+            """
+            # Most Unix systems should have stty, which asplodes when not run
+            # under a pty, and prints useful info otherwise
+            result = Connection("localhost").run(
+                "stty size", hide=True, pty=True
+            )
+            found = result.stdout.strip().split()
+            cols, rows = pty_size()
+            assert tuple(map(int, found)), rows == cols
+            # PTYs use \r\n, not \n, line separation
+            assert "\r\n" in result.stdout
+            assert result.pty is True
+
+    class local:
+        def wraps_invoke_run(self):
+            # NOTE: most of the interesting tests about this are in
+            # invoke.runners / invoke.integration.
+            cxn = Connection("localhost")
+            result = cxn.local("echo foo", hide=True)
+            assert result.stdout == "foo\n"
+            assert not cxn.is_connected  # meh way of proving it didn't use SSH
+
+    def mixed_use_of_local_and_run(self):
+        """
+        Run command truly locally, and over SSH via localhost
+        """
+        cxn = Connection("localhost")
+        result = cxn.local("echo foo", hide=True)
+        assert result.stdout == "foo\n"
+        assert not cxn.is_connected  # meh way of proving it didn't use SSH yet
+        result = cxn.run("echo foo", hide=True)
+        assert cxn.is_connected  # NOW it's using SSH
+        assert result.stdout == "foo\n"
+
+    class sudo:
+        def setup(self):
+            # NOTE: assumes a user configured for passworded (NOT
+            # passwordless)_sudo, whose password is 'mypass', is executing the
+            # test suite. I.e. our travis-ci setup.
+            config = Config(
+                {"sudo": {"password": "mypass"}, "run": {"hide": True}}
+            )
+            self.cxn = Connection("localhost", config=config)
+
+        def sudo_command(self):
+            """
+            Run command via sudo on host localhost
+            """
+            skip_outside_travis()
+            assert self.cxn.sudo("whoami").stdout.strip() == "root"
+
+        def mixed_sudo_and_normal_commands(self):
+            """
+            Run command via sudo, and not via sudo, on localhost
+            """
+            skip_outside_travis()
+            logname = os.environ["LOGNAME"]
+            assert self.cxn.run("whoami").stdout.strip() == logname
+            assert self.cxn.sudo("whoami").stdout.strip() == "root"
+
+    def large_remote_commands_finish_cleanly(self):
+        # Guards against e.g. cleanup finishing before actually reading all
+        # data from the remote end. Which is largely an issue in Invoke-level
+        # code but one that only really manifests when doing stuff over the
+        # network. Yay computers!
+        path = "/usr/share/dict/words"
+        cxn = Connection("localhost")
+        with open(path) as fd:
+            words = [x.strip() for x in fd.readlines()]
+        stdout = cxn.run("cat {}".format(path), hide=True).stdout
+        lines = [x.strip() for x in stdout.splitlines()]
+        # When bug present, # lines received is significantly fewer than the
+        # true count in the file (by thousands).
+        assert len(lines) == len(words)
+
+    class command_timeout:
+        def setup(self):
+            self.cxn = Connection("localhost")
+
+        def does_not_raise_exception_when_under_timeout(self):
+            assert self.cxn.run("sleep 1", timeout=3)
+
+        def raises_exception_when_over_timeout(self):
+            with raises(CommandTimedOut) as info:
+                start = time.time()
+                self.cxn.run("sleep 5", timeout=1)
+            elapsed = time.time() - start
+            assert info.value.timeout == 1
+            # Catch scenarios where we except but don't actually shut down
+            # early (w/ a bit of fudge time for overhead)
+            assert elapsed <= 2
diff -Nru fabric-1.14.0/integration/group.py fabric-2.5.0/integration/group.py
--- fabric-1.14.0/integration/group.py	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/integration/group.py	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,35 @@
+from socket import gaierror
+
+from fabric import ThreadingGroup as Group
+from fabric.exceptions import GroupException
+
+
+class Group_:
+    def simple_command(self):
+        group = Group("localhost", "127.0.0.1")
+        result = group.run("echo foo", hide=True)
+        outs = [x.stdout.strip() for x in result.values()]
+        assert ["foo", "foo"] == outs
+
+    def failed_command(self):
+        group = Group("localhost", "127.0.0.1")
+        try:
+            group.run("lolnope", hide=True)
+        except GroupException as e:
+            # GroupException.result -> GroupResult;
+            # GroupResult values will be UnexpectedExit in this case;
+            # UnexpectedExit.result -> Result, and thus .exited etc.
+            exits = [x.result.exited for x in e.result.values()]
+            assert [127, 127] == exits
+        else:
+            assert False, "Did not raise GroupException!"
+
+    def excepted_command(self):
+        group = Group("nopebadhost1", "nopebadhost2")
+        try:
+            group.run("lolnope", hide=True)
+        except GroupException as e:
+            for value in e.result.values():
+                assert isinstance(value, gaierror)
+        else:
+            assert False, "Did not raise GroupException!"
diff -Nru fabric-1.14.0/integration/_support/file.txt fabric-2.5.0/integration/_support/file.txt
--- fabric-1.14.0/integration/_support/file.txt	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/integration/_support/file.txt	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1 @@
+yup
diff -Nru fabric-1.14.0/integration/_support/funky-perms.txt fabric-2.5.0/integration/_support/funky-perms.txt
--- fabric-1.14.0/integration/_support/funky-perms.txt	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/integration/_support/funky-perms.txt	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1 @@
+wat
diff -Nru fabric-1.14.0/integration/test_contrib.py fabric-2.5.0/integration/test_contrib.py
--- fabric-1.14.0/integration/test_contrib.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/integration/test_contrib.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,130 +0,0 @@
-import os
-import re
-
-from fabric.api import run, local
-from fabric.contrib import files, project
-
-from utils import Integration
-
-
-def tildify(path):
-    home = run("echo ~", quiet=True).stdout.strip()
-    return path.replace('~', home)
-
-def expect(path):
-    assert files.exists(tildify(path))
-
-def expect_contains(path, value):
-    assert files.contains(tildify(path), value)
-
-def escape(path):
-    return path.replace(' ', r'\ ')
-
-
-class FileCleaner(Integration):
-    def setup(self):
-        self.local = []
-        self.remote = []
-
-    def teardown(self):
-        super(FileCleaner, self).teardown()
-        for created in self.local:
-            os.unlink(created)
-        for created in self.remote:
-            run("rm %s" % escape(created))
-
-
-class TestTildeExpansion(FileCleaner):
-    def test_append(self):
-        for target in ('~/append_test', '~/append_test with spaces'):
-            self.remote.append(target)
-            files.append(target, ['line'])
-            expect(target)
-
-    def test_exists(self):
-        for target in ('~/exists_test', '~/exists test with space'):
-            self.remote.append(target)
-            run("touch %s" % escape(target))
-            expect(target)
-     
-    def test_sed(self):
-        for target in ('~/sed_test', '~/sed test with space'):
-            self.remote.append(target)
-            run("echo 'before' > %s" % escape(target))
-            files.sed(target, 'before', 'after')
-            expect_contains(target, 'after')
-     
-    def test_upload_template(self):
-        for i, target in enumerate((
-            '~/upload_template_test',
-            '~/upload template test with space'
-        )):
-            src = "source%s" % i
-            local("touch %s" % src)
-            self.local.append(src)
-            self.remote.append(target)
-            files.upload_template(src, target)
-            expect(target)
-
-
-class TestIsLink(FileCleaner):
-    # TODO: add more of these. meh.
-    def test_is_link_is_true_on_symlink(self):
-        self.remote.extend(['/tmp/foo', '/tmp/bar'])
-        run("touch /tmp/foo")
-        run("ln -s /tmp/foo /tmp/bar")
-        assert files.is_link('/tmp/bar')
-
-    def test_is_link_is_false_on_non_link(self):
-        self.remote.append('/tmp/biz')
-        run("touch /tmp/biz")
-        assert not files.is_link('/tmp/biz')
-
-
-rsync_sources = (
-    'integration/',
-    'integration/test_contrib.py',
-    'integration/test_operations.py',
-    'integration/utils.py'
-)
-
-class TestRsync(Integration):
-    def rsync(self, id_, **kwargs):
-        remote = '/tmp/rsync-test-%s/' % id_
-        if files.exists(remote):
-            run("rm -rf %s" % remote)
-        return project.rsync_project(
-            remote_dir=remote,
-            local_dir='integration',
-            ssh_opts='-o StrictHostKeyChecking=no',
-            capture=True,
-            **kwargs
-        )
-
-    def test_existing_default_args(self):
-        """
-        Rsync uses -v by default
-        """
-        r = self.rsync(1)
-        for x in rsync_sources:
-            assert re.search(r'^%s$' % x, r.stdout, re.M), "'%s' was not found in '%s'" % (x, r.stdout)
-
-    def test_overriding_default_args(self):
-        """
-        Use of default_args kwarg can be used to nuke e.g. -v
-        """
-        r = self.rsync(2, default_opts='-pthrz')
-        for x in rsync_sources:
-            assert not re.search(r'^%s$' % x, r.stdout, re.M), "'%s' was found in '%s'" % (x, r.stdout)
-
-
-class TestUploadTemplate(FileCleaner):
-    def test_allows_pty_disable(self):
-        src = "source_file"
-        target = "remote_file"
-        local("touch %s" % src)
-        self.local.append(src)
-        self.remote.append(target)
-        # Just make sure it doesn't asplode. meh.
-        files.upload_template(src, target, pty=False)
-        expect(target)
diff -Nru fabric-1.14.0/integration/test_operations.py fabric-2.5.0/integration/test_operations.py
--- fabric-1.14.0/integration/test_operations.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/integration/test_operations.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,186 +0,0 @@
-from __future__ import with_statement
-
-from StringIO import StringIO
-import os
-import posixpath
-import shutil
-
-from fabric.api import (
-    run, path, put, sudo, env, cd, local, settings, get
-)
-from fabric.contrib.files import exists
-
-from utils import Integration
-
-
-def assert_mode(path, mode):
-    remote_mode = run("stat -c \"%%a\" \"%s\"" % path).stdout
-    assert remote_mode == mode, "remote %r != expected %r" % (remote_mode, mode)
-
-
-class TestOperations(Integration):
-    filepath = "/tmp/whocares"
-    dirpath = "/tmp/whatever/bin"
-    not_owned = "/tmp/notmine"
-
-    def setup(self):
-        super(TestOperations, self).setup()
-        run("mkdir -p %s" % " ".join([self.dirpath, self.not_owned]))
-
-    def teardown(self):
-        super(TestOperations, self).teardown()
-        # Revert any chown crap from put sudo tests
-        sudo("chown %s ." % env.user)
-        # Nuke to prevent bleed
-        sudo("rm -rf %s" % " ".join([self.dirpath, self.filepath]))
-        sudo("rm -rf %s" % self.not_owned)
-
-    def test_no_trailing_space_in_shell_path_in_run(self):
-        put(StringIO("#!/bin/bash\necho hi"), "%s/myapp" % self.dirpath, mode="0755")
-        with path(self.dirpath):
-            assert run('myapp').stdout == 'hi'
-
-    def test_string_put_mode_arg_doesnt_error(self):
-        put(StringIO("#!/bin/bash\necho hi"), self.filepath, mode="0755")
-        assert_mode(self.filepath, "755")
-
-    def test_int_put_mode_works_ok_too(self):
-        put(StringIO("#!/bin/bash\necho hi"), self.filepath, mode=0755)
-        assert_mode(self.filepath, "755")
-
-    def _chown(self, target):
-        sudo("chown root %s" % target)
-
-    def _put_via_sudo(self, source=None, target_suffix='myfile', **kwargs):
-        # Ensure target dir prefix is not owned by our user (so we fail unless
-        # the sudo part of things is working)
-        self._chown(self.not_owned)
-        source = source if source else StringIO("whatever")
-        # Drop temp file into that dir, via use_sudo, + any kwargs
-        return put(
-            source,
-            self.not_owned + '/' + target_suffix,
-            use_sudo=True,
-            **kwargs
-        )
-
-    def test_put_with_use_sudo(self):
-        self._put_via_sudo()
-
-    def test_put_with_dir_and_use_sudo(self):
-        # Test cwd should be root of fabric source tree. Use our own folder as
-        # the source, meh.
-        self._put_via_sudo(source='integration', target_suffix='')
-
-    def test_put_with_use_sudo_and_custom_temp_dir(self):
-        # TODO: allow dependency injection in sftp.put or w/e, test it in
-        # isolation instead.
-        # For now, just half-ass it by ensuring $HOME isn't writable
-        # temporarily.
-        self._chown('.')
-        self._put_via_sudo(temp_dir='/tmp')
-
-    def test_put_with_use_sudo_dir_and_custom_temp_dir(self):
-        self._chown('.')
-        self._put_via_sudo(source='integration', target_suffix='', temp_dir='/tmp')
-
-    def test_put_use_sudo_and_explicit_mode(self):
-        # Setup
-        target_dir = posixpath.join(self.filepath, 'blah')
-        subdir = "inner"
-        subdir_abs = posixpath.join(target_dir, subdir)
-        filename = "whatever.txt"
-        target_file = posixpath.join(subdir_abs, filename)
-        run("mkdir -p %s" % subdir_abs)
-        self._chown(subdir_abs)
-        local_path = os.path.join('/tmp', filename)
-        with open(local_path, 'w+') as fd:
-            fd.write('stuff\n')
-        # Upload + assert
-        with cd(target_dir):
-            put(local_path, subdir, use_sudo=True, mode='777')
-        assert_mode(target_file, '777')
-
-    def test_put_file_to_dir_with_use_sudo_and_mirror_mode(self):
-        # Ensure mode of local file, umask varies on eg travis vs various
-        # localhosts
-        source = 'whatever.txt'
-        try:
-            local("touch %s" % source)
-            local("chmod 644 %s" % source)
-            # Target for _put_via_sudo is a directory by default
-            uploaded = self._put_via_sudo(
-                source=source, mirror_local_mode=True
-            )
-            assert_mode(uploaded[0], '644')
-        finally:
-            local("rm -f %s" % source)
-
-    def test_put_directory_use_sudo_and_spaces(self):
-        localdir = 'I have spaces'
-        localfile = os.path.join(localdir, 'file.txt')
-        os.mkdir(localdir)
-        with open(localfile, 'w') as fd:
-            fd.write('stuff\n')
-        try:
-            uploaded = self._put_via_sudo(localdir, target_suffix='')
-            # Kinda dumb, put() would've died if it couldn't do it, but.
-            assert exists(uploaded[0])
-            assert exists(posixpath.dirname(uploaded[0]))
-        finally:
-            shutil.rmtree(localdir)
-
-    def test_agent_forwarding_functions(self):
-        # When paramiko #399 is present this will hang indefinitely
-        with settings(forward_agent=True):
-            run('ssh-add -L')
-
-    def test_get_with_use_sudo_unowned_file(self):
-        # Ensure target is not normally readable by us
-        target = self.filepath
-        sudo("echo 'nope' > %s" % target)
-        sudo("chown root:root %s" % target)
-        sudo("chmod 0440 %s" % target)
-        # Pull down with use_sudo, confirm contents
-        local_ = StringIO()
-        get(
-            local_path=local_,
-            remote_path=target,
-            use_sudo=True,
-        )
-        assert local_.getvalue() == "nope\n"
-
-    def test_get_with_use_sudo_groupowned_file(self):
-        # Issue #1226: file gotten w/ use_sudo, file normally readable via
-        # group perms (yes - so use_sudo not required - full use case involves
-        # full-directory get() where use_sudo *is* required). Prior to fix,
-        # temp file is chmod 404 which seems to cause perm denied due to group
-        # membership (despite 'other' readability).
-        target = self.filepath
-        sudo("echo 'nope' > %s" % target)
-        # Same group as connected user
-        gid = run("id -g")
-        sudo("chown root:%s %s" % (gid, target))
-        # Same perms as bug use case (only really need group read)
-        sudo("chmod 0640 %s" % target)
-        # Do eet
-        local_ = StringIO()
-        get(
-            local_path=local_,
-            remote_path=target,
-            use_sudo=True,
-        )
-        assert local_.getvalue() == "nope\n"
-
-    def test_get_from_unreadable_dir(self):
-        # Put file in dir as normal user
-        remotepath = "%s/myfile.txt" % self.dirpath
-        run("echo 'foo' > %s" % remotepath)
-        # Make dir unreadable (but still executable - impossible to obtain
-        # file if dir is both unreadable and unexecutable)
-        sudo("chown root:root %s" % self.dirpath)
-        sudo("chmod 711 %s" % self.dirpath)
-        # Try gettin' it
-        local_ = StringIO()
-        get(local_path=local_, remote_path=remotepath)
-        assert local_.getvalue() == 'foo\n'
diff -Nru fabric-1.14.0/integration/transfer.py fabric-2.5.0/integration/transfer.py
--- fabric-1.14.0/integration/transfer.py	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/integration/transfer.py	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,91 @@
+import os
+import stat
+from io import BytesIO
+
+from py import path
+
+from fabric import Connection
+
+
+def _support(*parts):
+    return os.path.join(os.path.dirname(__file__), "_support", *parts)
+
+
+class Transfer_:
+    class get:
+        def setup(self):
+            self.c = Connection("localhost")
+            self.remote = _support("file.txt")
+
+        def base_case(self, tmpdir):
+            # Copy file from support to tempdir
+            with tmpdir.as_cwd():
+                result = self.c.get(self.remote)
+
+            # Make sure it arrived
+            local = tmpdir.join("file.txt")
+            assert local.check()
+            assert local.read() == "yup\n"
+            # Sanity check result object
+            assert result.remote == self.remote
+            assert result.orig_remote == self.remote
+            assert result.local == str(local)
+            assert result.orig_local is None
+
+        def file_like_objects(self):
+            fd = BytesIO()
+            result = self.c.get(remote=self.remote, local=fd)
+            assert fd.getvalue() == b"yup\n"
+            assert result.remote == self.remote
+            assert result.local is fd
+
+        def mode_preservation(self, tmpdir):
+            # Use a dummy file which is given an unusual, highly unlikely to be
+            # default umask, set of permissions (oct 641, aka -rw-r----x)
+            local = tmpdir.join("funky-local.txt")
+            remote = tmpdir.join("funky-remote.txt")
+            remote.write("whatever")
+            remote.chmod(0o641)
+            self.c.get(remote=str(remote), local=str(local))
+            assert stat.S_IMODE(local.stat().mode) == 0o641
+
+    class put:
+        def setup(self):
+            self.c = Connection("localhost")
+            self.remote = path.local.mkdtemp().join("file.txt").realpath()
+
+        def base_case(self):
+            # Copy file from 'local' (support dir) to 'remote' (tempdir)
+            local_dir = _support()
+            with path.local(local_dir).as_cwd():
+                tmpdir = self.remote.dirpath()
+                # TODO: wrap chdir at the Connection level
+                self.c.sftp().chdir(str(tmpdir))
+                result = self.c.put("file.txt")
+            # Make sure it arrived
+            assert self.remote.check()
+            assert self.remote.read() == "yup\n"
+            # Sanity check result object
+            assert result.remote == self.remote
+            assert result.orig_remote is None
+            assert result.local == _support("file.txt")
+            assert result.orig_local == "file.txt"
+
+        def file_like_objects(self):
+            fd = BytesIO()
+            fd.write(b"yup\n")
+            remote_str = str(self.remote)
+            result = self.c.put(local=fd, remote=remote_str)
+            assert self.remote.read() == "yup\n"
+            assert result.remote == remote_str
+            assert result.local is fd
+
+        def mode_preservation(self, tmpdir):
+            # Use a dummy file which is given an unusual, highly unlikely to be
+            # default umask, set of permissions (oct 641, aka -rw-r----x)
+            local = tmpdir.join("funky-local.txt")
+            local.write("whatever")
+            local.chmod(0o641)
+            remote = tmpdir.join("funky-remote.txt")
+            self.c.put(remote=str(remote), local=str(local))
+            assert stat.S_IMODE(remote.stat().mode) == 0o641
diff -Nru fabric-1.14.0/integration/utils.py fabric-2.5.0/integration/utils.py
--- fabric-1.14.0/integration/utils.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/integration/utils.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,18 +0,0 @@
-import os
-import sys
-
-# Pull in regular tests' utilities
-mod = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'tests'))
-sys.path.insert(0, mod)
-# Clean up
-del sys.path[0]
-
-
-class Integration(object):
-    def setup(self):
-        # Just so subclasses can super() us w/o fear. Meh.
-        pass
-
-    def teardown(self):
-        # Just so subclasses can super() us w/o fear. Meh.
-        pass
diff -Nru fabric-1.14.0/LICENSE fabric-2.5.0/LICENSE
--- fabric-1.14.0/LICENSE	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/LICENSE	2019-08-06 23:57:28.000000000 +0100
@@ -1,5 +1,4 @@
-Copyright (c) 2009-2017 Jeffrey E. Forcier
-Copyright (c) 2008-2009 Christian Vest Hansen
+Copyright (c) 2019 Jeff Forcier.
 All rights reserved.
 
 Redistribution and use in source and binary forms, with or without
diff -Nru fabric-1.14.0/MANIFEST.in fabric-2.5.0/MANIFEST.in
--- fabric-1.14.0/MANIFEST.in	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/MANIFEST.in	2019-08-06 23:57:28.000000000 +0100
@@ -1,10 +1,10 @@
-include AUTHORS
-include INSTALL
 include LICENSE
 include README.rst
+include tasks.py
 recursive-include sites *
-recursive-exclude sites/docs/_build *
-recursive-exclude sites/www/_build *
-include requirements.txt
+recursive-exclude sites/*/_build *
+include dev-requirements.txt
 recursive-include tests *
 recursive-exclude tests *.pyc *.pyo
+recursive-include integration *
+recursive-exclude integration *.pyc *.pyo
diff -Nru fabric-1.14.0/README.rst fabric-2.5.0/README.rst
--- fabric-1.14.0/README.rst	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/README.rst	2019-08-06 23:57:28.000000000 +0100
@@ -1,38 +1,13 @@
-Fabric is a Python (2.5-2.7) library and command-line tool for
-streamlining the use of SSH for application deployment or systems
-administration tasks.
+Welcome to Fabric!
+==================
 
-It provides a basic suite of operations for executing local or remote shell
-commands (normally or via ``sudo``) and uploading/downloading files, as well as
-auxiliary functionality such as prompting the running user for input, or
-aborting execution.
- 
-Typical use involves creating a Python module containing one or more functions,
-then executing them via the ``fab`` command-line tool. Below is a small but
-complete "fabfile" containing a single task:
-
-.. code-block:: python
-
-    from fabric.api import run
-
-    def host_type():
-        run('uname -s')
-
-If you save the above as ``fabfile.py`` (the default module that
-``fab`` loads), you can run the tasks defined in it on one or more
-servers, like so::
-
-    $ fab -H localhost,linuxbox host_type
-    [localhost] run: uname -s
-    [localhost] out: Darwin
-    [linuxbox] run: uname -s
-    [linuxbox] out: Linux
-
-    Done.
-    Disconnecting from localhost... done.
-    Disconnecting from linuxbox... done.
-
-In addition to use via the ``fab`` tool, Fabric's components may be imported
-into other Python code, providing a Pythonic interface to the SSH protocol
-suite at a higher level than that provided by e.g. the ``Paramiko`` library
-(which Fabric itself uses.)
+Fabric is a high level Python (2.7, 3.4+) library designed to execute shell
+commands remotely over SSH, yielding useful Python objects in return. It builds
+on top of `Invoke <http://pyinvoke.org>`_ (subprocess command execution and
+command-line features) and `Paramiko <http://paramiko.org>`_ (SSH protocol
+implementation), extending their APIs to complement one another and provide
+additional functionality.
+
+For a high level introduction, including example code, please see
+`our main project website <http://fabfile.org>`_; or for detailed API docs, see
+`the versioned API website <http://docs.fabfile.org>`_.
diff -Nru fabric-1.14.0/requirements.txt fabric-2.5.0/requirements.txt
--- fabric-1.14.0/requirements.txt	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/requirements.txt	1970-01-01 01:00:00.000000000 +0100
@@ -1,10 +0,0 @@
-# These requirements are for DEVELOPMENT ONLY!
-# You do not need e.g. Sphinx or Fudge just to run the 'fab' tool.
-# Instead, these are necessary for executing the test suite or developing the
-# cutting edge (which may have different requirements from released versions.)
-
-# Development version of Paramiko, just in case we're in one of those phases.
--e git+https://github.com/paramiko/paramiko@1.17#egg=paramiko
-# Pull in actual "you already have local installed checkouts of Fabric +
-# Paramiko" dev deps.
--r dev-requirements.txt
diff -Nru fabric-1.14.0/setup.cfg fabric-2.5.0/setup.cfg
--- fabric-1.14.0/setup.cfg	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/setup.cfg	2019-08-06 23:57:28.000000000 +0100
@@ -1,2 +1,14 @@
+[wheel]
+universal = 1
+
 [flake8]
-exclude = tests/support,tests/Python26SocketServer.py,sites,fabric/api.py
+exclude = .git,sites
+ignore = E124,E125,E128,E261,E301,E302,E303,W503
+max-line-length = 79
+
+[metadata]
+license_file = LICENSE
+
+[tool:pytest]
+testpaths = tests
+python_files = *
diff -Nru fabric-1.14.0/setup.py fabric-2.5.0/setup.py
--- fabric-1.14.0/setup.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/setup.py	2019-08-06 23:57:28.000000000 +0100
@@ -1,76 +1,107 @@
 #!/usr/bin/env python
 
-from __future__ import with_statement
-
-import sys
-
-from setuptools import setup, find_packages
-
-from fabric.version import get_version
+import os
+import setuptools
 
+# Enable the option of building/installing Fabric 2.x as "fabric2". This allows
+# users migrating from 1.x to 2.x to have both in the same process space and
+# migrate piecemeal.
+#
+# NOTE: this requires some irritating tomfoolery; to wit:
+# - the repo has a fabric2/ symlink to fabric/ so that things looking for
+# fabric2/<whatever> will find it OK, whether that's code in here or deeper in
+# setuptools/wheel/etc
+# - wheels do _not_ execute this on install, only on generation, so maintainers
+# just build wheels with the env var below turned on, and those wheels install
+# 'fabric2' no problem
+# - sdists execute this _both_ on package creation _and_ on install, so the env
+# var only helps with inbound package metadata; on install by a user, if they
+# don't have the env var, they'd end up with errors because this file tries to
+# look in fabric/, not fabric2/
+# - thus, we use a different test that looks locally to see if only one dir
+# is present, and that overrides the env var test.
+#
+# See also sites/www/installing.txt.
+
+env_wants_v2 = os.environ.get("PACKAGE_AS_FABRIC2", False)
+
+here = os.path.abspath(os.path.dirname(__file__))
+fabric2_present = os.path.isdir(os.path.join(here, "fabric2"))
+fabric_present = os.path.isdir(os.path.join(here, "fabric"))
+only_v2_present = fabric2_present and not fabric_present
+
+package_name = "fabric"
+binary_name = "fab"
+if env_wants_v2 or only_v2_present:
+    package_name = "fabric2"
+    binary_name = "fab2"
+packages = setuptools.find_packages(
+    include=[package_name, "{}.*".format(package_name)]
+)
 
-with open('README.rst') as f:
-    readme = f.read()
+# Version info -- read without importing
+_locals = {}
+with open(os.path.join(package_name, "_version.py")) as fp:
+    exec(fp.read(), None, _locals)
+version = _locals["__version__"]
 
+# Frankenstein long_description: changelog note + README
 long_description = """
 To find out what's new in this version of Fabric, please see `the changelog
 <http://fabfile.org/changelog.html>`_.
 
-You can also install the `development version via ``pip install -e
-git+https://github.com/fabric/fabric/#egg=fabric``.
-
-----
-
-%s
-
-----
-
-For more information, please see the Fabric website or execute ``fab --help``.
-""" % (readme)
-
-if sys.version_info[:2] < (2, 6):
-    install_requires=['paramiko>=1.10,<1.13']
-else:
-    install_requires=['paramiko>=1.10,<3.0']
+{}
+""".format(
+    open("README.rst").read()
+)
 
+testing_deps = ["mock>=2.0.0,<3.0"]
+pytest_deps = ["pytest>=3.2.5,<4.0"]
 
-setup(
-    name='Fabric',
-    version=get_version('short'),
-    description='Fabric is a simple, Pythonic tool for remote execution and deployment.',
+setuptools.setup(
+    name=package_name,
+    version=version,
+    description="High level SSH command execution",
+    license="BSD",
     long_description=long_description,
-    author='Jeff Forcier',
-    author_email='jeff@bitprophet.org',
-    url='http://fabfile.org',
-    packages=find_packages(),
-    test_suite='nose.collector',
-    tests_require=['nose<2.0', 'fudge<1.0', 'jinja2<3.0'],
-    install_requires=install_requires,
+    author="Jeff Forcier",
+    author_email="jeff@bitprophet.org",
+    url="http://fabfile.org",
+    install_requires=["invoke>=1.3,<2.0", "paramiko>=2.4"],
+    extras_require={
+        "testing": testing_deps,
+        "pytest": testing_deps + pytest_deps,
+    },
+    packages=packages,
     entry_points={
-        'console_scripts': [
-            'fab = fabric.main:main',
+        "console_scripts": [
+            "{} = {}.main:program.run".format(binary_name, package_name)
         ]
     },
     classifiers=[
-          'Development Status :: 5 - Production/Stable',
-          'Environment :: Console',
-          'Intended Audience :: Developers',
-          'Intended Audience :: System Administrators',
-          'License :: OSI Approved :: BSD License',
-          'Operating System :: MacOS :: MacOS X',
-          'Operating System :: Unix',
-          'Operating System :: POSIX',
-          'Programming Language :: Python',
-          'Programming Language :: Python :: 2 :: Only',
-          'Programming Language :: Python :: 2.5',
-          'Programming Language :: Python :: 2.6',
-          'Programming Language :: Python :: 2.7',
-          'Topic :: Software Development',
-          'Topic :: Software Development :: Build Tools',
-          'Topic :: Software Development :: Libraries',
-          'Topic :: Software Development :: Libraries :: Python Modules',
-          'Topic :: System :: Clustering',
-          'Topic :: System :: Software Distribution',
-          'Topic :: System :: Systems Administration',
+        "Development Status :: 5 - Production/Stable",
+        "Environment :: Console",
+        "Intended Audience :: Developers",
+        "Intended Audience :: System Administrators",
+        "License :: OSI Approved :: BSD License",
+        "Operating System :: POSIX",
+        "Operating System :: Unix",
+        "Operating System :: MacOS :: MacOS X",
+        "Operating System :: Microsoft :: Windows",
+        "Programming Language :: Python",
+        "Programming Language :: Python :: 2",
+        "Programming Language :: Python :: 2.7",
+        "Programming Language :: Python :: 3",
+        "Programming Language :: Python :: 3.4",
+        "Programming Language :: Python :: 3.5",
+        "Programming Language :: Python :: 3.6",
+        "Programming Language :: Python :: 3.7",
+        "Topic :: Software Development",
+        "Topic :: Software Development :: Build Tools",
+        "Topic :: Software Development :: Libraries",
+        "Topic :: Software Development :: Libraries :: Python Modules",
+        "Topic :: System :: Clustering",
+        "Topic :: System :: Software Distribution",
+        "Topic :: System :: Systems Administration",
     ],
 )
diff -Nru fabric-1.14.0/sites/docs/api/config.rst fabric-2.5.0/sites/docs/api/config.rst
--- fabric-1.14.0/sites/docs/api/config.rst	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/sites/docs/api/config.rst	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,5 @@
+==============
+``config``
+==============
+
+.. automodule:: fabric.config
diff -Nru fabric-1.14.0/sites/docs/api/connection.rst fabric-2.5.0/sites/docs/api/connection.rst
--- fabric-1.14.0/sites/docs/api/connection.rst	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/sites/docs/api/connection.rst	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,5 @@
+==============
+``connection``
+==============
+
+.. automodule:: fabric.connection
diff -Nru fabric-1.14.0/sites/docs/api/contrib/console.rst fabric-2.5.0/sites/docs/api/contrib/console.rst
--- fabric-1.14.0/sites/docs/api/contrib/console.rst	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/sites/docs/api/contrib/console.rst	1970-01-01 01:00:00.000000000 +0100
@@ -1,5 +0,0 @@
-Console Output Utilities
-========================
-
-.. automodule:: fabric.contrib.console
-    :members:
diff -Nru fabric-1.14.0/sites/docs/api/contrib/django.rst fabric-2.5.0/sites/docs/api/contrib/django.rst
--- fabric-1.14.0/sites/docs/api/contrib/django.rst	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/sites/docs/api/contrib/django.rst	1970-01-01 01:00:00.000000000 +0100
@@ -1,6 +0,0 @@
-==================
-Django Integration
-==================
-
-.. automodule:: fabric.contrib.django
-    :members:
diff -Nru fabric-1.14.0/sites/docs/api/contrib/files.rst fabric-2.5.0/sites/docs/api/contrib/files.rst
--- fabric-1.14.0/sites/docs/api/contrib/files.rst	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/sites/docs/api/contrib/files.rst	1970-01-01 01:00:00.000000000 +0100
@@ -1,6 +0,0 @@
-=============================
-File and Directory Management
-=============================
-
-.. automodule:: fabric.contrib.files
-    :members:
diff -Nru fabric-1.14.0/sites/docs/api/contrib/project.rst fabric-2.5.0/sites/docs/api/contrib/project.rst
--- fabric-1.14.0/sites/docs/api/contrib/project.rst	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/sites/docs/api/contrib/project.rst	1970-01-01 01:00:00.000000000 +0100
@@ -1,6 +0,0 @@
-=============
-Project Tools
-=============
-
-.. automodule:: fabric.contrib.project
-    :members:
diff -Nru fabric-1.14.0/sites/docs/api/core/colors.rst fabric-2.5.0/sites/docs/api/core/colors.rst
--- fabric-1.14.0/sites/docs/api/core/colors.rst	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/sites/docs/api/core/colors.rst	1970-01-01 01:00:00.000000000 +0100
@@ -1,7 +0,0 @@
-======================
-Color output functions
-======================
-
-.. automodule:: fabric.colors
-    :members:
-    :undoc-members:
diff -Nru fabric-1.14.0/sites/docs/api/core/context_managers.rst fabric-2.5.0/sites/docs/api/core/context_managers.rst
--- fabric-1.14.0/sites/docs/api/core/context_managers.rst	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/sites/docs/api/core/context_managers.rst	1970-01-01 01:00:00.000000000 +0100
@@ -1,6 +0,0 @@
-================
-Context Managers
-================
-
-.. automodule:: fabric.context_managers
-    :members:
diff -Nru fabric-1.14.0/sites/docs/api/core/decorators.rst fabric-2.5.0/sites/docs/api/core/decorators.rst
--- fabric-1.14.0/sites/docs/api/core/decorators.rst	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/sites/docs/api/core/decorators.rst	1970-01-01 01:00:00.000000000 +0100
@@ -1,6 +0,0 @@
-==========
-Decorators
-==========
-
-.. automodule:: fabric.decorators
-    :members: hosts, roles, runs_once, serial, parallel, task, with_settings
diff -Nru fabric-1.14.0/sites/docs/api/core/docs.rst fabric-2.5.0/sites/docs/api/core/docs.rst
--- fabric-1.14.0/sites/docs/api/core/docs.rst	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/sites/docs/api/core/docs.rst	1970-01-01 01:00:00.000000000 +0100
@@ -1,6 +0,0 @@
-=====================
-Documentation helpers
-=====================
-
-.. automodule:: fabric.docs
-    :members:
diff -Nru fabric-1.14.0/sites/docs/api/core/network.rst fabric-2.5.0/sites/docs/api/core/network.rst
--- fabric-1.14.0/sites/docs/api/core/network.rst	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/sites/docs/api/core/network.rst	1970-01-01 01:00:00.000000000 +0100
@@ -1,7 +0,0 @@
-=======
-Network
-=======
-
-.. automodule:: fabric.network
-
-    .. autofunction:: disconnect_all
diff -Nru fabric-1.14.0/sites/docs/api/core/operations.rst fabric-2.5.0/sites/docs/api/core/operations.rst
--- fabric-1.14.0/sites/docs/api/core/operations.rst	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/sites/docs/api/core/operations.rst	1970-01-01 01:00:00.000000000 +0100
@@ -1,6 +0,0 @@
-==========
-Operations
-==========
-
-.. automodule:: fabric.operations
-    :members:
diff -Nru fabric-1.14.0/sites/docs/api/core/tasks.rst fabric-2.5.0/sites/docs/api/core/tasks.rst
--- fabric-1.14.0/sites/docs/api/core/tasks.rst	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/sites/docs/api/core/tasks.rst	1970-01-01 01:00:00.000000000 +0100
@@ -1,6 +0,0 @@
-=====
-Tasks
-=====
-
-.. automodule:: fabric.tasks
-    :members: Task, WrappedCallableTask, execute
diff -Nru fabric-1.14.0/sites/docs/api/core/utils.rst fabric-2.5.0/sites/docs/api/core/utils.rst
--- fabric-1.14.0/sites/docs/api/core/utils.rst	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/sites/docs/api/core/utils.rst	1970-01-01 01:00:00.000000000 +0100
@@ -1,6 +0,0 @@
-=====
-Utils
-=====
-
-.. automodule:: fabric.utils
-    :members:
diff -Nru fabric-1.14.0/sites/docs/api/exceptions.rst fabric-2.5.0/sites/docs/api/exceptions.rst
--- fabric-1.14.0/sites/docs/api/exceptions.rst	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/sites/docs/api/exceptions.rst	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,5 @@
+==============
+``exceptions``
+==============
+
+.. automodule:: fabric.exceptions
diff -Nru fabric-1.14.0/sites/docs/api/executor.rst fabric-2.5.0/sites/docs/api/executor.rst
--- fabric-1.14.0/sites/docs/api/executor.rst	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/sites/docs/api/executor.rst	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,5 @@
+============
+``executor``
+============
+
+.. automodule:: fabric.executor
diff -Nru fabric-1.14.0/sites/docs/api/group.rst fabric-2.5.0/sites/docs/api/group.rst
--- fabric-1.14.0/sites/docs/api/group.rst	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/sites/docs/api/group.rst	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,5 @@
+==============
+``group``
+==============
+
+.. automodule:: fabric.group
diff -Nru fabric-1.14.0/sites/docs/api/runners.rst fabric-2.5.0/sites/docs/api/runners.rst
--- fabric-1.14.0/sites/docs/api/runners.rst	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/sites/docs/api/runners.rst	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,5 @@
+===========
+``runners``
+===========
+
+.. automodule:: fabric.runners
diff -Nru fabric-1.14.0/sites/docs/api/tasks.rst fabric-2.5.0/sites/docs/api/tasks.rst
--- fabric-1.14.0/sites/docs/api/tasks.rst	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/sites/docs/api/tasks.rst	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,5 @@
+=========
+``tasks``
+=========
+
+.. automodule:: fabric.tasks
diff -Nru fabric-1.14.0/sites/docs/api/testing.rst fabric-2.5.0/sites/docs/api/testing.rst
--- fabric-1.14.0/sites/docs/api/testing.rst	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/sites/docs/api/testing.rst	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,25 @@
+.. _testing-subpackage:
+
+===========
+``testing``
+===========
+
+The ``fabric.testing`` subpackage contains a handful of test helper modules:
+
+- `fabric.testing.base` which only depends on things like ``mock`` and is
+  appropriate in just about any test paradigm;
+- `fabric.testing.fixtures`, containing ``pytest`` fixtures and thus only of
+  interest for users of ``pytest``.
+
+All are documented below. Please note the module-level documentation which
+contains install instructions!
+
+``testing.base``
+================
+
+.. automodule:: fabric.testing.base
+
+``testing.fixtures``
+====================
+
+.. automodule:: fabric.testing.fixtures
diff -Nru fabric-1.14.0/sites/docs/api/transfer.rst fabric-2.5.0/sites/docs/api/transfer.rst
--- fabric-1.14.0/sites/docs/api/transfer.rst	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/sites/docs/api/transfer.rst	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,6 @@
+============
+``transfer``
+============
+
+.. automodule:: fabric.transfer
+    :member-order: bysource
diff -Nru fabric-1.14.0/sites/docs/api/tunnels.rst fabric-2.5.0/sites/docs/api/tunnels.rst
--- fabric-1.14.0/sites/docs/api/tunnels.rst	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/sites/docs/api/tunnels.rst	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,5 @@
+==============
+``tunnels``
+==============
+
+.. automodule:: fabric.tunnels
diff -Nru fabric-1.14.0/sites/docs/api/util.rst fabric-2.5.0/sites/docs/api/util.rst
--- fabric-1.14.0/sites/docs/api/util.rst	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/sites/docs/api/util.rst	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,5 @@
+========
+``util``
+========
+
+.. automodule:: fabric.util
diff -Nru fabric-1.14.0/sites/docs/cli.rst fabric-2.5.0/sites/docs/cli.rst
--- fabric-1.14.0/sites/docs/cli.rst	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/sites/docs/cli.rst	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,132 @@
+======================
+Command-line interface
+======================
+
+This page documents the details of Fabric's command-line interface, ``fab``.
+
+
+Options & arguments
+===================
+
+.. note::
+    By default, ``fab`` honors all of the same CLI options as :ref:`Invoke's
+    'inv' program <inv>`; only additions and overrides are listed here!
+
+    For example, Fabric implements :option:`--prompt-for-passphrase` and
+    :option:`--prompt-for-login-password` because they are SSH specific, but
+    it inherits a related option -- :ref:`--prompt-for-sudo-password
+    <prompt-for-sudo-password>` -- from Invoke, which handles sudo autoresponse
+    concerns.
+
+.. option:: -H, --hosts
+
+    Takes a comma-separated string listing hostnames against which tasks
+    should be executed, in serial. See :ref:`runtime-hosts`.
+
+.. option:: -i, --identity
+
+    Overrides the ``key_filename`` value in the ``connect_kwargs`` config
+    setting (which is read by `.Connection`, and eventually makes its way into
+    Paramiko; see the docstring for `.Connection` for details.)
+
+    Typically this can be thought of as identical to ``ssh -i <path>``, i.e.
+    supplying a specific, runtime private key file. Like ``ssh -i``, it builds
+    an iterable of strings and may be given multiple times.
+
+    Default: ``[]``.
+
+.. option:: --prompt-for-login-password
+
+    Causes Fabric to prompt 'up front' for a value to store as the
+    ``connect_kwargs.password`` config setting (used by Paramiko when
+    authenticating via passwords and, in some versions, also used for key
+    passphrases.) Useful if you do not want to configure such values in on-disk
+    conf files or via shell environment variables.
+
+.. option:: --prompt-for-passphrase
+
+    Causes Fabric to prompt 'up front' for a value to store as the
+    ``connect_kwargs.passphrase`` config setting (used by Paramiko to decrypt
+    private key files.) Useful if you do not want to configure such values in
+    on-disk conf files or via shell environment variables.
+
+.. option:: -S, --ssh-config
+
+    Takes a path to load as a runtime SSH config file. See :ref:`ssh-config`.
+
+.. option:: -t, --connect-timeout
+
+    Takes an integer of seconds after which connection should time out.
+    Supplies the default value for the ``timeouts.connect`` config setting.
+
+
+Seeking & loading tasks
+=======================
+
+``fab`` follows all the same rules as Invoke's :ref:`collection loading
+<collection-discovery>`, with the sole exception that the default collection
+name sought is ``fabfile`` instead of ``tasks``. Thus, whenever Invoke's
+documentation mentions ``tasks`` or ``tasks.py``, Fabric substitutes
+``fabfile`` / ``fabfile.py``.
+
+For example, if your current working directory is
+``/home/myuser/projects/mywebapp``, running ``fab --list`` will cause Fabric to
+look for ``/home/myuser/projects/mywebapp/fabfile.py`` (or
+``/home/myuser/projects/mywebapp/fabfile/__init__.py`` - Python's import system
+treats both the same). If it's not found there,
+``/home/myuser/projects/fabfile.py`` is sought next; and so forth.
+
+
+.. _runtime-hosts:
+
+Runtime specification of host lists
+===================================
+
+While advanced use cases may need to take matters into their own hands, you can
+go reasonably far with the core :option:`--hosts` flag, which specifies one or
+more hosts the given task(s) should execute against.
+
+By default, execution is a serial process: for each task on the command line,
+run it once for each host given to :option:`--hosts`. Imagine tasks that simply
+print ``Running <task name> on <host>!``::
+
+    $ fab --hosts host1,host2,host3 taskA taskB
+    Running taskA on host1!
+    Running taskA on host2!
+    Running taskA on host3!
+    Running taskB on host1!
+    Running taskB on host2!
+    Running taskB on host3!
+
+.. note::
+    When :option:`--hosts` is not given, ``fab`` behaves similarly to Invoke's
+    :ref:`command-line interface <inv>`, generating regular instances of
+    `~invoke.context.Context` instead of `Connections <.Connection>`.
+
+Executing arbitrary/ad-hoc commands
+===================================
+
+``fab`` leverages a lesser-known command line convention and may be called in
+the following manner::
+
+    $ fab [options] -- [shell command]
+
+where everything after the ``--`` is turned into a temporary `.Connection.run`
+call, and is not parsed for ``fab`` options. If you've specified a host list
+via an earlier task or the core CLI flags, this usage will act like a one-line
+anonymous task.
+
+For example, let's say you wanted kernel info for a bunch of systems::
+
+    $ fab -H host1,host2,host3 -- uname -a
+
+Such a command is equivalent to the following Fabric library code::
+
+    from fabric import Group
+
+    Group('host1', 'host2', 'host3').run("uname -a")
+
+Most of the time you will want to just write out the task in your fabfile
+(anything you use once, you're likely to use again) but this feature provides a
+handy, fast way to dash off an SSH-borne command while leveraging predefined
+connection settings.
diff -Nru fabric-1.14.0/sites/docs/concepts/authentication.rst fabric-2.5.0/sites/docs/concepts/authentication.rst
--- fabric-1.14.0/sites/docs/concepts/authentication.rst	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/sites/docs/concepts/authentication.rst	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,96 @@
+==============
+Authentication
+==============
+
+Even in the 'vanilla' OpenSSH client, authenticating to remote servers involves
+multiple potential sources for secrets and configuration; Fabric not only
+supports most of those, but has more of its own. This document outlines the
+available methods for setting authentication secrets.
+
+.. note::
+    Since Fabric itself tries not to reinvent too much Paramiko functionality,
+    most of the time configuring authentication values boils down to "how to
+    set keyword argument values for `SSHClient.connect
+    <paramiko.client.SSHClient.connect>`", which in turn means to set values
+    inside either the ``connect_kwargs`` :doc:`config
+    </concepts/configuration>` subtree, or the ``connect_kwargs`` keyword
+    argument of `.Connection`.
+
+Private key files
+=================
+
+Private keys stored on-disk are probably the most common auth mechanism for
+SSH. Fabric offers multiple methods of configuring which paths to use, most of
+which end up merged into one list of paths handed to
+``SSHClient.connect(key_filename=[...])``, in the following order:
+
+- If a ``key_filename`` key exists in the ``connect_kwargs`` argument to
+  `.Connection`, they come first in the list. (This is basically the "runtime"
+  option for non-CLI users.)
+- The config setting ``connect_kwargs.key_filename`` can be set in a number of
+  ways (as per the :doc:`config docs </concepts/configuration>`) including via
+  the :option:`--identity` CLI flag (which sets the ``overrides`` level of the
+  config; so when this flag is used, key filename values from other config
+  sources will be overridden.) This value comes next in the overall list.
+- Using an :ref:`ssh_config <ssh-config>` file with ``IdentityFile``
+  directives lets you share configuration with other SSH clients; such values
+  come last.
+
+Encryption passphrases
+----------------------
+
+If your private key file is protected via a passphrase, it can be supplied in a
+handful of ways:
+
+- The ``connect_kwargs.passphrase`` config option is the most direct way to
+  supply a passphrase to be used automatically.
+
+  .. note::
+    Using actual on-disk config files for this type of material isn't always
+    wise, but recall that the :doc:`configuration system
+    </concepts/configuration>` is capable of loading data from other sources,
+    such as your shell environment or even arbitrary remote databases.
+
+- If you prefer to enter the passphrase manually at runtime, you may use the
+  command-line option :option:`--prompt-for-passphrase`, which will cause
+  Fabric to interactively prompt the user at the start of the process, and
+  store the entered value in ``connect_kwargs.passphrase`` (at the 'overrides'
+  level.)
+
+Private key objects
+===================
+
+Instantiate your own `PKey <paramiko.pkey.PKey>` object (see its subclasses'
+API docs for details) and place it into ``connect_kwargs.pkey``. That's it!
+You'll be responsible for any handling of passphrases, if the key material
+you're loading (these classes can load from file paths or strings) is
+encrypted.
+
+SSH agents
+==========
+
+By default (similar to how OpenSSH behaves) Paramiko will attempt to connect to
+a running SSH agent (Unix style, e.g. a live ``SSH_AUTH_SOCK``, or Pageant if
+one is on Windows). This can be disabled by setting
+``connect_kwargs.allow_agent`` to ``False``.
+
+Passwords
+=========
+
+Password authentication is relatively straightforward:
+
+- You can configure it via ``connect_kwargs.password`` directly.
+- If you want to be prompted for it at the start of a session, specify
+  :option:`--prompt-for-login-password`.
+
+.. TODO: host-configuration hooks are very important here, when implemented
+
+GSSAPI
+======
+
+Fabric doesn't provide any extra GSSAPI support on top of Paramiko's existing
+connect-time parameters (see e.g. ``gss_kex``/``gss_auth``/``gss_host``/etc in
+`SSHClient.connect <paramiko.client.SSHClient.connect>`) and the modules
+implementing the functionality itself (such as `paramiko.ssh_gss`.) Thus, as
+usual, you should be looking to modify the ``connect_kwargs`` configuration
+tree.
diff -Nru fabric-1.14.0/sites/docs/concepts/configuration.rst fabric-2.5.0/sites/docs/concepts/configuration.rst
--- fabric-1.14.0/sites/docs/concepts/configuration.rst	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/sites/docs/concepts/configuration.rst	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,248 @@
+.. _fab-configuration:
+
+=============
+Configuration
+=============
+
+Basics
+======
+
+The heart of Fabric's configuration system (as with much of the rest of Fabric)
+relies on Invoke functionality, namely `invoke.config.Config` (technically, a
+lightweight subclass, `fabric.config.Config`). For practical details on
+what this means re: configuring Fabric's behavior, please see :ref:`Invoke's
+configuration documentation <invoke:configuration>`.
+
+The primary differences from that document are as follows:
+
+* The configuration file paths sought are all named ``fabric.*`` instead of
+  ``invoke.*`` - e.g. ``/etc/fabric.yml`` instead of ``/etc/invoke.yml``,
+  ``~/.fabric.py`` instead of ``~/.invoke.py``, etc.
+* In addition to :ref:`Invoke's own default configuration values
+  <invoke:default-values>`, Fabric merges in some of its own, such as the fact
+  that SSH's default port number is 22. See :ref:`default-values` for details.
+* Fabric has facilities for loading SSH config files, and will automatically
+  create (or update) a configuration subtree on a per `Connection
+  <fabric.connection.Connection>` basis, loaded with the interpreted SSH
+  configuration for that specific host (since an SSH config file is only ever
+  useful via such a lens). See :ref:`ssh-config`.
+* Fabric plans to offer a framework for managing per-host and
+  per-host-collection configuration details and overrides, though this is not
+  yet implemented (it will be analogous to, but improved upon, the
+  ``env.hosts`` and ``env.roles`` structures from Fabric 1.x).
+
+    * This functionality will supplement that of the SSH config loading
+      described earlier; we expect most users will prefer to configure as much
+      as possible via an SSH config file, but not all Fabric settings have
+      ``ssh_config`` analogues, nor do all use cases fit neatly into such
+      files.
+
+
+.. _default-values:
+
+Default configuration values
+============================
+
+Overrides of Invoke-level defaults
+----------------------------------
+
+- ``run.replace_env``: ``True``, instead of ``False``, so that remote commands
+  run with a 'clean', empty environment instead of inheriting a copy of the
+  current process' environment.
+
+  This is for security purposes: leaking local environment data remotely by
+  default would be unsanitary. It's also compatible with the behavior of
+  OpenSSH.
+
+  .. seealso::
+    The warning under `paramiko.channel.Channel.set_environment_variable`.
+
+Extensions to Invoke-level defaults
+-----------------------------------
+
+- ``runners.remote``: In Invoke, the ``runners`` tree has a single subkey,
+  ``local`` (mapping to `~invoke.runners.Local`). Fabric adds this new subkey,
+  ``remote``, which is mapped to `~fabric.runners.Remote`.
+
+New default values defined by Fabric
+------------------------------------
+
+.. note::
+    Most of these settings are also available in the constructor of
+    `.Connection`, if they only need modification on a per-connection basis.
+
+.. warning::
+    Many of these are also configurable via :ref:`ssh_config files
+    <ssh-config>`. **Such values take precedence over those defined via the
+    core configuration**, so make sure you're aware of whether you're loading
+    such files (or :ref:`disable them to be sure <disabling-ssh-config>`).
+
+- ``connect_kwargs``: Keyword arguments (`dict`) given to `SSHClient.connect
+  <paramiko.client.SSHClient.connect>` when `.Connection` performs that method
+  call. This is the primary configuration vector for many SSH-related options,
+  such as selecting private keys, toggling forwarding of SSH agents, etc.
+  Default: ``{}``.
+- ``forward_agent``: Whether to attempt forwarding of your local SSH
+  authentication agent to the remote end. Default: ``False`` (same as in
+  OpenSSH.)
+- ``gateway``: Used as the default value of the ``gateway`` kwarg for
+  `.Connection`. May be any value accepted by that argument. Default: ``None``.
+- ``load_ssh_configs``: Whether to automatically seek out :ref:`SSH config
+  files <ssh-config>`. When ``False``, no automatic loading occurs. Default:
+  ``True``.
+- ``port``: TCP port number used by `.Connection` objects when not otherwise
+  specified. Default: ``22``.
+- ``inline_ssh_env``: Boolean serving as global default for the value of
+  `.Connection`'s ``inline_ssh_env`` parameter; see its docs for details.
+  Default: ``False``.
+- ``ssh_config_path``: Runtime SSH config path; see :ref:`ssh-config`. Default:
+  ``None``.
+- ``timeouts``: Various timeouts, specifically:
+
+    - ``connect``: Connection timeout, in seconds; defaults to ``None``,
+      meaning no timeout / block forever.
+
+- ``user``: Username given to the remote ``sshd`` when connecting. Default:
+  your local system username.
+
+
+.. _ssh-config:
+
+Loading and using ``ssh_config`` files
+======================================
+
+How files are loaded
+--------------------
+
+Fabric uses Paramiko's SSH config file machinery to load and parse
+``ssh_config``-format files (following OpenSSH's behavior re: which files to
+load, when possible):
+
+- An already-parsed `~paramiko.config.SSHConfig` object may be given to
+  `.Config.__init__` via its ``ssh_config`` keyword argument; if this value is
+  given, no files are loaded, even if they exist.
+- A runtime file path may be specified via configuration itself, as the
+  ``ssh_config_path`` key; such a path will be loaded into a new
+  `~paramiko.config.SSHConfig` object at the end of `.Config.__init__` and no
+  other files will be sought out.
+
+    - It will be filled in by the ``fab`` CLI tool if the
+      :option:`--ssh-config` flag is given.
+
+- If no runtime config (object or path) was given to `.Config.__init__`, it
+  will automatically seek out and load ``~/.ssh/config`` and/or
+  ``/etc/ssh/ssh_config``, if they exist (and in that order.)
+
+  .. note::
+      Rules present in both files will result in the user-level file 'winning',
+      as the first rule found during lookup is always used.
+
+- If none of the above vectors yielded SSH config data, a blank/empty
+  `~paramiko.config.SSHConfig` is the final result.
+- Regardless of how the object was generated, it is exposed as
+  ``Config.base_ssh_config``.
+
+.. _connection-ssh-config:
+
+``Connection``'s use of ``ssh_config`` values
+---------------------------------------------
+
+`.Connection` objects expose a per-host 'view' of their config's SSH data
+(obtained via `~paramiko.config.SSHConfig.lookup`) as `.Connection.ssh_config`.
+`.Connection` itself references these values as described in the following
+subsections, usually as simple defaults for the appropriate config key or
+parameter (``port``, ``forward_agent``, etc.)
+
+Unless otherwise specified, these values override regular configuration values
+for the same keys, but may themselves be overridden by `.Connection.__init__`
+parameters.
+
+Take for example a ``~/.fabric.yaml``:
+
+.. code:: yaml
+
+    user: foo
+
+Absent any other configuration, ``Connection('myhost')`` connects as the
+``foo`` user.
+
+If we also have an ``~/.ssh/config``::
+
+    Host *
+        User bar
+
+then ``Connection('myhost')`` connects as ``bar`` (the SSH config wins over
+the Fabric config.)
+
+*However*, in both cases, ``Connection('myhost', user='biz')`` will connect as
+``biz``.
+
+.. note::
+    The below sections use capitalized versions of ``ssh_config`` keys for
+    easier correlation with ``man ssh_config``, **but** the actual
+    `~paramiko.config.SSHConfig` data structure is normalized to lowercase
+    keys, since SSH config files are technically case-insensitive.
+
+Connection parameters
+~~~~~~~~~~~~~~~~~~~~~
+
+- ``Hostname``: replaces the original value of ``host`` (which is preserved as
+  ``.original_host``.)
+- ``Port``: supplies the default value for the ``port`` config option /
+  parameter.
+- ``User``: supplies the default value for the ``user`` config option /
+  parameter.
+- ``ConnectTimeout``: sets the default value for the ``timeouts.connect``
+  config option / ``timeout`` parameter.
+
+Proxying
+~~~~~~~~
+
+- ``ProxyCommand``: supplies default (string) value for ``gateway``.
+- ``ProxyJump``: supplies default (`Connection <fabric.connection.Connection>`)
+  value for ``gateway``.
+
+  - Nested-style ``ProxyJump``, i.e. ``user1@hop1.host,user2@hop2.host,...``,
+    will result in an appropriate series of nested ``gateway`` values under the
+    hood - as if the user had manually specified ``Connecton(...,
+    gateway=Connection('user1@hop1.host',
+    gateway=Connection('user2@hop2.host', gateway=...)))``.
+
+.. note::
+    If both are specified for a given host, ``ProxyJump`` will override
+    ``ProxyCommand``. This is slightly different from OpenSSH, where the order
+    the directives are loaded determines which one wins. Doing so on our end
+    (where we view the config as a dictionary structure) requires additional
+    work.
+
+.. TODO:
+    honor ProxyJump's comma-separated variant, which should translate to
+    (reverse-ordered) nested Connection-style gateways.
+
+Authentication
+~~~~~~~~~~~~~~
+
+- ``ForwardAgent``: controls default behavior of ``forward_agent``.
+- ``IdentityFile``: appends to the ``key_filename`` key within
+  ``connect_kwargs`` (similar to :option:`--identity`.)
+
+.. TODO: merge with per-host config when it's figured out
+
+
+.. _disabling-ssh-config:
+
+Disabling (most) ``ssh_config`` loading
+---------------------------------------
+
+Users who need tighter control over how their environment gets configured may
+want to disable the automatic loading of system/user level SSH config files;
+this can prevent hard-to-expect errors such as a new user's ``~/.ssh/config``
+overriding values that are being set in the regular config hierarchy.
+
+To do so, simply set the top level config option ``load_ssh_configs`` to
+``False``.
+
+.. note::
+    Changing this setting does *not* disable loading of runtime-level config
+    files (e.g. via :option:`-F`). If a user is explicitly telling us to load
+    such a file, we assume they know what they're doing.
diff -Nru fabric-1.14.0/sites/docs/concepts/networking.rst fabric-2.5.0/sites/docs/concepts/networking.rst
--- fabric-1.14.0/sites/docs/concepts/networking.rst	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/sites/docs/concepts/networking.rst	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,93 @@
+==========
+Networking
+==========
+
+.. _ssh-gateways:
+
+SSH connection gateways
+=======================
+
+Background
+----------
+
+When connecting to well-secured networks whose internal hosts are not directly
+reachable from the Internet, a common pattern is "bouncing", "gatewaying" or
+"proxying" SSH connections via an intermediate host (often called a "bastion",
+"gateway" or "jump box").
+
+Gatewaying requires making an initial/outer SSH connection to the gateway
+system, then using that connection as a transport for the "real"
+connection to the final/internal host.
+
+At a basic level, one could ``ssh gatewayhost``, then ``ssh internalhost`` from
+the resulting shell. This works for individual long-running sessions, but
+becomes a burden when it must be done frequently.
+
+There are two gateway solutions available in Fabric, mirroring the
+functionality of OpenSSH's client: ``ProxyJump`` style (easier, less overhead,
+can be nested) or ``ProxyCommand`` style (more overhead, can't be nested,
+sometimes more flexible). Both support the usual range of configuration
+sources: Fabric's own config framework, SSH config files, or runtime
+parameters.
+
+``ProxyJump``
+-------------
+
+This style of gateway uses the SSH protocol's ``direct-tcpip`` channel type - a
+lightweight method of requesting that the gateway's ``sshd`` open a connection
+on our behalf to another system. (This has been possible in OpenSSH server for
+a long time; support in OpenSSH's client is new as of 7.3.)
+
+Channel objects (instances of `paramiko.channel.Channel`) implement Python's
+socket API and are thus usable in place of real operating system sockets for
+nearly any Python code.
+
+``ProxyJump`` style gatewaying is simple to use: create a new `.Connection`
+object parameterized for the gateway, and supply it as the ``gateway``
+parameter when creating your inner/real `.Connection`::
+
+    from fabric import Connection
+
+    c = Connection('internalhost', gateway=Connection('gatewayhost'))
+
+As with any other `.Connection`, the gateway connection may be configured with
+its own username, port number, and so forth. (This includes ``gateway`` itself
+- they can be chained indefinitely!)
+
+.. TODO:
+    should it default to user/port from the 'outer' Connection? Some users may
+    assume it will? (Probably most likely to assume user is preserved; port
+    less so?)
+
+``ProxyCommand``
+----------------
+
+The traditional OpenSSH command-line client has long offered a ``ProxyCommand``
+directive (see `man ssh_config <http://man.openbsd.org/ssh_config>`_), which
+pipes the inner connection's input and output through an arbitrary local
+subprocess.
+
+Compared to ``ProxyJump`` style gateways, this adds overhead (the extra
+subprocess) and can't easily be nested. In trade, it allows for advanced tricks
+like use of SOCKS proxies, or custom filtering/gatekeeping applications.
+
+``ProxyCommand`` subprocesses are typically another ``ssh`` command, such as
+``ssh -W %h:%p gatewayhost``; or (on SSH versions lacking ``-W``) the widely
+available ``netcat``, via ``ssh gatewayhost nc %h %p``.
+
+Fabric supports ``ProxyCommand`` by accepting command string objects in the
+``gateway`` kwarg of `.Connection`; this is used to populate a
+`paramiko.proxy.ProxyCommand` object at connection time.
+
+Additional concerns
+-------------------
+
+If you're unsure which of the two approaches to use: use ``ProxyJump`` style.
+It performs better, uses fewer resources on your local system, and has an
+easier-to-use API.
+
+.. warning::
+    Requesting both types of gateways simultaneously to the same host (i.e.
+    supplying a `.Connection` as the ``gateway`` via kwarg or config, *and*
+    loading a config file containing ``ProxyCommand``) is considered an error
+    and will result in an exception.
diff -Nru fabric-1.14.0/sites/docs/conf.py fabric-2.5.0/sites/docs/conf.py
--- fabric-1.14.0/sites/docs/conf.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/sites/docs/conf.py	2019-08-06 23:57:28.000000000 +0100
@@ -1,28 +1,25 @@
 # Obtain shared config values
-import os, sys
+import sys
 from os.path import abspath, join, dirname
-sys.path.append(abspath(join(dirname(__file__), '..')))
-sys.path.append(abspath(join(dirname(__file__), '..', '..')))
-from shared_conf import *
 
-# Enable autodoc, intersphinx
-extensions.extend(['sphinx.ext.autodoc', 'sphinx.ext.intersphinx'])
+sys.path.append(abspath(join(dirname(__file__), "..")))
+sys.path.append(abspath(join(dirname(__file__), "..", "..")))
+from shared_conf import *
 
-# Autodoc settings
-autodoc_default_flags = ['members', 'special-members']
+# Enable & configure autodoc
+extensions.append("sphinx.ext.autodoc")
+autodoc_default_flags = ["members", "special-members"]
 
 # Default is 'local' building, but reference the public WWW site when building
 # under RTD.
-target = join(dirname(__file__), '..', 'www', '_build')
-if os.environ.get('READTHEDOCS') == 'True':
-    target = 'http://www.fabfile.org/'
-# Intersphinx connection to stdlib + www site
-intersphinx_mapping = {
-    'python': ('http://docs.python.org/2.6', None),
-    'www': (target, None),
-}
+target = join(dirname(__file__), "..", "www", "_build")
+if on_rtd:
+    target = "http://www.fabfile.org/"
+www = (target, None)
+# Intersphinx connection to www site
+intersphinx_mapping.update({"www": www})
 
 # Sister-site links to WWW
-html_theme_options['extra_nav_links'] = {
-    "Main website": 'http://www.fabfile.org',
+html_theme_options["extra_nav_links"] = {
+    "Main website": "http://www.fabfile.org"
 }
diff -Nru fabric-1.14.0/sites/docs/getting-started.rst fabric-2.5.0/sites/docs/getting-started.rst
--- fabric-1.14.0/sites/docs/getting-started.rst	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/sites/docs/getting-started.rst	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,433 @@
+===============
+Getting started
+===============
+
+Welcome! This tutorial highlights Fabric's core features; for further details,
+see the links within, or the documentation index which has links to conceptual
+and API doc sections.
+
+
+A note about imports
+====================
+
+Fabric composes a couple of other libraries as well as providing its own layer
+on top; user code will most often import from the ``fabric`` package, but
+you'll sometimes import directly from ``invoke`` or ``paramiko`` too:
+
+- `Invoke <https://pyinvoke.org>`_  implements CLI parsing, task organization,
+  and shell command execution (a generic framework plus specific implementation
+  for local commands.)
+
+    - Anything that isn't specific to remote systems tends to live in Invoke,
+      and it is often used standalone by programmers who don't need any remote
+      functionality.
+    - Fabric users will frequently import Invoke objects, in cases where Fabric
+      itself has no need to subclass or otherwise modify what Invoke provides.
+
+- `Paramiko <https://paramiko.org>`_ implements low/mid level SSH
+  functionality - SSH and SFTP sessions, key management, etc.
+
+    - Fabric mostly uses this under the hood; users will only rarely import
+      from Paramiko directly.
+
+- Fabric glues the other libraries together and provides its own high level
+  objects too, e.g.:
+
+    - Subclassing Invoke's context and command-runner classes, wrapping them
+      around Paramiko-level primitives;
+    - Extending Invoke's configuration system by using Paramiko's
+      ``ssh_config`` parsing machinery;
+    - Implementing new high-level primitives of its own, such as
+      port-forwarding context managers. (These may, in time, migrate downwards
+      into Paramiko.)
+
+.. TODO:
+    we should probably rename Collection to be Namespace or something; it's too
+    close to 'Connection'
+
+
+Run commands via Connections and ``run``
+========================================
+
+The most basic use of Fabric is to execute a shell command on a remote system
+via SSH, then (optionally) interrogate the result. By default, the remote
+program's output is printed directly to your terminal, *and* captured. A basic
+example:
+
+.. testsetup:: basic
+
+    mock = MockRemote()
+    mock.expect(out=b'Linux\n')
+
+.. testcleanup:: basic
+
+    mock.stop()
+
+.. doctest:: basic
+
+    >>> from fabric import Connection
+    >>> c = Connection('web1')
+    >>> result = c.run('uname -s')
+    Linux
+    >>> result.stdout.strip() == 'Linux'
+    True
+    >>> result.exited
+    0
+    >>> result.ok
+    True
+    >>> result.command
+    'uname -s'
+    >>> result.connection
+    <Connection host=web1>
+    >>> result.connection.host
+    'web1'
+
+Meet `.Connection`, which represents an SSH connection and provides the core of
+Fabric's API, such as `~.Connection.run`. `.Connection` objects need at least a
+hostname to be created successfully, and may be further parameterized by
+username and/or port number. You can give these explicitly via args/kwargs::
+
+    Connection(host='web1', user='deploy', port=2202)
+
+Or by stuffing a ``[user@]host[:port]`` string into the ``host`` argument
+(though this is purely convenience; always use kwargs whenever ambiguity
+appears!)::
+
+    Connection('deploy@web1:2202')
+
+`.Connection` objects' methods (like `~.Connection.run`) usually return
+instances of `invoke.runners.Result` (or subclasses thereof) exposing the sorts
+of details seen above: what was requested, what happened while the remote
+action occurred, and what the final result was.
+
+.. note::
+    Many lower-level SSH connection arguments (such as private keys and
+    timeouts) can be given directly to the SSH backend by using the
+    :ref:`connect_kwargs argument <connect_kwargs-arg>`.
+
+Superuser privileges via auto-response
+======================================
+
+Need to run things as the remote system's superuser? You could invoke the
+``sudo`` program via `~.Connection.run`, and (if your remote system isn't
+configured with passwordless sudo) respond to the password prompt by hand, as
+below. (Note how we need to request a remote pseudo-terminal; most ``sudo``
+implementations get grumpy at password-prompt time otherwise.)
+
+.. testsetup:: sudo-by-hand
+
+    mock = MockRemote()
+    mock.expect(commands=(
+        Command(out=b'[sudo] password:\n'),
+        Command(out=b'1001\n'),
+    ))
+
+.. testcleanup:: sudo-by-hand
+
+    mock.stop()
+
+.. doctest:: sudo-by-hand
+
+    >>> from fabric import Connection
+    >>> c = Connection('db1')
+    >>> c.run('sudo useradd mydbuser', pty=True)
+    [sudo] password:
+    <Result cmd='sudo useradd mydbuser' exited=0>
+    >>> c.run('id -u mydbuser')
+    1001
+    <Result cmd='id -u mydbuser' exited=0>
+
+Giving passwords by hand every time can get old; thankfully Invoke's powerful
+command-execution functionality includes the ability to :ref:`auto-respond
+<autoresponding>` to program output with pre-defined input. We can use this for
+``sudo``:
+
+.. testsetup:: sudo-with-responses
+
+    mock = MockRemote()
+    mock.expect(out=b'[sudo] password:\nroot\n', in_=b'mypassword\n')
+
+.. testcleanup:: sudo-with-responses
+
+    mock.stop()
+
+.. doctest:: sudo-with-responses
+
+    >>> from invoke import Responder
+    >>> from fabric import Connection
+    >>> c = Connection('host')
+    >>> sudopass = Responder(
+    ...     pattern=r'\[sudo\] password:',
+    ...     response='mypassword\n',
+    ... )
+    >>> c.run('sudo whoami', pty=True, watchers=[sudopass])
+    [sudo] password:
+    root
+    <Result cmd='sudo whoami' exited=0>
+
+It's difficult to show in a snippet, but when the above was executed, the user
+didn't need to type anything; ``mypassword`` was sent to the remote program
+automatically. Much easier!
+
+The ``sudo`` helper
+-------------------
+
+Using watchers/responders works well here, but it's a lot of boilerplate to set
+up every time - especially as real-world use cases need more work to detect
+failed/incorrect passwords.
+
+To help with that, Invoke provides a `Context.sudo
+<invoke.context.Context.sudo>` method which handles most of the boilerplate for
+you (as `.Connection` subclasses `~invoke.context.Context`, it gets this method
+for free.) `~invoke.context.Context.sudo` doesn't do anything users can't do
+themselves - but as always, common problems are best solved with commonly
+shared solutions.
+
+All the user needs to do is ensure the ``sudo.password`` :doc:`configuration
+value </concepts/configuration>` is filled in (via config file, environment
+variable, or :option:`--prompt-for-sudo-password`) and `.Connection.sudo`
+handles the rest. For the sake of clarity, here's an example where a
+library/shell user performs their own `getpass`-based password prompt:
+
+.. testsetup:: sudo
+
+    from __future__ import print_function
+    from mock import patch
+    gp_patcher = patch('getpass.getpass', side_effect=lambda x: print(x))
+    gp_patcher.start()
+    mock = MockRemote()
+    mock.expect(commands=(
+        Command(out=b'root\n'),
+        Command(),
+        Command(out=b'1001\n'),
+    ))
+
+.. testcleanup:: sudo
+
+    mock.stop()
+    gp_patcher.stop()
+
+.. doctest:: sudo
+    :options: +ELLIPSIS
+
+    >>> import getpass
+    >>> from fabric import Connection, Config
+    >>> sudo_pass = getpass.getpass("What's your sudo password?")
+    What's your sudo password?
+    >>> config = Config(overrides={'sudo': {'password': sudo_pass}})
+    >>> c = Connection('db1', config=config)
+    >>> c.sudo('whoami', hide='stderr')
+    root
+    <Result cmd="...whoami" exited=0>
+    >>> c.sudo('useradd mydbuser')
+    <Result cmd="...useradd mydbuser" exited=0>
+    >>> c.run('id -u mydbuser')
+    1001
+    <Result cmd='id -u mydbuser' exited=0>
+
+We filled in the sudo password up-front at runtime in this example; in
+real-world situations, you might also supply it via the configuration system
+(perhaps using environment variables, to avoid polluting config files), or
+ideally, use a secrets management system.
+
+
+Transfer files
+==============
+
+Besides shell command execution, the other common use of SSH connections is
+file transfer; `.Connection.put` and `.Connection.get` exist to fill this need.
+For example, say you had an archive file you wanted to upload:
+
+.. testsetup:: transfers
+
+    mock = MockSFTP()
+
+.. testcleanup:: transfers
+
+    mock.stop()
+
+.. doctest:: transfers
+
+    >>> from fabric import Connection
+    >>> result = Connection('web1').put('myfiles.tgz', remote='/opt/mydata/')
+    >>> print("Uploaded {0.local} to {0.remote}".format(result))
+    Uploaded /local/myfiles.tgz to /opt/mydata/
+
+These methods typically follow the behavior of ``cp`` and ``scp``/``sftp`` in
+terms of argument evaluation - for example, in the above snippet, we omitted
+the filename part of the remote path argument.
+
+
+Multiple actions
+================
+
+One-liners are good examples but aren't always realistic use cases - one
+typically needs multiple steps to do anything interesting. At the most basic
+level, you could do this by calling `.Connection` methods multiple times::
+
+    from fabric import Connection
+    c = Connection('web1')
+    c.put('myfiles.tgz', '/opt/mydata')
+    c.run('tar -C /opt/mydata -xzvf /opt/mydata/myfiles.tgz')
+
+You could (but don't have to) turn such blocks of code into functions,
+parameterized with a `.Connection` object from the caller, to encourage reuse::
+
+    def upload_and_unpack(c):
+        c.put('myfiles.tgz', '/opt/mydata')
+        c.run('tar -C /opt/mydata -xzvf /opt/mydata/myfiles.tgz')
+        
+As you'll see below, such functions can be handed to other API methods to
+enable more complex use cases as well.
+
+
+Multiple servers
+================
+
+Most real use cases involve doing things on more than one server. The
+straightforward approach could be to iterate over a list or tuple of
+`.Connection` arguments (or `.Connection` objects themselves, perhaps via
+``map``)::
+
+    >>> from fabric import Connection
+    >>> for host in ('web1', 'web2', 'mac1'):
+    >>>     result = Connection(host).run('uname -s')
+    ...     print("{}: {}".format(host, result.stdout.strip()))
+    ...
+    ...
+    web1: Linux
+    web2: Linux
+    mac1: Darwin
+    
+This approach works, but as use cases get more complex it can be
+useful to think of a collection of hosts as a single object. Enter `.Group`, a
+class wrapping one-or-more `.Connection` objects and offering a similar API;
+specifically, you'll want to use one of its concrete subclasses like
+`.SerialGroup` or `.ThreadingGroup`.
+
+The previous example, using `.Group` (`.SerialGroup` specifically), looks like
+this::
+
+    >>> from fabric import SerialGroup as Group
+    >>> results = Group('web1', 'web2', 'mac1').run('uname -s')
+    >>> print(results)
+    <GroupResult: {
+        <Connection 'web1'>: <CommandResult 'uname -s'>,
+        <Connection 'web2'>: <CommandResult 'uname -s'>,
+        <Connection 'mac1'>: <CommandResult 'uname -s'>,
+    }>
+    >>> for connection, result in results.items():
+    ...     print("{0.host}: {1.stdout}".format(connection, result))
+    ...
+    ...
+    web1: Linux
+    web2: Linux
+    mac1: Darwin
+
+Where `.Connection` methods return single ``Result`` objects (e.g.
+`fabric.runners.Result`), `.Group` methods return `.GroupResult` - `dict`-like
+objects offering access to individual per-connection results as well as
+metadata about the entire run.
+
+When any individual connections within the `.Group` encounter errors, the
+`.GroupResult` is lightly wrapped in a `.GroupException`, which is raised. Thus
+the aggregate behavior resembles that of individual `.Connection` methods,
+returning a value on success or raising an exception on failure.
+
+
+Bringing it all together
+========================
+
+Finally, we arrive at the most realistic use case: you've got a bundle of
+commands and/or file transfers and you want to apply it to multiple servers.
+You *could* use multiple `.Group` method calls to do this::
+
+    from fabric import SerialGroup as Group
+    pool = Group('web1', 'web2', 'web3')
+    pool.put('myfiles.tgz', '/opt/mydata')
+    pool.run('tar -C /opt/mydata -xzvf /opt/mydata/myfiles.tgz')
+
+That approach falls short as soon as logic becomes necessary - for example, if
+you only wanted to perform the copy-and-untar above when ``/opt/mydata`` is
+empty. Performing that sort of check requires execution on a per-server basis.
+
+You could fill that need by using iterables of `.Connection` objects (though
+this foregoes some benefits of using `Groups <.Group>`)::
+
+    from fabric import Connection
+    for host in ('web1', 'web2', 'web3'):
+        c = Connection(host)
+        if c.run('test -f /opt/mydata/myfile', warn=True).failed:
+            c.put('myfiles.tgz', '/opt/mydata')
+            c.run('tar -C /opt/mydata -xzvf /opt/mydata/myfiles.tgz')
+
+Alternatively, remember how we used a function in that earlier example? You can
+go that route instead::
+
+    from fabric import SerialGroup as Group
+
+    def upload_and_unpack(c):
+        if c.run('test -f /opt/mydata/myfile', warn=True).failed:
+            c.put('myfiles.tgz', '/opt/mydata')
+            c.run('tar -C /opt/mydata -xzvf /opt/mydata/myfiles.tgz')
+
+    for connection in Group('web1', 'web2', 'web3'):
+        upload_and_unpack(connection)
+
+The only convenience this final approach lacks is a useful analogue to
+`.Group.run` - if you want to track the results of all the
+``upload_and_unpack`` call as an aggregate, you have to do that yourself. Look
+to future feature releases for more in this space!
+
+
+Addendum: the ``fab`` command-line tool
+=======================================
+
+It's often useful to run Fabric code from a shell, e.g. deploying applications
+or running sysadmin jobs on arbitrary servers. You could use regular
+:ref:`Invoke tasks <defining-and-running-task-functions>` with Fabric library
+code in them, but another option is Fabric's own "network-oriented" tool,
+``fab``.
+
+``fab`` wraps Invoke's CLI mechanics with features like host selection, letting
+you quickly run tasks on various servers - without having to define ``host``
+kwargs on all your tasks or similar.
+
+.. note::
+    This mode was the primary API of Fabric 1.x; as of 2.0 it's just a
+    convenience. Whenever your use case falls outside these shortcuts, it
+    should be easy to revert to the library API directly (with or without
+    Invoke's less opinionated CLI tasks wrapped around it).
+
+For a final code example, let's adapt the previous example into a ``fab`` task
+module called ``fabfile.py``::
+
+    from fabric import task
+
+    @task
+    def upload_and_unpack(c):
+        if c.run('test -f /opt/mydata/myfile', warn=True).failed:
+            c.put('myfiles.tgz', '/opt/mydata')
+            c.run('tar -C /opt/mydata -xzvf /opt/mydata/myfiles.tgz')
+
+Not hard - all we did was copy our temporary task function into a file and slap
+a decorator on it. `~fabric.tasks.task` tells the CLI machinery to expose the
+task on the command line::
+
+    $ fab --list
+    Available tasks:
+
+      upload_and_unpack
+
+Then, when ``fab`` actually invokes a task, it knows how to stitch together
+arguments controlling target servers, and run the task once per server. To run
+the task once on a single server::
+
+    $ fab -H web1 upload_and_unpack
+
+When this occurs, ``c`` inside the task is set, effectively, to
+``Connection("web1")`` - as in earlier examples. Similarly, you can give more
+than one host, which runs the task multiple times, each time with a different
+`.Connection` instance handed in::
+
+    $ fab -H web1,web2,web3 upload_and_unpack
diff -Nru fabric-1.14.0/sites/docs/index.rst fabric-2.5.0/sites/docs/index.rst
--- fabric-1.14.0/sites/docs/index.rst	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/sites/docs/index.rst	2019-08-06 23:57:28.000000000 +0100
@@ -6,84 +6,55 @@
 Fabric is, including its public changelog & how the project is maintained,
 please see `the main project website <http://fabfile.org>`_.
 
+Getting started
+---------------
 
-Tutorial
---------
-
-For new users, and/or for an overview of Fabric's basic functionality, please
-see the :doc:`tutorial`. The rest of the documentation will assume you're
-at least passingly familiar with the material contained within.
+Many core ideas & API calls are explained in the tutorial/getting-started
+document:
 
 .. toctree::
-    :hidden:
+    :maxdepth: 2
 
-    tutorial
+    getting-started
 
+Upgrading from 1.x
+------------------
 
-.. _usage-docs:
+Looking to upgrade from Fabric 1.x? See our :ref:`detailed upgrade guide
+<upgrading>` on the nonversioned main project site.
 
-Usage documentation
--------------------
+.. _concepts-docs:
+
+Concepts
+--------
 
-The following list contains all major sections of Fabric's prose (non-API)
-documentation, which expands upon the concepts outlined in the
-:doc:`tutorial` and also covers advanced topics.
+Dig deeper into specific topics:
 
 .. toctree::
     :maxdepth: 2
     :glob:
 
-    usage/*
-
-
-.. _api_docs:
-
-API documentation
------------------
-
-Fabric maintains two sets of API documentation, autogenerated from the source
-code's docstrings (which are typically very thorough.)
-
-.. _core-api:
+    concepts/*
 
-Core API
-~~~~~~~~
+The ``fab`` CLI tool
+--------------------
 
-The **core** API is loosely defined as those functions, classes and methods
-which form the basic building blocks of Fabric (such as
-`~fabric.operations.run` and `~fabric.operations.sudo`) upon which everything
-else (the below "contrib" section, and user fabfiles) builds.
+Details on the CLI interface to Fabric, how it extends Invoke's CLI machinery,
+and examples of shortcuts for executing tasks across hosts or groups.
 
 .. toctree::
-    :maxdepth: 1
-    :glob:
-
-    api/core/*
+    cli
 
-.. _contrib-api:
+.. _api-docs:
 
-Contrib API
-~~~~~~~~~~~
+API
+---
 
-Fabric's **contrib** package contains commonly useful tools (often merged in
-from user fabfiles) for tasks such as user I/O, modifying remote files, and so
-forth. While the core API is likely to remain small and relatively unchanged
-over time, this contrib section will grow and evolve (while trying to remain
-backwards-compatible) as more use-cases are solved and added.
+Know what you're looking for & just need API details? View our auto-generated
+API documentation:
 
 .. toctree::
     :maxdepth: 1
     :glob:
 
-    api/contrib/*
-
-
-Contributing & Running Tests
-----------------------------
-
-For advanced users & developers looking to help fix bugs or add new features.
-
-.. toctree::
-    :hidden:
-
-    running_tests
+    api/*
diff -Nru fabric-1.14.0/sites/docs/running_tests.rst fabric-2.5.0/sites/docs/running_tests.rst
--- fabric-1.14.0/sites/docs/running_tests.rst	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/sites/docs/running_tests.rst	1970-01-01 01:00:00.000000000 +0100
@@ -1,48 +0,0 @@
-======================
-Running Fabric's Tests
-======================
-
-Fabric is maintained with 100% passing tests. Where possible, patches should
-include tests covering the changes, making things far easier to verify & merge.
-
-When developing on Fabric, it works best to establish a `virtualenv`_ to install
-the dependencies in isolation for running tests.
-
-.. _`virtualenv`: https://virtualenv.pypa.io/en/latest/
-
-.. _first-time-setup:
-
-First-time Setup
-================
-
-* Fork the `repository`_ on GitHub
-* Clone your new fork (e.g.
-  ``git clone git@github.com:<your_username>/fabric.git``)
-* ``cd fabric``
-* ``virtualenv env``
-* ``. env/bin/activate``
-* ``pip install -r requirements.txt``
-* ``python setup.py develop``
-
-.. _`repository`: https://github.com/fabric/fabric
-
-.. _running-tests:
-
-Running Tests
-=============
-
-Once your virtualenv is activated (``. env/bin/activate``) & you have the latest
-requirements, running tests is just::
-
-    nosetests tests/
-
-You should **always** run tests on ``master`` (or the release branch you're
-working with) to ensure they're passing before working on your own
-changes/tests.
-
-Alternatively, if you've run ``python setup.py develop`` on your Fabric clone,
-you can also run::
-
-    fab test
-
-This adds additional flags which enable running doctests & adds nice coloration.
diff -Nru fabric-1.14.0/sites/docs/tutorial.rst fabric-2.5.0/sites/docs/tutorial.rst
--- fabric-1.14.0/sites/docs/tutorial.rst	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/sites/docs/tutorial.rst	1970-01-01 01:00:00.000000000 +0100
@@ -1,468 +0,0 @@
-=====================
-Overview and Tutorial
-=====================
-
-Welcome to Fabric!
-
-This document is a whirlwind tour of Fabric's features and a quick guide to its
-use. Additional documentation (which is linked to throughout) can be found in
-the :ref:`usage documentation <usage-docs>` -- please make sure to check it out.
-
-
-What is Fabric?
-===============
-
-As the ``README`` says:
-
-    .. include:: ../../README.rst
-        :end-before: It provides
-
-More specifically, Fabric is:
-
-* A tool that lets you execute **arbitrary Python functions** via the **command
-  line**;
-* A library of subroutines (built on top of a lower-level library) to make
-  executing shell commands over SSH **easy** and **Pythonic**.
-
-Naturally, most users combine these two things, using Fabric to write and
-execute Python functions, or **tasks**, to automate interactions with remote
-servers. Let's take a look.
-
-
-Hello, ``fab``
-==============
-
-This wouldn't be a proper tutorial without "the usual"::
-
-    def hello():
-        print("Hello world!")
-
-Placed in a Python module file named ``fabfile.py`` in your current working
-directory, that ``hello`` function can be executed with the ``fab`` tool
-(installed as part of Fabric) and does just what you'd expect::
-
-    $ fab hello
-    Hello world!
-
-    Done.
-
-That's all there is to it. This functionality allows Fabric to be used as a
-(very) basic build tool even without importing any of its API.
-
-.. note::
-
-    The ``fab`` tool simply imports your fabfile and executes the function or
-    functions you instruct it to. There's nothing magic about it -- anything
-    you can do in a normal Python script can be done in a fabfile!
-
-.. seealso:: :ref:`execution-strategy`, :doc:`/usage/tasks`, :doc:`/usage/fab`
-
-
-Task arguments
-==============
-
-It's often useful to pass runtime parameters into your tasks, just as you might
-during regular Python programming. Fabric has basic support for this using a
-shell-compatible notation: ``<task name>:<arg>,<kwarg>=<value>,...``. It's
-contrived, but let's extend the above example to say hello to you personally::
-
-    def hello(name="world"):
-        print("Hello %s!" % name)
-
-By default, calling ``fab hello`` will still behave as it did before; but now
-we can personalize it::
-
-    $ fab hello:name=Jeff
-    Hello Jeff!
-
-    Done.
-
-Those already used to programming in Python might have guessed that this
-invocation behaves exactly the same way::
-
-    $ fab hello:Jeff
-    Hello Jeff!
-
-    Done.
-
-For the time being, your argument values will always show up in Python as
-strings and may require a bit of string manipulation for complex types such
-as lists. Future versions may add a typecasting system to make this easier.
-
-.. seealso:: :ref:`task-arguments`
-
-Local commands
-==============
-
-As used above, ``fab`` only really saves a couple lines of
-``if __name__ == "__main__"`` boilerplate. It's mostly designed for use with
-Fabric's API, which contains functions (or **operations**) for executing shell
-commands, transferring files, and so forth.
-
-Let's build a hypothetical Web application fabfile. This example scenario is
-as follows: The Web application is managed via Git on a remote host
-``vcshost``. On ``localhost``, we have a local clone of said Web application.
-When we push changes back to ``vcshost``, we want to be able to immediately
-install these changes on a remote host ``my_server`` in an automated fashion.
-We will do this by automating the local and remote Git commands.
-
-Fabfiles usually work best at the root of a project::
-
-    .
-    |-- __init__.py
-    |-- app.wsgi
-    |-- fabfile.py <-- our fabfile!
-    |-- manage.py
-    `-- my_app
-        |-- __init__.py
-        |-- models.py
-        |-- templates
-        |   `-- index.html
-        |-- tests.py
-        |-- urls.py
-        `-- views.py
-
-.. note::
-
-    We're using a Django application here, but only as an example -- Fabric is
-    not tied to any external codebase, save for its SSH library.
-
-For starters, perhaps we want to run our tests and commit to our VCS so we're
-ready for a deploy::
-
-    from fabric.api import local
-
-    def prepare_deploy():
-        local("./manage.py test my_app")
-        local("git add -p && git commit")
-        local("git push")
-
-The output of which might look a bit like this::
-
-    $ fab prepare_deploy
-    [localhost] run: ./manage.py test my_app
-    Creating test database...
-    Creating tables
-    Creating indexes
-    ..........................................
-    ----------------------------------------------------------------------
-    Ran 42 tests in 9.138s
-
-    OK
-    Destroying test database...
-
-    [localhost] run: git add -p && git commit
-
-    <interactive Git add / git commit edit message session>
-
-    [localhost] run: git push
-
-    <git push session, possibly merging conflicts interactively>
-
-    Done.
-
-The code itself is straightforward: import a Fabric API function,
-`~fabric.operations.local`, and use it to run and interact with local shell
-commands. The rest of Fabric's API is similar -- it's all just Python.
-
-.. seealso:: :doc:`api/core/operations`, :ref:`fabfile-discovery`
-
-
-Organize it your way
-====================
-
-Because Fabric is "just Python" you're free to organize your fabfile any way
-you want. For example, it's often useful to start splitting things up into
-subtasks::
-
-    from fabric.api import local
-
-    def test():
-        local("./manage.py test my_app")
-
-    def commit():
-        local("git add -p && git commit")
-
-    def push():
-        local("git push")
-
-    def prepare_deploy():
-        test()
-        commit()
-        push()
-
-The ``prepare_deploy`` task can be called just as before, but now you can make
-a more granular call to one of the sub-tasks, if desired.
-
-
-Failure
-=======
-
-Our base case works fine now, but what happens if our tests fail?  Chances are
-we want to put on the brakes and fix them before deploying.
-
-Fabric checks the return value of programs called via operations and will abort
-if they didn't exit cleanly. Let's see what happens if one of our tests
-encounters an error::
-
-    $ fab prepare_deploy
-    [localhost] run: ./manage.py test my_app
-    Creating test database...
-    Creating tables
-    Creating indexes
-    .............E............................
-    ======================================================================
-    ERROR: testSomething (my_project.my_app.tests.MainTests)
-    ----------------------------------------------------------------------
-    Traceback (most recent call last):
-    [...]
-
-    ----------------------------------------------------------------------
-    Ran 42 tests in 9.138s
-
-    FAILED (errors=1)
-    Destroying test database...
-
-    Fatal error: local() encountered an error (return code 2) while executing './manage.py test my_app'
-
-    Aborting.
-
-Great! We didn't have to do anything ourselves: Fabric detected the failure and
-aborted, never running the ``commit`` task.
-
-.. seealso:: :ref:`Failure handling (usage documentation) <failures>`
-
-Failure handling
-----------------
-
-But what if we wanted to be flexible and give the user a choice? A setting
-(or **environment variable**, usually shortened to **env var**) called
-:ref:`warn_only` lets you turn aborts into warnings, allowing flexible error
-handling to occur.
-
-Let's flip this setting on for our ``test`` function, and then inspect the
-result of the `~fabric.operations.local` call ourselves::
-
-    from __future__ import with_statement
-    from fabric.api import local, settings, abort
-    from fabric.contrib.console import confirm
-
-    def test():
-        with settings(warn_only=True):
-            result = local('./manage.py test my_app', capture=True)
-        if result.failed and not confirm("Tests failed. Continue anyway?"):
-            abort("Aborting at user request.")
-
-    [...]
-
-In adding this new feature we've introduced a number of new things:
-
-* The ``__future__`` import required to use ``with:`` in Python 2.5;
-* Fabric's `contrib.console <fabric.contrib.console>` submodule, containing the
-  `~fabric.contrib.console.confirm` function, used for simple yes/no prompts;
-* The `~fabric.context_managers.settings` context manager, used to apply
-  settings to a specific block of code;
-* Command-running operations like `~fabric.operations.local` can return objects
-  containing info about their result (such as ``.failed``, or
-  ``.return_code``);
-* And the `~fabric.utils.abort` function, used to manually abort execution.
-
-However, despite the additional complexity, it's still pretty easy to follow,
-and is now much more flexible.
-
-.. seealso:: :doc:`api/core/context_managers`, :ref:`env-vars`
-
-
-Making connections
-==================
-
-Let's start wrapping up our fabfile by putting in the keystone: a ``deploy``
-task that is destined to run on one or more remote server(s), and ensures the
-code is up to date::
-
-    def deploy():
-        code_dir = '/srv/django/myproject'
-        with cd(code_dir):
-            run("git pull")
-            run("touch app.wsgi")
-
-Here again, we introduce a handful of new concepts:
-
-* Fabric is just Python -- so we can make liberal use of regular Python code
-  constructs such as variables and string interpolation;
-* `~fabric.context_managers.cd`, an easy way of prefixing commands with a ``cd
-  /to/some/directory`` call. This is similar to  `~fabric.context_managers.lcd`
-  which does the same locally.
-* `~fabric.operations.run`, which is similar to `~fabric.operations.local` but
-  runs **remotely** instead of locally.
-
-We also need to make sure we import the new functions at the top of our file::
-
-    from __future__ import with_statement
-    from fabric.api import local, settings, abort, run, cd
-    from fabric.contrib.console import confirm
-
-With these changes in place, let's deploy::
-
-    $ fab deploy
-    No hosts found. Please specify (single) host string for connection: my_server
-    [my_server] run: git pull
-    [my_server] out: Already up-to-date.
-    [my_server] out:
-    [my_server] run: touch app.wsgi
-
-    Done.
-
-We never specified any connection info in our fabfile, so Fabric doesn't know
-on which host(s) the remote command should be executed. When this happens,
-Fabric prompts us at runtime. Connection definitions use SSH-like "host
-strings" (e.g. ``user@host:port``) and will use your local username as a
-default -- so in this example, we just had to specify the hostname,
-``my_server``.
-
-
-Remote interactivity
---------------------
-
-``git pull`` works fine if you've already got a checkout of your source code --
-but what if this is the first deploy? It'd be nice to handle that case too and
-do the initial ``git clone``::
-
-    def deploy():
-        code_dir = '/srv/django/myproject'
-        with settings(warn_only=True):
-            if run("test -d %s" % code_dir).failed:
-                run("git clone user@vcshost:/path/to/repo/.git %s" % code_dir)
-        with cd(code_dir):
-            run("git pull")
-            run("touch app.wsgi")
-
-As with our calls to `~fabric.operations.local` above, `~fabric.operations.run`
-also lets us construct clean Python-level logic based on executed shell
-commands. However, the interesting part here is the ``git clone`` call: since
-we're using Git's SSH method of accessing the repository on our Git server,
-this means our remote `~fabric.operations.run` call will need to authenticate
-itself.
-
-Older versions of Fabric (and similar high level SSH libraries) run remote
-programs in limbo, unable to be touched from the local end. This is
-problematic when you have a serious need to enter passwords or otherwise
-interact with the remote program.
-
-Fabric 1.0 and later breaks down this wall and ensures you can always talk to
-the other side. Let's see what happens when we run our updated ``deploy`` task
-on a new server with no Git checkout::
-
-    $ fab deploy
-    No hosts found. Please specify (single) host string for connection: my_server
-    [my_server] run: test -d /srv/django/myproject
-
-    Warning: run() encountered an error (return code 1) while executing 'test -d /srv/django/myproject'
-
-    [my_server] run: git clone user@vcshost:/path/to/repo/.git /srv/django/myproject
-    [my_server] out: Cloning into /srv/django/myproject...
-    [my_server] out: Password: <enter password>
-    [my_server] out: remote: Counting objects: 6698, done.
-    [my_server] out: remote: Compressing objects: 100% (2237/2237), done.
-    [my_server] out: remote: Total 6698 (delta 4633), reused 6414 (delta 4412)
-    [my_server] out: Receiving objects: 100% (6698/6698), 1.28 MiB, done.
-    [my_server] out: Resolving deltas: 100% (4633/4633), done.
-    [my_server] out:
-    [my_server] run: git pull
-    [my_server] out: Already up-to-date.
-    [my_server] out:
-    [my_server] run: touch app.wsgi
-
-    Done.
-
-Notice the ``Password:`` prompt -- that was our remote ``git`` call on our Web server, asking for the password to the Git server. We were able to type it in and the clone continued normally.
-
-.. seealso:: :doc:`/usage/interactivity`
-
-
-.. _defining-connections:
-
-Defining connections beforehand
--------------------------------
-
-Specifying connection info at runtime gets old real fast, so Fabric provides a
-handful of ways to do it in your fabfile or on the command line. We won't cover
-all of them here, but we will show you the most common one: setting the global
-host list, :ref:`env.hosts <hosts>`.
-
-:doc:`env <usage/env>` is a global dictionary-like object driving many of
-Fabric's settings, and can be written to with attributes as well (in fact,
-`~fabric.context_managers.settings`, seen above, is simply a wrapper for this.)
-Thus, we can modify it at module level near the top of our fabfile like so::
-
-    from __future__ import with_statement
-    from fabric.api import *
-    from fabric.contrib.console import confirm
-
-    env.hosts = ['my_server']
-
-    def test():
-        do_test_stuff()
-
-When ``fab`` loads up our fabfile, our modification of ``env`` will execute,
-storing our settings change. The end result is exactly as above: our ``deploy``
-task will run against the ``my_server`` server.
-
-This is also how you can tell Fabric to run on multiple remote systems at once:
-because ``env.hosts`` is a list, ``fab`` iterates over it, calling the given
-task once for each connection.
-
-.. seealso:: :doc:`usage/env`, :ref:`host-lists`
-
-
-Conclusion
-==========
-
-Our completed fabfile is still pretty short, as such things go. Here it is in
-its entirety::
-
-    from __future__ import with_statement
-    from fabric.api import *
-    from fabric.contrib.console import confirm
-
-    env.hosts = ['my_server']
-
-    def test():
-        with settings(warn_only=True):
-            result = local('./manage.py test my_app', capture=True)
-        if result.failed and not confirm("Tests failed. Continue anyway?"):
-            abort("Aborting at user request.")
-
-    def commit():
-        local("git add -p && git commit")
-
-    def push():
-        local("git push")
-
-    def prepare_deploy():
-        test()
-        commit()
-        push()
-
-    def deploy():
-        code_dir = '/srv/django/myproject'
-        with settings(warn_only=True):
-            if run("test -d %s" % code_dir).failed:
-                run("git clone user@vcshost:/path/to/repo/.git %s" % code_dir)
-        with cd(code_dir):
-            run("git pull")
-            run("touch app.wsgi")
-
-This fabfile makes use of a large portion of Fabric's feature set:
-
-* defining fabfile tasks and running them with :doc:`fab <usage/fab>`;
-* calling local shell commands with `~fabric.operations.local`;
-* modifying env vars with `~fabric.context_managers.settings`;
-* handling command failures, prompting the user, and manually aborting;
-* and defining host lists and `~fabric.operations.run`-ning remote commands.
-
-However, there's still a lot more we haven't covered here! Please make sure you
-follow the various "see also" links, and check out the documentation table of
-contents on :doc:`the main index page <index>`.
-
-Thanks for reading!
diff -Nru fabric-1.14.0/sites/docs/upgrading.rst fabric-2.5.0/sites/docs/upgrading.rst
--- fabric-1.14.0/sites/docs/upgrading.rst	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/sites/docs/upgrading.rst	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,9 @@
+:orphan:
+
+==================
+Upgrading - moved!
+==================
+
+If you're here, you're probably following an old link or bookmark. The
+upgrading page has moved to the unversioned main project site:
+:ref:`upgrading`. Please update your bookmarks and links!
diff -Nru fabric-1.14.0/sites/docs/usage/env.rst fabric-2.5.0/sites/docs/usage/env.rst
--- fabric-1.14.0/sites/docs/usage/env.rst	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/sites/docs/usage/env.rst	1970-01-01 01:00:00.000000000 +0100
@@ -1,973 +0,0 @@
-===================================
-The environment dictionary, ``env``
-===================================
-
-A simple but integral aspect of Fabric is what is known as the "environment": a
-Python dictionary subclass, which is used as a combination settings registry and
-shared inter-task data namespace.
-
-The environment dict is currently implemented as a global singleton,
-``fabric.state.env``, and is included in ``fabric.api`` for convenience. Keys
-in ``env`` are sometimes referred to as "env variables".
-
-Environment as configuration
-============================
-
-Most of Fabric's behavior is controllable by modifying ``env`` variables, such
-as ``env.hosts`` (as seen in :ref:`the tutorial <defining-connections>`). Other
-commonly-modified env vars include:
-
-* ``user``: Fabric defaults to your local username when making SSH connections,
-  but you can use ``env.user`` to override this if necessary. The :doc:`execution`
-  documentation also has info on how to specify usernames on a per-host basis.
-* ``password``: Used to explicitly set your default connection or sudo password
-  if desired. Fabric will prompt you when necessary if this isn't set or
-  doesn't appear to be valid.
-* ``warn_only``: a Boolean setting determining whether Fabric exits when
-  detecting errors on the remote end. See :doc:`execution` for more on this
-  behavior.
-
-There are a number of other env variables; for the full list, see
-:ref:`env-vars` at the bottom of this document.
-
-The `~fabric.context_managers.settings` context manager
--------------------------------------------------------
-
-In many situations, it's useful to only temporarily modify ``env`` vars so that
-a given settings change only applies to a block of code. Fabric provides a
-`~fabric.context_managers.settings` context manager, which takes any number of
-key/value pairs and will use them to modify ``env`` within its wrapped block.
-
-For example, there are many situations where setting ``warn_only`` (see below)
-is useful. To apply it to a few lines of code, use
-``settings(warn_only=True)``, as seen in this simplified version of the
-``contrib`` `~fabric.contrib.files.exists` function::
-
-    from fabric.api import settings, run
-
-    def exists(path):
-        with settings(warn_only=True):
-            return run('test -e %s' % path)
-
-See the :doc:`../api/core/context_managers` API documentation for details on
-`~fabric.context_managers.settings` and other, similar tools.
-
-Environment as shared state
-===========================
-
-As mentioned, the ``env`` object is simply a dictionary subclass, so your own
-fabfile code may store information in it as well. This is sometimes useful for
-keeping state between multiple tasks within a single execution run.
-
-.. note::
-
-    This aspect of ``env`` is largely historical: in the past, fabfiles were
-    not pure Python and thus the environment was the only way to communicate
-    between tasks. Nowadays, you may call other tasks or subroutines directly,
-    and even keep module-level shared state if you wish.
-
-    In future versions, Fabric will become threadsafe, at which point ``env``
-    may be the only easy/safe way to keep global state.
-
-Other considerations
-====================
-
-While it subclasses ``dict``, Fabric's ``env`` has been modified so that its
-values may be read/written by way of attribute access, as seen in some of the
-above material. In other words, ``env.host_string`` and ``env['host_string']``
-are functionally identical. We feel that attribute access can often save a bit
-of typing and makes the code more readable, so it's the recommended way to
-interact with ``env``.
-
-The fact that it's a dictionary can be useful in other ways, such as with
-Python's ``dict``-based string interpolation, which is especially handy if you
-need to insert multiple env vars into a single string. Using "normal" string
-interpolation might look like this::
-
-    print("Executing on %s as %s" % (env.host, env.user))
-
-Using dict-style interpolation is more readable and slightly shorter::
-
-        print("Executing on %(host)s as %(user)s" % env)
-
-.. _env-vars:
-
-Full list of env vars
-=====================
-
-Below is a list of all predefined (or defined by Fabric itself during
-execution) environment variables. While many of them may be manipulated
-directly, it's often best to use `~fabric.context_managers`, either generally
-via `~fabric.context_managers.settings` or via specific context managers such
-as `~fabric.context_managers.cd`.
-
-Note that many of these may be set via ``fab``'s command-line switches -- see
-:doc:`fab` for details. Cross-references are provided where appropriate.
-
-.. seealso:: :option:`--set`
-
-.. _abort-exception:
-
-``abort_exception``
--------------------
-
-**Default:** ``None``
-
-Fabric normally handles aborting by printing an error message to stderr and
-calling ``sys.exit(1)``. This setting allows you to override that behavior
-(which is what happens when ``env.abort_exception`` is ``None``.)
-
-Give it a callable which takes a string (the error message that would have been
-printed) and returns an exception instance.  That exception object is then
-raised instead of ``SystemExit`` (which is what ``sys.exit`` does.)
-
-Much of the time you'll want to simply set this to an exception class, as those
-fit the above description perfectly (callable, take a string, return an
-exception instance.) E.g. ``env.abort_exception = MyExceptionClass``.
-
-.. _abort-on-prompts:
-
-``abort_on_prompts``
---------------------
-
-**Default:** ``False``
-
-When ``True``, Fabric will run in a non-interactive mode, calling
-`~fabric.utils.abort` anytime it would normally prompt the user for input (such
-as password prompts, "What host to connect to?" prompts, fabfile invocation of
-`~fabric.operations.prompt`, and so forth.) This allows users to ensure a Fabric
-session will always terminate cleanly instead of blocking on user input forever
-when unforeseen circumstances arise.
-
-.. versionadded:: 1.1
-.. seealso:: :option:`--abort-on-prompts`
-
-
-``all_hosts``
--------------
-
-**Default:** ``[]``
-
-Set by ``fab`` to the full host list for the currently executing command. For
-informational purposes only.
-
-.. seealso:: :doc:`execution`
-
-.. _always-use-pty:
-
-``always_use_pty``
-------------------
-
-**Default:** ``True``
-
-When set to ``False``, causes `~fabric.operations.run`/`~fabric.operations.sudo`
-to act as if they have been called with ``pty=False``.
-
-.. seealso:: :option:`--no-pty`
-.. versionadded:: 1.0
-
-.. _colorize-errors:
-
-``colorize_errors``
--------------------
-
-**Default** ``False``
-
-When set to ``True``, error output to the terminal is colored red and warnings
-are colored magenta to make them easier to see.
-
-.. versionadded:: 1.7
-
-.. _combine-stderr:
-
-``combine_stderr``
-------------------
-
-**Default**: ``True``
-
-Causes the SSH layer to merge a remote program's stdout and stderr streams to
-avoid becoming meshed together when printed. See :ref:`combine_streams` for
-details on why this is needed and what its effects are.
-
-.. versionadded:: 1.0
-
-``command``
------------
-
-**Default:** ``None``
-
-Set by ``fab`` to the currently executing command name (e.g., when executed as
-``$ fab task1 task2``, ``env.command`` will be set to ``"task1"`` while
-``task1`` is executing, and then to ``"task2"``.) For informational purposes
-only.
-
-.. seealso:: :doc:`execution`
-
-``command_prefixes``
---------------------
-
-**Default:** ``[]``
-
-Modified by `~fabric.context_managers.prefix`, and prepended to commands
-executed by `~fabric.operations.run`/`~fabric.operations.sudo`.
-
-.. versionadded:: 1.0
-
-.. _command-timeout:
-
-``command_timeout``
--------------------
-
-**Default:** ``None``
-
-Remote command timeout, in seconds.
-
-.. versionadded:: 1.6
-.. seealso:: :option:`--command-timeout`
-
-.. _connection-attempts:
-
-``connection_attempts``
------------------------
-
-**Default:** ``1``
-
-Number of times Fabric will attempt to connect when connecting to a new server. For backwards compatibility reasons, it defaults to only one connection attempt.
-
-.. versionadded:: 1.4
-.. seealso:: :option:`--connection-attempts`, :ref:`timeout`
-
-``cwd``
--------
-
-**Default:** ``''``
-
-Current working directory. Used to keep state for the
-`~fabric.context_managers.cd` context manager.
-
-.. _dedupe_hosts:
-
-``dedupe_hosts``
-----------------
-
-**Default:** ``True``
-
-Deduplicate merged host lists so any given host string is only represented once
-(e.g. when using combinations of ``@hosts`` + ``@roles``, or ``-H`` and
-``-R``.)
-
-When set to ``False``, this option relaxes the deduplication, allowing users
-who explicitly want to run a task multiple times on the same host (say, in
-parallel, though it works fine serially too) to do so.
-
-.. versionadded:: 1.5
-
-.. _disable-known-hosts:
-
-``disable_known_hosts``
------------------------
-
-**Default:** ``False``
-
-If ``True``, the SSH layer will skip loading the user's known-hosts file.
-Useful for avoiding exceptions in situations where a "known host" changing its
-host key is actually valid (e.g. cloud servers such as EC2.)
-
-.. seealso:: :option:`--disable-known-hosts <-D>`, :doc:`ssh`
-
-
-.. _eagerly-disconnect:
-
-``eagerly_disconnect``
-----------------------
-
-**Default:** ``False``
-
-If ``True``, causes ``fab`` to close connections after each individual task
-execution, instead of at the end of the run. This helps prevent a lot of
-typically-unused network sessions from piling up and causing problems with
-limits on per-process open files, or network hardware.
-
-.. note::
-    When active, this setting will result in the disconnect messages appearing
-    throughout your output, instead of at the end. This may be improved in
-    future releases.
-
-.. _effective_roles:
-
-``effective_roles``
--------------------
-
-**Default:** ``[]``
-
-Set by ``fab`` to the roles list of the currently executing command. For
-informational purposes only.
-
-.. versionadded:: 1.9
-.. seealso:: :doc:`execution`
-
-.. _exclude-hosts:
-
-``exclude_hosts``
------------------
-
-**Default:** ``[]``
-
-Specifies a list of host strings to be :ref:`skipped over <exclude-hosts>`
-during ``fab`` execution. Typically set via :option:`--exclude-hosts/-x <-x>`.
-
-.. versionadded:: 1.1
-
-
-``fabfile``
------------
-
-**Default:** ``fabfile.py``
-
-Filename pattern which ``fab`` searches for when loading fabfiles.
-To indicate a specific file, use the full path to the file. Obviously, it
-doesn't make sense to set this in a fabfile, but it may be specified in a
-``.fabricrc`` file or on the command line.
-
-.. seealso:: :option:`--fabfile <-f>`, :doc:`fab`
-
-
-.. _gateway:
-
-``gateway``
------------
-
-**Default:** ``None``
-
-Enables SSH-driven gatewaying through the indicated host. The value should be a
-normal Fabric host string as used in e.g. :ref:`env.host_string <host_string>`.
-When this is set, newly created connections will be set to route their SSH
-traffic through the remote SSH daemon to the final destination.
-
-.. versionadded:: 1.5
-
-.. seealso:: :option:`--gateway <-g>`
-
-.. _kerberos:
-
-``gss_(auth|deleg|kex)``
-------------------------
-
-**Default:** ``False`` for all.
-
-These three options (``gss_auth``, ``gss_deleg``, and ``gss_kex``) are passed
-verbatim into Paramiko's ``Client.connect`` method, and control
-Kerberos/GSS-API behavior. For details, see Paramiko's docs: `GSS-API
-authentication <http://docs.paramiko.org/en/latest/api/ssh_gss.html>`_, `GSS-API key exchange <http://docs.paramiko.org/en/latest/api/kex_gss.html>`_.
-
-.. note::
-    This functionality requires Paramiko ``1.15`` or above! You will get
-    ``TypeError`` about unexpected keyword arguments with Paramiko ``1.14`` or
-    earlier, as it lacks Kerberos support.
-
-.. versionadded:: 1.11
-.. seealso:: :option:`--gss-auth`, :option:`--gss-deleg`, :option:`--gss-kex`
-
-.. _host_string:
-
-``host_string``
----------------
-
-**Default:** ``None``
-
-Defines the current user/host/port which Fabric will connect to when executing
-`~fabric.operations.run`, `~fabric.operations.put` and so forth. This is set by
-``fab`` when iterating over a previously set host list, and may also be
-manually set when using Fabric as a library.
-
-.. seealso:: :doc:`execution`
-
-
-.. _forward-agent:
-
-``forward_agent``
---------------------
-
-**Default:** ``False``
-
-If ``True``, enables forwarding of your local SSH agent to the remote end.
-
-.. versionadded:: 1.4
-
-.. seealso:: :option:`--forward-agent <-A>`
-
-.. _host:
-
-``host``
---------
-
-**Default:** ``None``
-
-Set to the hostname part of ``env.host_string`` by ``fab``. For informational
-purposes only.
-
-.. _hosts:
-
-``hosts``
----------
-
-**Default:** ``[]``
-
-The global host list used when composing per-task host lists.
-
-.. seealso:: :option:`--hosts <-H>`, :doc:`execution`
-
-.. _keepalive:
-
-``keepalive``
--------------
-
-**Default:** ``0`` (i.e. no keepalive)
-
-An integer specifying an SSH keepalive interval to use; basically maps to the
-SSH config option ``ServerAliveInterval``. Useful if you find connections are
-timing out due to meddlesome network hardware or what have you.
-
-.. seealso:: :option:`--keepalive`
-.. versionadded:: 1.1
-
-
-.. _key:
-
-``key``
-----------------
-
-**Default:** ``None``
-
-A string, or file-like object, containing an SSH key; used during connection
-authentication.
-
-.. note::
-    The most common method for using SSH keys is to set :ref:`key-filename`.
-
-.. versionadded:: 1.7
-
-
-.. _key-filename:
-
-``key_filename``
-----------------
-
-**Default:** ``None``
-
-May be a string or list of strings, referencing file paths to SSH key files to
-try when connecting. Passed through directly to the SSH layer. May be
-set/appended to with :option:`-i`.
-
-.. seealso:: `Paramiko's documentation for SSHClient.connect() <http://docs.paramiko.org/en/latest/api/client.html#paramiko.client.SSHClient.connect>`_
-
-.. _env-linewise:
-
-``linewise``
-------------
-
-**Default:** ``False``
-
-Forces buffering by line instead of by character/byte, typically when running
-in parallel mode. May be activated via :option:`--linewise`. This option is
-implied by :ref:`env.parallel <env-parallel>` -- even if ``linewise`` is False,
-if ``parallel`` is True then linewise behavior will occur.
-
-.. seealso:: :ref:`linewise-output`
-
-.. versionadded:: 1.3
-
-
-.. _local-user:
-
-``local_user``
---------------
-
-A read-only value containing the local system username. This is the same value
-as :ref:`user`'s initial value, but whereas :ref:`user` may be altered by CLI
-arguments, Python code or specific host strings, :ref:`local-user` will always
-contain the same value.
-
-.. _no_agent:
-
-``no_agent``
-------------
-
-**Default:** ``False``
-
-If ``True``, will tell the SSH layer not to seek out running SSH agents when
-using key-based authentication.
-
-.. versionadded:: 0.9.1
-.. seealso:: :option:`--no_agent <-a>`
-
-.. _no_keys:
-
-``no_keys``
-------------------
-
-**Default:** ``False``
-
-If ``True``, will tell the SSH layer not to load any private key files from
-one's ``$HOME/.ssh/`` folder. (Key files explicitly loaded via ``fab -i`` will
-still be used, of course.)
-
-.. versionadded:: 0.9.1
-.. seealso:: :option:`-k`
-
-.. _output_prefix:
-
-``output_prefix``
------------------
-
-**Default:** ``True``
-
-By default Fabric prefixes every line of output with either ``[hostname] out:``
-or ``[hostname] err:``. Those prefixes may be hidden by setting
-``env.output_prefix`` to ``False``.
-
-.. versionadded:: 1.0.0
-
-.. _env-parallel:
-
-``parallel``
--------------------
-
-**Default:** ``False``
-
-When ``True``, forces all tasks to run in parallel. Implies :ref:`env.linewise
-<env-linewise>`.
-
-.. versionadded:: 1.3
-.. seealso:: :option:`--parallel <-P>`, :doc:`parallel`
-
-.. _password:
-
-``password``
-------------
-
-**Default:** ``None``
-
-The default password used by the SSH layer when connecting to remote hosts,
-**and/or** when answering `~fabric.operations.sudo` prompts.
-
-.. seealso:: :option:`--initial-password-prompt <-I>`, :ref:`env.passwords <passwords>`, :ref:`password-management`
-
-.. _passwords:
-
-``passwords``
--------------
-
-**Default:** ``{}``
-
-This dictionary is largely for internal use, and is filled automatically as a
-per-host-string password cache. Keys are full :ref:`host strings
-<host-strings>` and values are passwords (strings).
-
-.. warning::
-    If you modify or generate this dict manually, **you must use fully
-    qualified host strings** with user and port values. See the link above for
-    details on the host string API.
-
-.. seealso:: :ref:`password-management`
-
-.. versionadded:: 1.0
-
-
-.. _env-path:
-
-``path``
---------
-
-**Default:** ``''``
-
-Used to set the ``$PATH`` shell environment variable when executing commands in
-`~fabric.operations.run`/`~fabric.operations.sudo`/`~fabric.operations.local`.
-It is recommended to use the `~fabric.context_managers.path` context manager
-for managing this value instead of setting it directly.
-
-.. versionadded:: 1.0
-
-
-.. _pool-size:
-
-``pool_size``
--------------
-
-**Default:** ``0``
-
-Sets the number of concurrent processes to use when executing tasks in parallel.
-
-.. versionadded:: 1.3
-.. seealso:: :option:`--pool-size <-z>`, :doc:`parallel`
-
-.. _prompts:
-
-``prompts``
--------------
-
-**Default:** ``{}``
-
-The ``prompts`` dictionary allows users to control interactive prompts. If a
-key in the dictionary is found in a command's standard output stream, Fabric
-will automatically answer with the corresponding dictionary value.
-
-.. versionadded:: 1.9
-
-.. _port:
-
-``port``
---------
-
-**Default:** ``None``
-
-Set to the port part of ``env.host_string`` by ``fab`` when iterating over a
-host list. May also be used to specify a default port.
-
-.. _real-fabfile:
-
-``real_fabfile``
-----------------
-
-**Default:** ``None``
-
-Set by ``fab`` with the path to the fabfile it has loaded up, if it got that
-far. For informational purposes only.
-
-.. seealso:: :doc:`fab`
-
-
-.. _remote-interrupt:
-
-``remote_interrupt``
---------------------
-
-**Default:** ``None``
-
-Controls whether Ctrl-C triggers an interrupt remotely or is captured locally,
-as follows:
-
-* ``None`` (the default): only `~fabric.operations.open_shell` will exhibit
-  remote interrupt behavior, and
-  `~fabric.operations.run`/`~fabric.operations.sudo` will capture interrupts
-  locally.
-* ``False``: even `~fabric.operations.open_shell` captures locally.
-* ``True``: all functions will send the interrupt to the remote end.
-
-.. versionadded:: 1.6
-
-
-.. _rcfile:
-
-``rcfile``
-----------
-
-**Default:** ``$HOME/.fabricrc``
-
-Path used when loading Fabric's local settings file.
-
-.. seealso:: :option:`--config <-c>`, :doc:`fab`
-
-.. _reject-unknown-hosts:
-
-``reject_unknown_hosts``
-------------------------
-
-**Default:** ``False``
-
-If ``True``, the SSH layer will raise an exception when connecting to hosts not
-listed in the user's known-hosts file.
-
-.. seealso:: :option:`--reject-unknown-hosts <-r>`, :doc:`ssh`
-
-.. _system-known-hosts:
-
-``system_known_hosts``
-------------------------
-
-**Default:** ``None``
-
-If set, should be the path to a :file:`known_hosts` file.  The SSH layer will
-read this file before reading the user's known-hosts file.
-
-.. seealso:: :doc:`ssh`
-
-.. _roledefs:
-
-``roledefs``
-------------
-
-**Default:** ``{}``
-
-Dictionary defining role name to host list mappings.
-
-.. seealso:: :doc:`execution`
-
-.. _roles:
-
-``roles``
----------
-
-**Default:** ``[]``
-
-The global role list used when composing per-task host lists.
-
-.. seealso:: :option:`--roles <-R>`, :doc:`execution`
-
-.. _shell:
-
-``shell``
----------
-
-**Default:** ``/bin/bash -l -c``
-
-Value used as shell wrapper when executing commands with e.g.
-`~fabric.operations.run`. Must be able to exist in the form ``<env.shell>
-"<command goes here>"`` -- e.g. the default uses Bash's ``-c`` option which
-takes a command string as its value.
-
-.. seealso:: :option:`--shell <-s>`,
-             :ref:`FAQ on bash as default shell <faq-bash>`, :doc:`execution`
-
-.. _skip-bad-hosts:
-
-``skip_bad_hosts``
-------------------
-
-**Default:** ``False``
-
-If ``True``, causes ``fab`` (or non-``fab`` use of `~fabric.tasks.execute`) to skip over hosts it can't connect to.
-
-.. versionadded:: 1.4
-.. seealso::
-    :option:`--skip-bad-hosts`, :ref:`excluding-hosts`, :doc:`execution`
-
-
-.. _skip-unknown-tasks:
-
-``skip_unknown_tasks``
-----------------------
-
-**Default:** ``False``
-
-If ``True``, causes ``fab`` (or non-``fab`` use of `~fabric.tasks.execute`)
-to skip over tasks not found, without aborting.
-
-.. seealso::
-    :option:`--skip-unknown-tasks`
-
-
-.. _ssh-config-path:
-
-``ssh_config_path``
--------------------
-
-**Default:** ``$HOME/.ssh/config``
-
-Allows specification of an alternate SSH configuration file path.
-
-.. versionadded:: 1.4
-.. seealso:: :option:`--ssh-config-path`, :ref:`ssh-config`
-
-``ok_ret_codes``
-------------------------
-
-**Default:** ``[0]``
-
-Return codes in this list are used to determine whether calls to
-`~fabric.operations.run`/`~fabric.operations.sudo`/`~fabric.operations.sudo`
-are considered successful.
-
-.. versionadded:: 1.6
-
-
-.. _sudo_password:
-
-``sudo_password``
------------------
-
-**Default:** ``None``
-
-The default password to submit to ``sudo`` password prompts. If empty or
-``None``, :ref:`env.password <password>` and/or :ref:`env.passwords
-<passwords>` is used as a fallback.
-
-.. seealso::
-    :ref:`password-management`, :option:`--sudo-password`,
-    :option:`--initial-sudo-password-prompt`
-.. versionadded:: 1.12
-
-.. _sudo_passwords:
-
-``sudo_passwords``
-------------------
-
-**Default:** ``{}``
-
-Identical to :ref:`passwords`, but used for sudo-only passwords.
-
-.. seealso:: :ref:`password-management`
-.. versionadded:: 1.12
-
-.. _sudo_prefix:
-
-``sudo_prefix``
----------------
-
-**Default:** ``"sudo -S -p '%(sudo_prompt)s' " % env``
-
-The actual ``sudo`` command prefixed onto `~fabric.operations.sudo` calls'
-command strings. Users who do not have ``sudo`` on their default remote
-``$PATH``, or who need to make other changes (such as removing the ``-p`` when
-passwordless sudo is in effect) may find changing this useful.
-
-.. seealso::
-
-    The `~fabric.operations.sudo` operation; :ref:`env.sudo_prompt
-    <sudo_prompt>`
-
-.. _sudo_prompt:
-
-``sudo_prompt``
----------------
-
-**Default:** ``"sudo password:"``
-
-Passed to the ``sudo`` program on remote systems so that Fabric may correctly
-identify its password prompt.
-
-.. seealso::
-
-    The `~fabric.operations.sudo` operation; :ref:`env.sudo_prefix
-    <sudo_prefix>`
-
-.. _sudo_user:
-
-``sudo_user``
--------------
-
-**Default:** ``None``
-
-Used as a fallback value for `~fabric.operations.sudo`'s ``user`` argument if
-none is given. Useful in combination with `~fabric.context_managers.settings`.
-
-.. seealso:: `~fabric.operations.sudo`
-
-.. _env-tasks:
-
-``tasks``
--------------
-
-**Default:** ``[]``
-
-Set by ``fab`` to the full tasks list to be executed for the currently
-executing command. For informational purposes only.
-
-.. seealso:: :doc:`execution`
-
-.. _timeout:
-
-``timeout``
------------
-
-**Default:** ``10``
-
-Network connection timeout, in seconds.
-
-.. versionadded:: 1.4
-.. seealso:: :option:`--timeout`, :ref:`connection-attempts`
-
-``use_shell``
--------------
-
-**Default:** ``True``
-
-Global setting which acts like the ``shell`` argument to
-`~fabric.operations.run`/`~fabric.operations.sudo`: if it is set to ``False``,
-operations will not wrap executed commands in ``env.shell``.
-
-
-.. _use-ssh-config:
-
-``use_ssh_config``
-------------------
-
-**Default:** ``False``
-
-Set to ``True`` to cause Fabric to load your local SSH config file.
-
-.. versionadded:: 1.4
-.. seealso:: :ref:`ssh-config`
-
-
-.. _user:
-
-``user``
---------
-
-**Default:** User's local username
-
-The username used by the SSH layer when connecting to remote hosts. May be set
-globally, and will be used when not otherwise explicitly set in host strings.
-However, when explicitly given in such a manner, this variable will be
-temporarily overwritten with the current value -- i.e. it will always display
-the user currently being connected as.
-
-To illustrate this, a fabfile::
-
-    from fabric.api import env, run
-
-    env.user = 'implicit_user'
-    env.hosts = ['host1', 'explicit_user@host2', 'host3']
-
-    def print_user():
-        with hide('running'):
-            run('echo "%(user)s"' % env)
-
-and its use::
-
-    $ fab print_user
-
-    [host1] out: implicit_user
-    [explicit_user@host2] out: explicit_user
-    [host3] out: implicit_user
-
-    Done.
-    Disconnecting from host1... done.
-    Disconnecting from host2... done.
-    Disconnecting from host3... done.
-
-As you can see, during execution on ``host2``, ``env.user`` was set to
-``"explicit_user"``, but was restored to its previous value
-(``"implicit_user"``) afterwards.
-
-.. note::
-
-    ``env.user`` is currently somewhat confusing (it's used for configuration
-    **and** informational purposes) so expect this to change in the future --
-    the informational aspect will likely be broken out into a separate env
-    variable.
-
-.. seealso:: :doc:`execution`, :option:`--user <-u>`
-
-``version``
------------
-
-**Default:** current Fabric version string
-
-Mostly for informational purposes. Modification is not recommended, but
-probably won't break anything either.
-
-.. seealso:: :option:`--version <-V>`
-
-.. _warn_only:
-
-``warn_only``
--------------
-
-**Default:** ``False``
-
-Specifies whether or not to warn, instead of abort, when
-`~fabric.operations` encounter error conditions.
-
-.. seealso:: :option:`--warn-only <-w>`, :doc:`execution`
diff -Nru fabric-1.14.0/sites/docs/usage/execution.rst fabric-2.5.0/sites/docs/usage/execution.rst
--- fabric-1.14.0/sites/docs/usage/execution.rst	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/sites/docs/usage/execution.rst	1970-01-01 01:00:00.000000000 +0100
@@ -1,905 +0,0 @@
-===============
-Execution model
-===============
-
-If you've read the :doc:`../tutorial`, you should already be familiar with how
-Fabric operates in the base case (a single task on a single host.) However, in
-many situations you'll find yourself wanting to execute multiple tasks and/or
-on multiple hosts. Perhaps you want to split a big task into smaller reusable
-parts, or crawl a collection of servers looking for an old user to remove. Such
-a scenario requires specific rules for when and how tasks are executed.
-
-This document explores Fabric's execution model, including the main execution
-loop, how to define host lists, how connections are made, and so forth.
-
-
-.. _execution-strategy:
-
-Execution strategy
-==================
-
-Fabric defaults to a single, serial execution method, though there is an
-alternative parallel mode available as of Fabric 1.3 (see
-:doc:`/usage/parallel`). This default behavior is as follows:
-
-* A list of tasks is created. Currently this list is simply the arguments given
-  to :doc:`fab <fab>`, preserving the order given.
-* For each task, a task-specific host list is generated from various
-  sources (see :ref:`host-lists` below for details.)
-* The task list is walked through in order, and each task is run once per host
-  in its host list.
-* Tasks with no hosts in their host list are considered local-only, and will
-  always run once and only once.
-
-Thus, given the following fabfile::
-
-    from fabric.api import run, env
-
-    env.hosts = ['host1', 'host2']
-
-    def taskA():
-        run('ls')
-
-    def taskB():
-        run('whoami')
-
-and the following invocation::
-
-    $ fab taskA taskB
-
-you will see that Fabric performs the following:
-
-* ``taskA`` executed on ``host1``
-* ``taskA`` executed on ``host2``
-* ``taskB`` executed on ``host1``
-* ``taskB`` executed on ``host2``
-
-While this approach is simplistic, it allows for a straightforward composition
-of task functions, and (unlike tools which push the multi-host functionality
-down to the individual function calls) enables shell script-like logic where
-you may introspect the output or return code of a given command and decide what
-to do next.
-
-
-Defining tasks
-==============
-
-For details on what constitutes a Fabric task and how to organize them, please see :doc:`/usage/tasks`.
-
-
-Defining host lists
-===================
-
-Unless you're using Fabric as a simple build system (which is possible, but not
-the primary use-case) having tasks won't do you any good without the ability to
-specify remote hosts on which to execute them. There are a number of ways to do
-so, with scopes varying from global to per-task, and it's possible mix and
-match as needed.
-
-.. _host-strings:
-
-Hosts
------
-
-Hosts, in this context, refer to what are also called "host strings": Python
-strings specifying a username, hostname and port combination, in the form of
-``username@hostname:port``. User and/or port (and the associated ``@`` or
-``:``) may be omitted, and will be filled by the executing user's local
-username, and/or port 22, respectively. Thus, ``admin@foo.com:222``,
-``deploy@website`` and ``nameserver1`` could all be valid host strings.
-
-IPv6 address notation is also supported, for example ``::1``, ``[::1]:1222``,
-``user@2001:db8::1`` or ``user@[2001:db8::1]:1222``. Square brackets
-are necessary only to separate the address from the port number. If no
-port number is used, the brackets are optional. Also if host string is
-specified via command-line argument, it may be necessary to escape
-brackets in some shells.
-
-.. note::
-    The user/hostname split occurs at the last ``@`` found, so e.g. email
-    address usernames are valid and will be parsed correctly.
-
-During execution, Fabric normalizes the host strings given and then stores each
-part (username/hostname/port) in the environment dictionary, for both its use
-and for tasks to reference if the need arises. See :doc:`env` for details.
-
-.. _execution-roles:
-
-Roles
------
-
-Host strings map to single hosts, but sometimes it's useful to arrange hosts in
-groups. Perhaps you have a number of Web servers behind a load balancer and
-want to update all of them, or want to run a task on "all client servers".
-Roles provide a way of defining strings which correspond to lists of host
-strings, and can then be specified instead of writing out the entire list every
-time.
-
-This mapping is defined as a dictionary, ``env.roledefs``, which must be
-modified by a fabfile in order to be used. A simple example::
-
-    from fabric.api import env
-
-    env.roledefs['webservers'] = ['www1', 'www2', 'www3']
-
-Since ``env.roledefs`` is naturally empty by default, you may also opt to
-re-assign to it without fear of losing any information (provided you aren't
-loading other fabfiles which also modify it, of course)::
-
-    from fabric.api import env
-
-    env.roledefs = {
-        'web': ['www1', 'www2', 'www3'],
-        'dns': ['ns1', 'ns2']
-    }
-
-Role definitions are not necessarily configuration of hosts only, they can
-also hold additional role specific settings of your choice. This is achieved
-by defining the roles as dicts and host strings under a ``hosts`` key::
-
-    from fabric.api import env
-
-    env.roledefs = {
-        'web': {
-            'hosts': ['www1', 'www2', 'www3'],
-            'foo': 'bar'
-        },
-        'dns': {
-            'hosts': ['ns1', 'ns2'],
-            'foo': 'baz'
-        }
-    }
-
-In addition to list/iterable object types, the values in ``env.roledefs``
-(or value of ``hosts`` key in dict style definition) may be callables, and will
-thus be called when looked up when tasks are run instead of at module load
-time. (For example, you could connect to remote servers to obtain role
-definitions, and not worry about causing delays at fabfile load time when
-calling e.g. ``fab --list``.)
-
-Use of roles is not required in any way -- it's simply a convenience in
-situations where you have common groupings of servers.
-
-.. versionchanged:: 0.9.2
-    Added ability to use callables as ``roledefs`` values.
-
-.. _host-lists:
-
-How host lists are constructed
-------------------------------
-
-There are a number of ways to specify host lists, either globally or per-task,
-and generally these methods override one another instead of merging together
-(though this may change in future releases.) Each such method is typically
-split into two parts, one for hosts and one for roles.
-
-Globally, via ``env``
-~~~~~~~~~~~~~~~~~~~~~
-
-The most common method of setting hosts or roles is by modifying two key-value
-pairs in the environment dictionary, :doc:`env <env>`: ``hosts`` and ``roles``.
-The value of these variables is checked at runtime, while constructing each
-tasks's host list.
-
-Thus, they may be set at module level, which will take effect when the fabfile
-is imported::
-
-    from fabric.api import env, run
-
-    env.hosts = ['host1', 'host2']
-
-    def mytask():
-        run('ls /var/www')
-
-Such a fabfile, run simply as ``fab mytask``, will run ``mytask`` on ``host1``
-followed by ``host2``.
-
-Since the env vars are checked for *each* task, this means that if you have the
-need, you can actually modify ``env`` in one task and it will affect all
-following tasks::
-
-    from fabric.api import env, run
-
-    def set_hosts():
-        env.hosts = ['host1', 'host2']
-
-    def mytask():
-        run('ls /var/www')
-
-When run as ``fab set_hosts mytask``, ``set_hosts`` is a "local" task -- its
-own host list is empty -- but ``mytask`` will again run on the two hosts given.
-
-.. note::
-
-    This technique used to be a common way of creating fake "roles", but is
-    less necessary now that roles are fully implemented. It may still be useful
-    in some situations, however.
-
-Alongside ``env.hosts`` is ``env.roles`` (not to be confused with
-``env.roledefs``!) which, if given, will be taken as a list of role names to
-look up in ``env.roledefs``.
-
-Globally, via the command line
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-In addition to modifying ``env.hosts``, ``env.roles``, and
-``env.exclude_hosts`` at the module level, you may define them by passing
-comma-separated string arguments to the command-line switches
-:option:`--hosts/-H <-H>` and :option:`--roles/-R <-R>`, e.g.::
-
-    $ fab -H host1,host2 mytask
-
-Such an invocation is directly equivalent to ``env.hosts = ['host1', 'host2']``
--- the argument parser knows to look for these arguments and will modify
-``env`` at parse time.
-
-.. note::
-
-    It's possible, and in fact common, to use these switches to set only a
-    single host or role. Fabric simply calls ``string.split(',')`` on the given
-    string, so a string with no commas turns into a single-item list.
-
-It is important to know that these command-line switches are interpreted
-**before** your fabfile is loaded: any reassignment to ``env.hosts`` or
-``env.roles`` in your fabfile will overwrite them.
-
-If you wish to nondestructively merge the command-line hosts with your
-fabfile-defined ones, make sure your fabfile uses ``env.hosts.extend()``
-instead::
-
-    from fabric.api import env, run
-
-    env.hosts.extend(['host3', 'host4'])
-
-    def mytask():
-        run('ls /var/www')
-
-When this fabfile is run as ``fab -H host1,host2 mytask``, ``env.hosts`` will
-then contain ``['host1', 'host2', 'host3', 'host4']`` at the time that
-``mytask`` is executed.
-
-.. note::
-
-    ``env.hosts`` is simply a Python list object -- so you may use
-    ``env.hosts.append()`` or any other such method you wish.
-
-.. _hosts-per-task-cli:
-
-Per-task, via the command line
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-Globally setting host lists only works if you want all your tasks to run on the
-same host list all the time. This isn't always true, so Fabric provides a few
-ways to be more granular and specify host lists which apply to a single task
-only. The first of these uses task arguments.
-
-As outlined in :doc:`fab`, it's possible to specify per-task arguments via a
-special command-line syntax. In addition to naming actual arguments to your
-task function, this may be used to set the ``host``, ``hosts``, ``role`` or
-``roles`` "arguments", which are interpreted by Fabric when building host lists
-(and removed from the arguments passed to the task itself.)
-
-.. note::
-
-    Since commas are already used to separate task arguments from one another,
-    semicolons must be used in the ``hosts`` or ``roles`` arguments to
-    delineate individual host strings or role names. Furthermore, the argument
-    must be quoted to prevent your shell from interpreting the semicolons.
-
-Take the below fabfile, which is the same one we've been using, but which
-doesn't define any host info at all::
-
-    from fabric.api import run
-
-    def mytask():
-        run('ls /var/www')
-
-To specify per-task hosts for ``mytask``, execute it like so::
-
-    $ fab mytask:hosts="host1;host2"
-
-This will override any other host list and ensure ``mytask`` always runs on
-just those two hosts.
-
-Per-task, via decorators
-~~~~~~~~~~~~~~~~~~~~~~~~
-
-If a given task should always run on a predetermined host list, you may wish to
-specify this in your fabfile itself. This can be done by decorating a task
-function with the `~fabric.decorators.hosts` or `~fabric.decorators.roles`
-decorators. These decorators take a variable argument list, like so::
-
-    from fabric.api import hosts, run
-
-    @hosts('host1', 'host2')
-    def mytask():
-        run('ls /var/www')
-
-They will also take an single iterable argument, e.g.::
-
-    my_hosts = ('host1', 'host2')
-    @hosts(my_hosts)
-    def mytask():
-        # ...
-
-When used, these decorators override any checks of ``env`` for that particular
-task's host list (though ``env`` is not modified in any way -- it is simply
-ignored.) Thus, even if the above fabfile had defined ``env.hosts`` or the call
-to :doc:`fab <fab>` uses :option:`--hosts/-H <-H>`, ``mytask`` would still run
-on a host list of ``['host1', 'host2']``.
-
-However, decorator host lists do **not** override per-task command-line
-arguments, as given in the previous section.
-
-Order of precedence
-~~~~~~~~~~~~~~~~~~~
-
-We've been pointing out which methods of setting host lists trump the others,
-as we've gone along. However, to make things clearer, here's a quick breakdown:
-
-* Per-task, command-line host lists (``fab mytask:host=host1``) override
-  absolutely everything else.
-* Per-task, decorator-specified host lists (``@hosts('host1')``) override the
-  ``env`` variables.
-* Globally specified host lists set in the fabfile (``env.hosts = ['host1']``)
-  *can* override such lists set on the command-line, but only if you're not
-  careful (or want them to.)
-* Globally specified host lists set on the command-line (``--hosts=host1``)
-  will initialize the ``env`` variables, but that's it.
-
-This logic may change slightly in the future to be more consistent (e.g.
-having :option:`--hosts <-H>` somehow take precedence over ``env.hosts`` in the
-same way that command-line per-task lists trump in-code ones) but only in a
-backwards-incompatible release.
-
-.. _combining-host-lists:
-
-Combining host lists
---------------------
-
-There is no "unionizing" of hosts between the various sources mentioned in
-:ref:`host-lists`. If ``env.hosts`` is set to ``['host1', 'host2', 'host3']``,
-and a per-function (e.g.  via `~fabric.decorators.hosts`) host list is set to
-just ``['host2', 'host3']``, that function will **not** execute on ``host1``,
-because the per-task decorator host list takes precedence.
-
-However, for each given source, if both roles **and** hosts are specified, they
-will be merged together into a single host list. Take, for example, this
-fabfile where both of the decorators are used::
-
-    from fabric.api import env, hosts, roles, run
-
-    env.roledefs = {'role1': ['b', 'c']}
-
-    @hosts('a', 'b')
-    @roles('role1')
-    def mytask():
-        run('ls /var/www')
-
-Assuming no command-line hosts or roles are given when ``mytask`` is executed,
-this fabfile will call ``mytask`` on a host list of ``['a', 'b', 'c']`` -- the
-union of ``role1`` and the contents of the `~fabric.decorators.hosts` call.
-
-
-.. _deduplication:
-
-Host list deduplication
------------------------
-
-By default, to support :ref:`combining-host-lists`, Fabric deduplicates the
-final host list so any given host string is only present once. However, this
-prevents explicit/intentional running of a task multiple times on the same
-target host, which is sometimes useful.
-
-To turn off deduplication, set :ref:`env.dedupe_hosts <dedupe_hosts>` to
-``False``.
-
-
-.. _excluding-hosts:
-
-Excluding specific hosts
-------------------------
-
-At times, it is useful to exclude one or more specific hosts, e.g. to override
-a few bad or otherwise undesirable hosts which are pulled in from a role or an
-autogenerated host list.
-
-.. note::
-    As of Fabric 1.4, you may wish to use :ref:`skip-bad-hosts` instead, which
-    automatically skips over any unreachable hosts.
-
-Host exclusion may be accomplished globally with :option:`--exclude-hosts/-x
-<-x>`::
-
-    $ fab -R myrole -x host2,host5 mytask
-
-If ``myrole`` was defined as ``['host1', 'host2', ..., 'host15']``, the above
-invocation would run with an effective host list of ``['host1', 'host3',
-'host4', 'host6', ..., 'host15']``.
-
-    .. note::
-        Using this option does not modify ``env.hosts`` -- it only causes the
-        main execution loop to skip the requested hosts.
-
-Exclusions may be specified per-task by using an extra ``exclude_hosts`` kwarg,
-which is implemented similarly to the abovementioned ``hosts`` and ``roles``
-per-task kwargs, in that it is stripped from the actual task invocation. This
-example would have the same result as the global exclude above::
-
-    $ fab mytask:roles=myrole,exclude_hosts="host2;host5"
-
-Note that the host list is semicolon-separated, just as with the ``hosts``
-per-task argument.
-
-Combining exclusions
-~~~~~~~~~~~~~~~~~~~~
-
-Host exclusion lists, like host lists themselves, are not merged together
-across the different "levels" they can be declared in. For example, a global
-``-x`` option will not affect a per-task host list set with a decorator or
-keyword argument, nor will per-task ``exclude_hosts`` keyword arguments affect
-a global ``-H`` list.
-
-There is one minor exception to this rule, namely that CLI-level keyword
-arguments (``mytask:exclude_hosts=x,y``) **will** be taken into account when
-examining host lists set via ``@hosts`` or ``@roles``. Thus a task function
-decorated with ``@hosts('host1', 'host2')`` executed as ``fab
-taskname:exclude_hosts=host2`` will only run on ``host1``.
-
-As with the host list merging, this functionality is currently limited (partly
-to keep the implementation simple) and may be expanded in future releases.
-
-
-.. _execute:
-
-Intelligently executing tasks with ``execute``
-==============================================
-
-.. versionadded:: 1.3
-
-Most of the information here involves "top level" tasks executed via :doc:`fab
-<fab>`, such as the first example where we called ``fab taskA taskB``.
-However, it's often convenient to wrap up multi-task invocations like this into
-their own, "meta" tasks.
-
-Prior to Fabric 1.3, this had to be done by hand, as outlined in
-:doc:`/usage/library`. Fabric's design eschews magical behavior, so simply
-*calling* a task function does **not** take into account decorators such as
-`~fabric.decorators.roles`.
-
-New in Fabric 1.3 is the `~fabric.tasks.execute` helper function, which takes a
-task object or name as its first argument. Using it is effectively the same as
-calling the given task from the command line: all the rules given above in
-:ref:`host-lists` apply. (The ``hosts`` and ``roles`` keyword arguments to
-`~fabric.tasks.execute` are analogous to :ref:`CLI per-task arguments
-<hosts-per-task-cli>`, including how they override all other host/role-setting
-methods.)
-
-As an example, here's a fabfile defining two stand-alone tasks for deploying a
-Web application::
-
-    from fabric.api import run, roles
-
-    env.roledefs = {
-        'db': ['db1', 'db2'],
-        'web': ['web1', 'web2', 'web3'],
-    }
-
-    @roles('db')
-    def migrate():
-        # Database stuff here.
-        pass
-
-    @roles('web')
-    def update():
-        # Code updates here.
-        pass
-
-In Fabric <=1.2, the only way to ensure that ``migrate`` runs on the DB servers
-and that ``update`` runs on the Web servers (short of manual
-``env.host_string`` manipulation) was to call both as top level tasks::
-
-    $ fab migrate update
-
-Fabric >=1.3 can use `~fabric.tasks.execute` to set up a meta-task. Update the
-``import`` line like so::
-
-    from fabric.api import run, roles, execute
-
-and append this to the bottom of the file::
-
-    def deploy():
-        execute(migrate)
-        execute(update)
-
-That's all there is to it; the `~fabric.decorators.roles` decorators will be honored as expected, resulting in the following execution sequence:
-
-* `migrate` on `db1`
-* `migrate` on `db2`
-* `update` on `web1`
-* `update` on `web2`
-* `update` on `web3`
-
-.. warning::
-    This technique works because tasks that themselves have no host list (this
-    includes the global host list settings) only run one time. If used inside a
-    "regular" task that is going to run on multiple hosts, calls to
-    `~fabric.tasks.execute` will also run multiple times, resulting in
-    multiplicative numbers of subtask calls -- be careful!
-
-    If you would like your `execute` calls to only be called once, you
-    may use the `~fabric.decorators.runs_once` decorator.
-
-.. seealso:: `~fabric.tasks.execute`, `~fabric.decorators.runs_once`
-
-
-.. _leveraging-execute-return-value:
-
-Leveraging ``execute`` to access multi-host results
----------------------------------------------------
-
-In nontrivial Fabric runs, especially parallel ones, you may want to gather up
-a bunch of per-host result values at the end - e.g. to present a summary table,
-perform calculations, etc.
-
-It's not possible to do this in Fabric's default "naive" mode (one where you
-rely on Fabric looping over host lists on your behalf), but with `.execute`
-it's pretty easy. Simply switch from calling the actual work-bearing task, to
-calling a "meta" task which takes control of execution with `.execute`::
-
-    from fabric.api import task, execute, run, runs_once
-
-    @task
-    def workhorse():
-        return run("get my infos")
-
-    @task
-    @runs_once
-    def go():
-        results = execute(workhorse)
-        print results
-
-In the above, ``workhorse`` can do any Fabric stuff at all -- it's literally
-your old "naive" task -- except that it needs to return something useful.
-
-``go`` is your new entry point (to be invoked as ``fab go``, or whatnot) and
-its job is to take the ``results`` dictionary from the `.execute` call and do
-whatever you need with it. Check the API docs for details on the structure of
-that return value.
-
-
-.. _dynamic-hosts:
-
-Using ``execute`` with dynamically-set host lists
--------------------------------------------------
-
-A common intermediate-to-advanced use case for Fabric is to parameterize lookup
-of one's target host list at runtime (when use of :ref:`execution-roles` does not
-suffice). ``execute`` can make this extremely simple, like so::
-
-    from fabric.api import run, execute, task
-
-    # For example, code talking to an HTTP API, or a database, or ...
-    from mylib import external_datastore
-
-    # This is the actual algorithm involved. It does not care about host
-    # lists at all.
-    def do_work():
-        run("something interesting on a host")
-
-    # This is the user-facing task invoked on the command line.
-    @task
-    def deploy(lookup_param):
-        # This is the magic you don't get with @hosts or @roles.
-        # Even lazy-loading roles require you to declare available roles
-        # beforehand. Here, the sky is the limit.
-        host_list = external_datastore.query(lookup_param)
-        # Put this dynamically generated host list together with the work to be
-        # done.
-        execute(do_work, hosts=host_list)
-    
-For example, if ``external_datastore`` was a simplistic "look up hosts by tag
-in a database" service, and you wanted to run a task on all hosts tagged as
-being related to your application stack, you might call the above like this::
-
-    $ fab deploy:app
-
-But wait! A data migration has gone awry on the DB servers. Let's fix up our
-migration code in our source repo, and deploy just the DB boxes again::
-
-    $ fab deploy:db
-
-This use case looks similar to Fabric's roles, but has much more potential, and
-is by no means limited to a single argument. Define the task however you wish,
-query your external data store in whatever way you need -- it's just Python.
-
-The alternate approach
-~~~~~~~~~~~~~~~~~~~~~~
-
-Similar to the above, but using ``fab``'s ability to call multiple tasks in
-succession instead of an explicit ``execute`` call, is to mutate
-:ref:`env.hosts <hosts>` in a host-list lookup task and then call ``do_work``
-in the same session::
-
-    from fabric.api import run, task
-
-    from mylib import external_datastore
-
-    # Marked as a publicly visible task, but otherwise unchanged: still just
-    # "do the work, let somebody else worry about what hosts to run on".
-    @task
-    def do_work():
-        run("something interesting on a host")
-
-    @task
-    def set_hosts(lookup_param):
-        # Update env.hosts instead of calling execute()
-        env.hosts = external_datastore.query(lookup_param)
-
-Then invoke like so::
-
-    $ fab set_hosts:app do_work
-
-One benefit of this approach over the previous one is that you can replace
-``do_work`` with any other "workhorse" task::
-
-    $ fab set_hosts:db snapshot
-    $ fab set_hosts:cassandra,cluster2 repair_ring
-    $ fab set_hosts:redis,environ=prod status
-
-
-.. _failures:
-
-Failure handling
-================
-
-Once the task list has been constructed, Fabric will start executing them as
-outlined in :ref:`execution-strategy`, until all tasks have been run on the
-entirety of their host lists. However, Fabric defaults to a "fail-fast"
-behavior pattern: if anything goes wrong, such as a remote program returning a
-nonzero return value or your fabfile's Python code encountering an exception,
-execution will halt immediately.
-
-This is typically the desired behavior, but there are many exceptions to the
-rule, so Fabric provides ``env.warn_only``, a Boolean setting. It defaults to
-``False``, meaning an error condition will result in the program aborting
-immediately. However, if ``env.warn_only`` is set to ``True`` at the time of
-failure -- with, say, the `~fabric.context_managers.settings` context
-manager -- Fabric will emit a warning message but continue executing.
-
-To signal a failure error from a Fabric task, use the `~fabric.utils.abort`.
-`~fabric.utils.abort` signals an error as if it had been detected by Fabric and
-follows the regular execution model for control flow.
-
-
-.. _connections:
-
-Connections
-===========
-
-``fab`` itself doesn't actually make any connections to remote hosts. Instead,
-it simply ensures that for each distinct run of a task on one of its hosts, the
-env var ``env.host_string`` is set to the right value. Users wanting to
-leverage Fabric as a library may do so manually to achieve similar effects
-(though as of Fabric 1.3, using `~fabric.tasks.execute` is preferred and more
-powerful.)
-
-``env.host_string`` is (as the name implies) the "current" host string, and is
-what Fabric uses to determine what connections to make (or re-use) when
-network-aware functions are run. Operations like `~fabric.operations.run` or
-`~fabric.operations.put` use ``env.host_string`` as a lookup key in a shared
-dictionary which maps host strings to SSH connection objects.
-
-.. note::
-
-    The connections dictionary (currently located at
-    ``fabric.state.connections``) acts as a cache, opting to return previously
-    created connections if possible in order to save some overhead, and
-    creating new ones otherwise.
-
-Lazy connections
-----------------
-
-Because connections are driven by the individual operations, Fabric will not
-actually make connections until they're necessary. Take for example this task
-which does some local housekeeping prior to interacting with the remote
-server::
-
-    from fabric.api import *
-
-    @hosts('host1')
-    def clean_and_upload():
-        local('find assets/ -name "*.DS_Store" -exec rm '{}' \;')
-        local('tar czf /tmp/assets.tgz assets/')
-        put('/tmp/assets.tgz', '/tmp/assets.tgz')
-        with cd('/var/www/myapp/'):
-            run('tar xzf /tmp/assets.tgz')
-
-What happens, connection-wise, is as follows:
-
-#. The two `~fabric.operations.local` calls will run without making any network
-   connections whatsoever;
-#. `~fabric.operations.put` asks the connection cache for a connection to
-   ``host1``;
-#. The connection cache fails to find an existing connection for that host
-   string, and so creates a new SSH connection, returning it to
-   `~fabric.operations.put`;
-#. `~fabric.operations.put` uploads the file through that connection;
-#. Finally, the `~fabric.operations.run` call asks the cache for a connection
-   to that same host string, and is given the existing, cached connection for
-   its own use.
-
-Extrapolating from this, you can also see that tasks which don't use any
-network-borne operations will never actually initiate any connections (though
-they will still be run once for each host in their host list, if any.)
-
-Closing connections
--------------------
-
-Fabric's connection cache never closes connections itself -- it leaves this up
-to whatever is using it. The :doc:`fab <fab>` tool does this bookkeeping for
-you: it iterates over all open connections and closes them just before it exits
-(regardless of whether the tasks failed or not.)
-
-Library users will need to ensure they explicitly close all open connections
-before their program exits. This can be accomplished by calling
-`~fabric.network.disconnect_all` at the end of your script.
-
-.. note::
-    `~fabric.network.disconnect_all` may be moved to a more public location in
-    the future; we're still working on making the library aspects of Fabric
-    more solidified and organized.
-
-Multiple connection attempts and skipping bad hosts
----------------------------------------------------
-
-As of Fabric 1.4, multiple attempts may be made to connect to remote servers
-before aborting with an error: Fabric will try connecting
-:ref:`env.connection_attempts <connection-attempts>` times before giving up,
-with a timeout of :ref:`env.timeout <timeout>` seconds each time. (These
-currently default to 1 try and 10 seconds, to match previous behavior, but they
-may be safely changed to whatever you need.)
-
-Furthermore, even total failure to connect to a server is no longer an absolute
-hard stop: set :ref:`env.skip_bad_hosts <skip-bad-hosts>` to ``True`` and in
-most situations (typically initial connections) Fabric will simply warn and
-continue, instead of aborting.
-
-.. versionadded:: 1.4
-
-.. _password-management:
-
-Password management
-===================
-
-Fabric maintains an in-memory password cache of your login and sudo passwords
-in certain situations; this helps avoid tedious re-entry when multiple systems
-share the same password [#]_, or if a remote system's ``sudo`` configuration
-doesn't do its own caching.
-
-Pre-filling the password caches
--------------------------------
-
-The first layer is a simple default or fallback password value,
-:ref:`env.password <password>` (which may also be set at the command line via
-:option:`--password <-p>` or :option:`--initial-password-prompt <-I>`). This
-env var stores a single password which (if non-empty) will be tried in the
-event that the host-specific cache (see below) has no entry for the current
-:ref:`host string <host_string>`.
-
-:ref:`env.passwords <passwords>` (plural!) serves as a per-user/per-host cache,
-storing the most recently entered password for every unique user/host/port
-combination (**note** that you must include **all three values** if modifying
-the structure by hand - see the above link for details). Due to this cache,
-connections to multiple different users and/or hosts in the same session will
-only require a single password entry for each. (Previous versions of Fabric
-used only the single, default password cache and thus required password
-re-entry every time the previously entered password became invalid.)
-
-Auto-filling/updating from user input
--------------------------------------
-
-Depending on your configuration and the number of hosts your session will
-connect to, you may find setting either or both of the above env vars to be
-useful. However, Fabric will automatically fill them in as necessary without
-any additional configuration.
-
-Specifically, each time a password prompt is presented to the user, the value
-entered is used to update both the single default password cache, and the cache
-value for the current value of ``env.host_string``.
-
-.. _sudo-passwords:
-
-Specifying ``sudo``-only passwords
-----------------------------------
-
-In some situations (such as those involving two-factor authentication, or any
-other situation where submitting a password at login time is not desired or
-correct) you may want to only cache passwords intended for ``sudo``, instead of
-reusing the values for both login and ``sudo`` purposes.
-
-To do this, you may set :ref:`env.sudo_password <sudo_password>` or populate
-:ref:`env.sudo_passwords <sudo_passwords>`, which mirror ``env.password`` and
-``env.passwords`` (described above). These values will **only** be used in
-responding to ``sudo`` password prompts, and will never be submitted at
-connection time.
-
-There is also an analogue to the ``--password`` command line flag, named
-:option:`--sudo-password`, and like :option:`--initial-password-prompt <-I>`,
-there exists :option:`--initial-sudo-password-prompt`.
-
-.. note::
-    When both types of passwords are filled in (e.g. if ``env.password =
-    "foo"`` and ``env.sudo_password = "bar"``), the ``sudo`` specific passwords
-    will be used.
-
-.. note::
-    Due to backwards compatibility concerns, user-entered ``sudo`` passwords
-    will still be cached into ``env.password``/``env.passwords``;
-    ``env.sudo_password``/``env.sudo_passwords`` are purely for noninteractive
-    use.
-
-.. [#] We highly recommend the use of SSH `key-based access
-    <http://en.wikipedia.org/wiki/Public_key>`_ instead of relying on
-    homogeneous password setups, as it's significantly more secure.
-
-
-.. _ssh-config:
-
-Leveraging native SSH config files
-==================================
-
-Command-line SSH clients (such as the one provided by `OpenSSH
-<http://openssh.org>`_) make use of a specific configuration format typically
-known as ``ssh_config``, and will read from a file in the platform-specific
-location ``$HOME/.ssh/config`` (or an arbitrary path given to
-:option:`--ssh-config-path`/:ref:`env.ssh_config_path <ssh-config-path>`.) This
-file allows specification of various SSH options such as default or per-host
-usernames, hostname aliases, and toggling other settings (such as whether to
-use :ref:`agent forwarding <forward-agent>`.)
-
-Fabric's SSH implementation allows loading a subset of these options from one's
-actual SSH config file, should it exist. This behavior is not enabled by
-default (in order to be backwards compatible) but may be turned on by setting
-:ref:`env.use_ssh_config <use-ssh-config>` to ``True`` at the top of your
-fabfile.
-
-If enabled, the following SSH config directives will be loaded and honored by Fabric:
-
-* ``User`` and ``Port`` will be used to fill in the appropriate connection
-  parameters when not otherwise specified, in the following fashion:
-
-  * Globally specified ``User``/``Port`` will be used in place of the current
-    defaults (local username and 22, respectively) if the appropriate env vars
-    are not set.
-  * However, if :ref:`env.user <user>`/:ref:`env.port <port>` *are* set, they
-    override global ``User``/``Port`` values.
-  * User/port values in the host string itself (e.g. ``hostname:222``) will
-    override everything, including any ``ssh_config`` values.
-* ``HostName`` can be used to replace the given hostname, just like with
-  regular ``ssh``. So a ``Host foo`` entry specifying ``HostName example.com``
-  will allow you to give Fabric the hostname ``'foo'`` and have that expanded
-  into ``'example.com'`` at connection time.
-* ``IdentityFile`` will extend (not replace) :ref:`env.key_filename
-  <key-filename>`.
-* ``ForwardAgent`` will augment :ref:`env.forward_agent <forward-agent>` in an
-  "OR" manner: if either is set to a positive value, agent forwarding will be
-  enabled.
-* ``ProxyCommand`` will trigger use of a proxy command for host connections,
-  just as with regular ``ssh``.
-
-  .. note::
-    If all you want to do is bounce SSH traffic off a gateway, you may find
-    :ref:`env.gateway <gateway>` to be a more efficient connection method
-    (which will also honor more Fabric-level settings) than the typical ``ssh
-    gatewayhost nc %h %p`` method of using ``ProxyCommand`` as a gateway.
-
-  .. note::
-    If your SSH config file contains ``ProxyCommand`` directives *and* you have
-    set :ref:`env.gateway <gateway>` to a non-``None`` value, ``env.gateway``
-    will take precedence and the ``ProxyCommand`` will be ignored.
-
-    If one has a pre-created SSH config file, rationale states it will be
-    easier for you to modify ``env.gateway`` (e.g. via
-    `~fabric.context_managers.settings`) than to work around your conf file's
-    contents entirely.
diff -Nru fabric-1.14.0/sites/docs/usage/fabfiles.rst fabric-2.5.0/sites/docs/usage/fabfiles.rst
--- fabric-1.14.0/sites/docs/usage/fabfiles.rst	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/sites/docs/usage/fabfiles.rst	1970-01-01 01:00:00.000000000 +0100
@@ -1,92 +0,0 @@
-============================
-Fabfile construction and use
-============================
-
-This document contains miscellaneous sections about fabfiles, both how to best
-write them, and how to use them once written.
-
-.. _fabfile-discovery:
-
-Fabfile discovery
-=================
-
-Fabric is capable of loading Python modules (e.g. ``fabfile.py``) or packages
-(e.g. a ``fabfile/`` directory containing an ``__init__.py``). By default, it
-looks for something named (to Python's import machinery) ``fabfile`` - so
-either ``fabfile/`` or ``fabfile.py``.
-
-The fabfile discovery algorithm searches in the invoking user's current working
-directory or any parent directories. Thus, it is oriented around "project" use,
-where one keeps e.g. a ``fabfile.py`` at the root of a source code tree. Such a
-fabfile will then be discovered no matter where in the tree the user invokes
-``fab``.
-
-The specific name to be searched for may be overridden on the command-line with
-the :option:`-f` option, or by adding a :ref:`fabricrc <fabricrc>` line which
-sets the value of ``fabfile``. For example, if you wanted to name your fabfile
-``fab_tasks.py``, you could create such a file and then call ``fab -f
-fab_tasks.py <task name>``, or add ``fabfile = fab_tasks.py`` to
-``~/.fabricrc``.
-
-If the given fabfile name contains path elements other than a filename (e.g.
-``../fabfile.py`` or ``/dir1/dir2/custom_fabfile``) it will be treated as a
-file path and directly checked for existence without any sort of searching.
-When in this mode, tilde-expansion will be applied, so one may refer to e.g.
-``~/personal_fabfile.py``.
-
-.. note::
-
-    Fabric does a normal ``import`` (actually an ``__import__``) of your
-    fabfile in order to access its contents -- it does not do any ``eval``-ing
-    or similar. In order for this to work, Fabric temporarily adds the found
-    fabfile's containing folder to the Python load path (and removes it
-    immediately afterwards.)
-
-.. versionchanged:: 0.9.2
-    The ability to load package fabfiles.
-
-
-.. _importing-the-api:
-
-Importing Fabric
-================
-
-Because Fabric is just Python, you *can* import its components any way you
-want. However, for the purposes of encapsulation and convenience (and to make
-life easier for Fabric's packaging script) Fabric's public API is maintained in
-the ``fabric.api`` module.
-
-All of Fabric's :doc:`../api/core/operations`,
-:doc:`../api/core/context_managers`, :doc:`../api/core/decorators` and
-:doc:`../api/core/utils` are included in this module as a single, flat
-namespace. This enables a very simple and consistent interface to Fabric within
-your fabfiles::
-
-    from fabric.api import *
-
-    # call run(), sudo(), etc etc
-
-This is not technically best practices (for `a
-number of reasons`_) and if you're only using a couple of
-Fab API calls, it *is* probably a good idea to explicitly ``from fabric.api
-import env, run`` or similar. However, in most nontrivial fabfiles, you'll be
-using all or most of the API, and the star import::
-
-    from fabric.api import *
-
-will be a lot easier to write and read than::
-
-    from fabric.api import abort, cd, env, get, hide, hosts, local, prompt, \
-        put, require, roles, run, runs_once, settings, show, sudo, warn
-
-so in this case we feel pragmatism overrides best practices.
-
-.. _a number of reasons: http://python.net/~goodger/projects/pycon/2007/idiomatic/handout.html#importing
-
-
-Defining tasks and importing callables
-======================================
-
-For important information on what exactly Fabric will consider as a task when
-it loads your fabfile, as well as notes on how best to import other code,
-please see :doc:`/usage/tasks` in the :doc:`execution` documentation.
diff -Nru fabric-1.14.0/sites/docs/usage/fab.rst fabric-2.5.0/sites/docs/usage/fab.rst
--- fabric-1.14.0/sites/docs/usage/fab.rst	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/sites/docs/usage/fab.rst	1970-01-01 01:00:00.000000000 +0100
@@ -1,542 +0,0 @@
-=============================
-``fab`` options and arguments
-=============================
-
-The most common method for utilizing Fabric is via its command-line tool,
-``fab``, which should have been placed on your shell's executable path when
-Fabric was installed. ``fab`` tries hard to be a good Unix citizen, using a
-standard style of command-line switches, help output, and so forth.
-
-
-Basic use
-=========
-
-In its most simple form, ``fab`` may be called with no options at all, and
-with one or more arguments, which should be task names, e.g.::
-
-    $ fab task1 task2
-
-As detailed in :doc:`../tutorial` and :doc:`execution`, this will run ``task1``
-followed by ``task2``, assuming that Fabric was able to find a fabfile nearby
-containing Python functions with those names.
-
-However, it's possible to expand this simple usage into something more
-flexible, by using the provided options and/or passing arguments to individual
-tasks.
-
-
-.. _arbitrary-commands:
-
-Arbitrary remote shell commands
-===============================
-
-.. versionadded:: 0.9.2
-
-Fabric leverages a lesser-known command line convention and may be called in
-the following manner::
-
-    $ fab [options] -- [shell command]
-
-where everything after the ``--`` is turned into a temporary
-`~fabric.operations.run` call, and is not parsed for ``fab`` options. If you've
-defined a host list at the module level or on the command line, this usage will
-act like a one-line anonymous task.
-
-For example, let's say you just wanted to get the kernel info for a bunch of
-systems; you could do this::
-
-    $ fab -H system1,system2,system3 -- uname -a
-
-which would be literally equivalent to the following fabfile::
-
-    from fabric.api import run
-
-    def anonymous():
-        run("uname -a")
-
-as if it were executed thusly::
-
-    $ fab -H system1,system2,system3 anonymous
-
-Most of the time you will want to just write out the task in your fabfile
-(anything you use once, you're likely to use again) but this feature provides a
-handy, fast way to quickly dash off an SSH-borne command while leveraging your
-fabfile's connection settings.
-
-
-.. _command-line-options:
-
-Command-line options
-====================
-
-A quick overview of all possible command line options can be found via ``fab
---help``. If you're looking for details on a specific option, we go into detail
-below.
-
-.. note::
-
-    ``fab`` uses Python's `optparse`_ library, meaning that it honors typical
-    Linux or GNU style short and long options, as well as freely mixing options
-    and arguments. E.g. ``fab task1 -H hostname task2 -i path/to/keyfile`` is
-    just as valid as the more straightforward ``fab -H hostname -i
-    path/to/keyfile task1 task2``.
-
-.. _optparse: http://docs.python.org/library/optparse.html
-
-.. cmdoption:: -a, --no_agent
-
-    Sets :ref:`env.no_agent <no_agent>` to ``True``, forcing our SSH layer not
-    to talk to the SSH agent when trying to unlock private key files.
-
-    .. versionadded:: 0.9.1
-
-.. cmdoption:: -A, --forward-agent
-
-    Sets :ref:`env.forward_agent <forward-agent>` to ``True``, enabling agent
-    forwarding.
-
-    .. versionadded:: 1.4
-
-.. cmdoption:: --abort-on-prompts
-
-    Sets :ref:`env.abort_on_prompts <abort-on-prompts>` to ``True``, forcing
-    Fabric to abort whenever it would prompt for input.
-
-    .. versionadded:: 1.1
-
-.. cmdoption:: -c RCFILE, --config=RCFILE
-
-    Sets :ref:`env.rcfile <rcfile>` to the given file path, which Fabric will
-    try to load on startup and use to update environment variables.
-
-.. cmdoption:: -d COMMAND, --display=COMMAND
-
-    Prints the entire docstring for the given task, if there is one. Does not
-    currently print out the task's function signature, so descriptive
-    docstrings are a good idea. (They're *always* a good idea, of course --
-    just moreso here.)
-
-.. cmdoption:: --connection-attempts=M, -n M
-
-    Set number of times to attempt connections. Sets
-    :ref:`env.connection_attempts <connection-attempts>`.
-
-    .. seealso::
-        :ref:`env.connection_attempts <connection-attempts>`,
-        :ref:`env.timeout <timeout>`
-    .. versionadded:: 1.4
-
-.. cmdoption:: -D, --disable-known-hosts
-
-    Sets :ref:`env.disable_known_hosts <disable-known-hosts>` to ``True``,
-    preventing Fabric from loading the user's SSH :file:`known_hosts` file.
-
-.. cmdoption:: -f FABFILE, --fabfile=FABFILE
-
-    The fabfile name pattern to search for (defaults to ``fabfile.py``), or
-    alternately an explicit file path to load as the fabfile (e.g.
-    ``/path/to/my/fabfile.py``.)
-
-    .. seealso:: :doc:`fabfiles`
-
-.. cmdoption:: -F LIST_FORMAT, --list-format=LIST_FORMAT
-
-    Allows control over the output format of :option:`--list <-l>`. ``short`` is
-    equivalent to :option:`--shortlist`, ``normal`` is the same as simply
-    omitting this option entirely (i.e. the default), and ``nested`` prints out
-    a nested namespace tree.
-
-    .. versionadded:: 1.1
-    .. seealso:: :option:`--shortlist`, :option:`--list <-l>`
-
-.. cmdoption:: -g HOST, --gateway=HOST
-
-    Sets :ref:`env.gateway <gateway>` to ``HOST`` host string.
-
-    .. versionadded:: 1.5
-
-.. cmdoption:: --gss-auth
-
-    Toggles use of GSS-API authentication.
-
-    .. seealso:: :ref:`kerberos`
-    .. versionadded:: 1.11
-
-.. cmdoption:: --gss-deleg
-
-    Toggles whether GSS-API client credentials are delegated.
-
-    .. seealso:: :ref:`kerberos`
-    .. versionadded:: 1.11
-
-.. cmdoption:: --gss-kex
-
-    Toggles whether GSS-API key exchange is used.
-
-    .. seealso:: :ref:`kerberos`
-    .. versionadded:: 1.11
-
-.. cmdoption:: -h, --help
-
-    Displays a standard help message, with all possible options and a brief
-    overview of what they do, then exits.
-
-.. cmdoption:: --hide=LEVELS
-
-    A comma-separated list of :doc:`output levels <output_controls>` to hide by
-    default.
-
-
-.. cmdoption:: -H HOSTS, --hosts=HOSTS
-
-    Sets :ref:`env.hosts <hosts>` to the given comma-delimited list of host
-    strings.
-
-.. cmdoption:: -x HOSTS, --exclude-hosts=HOSTS
-
-    Sets :ref:`env.exclude_hosts <exclude-hosts>` to the given comma-delimited
-    list of host strings to then keep out of the final host list.
-
-    .. versionadded:: 1.1
-
-.. cmdoption:: -i KEY_FILENAME
-
-    When set to a file path, will load the given file as an SSH identity file
-    (usually a private key.) This option may be repeated multiple times. Sets
-    (or appends to) :ref:`env.key_filename <key-filename>`.
-
-.. cmdoption:: -I, --initial-password-prompt
-
-    Forces a password prompt at the start of the session (after fabfile load
-    and option parsing, but before executing any tasks) in order to pre-fill
-    :ref:`env.password <password>`.
-
-    This is useful for fire-and-forget runs (especially parallel sessions, in
-    which runtime input is not possible) when setting the password via
-    :option:`--password <-p>` or by setting :ref:`env.password <password>` in
-    your fabfile, is undesirable.
-
-    .. note:: The value entered into this prompt will *overwrite* anything
-      supplied via :ref:`env.password <password>` at module level, or via
-      :option:`--password <-p>`.
-
-    .. seealso:: :ref:`password-management`
-
-.. cmdoption:: --initial-sudo-password-prompt
-
-    Like :option:`--initial-password-prompt <-I>`, but for prefilling
-    :ref:`sudo_password` instead of :ref:`password`.
-
-    .. versionadded:: 1.12
-
-.. cmdoption:: -k
-
-    Sets :ref:`env.no_keys <no_keys>` to ``True``, forcing the SSH layer to not
-    look for SSH private key files in one's home directory.
-
-    .. versionadded:: 0.9.1
-
-.. cmdoption:: --keepalive=KEEPALIVE
-
-    Sets :ref:`env.keepalive <keepalive>` to the given (integer) value, specifying an SSH keepalive interval.
-
-    .. versionadded:: 1.1
-
-.. cmdoption:: --linewise
-
-    Forces output to be buffered line-by-line instead of byte-by-byte. Often useful or required for :ref:`parallel execution <linewise-output>`.
-
-    .. versionadded:: 1.3
-
-.. cmdoption:: -l, --list
-
-    Imports a fabfile as normal, but then prints a list of all discovered tasks
-    and exits. Will also print the first line of each task's docstring, if it
-    has one, next to it (truncating if necessary.)
-
-    .. versionchanged:: 0.9.1
-        Added docstring to output.
-    .. seealso:: :option:`--shortlist`, :option:`--list-format <-F>`
-
-.. cmdoption:: -p PASSWORD, --password=PASSWORD
-
-    Sets :ref:`env.password <password>` to the given string; it will then be
-    used as the default password when making SSH connections or calling the
-    ``sudo`` program.
-
-    .. seealso::
-        :option:`--initial-password-prompt <-I>`,
-        :option:`--sudo-password`
-
-.. cmdoption:: -P, --parallel
-
-    Sets :ref:`env.parallel <env-parallel>` to ``True``, causing
-    tasks to run in parallel.
-
-    .. versionadded:: 1.3
-    .. seealso:: :doc:`/usage/parallel`
-
-.. cmdoption:: --no-pty
-
-    Sets :ref:`env.always_use_pty <always-use-pty>` to ``False``, causing all
-    `~fabric.operations.run`/`~fabric.operations.sudo` calls to behave as if
-    one had specified ``pty=False``.
-
-    .. versionadded:: 1.0
-
-.. cmdoption:: -r, --reject-unknown-hosts
-
-    Sets :ref:`env.reject_unknown_hosts <reject-unknown-hosts>` to ``True``,
-    causing Fabric to abort when connecting to hosts not found in the user's SSH
-    :file:`known_hosts` file.
-
-.. cmdoption:: -R ROLES, --roles=ROLES
-
-    Sets :ref:`env.roles <roles>` to the given comma-separated list of role
-    names.
-
-.. cmdoption:: --set KEY=VALUE,...
-
-    Allows you to set default values for arbitrary Fabric env vars. Values set
-    this way have a low precedence -- they will not override more specific env
-    vars which are also specified on the command line. E.g.::
-
-        fab --set password=foo --password=bar
-
-    will result in ``env.password = 'bar'``, not ``'foo'``
-
-    Multiple ``KEY=VALUE`` pairs may be comma-separated, e.g. ``fab --set
-    var1=val1,var2=val2``.
-
-    Other than basic string values, you may also set env vars to True by
-    omitting the ``=VALUE`` (e.g. ``fab --set KEY``), and you may set values to
-    the empty string (and thus a False-equivalent value) by keeping the equals
-    sign, but omitting ``VALUE`` (e.g. ``fab --set KEY=``.)
-
-    .. versionadded:: 1.4
-
-.. cmdoption:: -s SHELL, --shell=SHELL
-
-    Sets :ref:`env.shell <shell>` to the given string, overriding the default
-    shell wrapper used to execute remote commands.
-
-.. cmdoption:: --shortlist
-
-    Similar to :option:`--list <-l>`, but without any embellishment, just task
-    names separated by newlines with no indentation or docstrings.
-
-    .. versionadded:: 0.9.2
-    .. seealso:: :option:`--list <-l>`
-
-.. cmdoption:: --show=LEVELS
-
-    A comma-separated list of :doc:`output levels <output_controls>` to
-    be added to those that are shown by
-    default.
-
-    .. seealso:: `~fabric.operations.run`, `~fabric.operations.sudo`
-
-.. cmdoption:: --ssh-config-path
-
-    Sets :ref:`env.ssh_config_path <ssh-config-path>`.
-
-    .. versionadded:: 1.4
-    .. seealso:: :ref:`ssh-config`
-
-.. cmdoption:: --skip-bad-hosts
-
-    Sets :ref:`env.skip_bad_hosts <skip-bad-hosts>`, causing Fabric to skip
-    unavailable hosts.
-
-    .. versionadded:: 1.4
-
-.. cmdoption:: --skip-unknown-tasks
-
-    Sets :ref:`env.skip_unknown_tasks <skip-unknown-tasks>`, causing Fabric to
-    skip unknown tasks.
-
-    .. seealso::
-        :ref:`env.skip_unknown_tasks <skip-unknown-tasks>`
-
-.. cmdoption:: --sudo-password
-
-    Sets :ref:`env.sudo_password <sudo_password>`.
-
-    .. versionadded:: 1.12
-
-.. cmdoption:: --timeout=N, -t N
-
-    Set connection timeout in seconds. Sets :ref:`env.timeout <timeout>`.
-
-    .. seealso::
-        :ref:`env.timeout <timeout>`,
-        :ref:`env.connection_attempts <connection-attempts>`
-    .. versionadded:: 1.4
-
-.. cmdoption:: --command-timeout=N, -T N
-
-   Set remote command timeout in seconds. Sets
-   :ref:`env.command_timeout <command-timeout>`.
-
-   .. seealso::
-	:ref:`env.command_timeout <command-timeout>`,
-
-   .. versionadded:: 1.6
-
-.. cmdoption:: -u USER, --user=USER
-
-    Sets :ref:`env.user <user>` to the given string; it will then be used as the
-    default username when making SSH connections.
-
-.. cmdoption:: -V, --version
-
-    Displays Fabric's version number, then exits.
-
-.. cmdoption:: -w, --warn-only
-
-    Sets :ref:`env.warn_only <warn_only>` to ``True``, causing Fabric to
-    continue execution even when commands encounter error conditions.
-
-.. cmdoption:: -z, --pool-size
-
-    Sets :ref:`env.pool_size <pool-size>`, which specifies how many processes
-    to run concurrently during parallel execution.
-
-    .. versionadded:: 1.3
-    .. seealso:: :doc:`/usage/parallel`
-
-
-.. _task-arguments:
-
-Per-task arguments
-==================
-
-The options given in :ref:`command-line-options` apply to the invocation of
-``fab`` as a whole; even if the order is mixed around, options still apply to
-all given tasks equally. Additionally, since tasks are just Python functions,
-it's often desirable to pass in arguments to them at runtime.
-
-Answering both these needs is the concept of "per-task arguments", which is a
-special syntax you can tack onto the end of any task name:
-
-* Use a colon (``:``) to separate the task name from its arguments;
-* Use commas (``,``) to separate arguments from one another (may be escaped
-  by using a backslash, i.e. ``\,``);
-* Use equals signs (``=``) for keyword arguments, or omit them for positional
-  arguments. May also be escaped with backslashes.
-
-Additionally, since this process involves string parsing, all values will end
-up as Python strings, so plan accordingly. (We hope to improve upon this in
-future versions of Fabric, provided an intuitive syntax can be found.)
-
-For example, a "create a new user" task might be defined like so (omitting most
-of the actual logic for brevity)::
-
-    def new_user(username, admin='no', comment="No comment provided"):
-        print("New User (%s): %s" % (username, comment))
-        pass
-
-You can specify just the username::
-
-    $ fab new_user:myusername
-
-Or treat it as an explicit keyword argument::
-
-    $ fab new_user:username=myusername
-
-If both args are given, you can again give them as positional args::
-
-    $ fab new_user:myusername,yes
-
-Or mix and match, just like in Python::
-
-    $ fab new_user:myusername,admin=yes
-
-The ``print`` call above is useful for illustrating escaped commas, like
-so::
-
-    $ fab new_user:myusername,admin=no,comment='Gary\, new developer (starts Monday)'
-
-.. note::
-    Quoting the backslash-escaped comma is required, as not doing so will cause
-    shell syntax errors. Quotes are also needed whenever an argument involves
-    other shell-related characters such as spaces.
-
-All of the above are translated into the expected Python function calls. For
-example, the last call above would become::
-
-    >>> new_user('myusername', admin='yes', comment='Gary, new developer (starts Monday)')
-
-Roles and hosts
----------------
-
-As mentioned in :ref:`the section on task execution <hosts-per-task-cli>`,
-there are a handful of per-task keyword arguments (``host``, ``hosts``,
-``role`` and ``roles``) which do not actually map to the task functions
-themselves, but are used for setting per-task host and/or role lists.
-
-These special kwargs are **removed** from the args/kwargs sent to the task
-function itself; this is so that you don't run into TypeErrors if your task
-doesn't define the kwargs in question. (It also means that if you **do** define
-arguments with these names, you won't be able to specify them in this manner --
-a regrettable but necessary sacrifice.)
-
-.. note::
-
-    If both the plural and singular forms of these kwargs are given, the value
-    of the plural will win out and the singular will be discarded.
-
-When using the plural form of these arguments, one must use semicolons (``;``)
-since commas are already being used to separate arguments from one another.
-Furthermore, since your shell is likely to consider semicolons a special
-character, you'll want to quote the host list string to prevent shell
-interpretation, e.g.::
-
-    $ fab new_user:myusername,hosts="host1;host2"
-
-Again, since the ``hosts`` kwarg is removed from the argument list sent to the
-``new_user`` task function, the actual Python invocation would be
-``new_user('myusername')``, and the function would be executed on a host list
-of ``['host1', 'host2']``.
-
-.. _fabricrc:
-
-Settings files
-==============
-
-Fabric currently honors a simple user settings file, or ``fabricrc`` (think
-``bashrc`` but for ``fab``) which should contain one or more key-value pairs,
-one per line. These lines will be subject to ``string.split('=')``, and thus
-can currently only be used to specify string settings. Any such key-value pairs
-will be used to update :doc:`env <env>` when ``fab`` runs, and is loaded prior
-to the loading of any fabfile.
-
-By default, Fabric looks for ``~/.fabricrc``, and this may be overridden by
-specifying the :option:`-c` flag to ``fab``.
-
-For example, if your typical SSH login username differs from your workstation
-username, and you don't want to modify ``env.user`` in a project's fabfile
-(possibly because you expect others to use it as well) you could write a
-``fabricrc`` file like so::
-
-    user = ssh_user_name
-
-Then, when running ``fab``, your fabfile would load up with ``env.user`` set to
-``'ssh_user_name'``. Other users of that fabfile could do the same, allowing
-the fabfile itself to be cleanly agnostic regarding the default username.
-
-Exit status
-============
-
-If ``fab`` executes all commands on all hosts successfully, success (0) is returned.
-
-Otherwise,
-
-* If an invalid command or option is specified, ``fab`` aborts with an exit
-  status of 1.
-* If a connection to a host fails, ``fab`` aborts with an exit status of 1. It
-  will not try the next host.
-* If a local or remote command fails (returns non-zero status), ``fab`` aborts
-  with an exit status of 1. The exit status of the original command can be
-  found in the log.
-* If a Python exception is thrown, ``fab`` aborts with an exit status of 1.
diff -Nru fabric-1.14.0/sites/docs/usage/interactivity.rst fabric-2.5.0/sites/docs/usage/interactivity.rst
--- fabric-1.14.0/sites/docs/usage/interactivity.rst	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/sites/docs/usage/interactivity.rst	1970-01-01 01:00:00.000000000 +0100
@@ -1,148 +0,0 @@
-================================
-Interaction with remote programs
-================================
-
-Fabric's primary operations, `~fabric.operations.run` and
-`~fabric.operations.sudo`, are capable of sending local input to the remote
-end, in a manner nearly identical to the ``ssh`` program. For example, programs
-which display password prompts (e.g. a database dump utility, or changing a
-user's password) will behave just as if you were interacting with them
-directly.
-
-However, as with ``ssh`` itself, Fabric's implementation of this feature is
-subject to a handful of limitations which are not always intuitive. This
-document discusses such issues in detail.
-
-.. note::
-    Readers unfamiliar with the basics of Unix stdout and stderr pipes, and/or
-    terminal devices, may wish to visit the Wikipedia pages for `Unix pipelines
-    <http://en.wikipedia.org/wiki/Pipe_(Unix)>`_ and `Pseudo terminals
-    <http://en.wikipedia.org/wiki/Pseudo_terminal>`_ respectively.
-
-
-.. _combine_streams:
-
-Combining stdout and stderr
-===========================
-
-The first issue to be aware of is that of the stdout and stderr streams, and
-why they are separated or combined as needed.
-
-Buffering
----------
-
-Fabric 0.9.x and earlier, and Python itself, buffer output on a line-by-line
-basis: text is not printed to the user until a newline character is found.
-This works fine in most situations but becomes problematic when one needs to
-deal with partial-line output such as prompts.
-
-.. note::
-    Line-buffered output can make programs appear to halt or freeze for no
-    reason, as prompts print out text without a newline, waiting for the user
-    to enter their input and press Return.
-
-Newer Fabric versions buffer both input and output on a character-by-character
-basis in order to make interaction with prompts possible. This has the
-convenient side effect of enabling interaction with complex programs utilizing
-the "curses" libraries or which otherwise redraw the screen (think ``top``).
-
-Crossing the streams
---------------------
-
-Unfortunately, printing to stderr and stdout simultaneously (as many programs
-do) means that when the two streams are printed independently one byte at a
-time, they can become garbled or meshed together. While this can sometimes be
-mitigated by line-buffering one of the streams and not the other, it's still a
-serious issue.
-
-To solve this problem, Fabric uses a setting in our SSH layer which merges the
-two streams at a low level and causes output to appear more naturally. This
-setting is represented in Fabric as the :ref:`combine-stderr` env var and
-keyword argument, and is ``True`` by default.
-
-Due to this default setting, output will appear correctly, but at the
-cost of an empty ``.stderr`` attribute on the return values of
-`~fabric.operations.run`/`~fabric.operations.sudo`, as all output will appear
-to be stdout.
-
-Conversely, users requiring a distinct stderr stream at the Python level and
-who aren't bothered by garbled user-facing output (or who are hiding stdout and
-stderr from the command in question) may opt to set this to ``False`` as
-needed.
-
-
-.. _pseudottys:
-
-Pseudo-terminals
-================
-
-The other main issue to consider when presenting interactive prompts to users
-is that of echoing the user's own input.
-
-Echoes
-------
-
-Typical terminal applications or bona fide text terminals (e.g. when using a
-Unix system without a running GUI) present programs with a terminal device
-called a tty or pty (for pseudo-terminal). These automatically echo all text
-typed into them back out to the user (via stdout), as interaction without
-seeing what you had just typed would be difficult. Terminal devices are also
-able to conditionally turn off echoing, allowing secure password prompts.
-
-However, it's possible for programs to be run without a tty or pty present at
-all (consider cron jobs, for example) and in this situation, any stdin data
-being fed to the program won't be echoed. This is desirable for programs being
-run without any humans around, and it's also Fabric's old default mode of
-operation.
-
-Fabric's approach
------------------
-
-Unfortunately, in the context of executing commands via Fabric, when no pty is
-present to echo a user's stdin, Fabric must echo it for them. This is
-sufficient for many applications, but it presents problems for password
-prompts, which become insecure.
-
-In the interests of security and meeting the principle of least surprise
-(insofar as users are typically expecting things to behave as they would when
-run in a terminal emulator), Fabric 1.0 and greater force a pty by default.
-With a pty enabled, Fabric simply allows the remote end to handle echoing or
-hiding of stdin and does not echo anything itself.
-
-.. note::
-    In addition to allowing normal echo behavior, a pty also means programs
-    that behave differently when attached to a terminal device will then do so.
-    For example, programs that colorize output on terminals but not when run in
-    the background will print colored output. Be wary of this if you inspect
-    the return value of `~fabric.operations.run` or `~fabric.operations.sudo`!
-
-For situations requiring the pty behavior turned off, the :option:`--no-pty`
-command-line argument and :ref:`always-use-pty` env var may be used.
-
-
-Combining the two
-=================
-
-As a final note, keep in mind that use of pseudo-terminals effectively implies
-combining stdout and stderr -- in much the same way as the :ref:`combine_stderr
-<combine_streams>` setting does. This is because a terminal device naturally
-sends both stdout and stderr to the same place -- the user's display -- thus
-making it impossible to differentiate between them.
-
-However, at the Fabric level, the two groups of settings are distinct from one
-another and may be combined in various ways. The default is for both to be set
-to ``True``; the other combinations are as follows:
-
-* ``run("cmd", pty=False, combine_stderr=True)``: will cause Fabric to echo all
-  stdin itself, including passwords, as well as potentially altering ``cmd``'s
-  behavior. Useful if ``cmd`` behaves undesirably when run under a pty and
-  you're not concerned about password prompts.
-* ``run("cmd", pty=False, combine_stderr=False)``: with both settings
-  ``False``, Fabric will echo stdin and won't issue a pty -- and this is highly
-  likely to result in undesired behavior for all but the simplest commands.
-  However, it is also the only way to access a distinct stderr stream, which is
-  occasionally useful.
-* ``run("cmd", pty=True, combine_stderr=False)``: valid, but won't really make
-  much of a difference, as ``pty=True`` will still result in merged streams.
-  May be useful for avoiding any edge case problems in ``combine_stderr`` (none
-  are presently known).
diff -Nru fabric-1.14.0/sites/docs/usage/library.rst fabric-2.5.0/sites/docs/usage/library.rst
--- fabric-1.14.0/sites/docs/usage/library.rst	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/sites/docs/usage/library.rst	1970-01-01 01:00:00.000000000 +0100
@@ -1,61 +0,0 @@
-===========
-Library Use
-===========
-
-Fabric's primary use case is via fabfiles and the :doc:`fab </usage/fab>` tool,
-and this is reflected in much of the documentation. However, Fabric's internals
-are written in such a manner as to be easily used without ``fab`` or fabfiles
-at all -- this document will show you how.
-
-There's really only a couple of considerations one must keep in mind, when
-compared to writing a fabfile and using ``fab`` to run it: how connections are
-really made, and how disconnections occur.
-
-Connections
-===========
-
-We've documented how Fabric really connects to its hosts before, but it's
-currently somewhat buried in the middle of the overall :doc:`execution docs
-</usage/execution>`. Specifically, you'll want to skip over to the 
-:ref:`connections` section and read it real quick. (You should really give that
-entire document a once-over, but it's not absolutely required.)
-
-As that section mentions, the key is simply that `~fabric.operations.run`,
-`~fabric.operations.sudo` and the other operations only look in one place when
-connecting: :ref:`env.host_string <host_string>`. All of the other mechanisms
-for setting hosts are interpreted by the ``fab`` tool when it runs, and don't
-matter when running as a library.
-
-That said, most use cases where you want to marry a given task ``X`` and a given list of hosts ``Y`` can, as of Fabric 1.3, be handled with the `~fabric.tasks.execute` function via ``execute(X, hosts=Y)``. Please see `~fabric.tasks.execute`'s documentation for details -- manual host string manipulation should be rarely necessary.
-
-Disconnecting
-=============
-
-The other main thing that ``fab`` does for you is to disconnect from all hosts
-at the end of a session; otherwise, Python will sit around forever waiting for
-those network resources to be released.
-
-Fabric 0.9.4 and newer have a function you can use to do this easily:
-`~fabric.network.disconnect_all`. Simply make sure your code calls this when it
-terminates (typically in the ``finally`` clause of an outer ``try: finally``
-statement -- lest errors in your code prevent disconnections from happening!)
-and things ought to work pretty well.
-
-If you're on Fabric 0.9.3 or older, you can simply do this (``disconnect_all``
-just adds a bit of nice output to this logic)::
-
-    from fabric.state import connections
-
-    for key in connections.keys():
-        connections[key].close()
-        del connections[key]
-
-
-Final note
-==========
-
-This document is an early draft, and may not cover absolutely every difference
-between ``fab`` use and library use. However, the above should highlight the
-largest stumbling blocks. When in doubt, note that in the Fabric source code,
-``fabric/main.py`` contains the bulk of the extra work done by ``fab``, and may
-serve as a useful reference.
diff -Nru fabric-1.14.0/sites/docs/usage/output_controls.rst fabric-2.5.0/sites/docs/usage/output_controls.rst
--- fabric-1.14.0/sites/docs/usage/output_controls.rst	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/sites/docs/usage/output_controls.rst	1970-01-01 01:00:00.000000000 +0100
@@ -1,169 +0,0 @@
-===============
-Managing output
-===============
-
-The ``fab`` tool is very verbose by default and prints out almost everything it
-can, including the remote end's stderr and stdout streams, the command strings
-being executed, and so forth. While this is necessary in many cases in order to
-know just what's going on, any nontrivial Fabric task will quickly become
-difficult to follow as it runs.
-
-
-Output levels
-=============
-
-To aid in organizing task output, Fabric output is grouped into a number of
-non-overlapping levels or groups, each of which may be turned on or off
-independently. This provides flexible control over what is displayed to the
-user.
-
-.. note::
-
-    All levels, save for ``debug`` and ``exceptions``, are on by default.
-
-Standard output levels
-----------------------
-
-The standard, atomic output levels/groups are as follows:
-
-* **status**: Status messages, i.e. noting when Fabric is done running, if
-  the user used a keyboard interrupt, or when servers are disconnected from.
-  These messages are almost always relevant and rarely verbose.
-
-* **aborts**: Abort messages. Like status messages, these should really only be
-  turned off when using Fabric as a library, and possibly not even then. Note
-  that even if this output group is turned off, aborts will still occur --
-  there just won't be any output about why Fabric aborted!
-
-* **warnings**: Warning messages. These are often turned off when one expects a
-  given operation to fail, such as when using ``grep`` to test existence of
-  text in a file. If paired with setting ``env.warn_only`` to True, this
-  can result in fully silent warnings when remote programs fail. As with
-  ``aborts``, this setting does not control actual warning behavior, only
-  whether warning messages are printed or hidden.
-
-* **running**: Printouts of commands being executed or files transferred, e.g.
-  ``[myserver] run: ls /var/www``. Also controls printing of tasks being run,
-  e.g. ``[myserver] Executing task 'foo'``.
-
-* **stdout**: Local, or remote, stdout, i.e. non-error output from commands.
-
-* **stderr**: Local, or remote, stderr, i.e. error-related output from commands.
-
-* **user**: User-generated output, i.e. local output printed by fabfile code
-  via use of the `~fabric.utils.fastprint` or `~fabric.utils.puts` functions.
-  
-.. versionchanged:: 0.9.2
-    Added "Executing task" lines to the ``running`` output level.
-
-.. versionchanged:: 0.9.2
-    Added the ``user`` output level.
-
-Debug output
-------------
-
-There are two more atomic output levels for use when troubleshooting:
-``debug``, which behaves slightly differently from the rest, and
-``exceptions``, whose behavior is included in ``debug`` but may be enabled
-separately.
-
-* **debug**: Turn on debugging (which is off by default.) Currently, this is
-  largely used to view the "full" commands being run; take for example this
-  `~fabric.operations.run` call::
-
-      run('ls "/home/username/Folder Name With Spaces/"')
-
-  Normally, the ``running`` line will show exactly what is passed into
-  `~fabric.operations.run`, like so::
-
-      [hostname] run: ls "/home/username/Folder Name With Spaces/"
-
-  With ``debug`` on, and assuming you've left :ref:`shell` set to ``True``, you
-  will see the literal, full string as passed to the remote server::
-
-      [hostname] run: /bin/bash -l -c "ls \"/home/username/Folder Name With Spaces\""
-
-  Enabling ``debug`` output will also display full Python tracebacks during
-  aborts (as if ``exceptions`` output was enabled).
-  
-  .. note::
-  
-      Where modifying other pieces of output (such as in the above example
-      where it modifies the 'running' line to show the shell and any escape
-      characters), this setting takes precedence over the others; so if
-      ``running`` is False but ``debug`` is True, you will still be shown the
-      'running' line in its debugging form.
-
-* **exceptions**: Enables display of tracebacks when exceptions occur; intended
-  for use when ``debug`` is set to ``False`` but one is still interested in
-  detailed error info.
-
-.. versionchanged:: 1.0
-    Debug output now includes full Python tracebacks during aborts.
-
-.. versionchanged:: 1.11
-    Added the ``exceptions`` output level.
-
-.. _output-aliases:
-
-Output level aliases
---------------------
-
-In addition to the atomic/standalone levels above, Fabric also provides a
-couple of convenience aliases which map to multiple other levels. These may be
-referenced anywhere the other levels are referenced, and will effectively
-toggle all of the levels they are mapped to.
-
-* **output**: Maps to both ``stdout`` and ``stderr``. Useful for when you only
-  care to see the 'running' lines and your own print statements (and warnings).
-
-* **everything**: Includes ``warnings``, ``running``, ``user`` and ``output``
-  (see above.) Thus, when turning off ``everything``, you will only see a bare
-  minimum of output (just ``status`` and ``debug`` if it's on), along with your
-  own print statements.
-
-* **commands**: Includes ``stdout`` and ``running``. Good for hiding
-  non-erroring commands entirely, while still displaying any stderr output.
-
-.. versionchanged:: 1.4
-    Added the ``commands`` output alias.
-
-
-Hiding and/or showing output levels
-===================================
-
-You may toggle any of Fabric's output levels in a number of ways; for examples,
-please see the API docs linked in each bullet point:
-
-* **Direct modification of fabric.state.output**: `fabric.state.output` is a
-  dictionary subclass (similar to :doc:`env <env>`) whose keys are the output
-  level names, and whose values are either True (show that particular type of
-  output) or False (hide it.)
-  
-  `fabric.state.output` is the lowest-level implementation of output levels and
-  is what Fabric's internals reference when deciding whether or not to print
-  their output.
-
-* **Context managers**: `~fabric.context_managers.hide` and
-  `~fabric.context_managers.show` are twin context managers that take one or
-  more output level names as strings, and either hide or show them within the
-  wrapped block. As with Fabric's other context managers, the prior values are
-  restored when the block exits.
-
-  .. seealso::
-
-      `~fabric.context_managers.settings`, which can nest calls to
-      `~fabric.context_managers.hide` and/or `~fabric.context_managers.show`
-      inside itself.
-
-* **Command-line arguments**: You may use the :option:`--hide` and/or
-  :option:`--show` arguments to :doc:`fab`, which behave exactly like the
-  context managers of the same names (but are, naturally, globally applied) and
-  take comma-separated strings as input.
-
-Prefix output
-=============
-
-By default Fabric prefixes every line of ouput with either ``[hostname] out:``
-or ``[hostname] err:``. Those prefixes may be hidden by setting
-``env.output_prefix`` to ``False``.
\ No newline at end of file
diff -Nru fabric-1.14.0/sites/docs/usage/parallel.rst fabric-2.5.0/sites/docs/usage/parallel.rst
--- fabric-1.14.0/sites/docs/usage/parallel.rst	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/sites/docs/usage/parallel.rst	1970-01-01 01:00:00.000000000 +0100
@@ -1,171 +0,0 @@
-==================
-Parallel execution
-==================
-
-.. _parallel-execution:
-
-.. versionadded:: 1.3
-
-By default, Fabric executes all specified tasks **serially** (see
-:ref:`execution-strategy` for details.) This document describes Fabric's
-options for running tasks on multiple hosts in **parallel**, via per-task
-decorators and/or global command-line switches.
-
-
-What it does
-============
-
-Because Fabric 1.x is not fully threadsafe (and because in general use, task
-functions do not typically interact with one another) this functionality is
-implemented via the Python `multiprocessing
-<http://docs.python.org/library/multiprocessing.html>`_ module. It creates one
-new process for each host and task combination, optionally using a
-(configurable) sliding window to prevent too many processes from running at the
-same time.
-
-For example, imagine a scenario where you want to update Web application code
-on a number of Web servers, and then reload the servers once the code has been
-distributed everywhere (to allow for easier rollback if code updates fail.) One
-could implement this with the following fabfile::
-
-    from fabric.api import *
-
-    def update():
-        with cd("/srv/django/myapp"):
-            run("git pull")
-
-    def reload():
-        sudo("service apache2 reload")
-
-and execute it on a set of 3 servers, in serial, like so::
-
-    $ fab -H web1,web2,web3 update reload
-
-Normally, without any parallel execution options activated, Fabric would run
-in order:
-
-#. ``update`` on ``web1``
-#. ``update`` on ``web2``
-#. ``update`` on ``web3``
-#. ``reload`` on ``web1``
-#. ``reload`` on ``web2``
-#. ``reload`` on ``web3``
-
-With parallel execution activated (via :option:`-P` -- see below for details),
-this turns into:
-
-#. ``update`` on ``web1``, ``web2``, and ``web3``
-#. ``reload`` on ``web1``, ``web2``, and ``web3``
-
-Hopefully the benefits of this are obvious -- if ``update`` took 5 seconds to
-run and ``reload`` took 2 seconds, serial execution takes (5+2)*3 = 21 seconds
-to run, while parallel execution takes only a third of the time, (5+2) = 7
-seconds on average.
-
-
-How to use it
-=============
-
-Decorators
-----------
-
-Since the minimum "unit" that parallel execution affects is a task, the
-functionality may be enabled or disabled on a task-by-task basis using the
-`~fabric.decorators.parallel` and `~fabric.decorators.serial` decorators. For
-example, this fabfile::
-
-    from fabric.api import *
-
-    @parallel
-    def runs_in_parallel():
-        pass
-
-    def runs_serially():
-        pass
-
-when run in this manner::
-
-    $ fab -H host1,host2,host3 runs_in_parallel runs_serially
-
-will result in the following execution sequence:
-
-#. ``runs_in_parallel`` on ``host1``, ``host2``, and ``host3``
-#. ``runs_serially`` on ``host1``
-#. ``runs_serially`` on ``host2``
-#. ``runs_serially`` on ``host3``
-
-Command-line flags
-------------------
-
-One may also force all tasks to run in parallel by using the command-line flag
-:option:`-P` or the env variable :ref:`env.parallel <env-parallel>`.  However,
-any task specifically wrapped with `~fabric.decorators.serial` will ignore this
-setting and continue to run serially.
-
-For example, the following fabfile will result in the same execution sequence
-as the one above::
-
-    from fabric.api import *
-
-    def runs_in_parallel():
-        pass
-
-    @serial
-    def runs_serially():
-        pass
-
-when invoked like so::
-
-    $ fab -H host1,host2,host3 -P runs_in_parallel runs_serially
-
-As before, ``runs_in_parallel`` will run in parallel, and ``runs_serially`` in
-sequence.
-
-
-Bubble size
-===========
-
-With large host lists, a user's local machine can get overwhelmed by running
-too many concurrent Fabric processes. Because of this, you may opt to use a
-moving bubble approach that limits Fabric to a specific number of concurrently
-active processes.
-
-By default, no bubble is used and all hosts are run in one concurrent pool. You
-can override this on a per-task level by specifying the ``pool_size`` keyword
-argument to `~fabric.decorators.parallel`, or globally via :option:`-z`.
-
-For example, to run on 5 hosts at a time::
-
-    from fabric.api import *
-
-    @parallel(pool_size=5)
-    def heavy_task():
-        # lots of heavy local lifting or lots of IO here
-
-Or skip the ``pool_size`` kwarg and instead::
-
-    $ fab -P -z 5 heavy_task
-
-.. _linewise-output:
-
-Linewise vs bytewise output
-===========================
-
-Fabric's default mode of printing to the terminal is byte-by-byte, in order to
-support :doc:`/usage/interactivity`. This often gives poor results when running
-in parallel mode, as the multiple processes may write to your terminal's
-standard out stream simultaneously.
-
-To help offset this problem, Fabric's option for linewise output is
-automatically enabled whenever parallelism is active. This will cause you to
-lose most of the benefits outlined in the above link Fabric's remote
-interactivity features, but as those do not map well to parallel invocations,
-it's typically a fair trade.
-
-There's no way to avoid the multiple processes mixing up on a line-by-line
-basis, but you will at least be able to tell them apart by the host-string line
-prefix.
-
-.. note::
-    Future versions will add improved logging support to make troubleshooting
-    parallel runs easier.
diff -Nru fabric-1.14.0/sites/docs/usage/ssh.rst fabric-2.5.0/sites/docs/usage/ssh.rst
--- fabric-1.14.0/sites/docs/usage/ssh.rst	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/sites/docs/usage/ssh.rst	1970-01-01 01:00:00.000000000 +0100
@@ -1,64 +0,0 @@
-============
-SSH behavior
-============
-
-Fabric currently makes use of a pure-Python SSH re-implementation for managing
-connections, meaning that there are occasionally spots where it is limited by
-that library's capabilities. Below are areas of note where Fabric will exhibit
-behavior that isn't consistent with, or as flexible as, the behavior of the
-``ssh`` command-line program.
-
-
-Unknown hosts
-=============
-
-SSH's host key tracking mechanism keeps tabs on all the hosts you attempt to
-connect to, and maintains a ``~/.ssh/known_hosts`` file with mappings between
-identifiers (IP address, sometimes with a hostname as well) and SSH keys. (For
-details on how this works, please see the `OpenSSH documentation
-<http://openssh.org/manual.html>`_.)
-
-The ``paramiko`` library is capable of loading up your ``known_hosts`` file,
-and will then compare any host it connects to, with that mapping. Settings are
-available to determine what happens when an unknown host (a host whose username
-or IP is not found in ``known_hosts``) is seen:
-
-* **Reject**: the host key is rejected and the connection is not made. This
-  results in a Python exception, which will terminate your Fabric session with a
-  message that the host is unknown.
-* **Add**: the new host key is added to the in-memory list of known hosts, the
-  connection is made, and things continue normally. Note that this does **not**
-  modify your on-disk ``known_hosts`` file!
-* **Ask**: not yet implemented at the Fabric level, this is a ``paramiko``
-  library option which would result in the user being prompted about the
-  unknown key and whether to accept it.
-
-Whether to reject or add hosts, as above, is controlled in Fabric via the
-:ref:`env.reject_unknown_hosts <reject-unknown-hosts>` option, which is False
-by default for convenience's sake. We feel this is a valid tradeoff between
-convenience and security; anyone who feels otherwise can easily modify their
-fabfiles at module level to set ``env.reject_unknown_hosts = True``.
-
-
-Known hosts with changed keys
-=============================
-
-The point of SSH's key/fingerprint tracking is so that man-in-the-middle
-attacks can be detected: if an attacker redirects your SSH traffic to a
-computer under his control, and pretends to be your original destination
-server, the host keys will not match. Thus, the default behavior of SSH (and
-its Python implementation) is to immediately abort the connection when a host
-previously recorded in ``known_hosts`` suddenly starts sending us a different
-host key.
-
-In some edge cases such as some EC2 deployments, you may want to ignore this
-potential problem. Our SSH layer, at the time of writing, doesn't give us
-control over this exact behavior, but we can sidestep it by simply skipping the
-loading of ``known_hosts`` -- if the host list being compared to is empty, then
-there's no problem. Set :ref:`env.disable_known_hosts <disable-known-hosts>` to
-True when you want this behavior; it is False by default, in order to preserve
-default SSH behavior.
-
-.. warning::
-    Enabling :ref:`env.disable_known_hosts <disable-known-hosts>` will leave
-    you wide open to man-in-the-middle attacks! Please use with caution.
diff -Nru fabric-1.14.0/sites/docs/usage/tasks.rst fabric-2.5.0/sites/docs/usage/tasks.rst
--- fabric-1.14.0/sites/docs/usage/tasks.rst	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/sites/docs/usage/tasks.rst	1970-01-01 01:00:00.000000000 +0100
@@ -1,540 +0,0 @@
-==============
-Defining tasks
-==============
-
-As of Fabric 1.1, there are two distinct methods you may use in order to define
-which objects in your fabfile show up as tasks:
-
-* The "new" method starting in 1.1 considers instances of `~fabric.tasks.Task`
-  or its subclasses, and also descends into imported modules to allow building
-  nested namespaces.
-* The "classic" method from 1.0 and earlier considers all public callable
-  objects (functions, classes etc) and only considers the objects in the
-  fabfile itself with no recursing into imported module.
-
-.. note::
-    These two methods are **mutually exclusive**: if Fabric finds *any*
-    new-style task objects in your fabfile or in modules it imports, it will
-    assume you've committed to this method of task declaration and won't
-    consider any non-`~fabric.tasks.Task` callables. If *no* new-style tasks
-    are found, it reverts to the classic behavior.
-
-The rest of this document explores these two methods in detail.
-
-.. note::
-
-    To see exactly what tasks in your fabfile may be executed via ``fab``, use
-    :option:`fab --list <-l>`.
-
-.. _new-style-tasks:
-
-New-style tasks
-===============
-
-Fabric 1.1 introduced the `~fabric.tasks.Task` class to facilitate new features
-and enable some programming best practices, specifically:
-
-* **Object-oriented tasks**. Inheritance and all that comes with it can make
-  for much more sensible code reuse than passing around simple function
-  objects.  The classic style of task declaration didn't entirely rule this
-  out, but it also didn't make it terribly easy.
-* **Namespaces**. Having an explicit method of declaring tasks makes it easier
-  to set up recursive namespaces without e.g. polluting your task list with the
-  contents of Python's ``os`` module (which would show up as valid "tasks"
-  under the classic methodology.)
-
-With the introduction of `~fabric.tasks.Task`, there are two ways to set up new
-tasks:
-
-* Decorate a regular module level function with `@task
-  <fabric.decorators.task>`, which transparently wraps the function in a
-  `~fabric.tasks.Task` subclass.  The function name will be used as the task
-  name when invoking.
-* Subclass `~fabric.tasks.Task` (`~fabric.tasks.Task` itself is intended to be
-  abstract), define a ``run`` method, and instantiate your subclass at module
-  level. Instances' ``name`` attributes are used as the task name; if omitted
-  the instance's variable name will be used instead.
-
-Use of new-style tasks also allows you to set up :ref:`namespaces
-<namespaces>`.
-
-
-.. _task-decorator:
-
-The ``@task`` decorator
------------------------
-
-The quickest way to make use of new-style task features is to wrap basic task functions with `@task <fabric.decorators.task>`::
-
-    from fabric.api import task, run
-
-    @task
-    def mytask():
-        run("a command")
-
-
-When this decorator is used, it signals to Fabric that *only* functions wrapped in the decorator are to be loaded up as valid tasks. (When not present, :ref:`classic-style task <classic-tasks>` behavior kicks in.)
-
-.. _task-decorator-arguments:
-
-Arguments
-~~~~~~~~~
-
-`@task <fabric.decorators.task>` may also be called with arguments to
-customize its behavior. Any arguments not documented below are passed into the
-constructor of the ``task_class`` being used, with the function itself as the
-first argument (see :ref:`task-decorator-and-classes` for details.)
-
-* ``task_class``: The `~fabric.tasks.Task` subclass used to wrap the decorated
-  function. Defaults to `~fabric.tasks.WrappedCallableTask`.
-* ``aliases``: An iterable of string names which will be used as aliases for
-  the wrapped function. See :ref:`task-aliases` for details.
-* ``alias``: Like ``aliases`` but taking a single string argument instead of an
-  iterable. If both ``alias`` and ``aliases`` are specified, ``aliases`` will
-  take precedence.
-* ``default``: A boolean value determining whether the decorated task also
-  stands in for its containing module as a task name. See :ref:`default-tasks`.
-* ``name``: A string setting the name this task appears as to the command-line
-  interface. Useful for task names that would otherwise shadow Python builtins
-  (which is technically legal but frowned upon and bug-prone.)
-
-.. _task-aliases:
-
-Aliases
-~~~~~~~
-
-Here's a quick example of using the ``alias`` keyword argument to facilitate
-use of both a longer human-readable task name, and a shorter name which is
-quicker to type::
-
-    from fabric.api import task
-
-    @task(alias='dwm')
-    def deploy_with_migrations():
-        pass
-
-Calling :option:`--list <-l>` on this fabfile would show both the original
-``deploy_with_migrations`` and its alias ``dwm``::
-
-    $ fab --list
-    Available commands:
-
-        deploy_with_migrations
-        dwm
-
-When more than one alias for the same function is needed, simply swap in the
-``aliases`` kwarg, which takes an iterable of strings instead of a single
-string.
-
-.. _default-tasks:
-
-Default tasks
-~~~~~~~~~~~~~
-
-In a similar manner to :ref:`aliases <task-aliases>`, it's sometimes useful to
-designate a given task within a module as the "default" task, which may be
-called by referencing *just* the module name. This can save typing and/or
-allow for neater organization when there's a single "main" task and a number
-of related tasks or subroutines.
-
-For example, a ``deploy`` submodule might contain tasks for provisioning new
-servers, pushing code, migrating databases, and so forth -- but it'd be very
-convenient to highlight a task as the default "just deploy" action. Such a
-``deploy.py`` module might look like this::
-
-    from fabric.api import task
-
-    @task
-    def migrate():
-        pass
-
-    @task
-    def push():
-        pass
-
-    @task
-    def provision():
-        pass
-
-    @task
-    def full_deploy():
-        if not provisioned:
-            provision()
-        push()
-        migrate()
-
-With the following task list (assuming a simple top level ``fabfile.py`` that just imports ``deploy``)::
-
-    $ fab --list
-    Available commands:
-
-        deploy.full_deploy
-        deploy.migrate
-        deploy.provision
-        deploy.push
-
-Calling ``deploy.full_deploy`` on every deploy could get kind of old, or somebody new to the team might not be sure if that's really the right task to run.
-
-Using the ``default`` kwarg to `@task <fabric.decorators.task>`, we can tag
-e.g. ``full_deploy`` as the default task::
-
-    @task(default=True)
-    def full_deploy():
-        pass
-
-Doing so updates the task list like so::
-
-    $ fab --list
-    Available commands:
-
-        deploy
-        deploy.full_deploy
-        deploy.migrate
-        deploy.provision
-        deploy.push
-
-Note that ``full_deploy`` still exists as its own explicit task -- but now
-``deploy`` shows up as a sort of top level alias for ``full_deploy``.
-
-If multiple tasks within a module have ``default=True`` set, the last one to
-be loaded (typically the one lowest down in the file) will take precedence.
-
-Top-level default tasks
-~~~~~~~~~~~~~~~~~~~~~~~
-
-Using ``@task(default=True)`` in the top level fabfile will cause the denoted
-task to execute when a user invokes ``fab`` without any task names (similar to
-e.g. ``make``.) When using this shortcut, it is not possible to specify
-arguments to the task itself -- use a regular invocation of the task if this
-is necessary.
-
-.. _task-subclasses:
-
-``Task`` subclasses
--------------------
-
-If you're used to :ref:`classic-style tasks <classic-tasks>`, an easy way to
-think about `~fabric.tasks.Task` subclasses is that their ``run`` method is
-directly equivalent to a classic task; its arguments are the task arguments
-(other than ``self``) and its body is what gets executed.
-
-For example, this new-style task::
-
-    class MyTask(Task):
-        name = "deploy"
-        def run(self, environment, domain="whatever.com"):
-            run("git clone foo")
-            sudo("service apache2 restart")
-
-    instance = MyTask()
-
-is exactly equivalent to this function-based task::
-
-    @task
-    def deploy(environment, domain="whatever.com"):
-        run("git clone foo")
-        sudo("service apache2 restart")
-
-Note how we had to instantiate an instance of our class; that's simply normal
-Python object-oriented programming at work. While it's a small bit of
-boilerplate right now -- for example, Fabric doesn't care about the name you
-give the instantiation, only the instance's ``name`` attribute -- it's well
-worth the benefit of having the power of classes available.
-
-We plan to extend the API in the future to make this experience a bit smoother.
-
-.. _task-decorator-and-classes:
-
-Using custom subclasses with ``@task``
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-It's possible to marry custom `~fabric.tasks.Task` subclasses with `@task
-<fabric.decorators.task>`. This may be useful in cases where your core
-execution logic doesn't do anything class/object-specific, but you want to
-take advantage of class metaprogramming or similar techniques.
-
-Specifically, any `~fabric.tasks.Task` subclass which is designed to take in a
-callable as its first constructor argument (as the built-in
-`~fabric.tasks.WrappedCallableTask` does) may be specified as the
-``task_class`` argument to `@task <fabric.decorators.task>`.
-
-Fabric will automatically instantiate a copy of the given class, passing in
-the wrapped function as the first argument. All other args/kwargs given to the
-decorator (besides the "special" arguments documented in
-:ref:`task-decorator-arguments`) are added afterwards.
-
-Here's a brief and somewhat contrived example to make this obvious::
-
-    from fabric.api import task
-    from fabric.tasks import Task
-
-    class CustomTask(Task):
-        def __init__(self, func, myarg, *args, **kwargs):
-            super(CustomTask, self).__init__(*args, **kwargs)
-            self.func = func
-            self.myarg = myarg
-
-        def run(self, *args, **kwargs):
-            return self.func(*args, **kwargs)
-
-    @task(task_class=CustomTask, myarg='value', alias='at')
-    def actual_task():
-        pass
-
-When this fabfile is loaded, a copy of ``CustomTask`` is instantiated, effectively calling::
-
-    task_obj = CustomTask(actual_task, myarg='value')
-
-Note how the ``alias`` kwarg is stripped out by the decorator itself and never
-reaches the class instantiation; this is identical in function to how
-:ref:`command-line task arguments <task-arguments>` work.
-
-.. _namespaces:
-
-Namespaces
-----------
-
-With :ref:`classic tasks <classic-tasks>`, fabfiles were limited to a single,
-flat set of task names with no real way to organize them.  In Fabric 1.1 and
-newer, if you declare tasks the new way (via `@task <fabric.decorators.task>`
-or your own `~fabric.tasks.Task` subclass instances) you may take advantage
-of **namespacing**:
-
-* Any module objects imported into your fabfile will be recursed into, looking
-  for additional task objects.
-* Within submodules, you may control which objects are "exported" by using the
-  standard Python ``__all__`` module-level variable name (thought they should
-  still be valid new-style task objects.)
-* These tasks will be given new dotted-notation names based on the modules they
-  came from, similar to Python's own import syntax.
-
-Let's build up a fabfile package from simple to complex and see how this works.
-
-Basic
-~~~~~
-
-We start with a single `__init__.py` containing a few tasks (the Fabric API
-import omitted for brevity)::
-
-    @task
-    def deploy():
-        ...
-
-    @task
-    def compress():
-        ...
-
-The output of ``fab --list`` would look something like this::
-
-    deploy
-    compress
-
-There's just one namespace here: the "root" or global namespace. Looks simple
-now, but in a real-world fabfile with dozens of tasks, it can get difficult to
-manage.
-
-Importing a submodule
-~~~~~~~~~~~~~~~~~~~~~
-
-As mentioned above, Fabric will examine any imported module objects for tasks,
-regardless of where that module exists on your Python import path.  For now we
-just want to include our own, "nearby" tasks, so we'll make a new submodule in
-our package for dealing with, say, load balancers -- ``lb.py``::
-
-    @task
-    def add_backend():
-        ...
-
-And we'll add this to the top of ``__init__.py``::
-
-    import lb
-
-Now ``fab --list`` shows us::
-
-    deploy
-    compress
-    lb.add_backend
-
-Again, with only one task in its own submodule, it looks kind of silly, but the
-benefits should be pretty obvious.
-
-Going deeper
-~~~~~~~~~~~~
-
-Namespacing isn't limited to just one level. Let's say we had a larger setup
-and wanted a namespace for database related tasks, with additional
-differentiation inside that. We make a sub-package named ``db/`` and inside it,
-a ``migrations.py`` module::
-
-    @task
-    def list():
-        ...
-
-    @task
-    def run():
-        ...
-
-We need to make sure that this module is visible to anybody importing ``db``,
-so we add it to the sub-package's ``__init__.py``::
-
-    import migrations
-
-As a final step, we import the sub-package into our root-level ``__init__.py``,
-so now its first few lines look like this::
-
-   import lb
-   import db
-
-After all that, our file tree looks like this::
-
-    .
-    ├── __init__.py
-    ├── db
-    │   ├── __init__.py
-    │   └── migrations.py
-    └── lb.py
-
-and ``fab --list`` shows::
-
-    deploy
-    compress
-    lb.add_backend
-    db.migrations.list
-    db.migrations.run
-
-We could also have specified (or imported) tasks directly into
-``db/__init__.py``, and they would show up as ``db.<whatever>`` as you might
-expect.
-
-Limiting with ``__all__``
-~~~~~~~~~~~~~~~~~~~~~~~~~
-
-You may limit what Fabric "sees" when it examines imported modules, by using
-the Python convention of a module level ``__all__`` variable (a list of
-variable names.) If we didn't want the ``db.migrations.run`` task to show up by
-default for some reason, we could add this to the top of ``db/migrations.py``::
-
-    __all__ = ['list']
-
-Note the lack of ``'run'`` there. You could, if needed, import ``run`` directly
-into some other part of the hierarchy, but otherwise it'll remain hidden.
-
-Switching it up
-~~~~~~~~~~~~~~~
-
-We've been keeping our fabfile package neatly organized and importing it in a
-straightforward manner, but the filesystem layout doesn't actually matter here.
-All Fabric's loader cares about is the names the modules are given when they're
-imported.
-
-For example, if we changed the top of our root ``__init__.py`` to look like
-this::
-
-    import db as database
-
-Our task list would change thusly::
-
-    deploy
-    compress
-    lb.add_backend
-    database.migrations.list
-    database.migrations.run
-
-This applies to any other import -- you could import third party modules into
-your own task hierarchy, or grab a deeply nested module and make it appear near
-the top level.
-
-Nested list output
-~~~~~~~~~~~~~~~~~~
-
-As a final note, we've been using the default Fabric :option:`--list <-l>`
-output during this section -- it makes it more obvious what the actual task
-names are. However, you can get a more nested or tree-like view by passing
-``nested`` to the :option:`--list-format <-F>` option::
-
-    $ fab --list-format=nested --list
-    Available commands (remember to call as module.[...].task):
-
-        deploy
-        compress
-        lb:
-            add_backend
-        database:
-            migrations:
-                list
-                run
-
-While it slightly obfuscates the "real" task names, this view provides a handy
-way of noting the organization of tasks in large namespaces.
-
-
-.. _classic-tasks:
-
-Classic tasks
-=============
-
-When no new-style `~fabric.tasks.Task`-based tasks are found, Fabric will
-consider any callable object found in your fabfile, **except** the following:
-
-* Callables whose name starts with an underscore (``_``). In other words,
-  Python's usual "private" convention holds true here.
-* Callables defined within Fabric itself. Fabric's own functions such as
-  `~fabric.operations.run` and `~fabric.operations.sudo`  will not show up in
-  your task list.
-
-
-Imports
--------
-
-Python's ``import`` statement effectively includes the imported objects in your
-module's namespace. Since Fabric's fabfiles are just Python modules, this means
-that imports are also considered as possible classic-style tasks, alongside
-anything defined in the fabfile itself.
-
-    .. note::
-        This only applies to imported *callable objects* -- not modules.
-        Imported modules only come into play if they contain :ref:`new-style
-        tasks <new-style-tasks>`, at which point this section no longer
-        applies.
-
-Because of this, we strongly recommend that you use the ``import module`` form
-of importing, followed by ``module.callable()``, which will result in a cleaner
-fabfile API than doing ``from module import callable``.
-
-For example, here's a sample fabfile which uses ``urllib.urlopen`` to get some
-data out of a webservice::
-
-    from urllib import urlopen
-
-    from fabric.api import run
-
-    def webservice_read():
-        objects = urlopen('http://my/web/service/?foo=bar').read().split()
-        print(objects)
-
-This looks simple enough, and will run without error. However, look what
-happens if we run :option:`fab --list <-l>` on this fabfile::
-
-    $ fab --list
-    Available commands:
-
-      webservice_read   List some directories.   
-      urlopen           urlopen(url [, data]) -> open file-like object
-
-Our fabfile of only one task is showing two "tasks", which is bad enough, and
-an unsuspecting user might accidentally try to call ``fab urlopen``, which
-probably won't work very well. Imagine any real-world fabfile, which is likely
-to be much more complex, and hopefully you can see how this could get messy
-fast.
-
-For reference, here's the recommended way to do it::
-
-    import urllib
-
-    from fabric.api import run
-
-    def webservice_read():
-        objects = urllib.urlopen('http://my/web/service/?foo=bar').read().split()
-        print(objects)
-
-It's a simple change, but it'll make anyone using your fabfile a bit happier.
diff -Nru fabric-1.14.0/sites/shared_conf.py fabric-2.5.0/sites/shared_conf.py
--- fabric-1.14.0/sites/shared_conf.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/sites/shared_conf.py	2019-08-06 23:57:28.000000000 +0100
@@ -1,4 +1,5 @@
-from os.path import join
+import os
+from os.path import join, dirname, abspath
 from datetime import datetime
 
 import alabaster
@@ -6,38 +7,72 @@
 
 # Alabaster theme + mini-extension
 html_theme_path = [alabaster.get_path()]
-extensions = ['alabaster']
+extensions = ["alabaster", "sphinx.ext.intersphinx"]
+
 # Paths relative to invoking conf.py - not this shared file
-html_static_path = [join('..', '_shared_static')]
-html_theme = 'alabaster'
+html_static_path = [join("..", "_shared_static")]
+html_theme = "alabaster"
 html_theme_options = {
-    'logo': 'logo.png',
-    'logo_name': True,
-    'logo_text_align': 'center',
-    'description': "Pythonic remote execution",
-    'github_user': 'fabric',
-    'github_repo': 'fabric',
-    'travis_button': True,
-    'analytics_id': 'UA-18486793-1',
-
-    'link': '#3782BE',
-    'link_hover': '#3782BE',
+    "logo": "logo.png",
+    "logo_name": True,
+    "logo_text_align": "center",
+    "description": "Pythonic remote execution",
+    "github_user": "fabric",
+    "github_repo": "fabric",
+    "travis_button": True,
+    "codecov_button": True,
+    "tidelift_url": "https://tidelift.com/subscription/pkg/pypi-fabric?utm_source=pypi-fabric&utm_medium=referral&utm_campaign=docs",
+    "analytics_id": "UA-18486793-1",
+    "link": "#3782BE",
+    "link_hover": "#3782BE",
+    # Wide enough that 80-col code snippets aren't truncated on default font
+    # settings (at least for bitprophet's Chrome-on-OSX-Yosemite setup)
+    "page_width": "1024px",
 }
 html_sidebars = {
-    '**': [
-        'about.html',
-        'navigation.html',
-        'searchbox.html',
-        'donate.html',
-    ]
+    "**": ["about.html", "navigation.html", "searchbox.html", "donate.html"]
+}
+
+# Enable & configure doctest
+extensions.append("sphinx.ext.doctest")
+doctest_global_setup = r"""
+from fabric.testing.base import MockRemote, MockSFTP, Session, Command
+"""
+
+on_rtd = os.environ.get("READTHEDOCS") == "True"
+on_travis = os.environ.get("TRAVIS", False)
+on_dev = not (on_rtd or on_travis)
+
+# Invoke (docs + www)
+inv_target = join(
+    dirname(__file__), "..", "..", "invoke", "sites", "docs", "_build"
+)
+if not on_dev:
+    inv_target = "http://docs.pyinvoke.org/en/latest/"
+inv_www_target = join(
+    dirname(__file__), "..", "..", "invoke", "sites", "www", "_build"
+)
+if not on_dev:
+    inv_www_target = "http://pyinvoke.org/"
+# Paramiko (docs)
+para_target = join(
+    dirname(__file__), "..", "..", "paramiko", "sites", "docs", "_build"
+)
+if not on_dev:
+    para_target = "http://docs.paramiko.org/en/latest/"
+intersphinx_mapping = {
+    "python": ("http://docs.python.org/", None),
+    "invoke": (inv_target, None),
+    "invoke_www": (inv_www_target, None),
+    "paramiko": (para_target, None),
 }
 
 # Regular settings
-project = 'Fabric'
+project = "Fabric"
 year = datetime.now().year
-copyright = '%d Jeff Forcier' % year
-master_doc = 'index'
-templates_path = ['_templates']
-exclude_trees = ['_build']
-source_suffix = '.rst'
-default_role = 'obj'
+copyright = "%d Jeff Forcier" % year
+master_doc = "index"
+templates_path = ["_templates"]
+exclude_trees = ["_build"]
+source_suffix = ".rst"
+default_role = "obj"
diff -Nru fabric-1.14.0/sites/www/changelog.rst fabric-2.5.0/sites/www/changelog.rst
--- fabric-1.14.0/sites/www/changelog.rst	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/sites/www/changelog.rst	2019-08-06 23:57:28.000000000 +0100
@@ -2,816 +2,183 @@
 Changelog
 =========
 
-* :release:`1.14.0 <2017-08-25>`
-* :feature:`1475` Honor ``env.timeout`` when opening new remote sessions (as
-  opposed to the initial overall connection, which already honored timeout
-  settings.) Thanks to ``@EugeniuZ`` for the report & ``@jrmsgit`` for the
-  first draft of the patch.
+.. note::
+    Looking for the Fabric 1.x changelog? See :doc:`/changelog-v1`.
 
-  .. note::
-    This feature only works with Paramiko 1.14.3 and above; if your Paramiko
-    version is older, no timeout can be set, and the previous behavior will
-    occur instead.
-
-* :release:`1.13.2 <2017-04-24>`
-* :release:`1.12.2 <2017-04-24>`
-* :bug:`1542` (via :issue:`1543`) Catch Paramiko-level gateway connection
-  errors (``ChannelError``) when raising ``NetworkError``; this prevents an
-  issue where gateway related issues were being treated as authentication
-  errors. Thanks to Charlie Stanley for catch & patch.
-* :bug:`1555` Multiple simultaneous `~fabric.operations.get` and/or
-  `~fabric.operations.put` with ``use_sudo=True`` and for the same remote host
-  and path could fail unnecessarily. Thanks ``@arnimarj`` for the report and
-  Pierce Lopez for the patch.
-* :bug:`1427` (via :issue:`1428`) Locate ``.pyc`` files when searching for
-  fabfiles to load; previously we only used the presence of ``.py`` files to
-  determine whether loading should be attempted. Credit: Ray Chen.
-* :bug:`1294` fix text escaping for `~fabric.contrib.files.contains` and
-  `~fabric.contrib.files.append` which would fail if the text contained e.g.
-  ``>``. Thanks to ``@ecksun`` for report & Pierce Lopez for the patch.
-* :support:`1065 backported` Fix incorrect SSH config reference in the docs for
-  ``env.keepalive``; it corresponds to ``ServerAliveInterval``, not
-  ``ClientAliveInterval``. Credit: Harry Percival.
-* :bug:`1574` `~fabric.contrib.project.upload_project` failed for folder in
-  current directory specified without any path separator. Thanks ``@aidanmelen``
-  for the report and Pierce Lopez for the patch.
-* :support:`1590 backported` Replace a reference to ``fab`` in a test
-  subprocess, to use the ``python -m <package>`` style instead; this allows
-  ``python setup.py test`` to run the test suite without having Fabric already
-  installed. Thanks to ``@BenSturmfels`` for catch & patch.
-* :support:`- backported` Backport :issue:`1462` to 1.12.x (was previously only
-  backported to 1.13.x.)
-* :support:`1416 backported` Add explicit "Python 2 only" note to ``setup.py``
-  trove classifiers to help signal that fact to various info-gathering tools.
-  Patch courtesy of Gavin Bisesi.
-* :bug:`1526` Disable use of PTY and shell for a background command execution
-  within `contrib.sed <fabric.contrib.files.sed>`, preventing a small class of
-  issues on some platforms/environments. Thanks to ``@doflink`` for the report
-  and Pierce Lopez for the final patch.
-* :support:`1539 backported` Add documentation for :ref:`env.output_prefix
-  <output_prefix>`. Thanks ``@jphalip``.
-* :bug:`1514` Compatibility with Python 2.5 was broken by using the ``format()``
-  method of a string (only in 1.11+). Report by ``@pedrudehuere``.
-* :release:`1.13.1 <2016-12-09>`
-* :bug:`1462` Make a PyCrypto-specific import and method call optional to avoid
-  ``ImportError`` problems under Paramiko 2.x. Thanks to Alex Gaynor for catch
-  & patch!
-* :release:`1.13.0 <2016-12-09>`
-* :support:`1461` Update setup requirements to allow Paramiko 2.x, now that
-  it's stable and been out in the wild for some time. Paramiko 1.x still works
-  like it always did; the only change to Paramiko 2 was the backend moving from
-  PyCrypto to Cryptography.
-
-  .. warning::
-    If you are upgrading an existing environment, the install dependencies have
-    changed; please see Paramiko's installation docs for details:
-    http://www.paramiko.org/installing.html
-
-* :release:`1.12.1 <2016-12-05>`
-* :release:`1.11.3 <2016-12-05>`
-* :release:`1.10.5 <2016-12-05>`
-* :bug:`1470` When using `~fabric.operations.get` with glob expressions, a lack
-  of matches for the glob would result in an empty file named after the glob
-  expression (in addition to raising an error). This has been fixed so the
-  empty file is no longer generated. Thanks to Georgy Kibardin for the catch &
-  initial patch.
-* :feature:`1495` Update the internals of `~fabric.contrib.files` so its
-  members work with SSH servers running on Windows. Thanks to Hamdi Sahloul for
-  the patch.
-* :support:`1483 backported` (also re: :issue:`1386`, :issue:`1374`,
-  :issue:`1300`) Add :ref:`an FAQ <faq-csh>` about quote problems in remote
-  ``csh`` causing issues with Fabric's shell-wrapping and quote-escaping.
-  Thanks to Michael Radziej for the update.
-* :support:`1379 backported` (also :issue:`1464`) Clean up a lot of unused
-  imports and similar cruft (many found via ``flake8 --select E4``). Thanks to
-  Mathias Ertl for the original patches.
-* :bug:`1458` Detect ``known_hosts``-related instances of
-  ``paramiko.SSHException`` and prevent them from being handled like
-  authentication errors (which is the default behavior). This fixes
-  issues with incorrect password prompts or prompt-related exceptions when
-  using ``reject_unknown_hosts`` and encountering missing or bad
-  ``known_hosts`` entries. Thanks to Lukáš Doktor for catch & patch.
-* :release:`1.12.0 <2016-07-25>`
-* :release:`1.11.2 <2016-07-25>`
-* :release:`1.10.4 <2016-07-25>`
-* :feature:`1491` Implement ``sudo``-specific password caching (:ref:`docs
-  <sudo-passwords>`). This can be used to work around issues where over-eager
-  submission of ``env.password`` at login time causes authentication problems
-  (e.g. during two-factor auth).
-* :bug:`1447` Fix a relative import in ``fabric.network`` to be
-  correctly/consistently absolute instead. Thanks to ``@bildzeitung`` for catch
-  & patch.
-* :release:`1.11.1 <2016-04-09>`
-* :bug:`- (==1.11)` Bumped version to ``1.11.1`` due to apparently accidentally
-  uploading a false ``1.11.0`` to PyPI sometime in the past (PyPI is secure &
-  prevents reusing deleted filenames.) We have no memory of this, but databases
-  don't lie!
-* :release:`1.11.0 <2016-04-09>`
-* :release:`1.10.3 <2016-04-09>`
-* :bug:`1135` (via :issue:`1241`) Modified order of operations in
-  `~fabric.operations.run`/`~fabric.operations.sudo` to apply environment vars
-  before prefixing commands (instead of after). Report by ``@warsamebashir``,
-  patch by Curtis Mattoon.
-* :feature:`1203` (via :issue:`1240`) Add a ``case_sensitive`` kwarg to
-  `~fabric.contrib.files.contains` (which toggles use of ``egrep -i``). Report
-  by ``@xoul``, patch by Curtis Mattoon.
-* :feature:`800` Add ``capture_buffer_size`` kwarg to
-  `~fabric.operations.run`/`~fabric.operations.sudo` so users can limit memory
-  usage in situations where subprocesses generate very large amounts of
-  stdout/err. Thanks to Jordan Starcher for the report & Omri Bahumi for an
-  early version of the patchset.
-* :feature:`1161` Add ``use_sudo`` kwarg to `~fabric.operations.reboot`.
-  Credit: Bryce Verdier.
-* :support:`943 backported` Tweak ``env.warn_only`` docs to note that it
-  applies to all operations, not just ``run``/``sudo``. Thanks ``@akitada``.
-* :feature:`932` Add a ``temp_dir`` kwarg to
-  `~fabric.contrib.files.upload_template` which is passed into its inner
-  `~fabric.operations.put` call. Thanks to ``@nburlett`` for the patch.
-* :support:`1257 backported` Add notes to the usage docs for ``fab`` regarding
-  the program's exit status. Credit: ``@koalaman``.
-* :feature:`1261` Expose Paramiko's Kerberos functionality as Fabric config
-  vars & command-line options. Thanks to Ramanan Sivaranjan for catch & patch,
-  and to Johannes Löthberg & Michael Bennett for additional testing.
-* :feature:`1271` Allow users whose fabfiles use `fabric.colors` to disable
-  colorization at runtime by specifying ``FABRIC_DISABLE_COLORS=1`` (or any
-  other non-empty value). Credit: Eric Berg.
-* :feature:`1326` Make `~fabric.contrib.project.rsync_project` aware of
-  ``env.gateway``, using a ``ProxyCommand`` under the hood. Credit: David
-  Rasch.
-* :support:`1359` Add a more-visible top-level ``CHANGELOG.rst`` pointing users
-  to the actual changelog stored within the Sphinx directory tree. Thanks to
-  Jonathan Vanasco for catch & patch.
-* :feature:`1388` Expose Jinja's ``keep_trailing_newline`` parameter in
-  `~fabric.contrib.files.upload_template` so users can force template renders
-  to preserve trailing newlines. Thanks to Chen Lei for the patch.
-* :bug:`1389 major` Gently overhaul SSH port derivation so it's less
-  surprising; previously, any non-default value stored in ``env.port`` was
-  overriding all SSH-config derived values. See the API docs for
-  `~fabric.network.normalize` for details on how it now behaves. Thanks to
-  Harry Weppner for catch & patch.
-* :support:`1454 backported` Remove use of ``:option:`` directives in the
-  changelog, it's currently broken in modern Sphinx & doesn't seem to have
-  actually functioned on Renaissance-era Sphinx either.
-* :bug:`1365` (via :issue:`1372`) Classic-style fabfiles (ones not using
-  ``@task``) erroneously included custom exception subclasses when collecting
-  tasks. This is now fixed thanks to ``@mattvonrocketstein``.
-* :bug:`1348` (via :issue:`1361`) Fix a bug in `~fabric.operations.get` where
-  remote file paths containing Python string formatting escape codes caused an
-  exception. Thanks to ``@natecode`` for the report and Bradley Spink for the
-  fix.
-* :release:`1.10.2 <2015-06-19>`
-* :support:`1325` Clarify `~fabric.operations.put` docs re: the ``mode``
-  argument. Thanks to ``@mjmare`` for the catch.
-* :bug:`1318` Update functionality added in :issue:`1213` so abort error
-  messages don't get printed twice (once by us, once by ``sys.exit``) but the
-  annotated exception error message is retained. Thanks to Felix Almeida for
-  the report.
-* :bug:`1305` (also :issue:`1313`) Fix a couple minor issues with the operation
-  of & demo code for the ``JobQueue`` class. Thanks to ``@dioh`` and Horst
-  Gutmann for the report & Cameron Lane for the patch.
-* :bug:`980` (also :issue:`1312`) Redirect output of ``cd`` to ``/dev/null`` so
-  users enabling bash's ``CDPATH`` (or similar features in other shells) don't
-  have polluted output captures. Thanks to Alex North-Keys for the original
-  report & Steve Ivy for the fix.
-* :bug:`1289` Fix "NameError: free variable referenced before assignment in
-  enclosing scope". Thanks to ``@SamuelMarks`` for catch & patch.
-* :bug:`1286` (also :issue:`971`, :issue:`1032`) Recursively unwrap decorators
-  instead of only unwrapping a single decorator level, when obtaining task
-  docstrings. Thanks to Avishai Ish-Shalom for the original report & Max Kovgan
-  for the patch.
-* :bug:`1273` Fix issue with ssh/config not having a cross-platform default
-  path. Thanks to ``@SamuelMarks`` for catch & patch.
-* :feature:`1200` Introduced ``exceptions`` output level, so users don't have to
-  deal with the debug output just to see tracebacks.
-* :support:`1239` Update README to work better under raw docutils so the
-  example code block is highlighted as Python on PyPI (and not just on our
-  Sphinx-driven website). Thanks to Marc Abramowitz.
-* :release:`1.10.1 <2014-12-19>`
-* :release:`1.9.2 <2014-12-19>`
-* :bug:`1201` Don't naively glob all `~fabric.operations.get` targets - only
-  glob actual directories. This avoids incorrectly yielding permission errors
-  in edge cases where a requested file is within a directory lacking the read
-  permission bit. Thanks to Sassa Nf for the original report.
-* :bug:`1019` (also :issue:`1022`, :issue:`1186`) Fix "is a tty" tests in
-  environments where streams (eg ``sys.stdout``) have been replaced with
-  objects lacking a ``.isatty()`` method. Thanks to Miki Tebeka for the
-  original report, Lele Long for a subsequent patch, and Julien Phalip
-  for the final/merged patch.
-* :support:`1213 backported` Add useful exception message to the implicit
-  ``SystemExit`` raised by Fabric's use of ``sys.exit`` inside the
-  `~fabric.api.abort` function. This allows client code catching ``SystemExit``
-  to have better introspection into the error. Thanks to Ioannis Panousis.
-* :bug:`1228` Update the ``CommandTimeout`` class so it has a useful ``str``
-  instead of appearing blank when caught by Fabric's top level exception
-  handling. Catch & patch from Tomaz Muraus.
-* :bug:`1180` Fix issue with unicode steam outputs crashing if stream encoding
-  type is None. Thanks to ``@joekiller`` for catch & patch.
-* :support:`958 backported` Remove the Git SHA portion of our version string
-  generation; it was rarely useful & occasionally caused issues for users with
-  non-Git-based source checkouts.
-* :support:`1229 backported` Add some missing API doc hyperlink references.
-  Thanks to Tony Narlock.
-* :bug:`1226` Update `~fabric.operations.get` to ensure that `env.user` has
-  access to tempfiles before changing permissions. Also corrected permissions
-  from 404 to 0400 to match comment. Patch by Curtis Mattoon; original report
-  from Daniel Watkins.
-* :release:`1.10.0 <2014-09-04>`
-* :bug:`1188 major` Update `~fabric.operations.local` to close non-pipe file
-  descriptors in the child process so subsequent calls to
-  `~fabric.operations.local` aren't blocked on e.g. already-connected network
-  sockets. Thanks to Tolbkni Kao for catch & patch.
-* :feature:`700` Added ``use_sudo`` and ``temp_dir`` params to
-  `~fabric.operations.get`. This allows downloading files normally not
-  accessible to the user using ``sudo``. Thanks to Jason Coombs for initial
-  report and to Alex Plugaru for the patch (:issue:`1121`).
-* :feature:`1098` Add support for dict style roledefs. Thanks to Jonas
-  Lundberg.
-* :feature:`1090` Add option to skip unknown tasks. Credit goes to Jonas
-  Lundberg.
-* :feature:`975` Fabric can now be invoked via ``python -m fabric`` in addition
-  to the typical use of the ``fab`` entrypoint. Patch courtesy of Jason Coombs.
-
-  .. note:: This functionality is only available under Python 2.7.
-
-* :release:`1.9.1 <2014-08-06>`
-* :release:`1.8.5 <2014-08-06>`
-* :release:`1.7.5 <2014-08-06>`
-* :bug:`1165` Prevent infinite loop condition when a gateway host is enabled &
-  the same host is in the regular target host list. Thanks to ``@CzBiX`` for
-  catch & patch.
-* :bug:`1147` Use ``stat`` instead of ``lstat`` when testing directory-ness in
-  the SFTP module. This allows recursive downloads to avoid recursing into
-  symlinks unexpectedly. Thanks to Igor Kalnitsky for the patch.
-* :bug:`1146` Fix a bug where `~fabric.contrib.files.upload_template` failed to
-  honor ``lcd`` when ``mirror_local_mode`` is ``True``. Thanks to Laszlo Marai
-  for catch & patch.
-* :bug:`1134` Skip bad hosts when the tasks are executed in parallel. Thanks to
-  Igor Maravić ``@i-maravic``.
-* :bug:`852` Fix to respect ``template_dir`` for non Jinja2 templates in
-  `~fabric.contrib.files.upload_template`. Thanks to Adam Kowalski for the
-  patch and Alex Plugaru for the initial test case.
-* :bug:`1096` Encode Unicode text appropriately for its target stream object to
-  avoid issues on non-ASCII systems. Thanks to Toru Uetani for the original
-  patch.
-* :bug:`1059` Update IPv6 support to work with link-local address formats.
-  Fix courtesy of ``@obormot``.
-* :bug:`1026` Fix a typo preventing quiet operation of
-  `~fabric.contrib.files.is_link`. Caught by ``@dongweiming``.
-* :bug:`600` Clear out connection caches in full when prepping
-  parallel-execution subprocesses. This avoids corner cases causing
-  hangs/freezes due to client/socket reuse. Thanks to Ruslan Lutsenko for the
-  initial report and Romain Chossart for the suggested fix.
-* :bug:`1167` Add Jinja to ``test_requires`` in ``setup.py`` for the couple of
-  newish tests that now require it. Thanks to Kubilay Kocak for the catch.
-* :release:`1.9.0 <2014-06-08>`
-* :feature:`1078` Add ``.command`` and ``.real_command`` attributes to
-  ``local`` return value.  Thanks to Alexander Teves (``@alexanderteves``) and
-  Konrad Hałas (``@konradhalas``).
-* :feature:`938` Add an env var :ref:`env.effective_roles <effective_roles>`
-  specifying roles used in the currently executing command. Thanks to
-  Piotr Betkier for the patch.
-* :feature:`1101` Reboot operation now supports custom command. Thanks to Jonas
-  Lejon.
-* :support:`1106` Fix a misleading/ambiguous example snippet in the ``fab``
-  usage docs to be clearer. Thanks to ``@zed``.
-* :release:`1.8.4 <2014-06-08>`
-* :release:`1.7.4 <2014-06-08>`
-* :bug:`898` Treat paths that begin with tilde "~" as absolute paths instead of
-  relative. Thanks to Alex Plugaru for the patch and Dan Craig for the
-  suggestion.
-* :support:`1105 backported` Enhance ``setup.py`` to allow Paramiko 1.13+ under
-  Python 2.6+. Thanks to to ``@Arfrever`` for catch & patch.
-* :release:`1.8.3 <2014-03-21>`
-* :release:`1.7.3 <2014-03-21>`
-* :support:`- backported` Modified packaging data to reflect that Fabric
-  requires Paramiko < 1.13 (which dropped Python 2.5 support.)
-* :feature:`1082` Add ``pty`` passthrough kwarg to
-  `~fabric.contrib.files.upload_template`.
-* :release:`1.8.2 <2014-02-14>`
-* :release:`1.7.2 <2014-02-14>`
-* :bug:`955` Quote directories created as part of ``put``'s recursive directory
-  uploads when ``use_sudo=True`` so directories with shell meta-characters
-  (such as spaces) work correctly. Thanks to John Harris for the catch.
-* :bug:`917` Correct an issue with ``put(use_sudo=True, mode=xxx)`` where the
-  ``chmod`` was trying to apply to the wrong location. Thanks to Remco
-  (``@nl5887``) for catch & patch.
-* :bug:`1046` Fix typo preventing use of ProxyCommand in some situations.
-  Thanks to Keith Yang.
-* :release:`1.8.1 <2013-12-24>`
-* :release:`1.7.1 <2013-12-24>`
-* :release:`1.6.4 <2013-12-24>` 956, 957
-* :release:`1.5.5 <2013-12-24>` 956, 957
-* :bug:`956` Fix pty size detection when running inside Emacs. Thanks to
-  `@akitada` for catch & patch.
-* :bug:`957` Fix bug preventing use of :ref:`env.gateway <gateway>` with
-  targets requiring password authentication. Thanks to Daniel González,
-  `@Bengrunt` and `@adrianbn` for their bug reports.
-* :feature:`741` Add :ref:`env.prompts <prompts>` dictionary, allowing
-  users to set up custom prompt responses (similar to the built-in sudo prompt
-  auto-responder.) Thanks to Nigel Owens and David Halter for the patch.
-* :bug:`965 major` Tweak IO flushing behavior when in linewise (& thus
-  parallel) mode so interwoven output is less frequent. Thanks to `@akidata`
-  for catch & patch.
-* :bug:`948` Handle connection failures due to server load and try connecting
-  to hosts a number of times specified in :ref:`env.connection_attempts
-  <connection-attempts>`.
-* :release:`1.8.0 <2013-09-20>`
-* :feature:`931` Allow overriding of `.abort` behavior via a custom
-  exception-returning callable set as :ref:`env.abort_exception
-  <abort-exception>`. Thanks to Chris Rose for the patch.
-* :support:`984 backported` Make this changelog easier to read! Now with
-  per-release sections, generated automatically from the old timeline source
-  format.
-* :feature:`910` Added a keyword argument to rsync_project to configure the
-  default options. Thanks to ``@moorepants`` for the patch.
-* :release:`1.7.0 <2013-07-26>`
-* :release:`1.6.2 <2013-07-26>`
-* :feature:`925` Added `contrib.files.is_link <.is_link>`. Thanks to `@jtangas`
-  for the patch.
-* :feature:`922` Task argument strings are now displayed when using
-  ``fab -d``. Thanks to Kevin Qiu for the patch.
-* :bug:`912` Leaving ``template_dir`` un-specified when using
-  `.upload_template` in Jinja mode used to cause ``'NoneType' has no attribute
-  'startswith'`` errors. This has been fixed. Thanks to Erick Yellott for catch
-  & to Erick Yellott + Kevin Williams for patches.
-* :feature:`924` Add new env var option :ref:`colorize-errors` to enable
-  coloring errors and warnings. Thanks to Aaron Meurer for the patch.
-* :bug:`593` Non-ASCII character sets in Jinja templates rendered within
-  `.upload_template` would cause ``UnicodeDecodeError`` when uploaded. This has
-  been addressed by encoding as ``utf-8`` prior to upload. Thanks to Sébastien
-  Fievet for the catch.
-* :feature:`908` Support loading SSH keys from memory. Thanks to Caleb Groom
-  for the patch.
-* :bug:`171` Added missing cross-references from ``env`` variables documentation
-  to corresponding command-line options. Thanks to Daniel D. Beck for the
-  contribution.
-* :bug:`884` The password cache feature was not working correctly with
-  password-requiring SSH gateway connections. That's fixed now. Thanks to Marco
-  Nenciarini for the catch.
-* :feature:`826` Enable sudo extraction of compressed archive via `use_sudo`
-  kwarg in `.upload_project`. Thanks to ``@abec`` for the patch.
-* :bug:`694 major` Allow users to work around ownership issues in the default
-  remote login directory: add ``temp_dir`` kwarg for explicit specification of
-  which "bounce" folder to use when calling `.put` with ``use_sudo=True``.
-  Thanks to Devin Bayer for the report & Dieter Plaetinck / Jesse Myers for
-  suggesting the workaround.
-* :bug:`882` Fix a `.get` bug regarding spaces in remote working directory
-  names. Thanks to Chris Rose for catch & patch.
-* :release:`1.6.1 <2013-05-23>`
-* :bug:`868` Substantial speedup of parallel tasks by removing an unnecessary
-  blocking timeout in the ``JobQueue`` loop. Thanks to Simo Kinnunen for the
-  patch.
-* :bug:`328` `.lcd` was no longer being correctly applied to
-  `.upload_template`; this has been fixed. Thanks to Joseph Lawson for the
-  catch.
-* :feature:`812` Add ``use_glob`` option to `.put` so users trying to upload
-  real filenames containing glob patterns (``*``, ``[`` etc) can disable the
-  default globbing behavior. Thanks to Michael McHugh for the patch.
-* :bug:`864 major` Allow users to disable Fabric's auto-escaping in
-  `.run`/`.sudo`.  Thanks to Christian Long and Michael McHugh for the patch.
-* :bug:`870` Changes to shell env var escaping highlighted some extraneous and
-  now damaging whitespace in `with path(): <.path>`. This has been removed and
-  a regression test added.
-* :bug:`871` Use of string mode values in `put(local, remote, mode="NNNN")
-  <.put>` would sometimes cause ``Unsupported operand`` errors. This has been
-  fixed.
-* :bug:`84 major` Fixed problem with missing -r flag in Mac OS X sed version.
-  Thanks to Konrad Hałas for the patch.
-* :bug:`861` Gracefully handle situations where users give a single string
-  literal to ``env.hosts``. Thanks to Bill Tucker for catch & patch.
-* :bug:`367` Expand paths with tilde inside (``contrib.files``). Thanks to
-  Konrad Hałas for catch & patch.
-* :feature:`845 backported` Downstream synchronization option implemented for
-  `~fabric.contrib.project.rsync_project`. Thanks to Antonio Barrero for the
-  patch.
-* :release:`1.6.0 <2013-03-01>`
-* :release:`1.5.4 <2013-03-01>`
-* :bug:`844` Account for SSH config overhaul in Paramiko 1.10 by e.g. updating
-  treatment of ``IdentityFile`` to handle multiple values. **This and related
-  SSH config parsing changes are backwards incompatible**; we are including
-  them in this release because they do fix incorrect, off-spec behavior.
-* :bug:`843` Ensure string ``pool_size`` values get run through ``int()``
-  before deriving final result (stdlib ``min()`` has odd behavior here...).
-  Thanks to Chris Kastorff for the catch.
-* :bug:`839` Fix bug in `~fabric.contrib.project.rsync_project` where IPv6
-  address were not always correctly detected. Thanks to Antonio Barrero for
-  catch & patch.
-* :bug:`587` Warn instead of aborting when :ref:`env.use_ssh_config
-  <use-ssh-config>` is True but the configured SSH conf file doesn't exist.
-  This allows multi-user fabfiles to enable SSH config without causing hard
-  stops for users lacking SSH configs. Thanks to Rodrigo Pimentel for the
-  report.
-* :feature:`821` Add `~fabric.context_managers.remote_tunnel` to allow reverse
-  SSH tunneling (exposing locally-visible network ports to the remote end).
-  Thanks to Giovanni Bajo for the patch.
-* :feature:`823` Add :ref:`env.remote_interrupt <remote-interrupt>` which
-  controls whether Ctrl-C is forwarded to the remote end or is captured locally
-  (previously, only the latter behavior was implemented). Thanks to Geert
-  Jansen for the patch.
-* :release:`1.5.3 <2013-01-28>`
-* :bug:`806` Force strings given to ``getpass`` during password prompts to be
-  ASCII, to prevent issues on some platforms when Unicode is encountered.
-  Thanks to Alex Louden for the patch.
-* :bug:`805` Update `~fabric.context_managers.shell_env` to play nice with
-  Windows (7, at least) systems and `~fabric.operations.local`. Thanks to
-  Fernando Macedo for the patch.
-* :bug:`654` Parallel runs whose sum total of returned data was large (e.g.
-  large return values from the task, or simply a large number of hosts in the
-  host list) were causing frustrating hangs. This has been fixed.
-* :feature:`402` Attempt to detect stale SSH sessions and reconnect when they
-  arise. Thanks to `@webengineer` for the patch.
-* :bug:`791` Cast `~fabric.operations.reboot`'s ``wait`` parameter to a numeric
-  type in case the caller submitted a string by mistake. Thanks to Thomas
-  Schreiber for the patch.
-* :bug:`703 major` Add a ``shell`` kwarg to many methods in
-  `~fabric.contrib.files` to help avoid conflicts with
-  `~fabric.context_managers.cd` and similar.  Thanks to `@mikek` for the patch.
-* :feature:`730` Add :ref:`env.system_known_hosts/--system-known-hosts
-  <system-known-hosts>` to allow loading a user-specified system-level SSH
-  ``known_hosts`` file. Thanks to Roy Smith for the patch.
-* :release:`1.5.2 <2013-01-15>`
-* :feature:`818` Added :ref:`env.eagerly_disconnect <eagerly-disconnect>`
-  option to help prevent pile-up of many open connections.
-* :feature:`706` Added :ref:`env.tasks <env-tasks>`, returning list of tasks to
-  be executed by current ``fab`` command.
-* :bug:`766` Use the variable name of a new-style ``fabric.tasks.Task``
-  subclass object when the object name attribute is undefined.  Thanks to
-  `@todddeluca` for the patch.
-* :bug:`604` Fixed wrong treatment of backslashes in put operation when uploading
-  directory tree on Windows. Thanks to Jason Coombs for the catch and
-  `@diresys` & Oliver Janik for the patch.
-  for the patch.
-* :bug:`792` The newish `~fabric.context_managers.shell_env` context manager
-  was incorrectly omitted from the ``fabric.api`` import endpoint. This has
-  been remedied. Thanks to Vishal Rana for the catch.
-* :feature:`735` Add ``ok_ret_codes`` option to ``env`` to allow alternate
-  return codes to be treated os "ok". Thanks to Andy Kraut for the pull request.
-* :bug:`775` Shell escaping was incorrectly applied to the value of ``$PATH``
-  updates in our shell environment handling, causing (at the very least)
-  `~fabric.operations.local` binary paths to become inoperable in certain
-  situations.  This has been fixed.
-* :feature:`787` Utilize new Paramiko feature allowing us to skip the use of
-  temporary local files when using file-like objects in
-  `~fabric.operations.get`/`~fabric.operations.put`.
-* :feature:`249` Allow specification of remote command timeout value by
-  setting :ref:`env.command_timeout <command-timeout>`. Thanks to Paul
-  McMillan for suggestion & initial patch.
-* Added current host string to prompt abort error messages.
-* :release:`1.5.1 <2012-11-15>`
-* :bug:`776` Fixed serious-but-non-obvious bug in direct-tcpip driven
-  gatewaying (e.g. that triggered by ``-g`` or ``env.gateway``.) Should work
-  correctly now.
-* :bug:`771` Sphinx autodoc helper `~fabric.docs.unwrap_tasks` didn't play nice
-  with ``@task(name=xxx)`` in some situations. This has been fixed.
-* :release:`1.5.0 <2012-11-06>`
-* :release:`1.4.4 <2012-11-06>`
-* :feature:`38` (also :issue:`698`) Implement both SSH-level and
-  ``ProxyCommand``-based gatewaying for SSH traffic. (This is distinct from
-  tunneling non-SSH traffic over the SSH connection, which is :issue:`78` and
-  not implemented yet.)
+- :release:`2.5.0 <2019-08-06>`
+- :support:`-` Update minimum Invoke version requirement to ``>=1.3``.
+- :feature:`1985` Add support for explicitly closing remote subprocess' stdin
+  when local stdin sees an EOF, by implementing a new command-runner method
+  recently added to Invoke; this prevents remote programs that 'follow' stdin
+  from blocking forever.
+- :bug:`- major` Anonymous/'remainder' subprocess execution (eg ``fab -H host
+  -- command``, as opposed to the use of `Connection.run
+  <fabric.connection.Connection.run>` inside tasks) was explicitly specifying
+  ``in_stream=False`` (i.e. "disconnect from stdin") under the hood; this was
+  leftover from early development and prevented use of interactive (or other
+  stdin-reading) programs via this avenue.
+
+  It has been removed; ``cat 'text' | fab -H somehost -- reads-from-stdin`` (or
+  similar use cases) should work again.
+- :support:`-` Removed unnecessary Cryptography version pin from packaging
+  metadata; this was an artifact from early development. At this point in
+  time, only Paramiko's own direct dependency specification should matter.
+
+  This is unlikely to affect anybody's install, since Paramiko has required
+  newer Cryptography versions for a number of years now.
+- :feature:`-` Allow specifying connection timeouts (already available via
+  `~fabric.connection.Connection` constructor argument and configuration
+  option) on the command-line, via :option:`-t/--connect-timeout <-t>`.
+- :feature:`1989` Reinstate command timeouts, by supporting the implementation
+  of that feature in Invoke (`pyinvoke/invoke#539
+  <https://github.com/pyinvoke/invoke/issues/539>`_). Thanks to Israel Fruchter
+  for report and early patchset.
+- :release:`2.4.0 <2018-09-13>`
+- :release:`2.3.2 <2018-09-13>`
+- :release:`2.2.3 <2018-09-13>`
+- :release:`2.1.6 <2018-09-13>`
+- :release:`2.0.5 <2018-09-13>`
+- :feature:`1849` Add `Connection.from_v1
+  <fabric.connection.Connection.from_v1>` (and `Config.from_v1
+  <fabric.config.Config.from_v1>`) for easy creation of modern
+  ``Connection``/``Config`` objects from the currently configured Fabric 1.x
+  environment. Should make upgrading piecemeal much easier for many use cases.
+- :feature:`1780` Add context manager behavior to `~fabric.group.Group`, to
+  match the same feature in `~fabric.connection.Connection`. Feature request by
+  István Sárándi.
+- :feature:`1709` Add `Group.close <fabric.group.Group.close>` to allow closing
+  an entire group's worth of connections at once. Patch via Johannes Löthberg.
+- :bug:`-` Fix a bug preventing tab completion (using the Invoke-level
+  ``--complete`` flag) from completing task names correctly (behavior was to
+  act as if there were never any tasks present, even if there was a valid
+  fabfile nearby).
+- :bug:`1850` Skip over ``ProxyJump`` configuration directives in SSH config
+  data when they would cause self-referential ``RecursionError`` (e.g. due to
+  wildcard-using ``Host`` stanzas which include the jump server itself).
+  Reported by Chris Adams.
+- :bug:`-` Some debug logging was reusing Invoke's logger object, generating
+  log messages "named" after ``invoke`` instead of ``fabric``. This has been
+  fixed by using Fabric's own logger everywhere instead.
+- :bug:`1852` Grant internal `~fabric.connection.Connection` objects created
+  during ``ProxyJump`` based gateways/proxies a copy of the outer
+  ``Connection``'s configuration object. This was not previously done, which
+  among other things meant one could not fully disable SSH config file loading
+  (as the internal ``Connection`` objects would revert to the default
+  behavior). Thanks to Chris Adams for the report.
+- :release:`2.3.1 <2018-08-08>`
+- :bug:`- (2.3+)` Update the new functionality added for :issue:`1826` so it
+  uses ``export``; without this, nontrivial shell invocations like ``command1
+  && command2`` end up only applying the env vars to the first command.
+- :release:`2.3.0 <2018-08-08>`
+- :feature:`1826` Add a new Boolean configuration and
+  `~fabric.connection.Connection` parameter, ``inline_ssh_env``, which (when
+  set to ``True``) changes how Fabric submits shell environment variables to
+  remote servers; this feature helps work around commonly restrictive
+  ``AcceptEnv`` settings on SSH servers. Thanks to Massimiliano Torromeo and
+  Max Arnold for the reports.
+- :release:`2.2.2 <2018-07-31>`
+- :release:`2.1.5 <2018-07-31>`
+- :release:`2.0.4 <2018-07-31>`
+- :bug:`-` Implement ``__lt__`` on `~fabric.connection.Connection` so it can be
+  sorted; this was overlooked when implementing things like ``__eq__`` and
+  ``__hash__``. (No, sorting doesn't usually matter much for this object type,
+  but when you gotta, you gotta...)
+- :support:`1819 backported` Moved example code from the README into the Sphinx
+  landing page so that we could apply doctests; includes a bunch of corrections
+  to invalid example code! Thanks to Antonio Feitosa for the initial catch &
+  patch.
+- :bug:`1749` Improve `~fabric.transfer.Transfer.put` behavior when uploading
+  to directory (vs file) paths, which was documented as working but had not
+  been fully implemented. The local path's basename (or file-like objects'
+  ``.name`` attribute) is now appended to the remote path in this case. Thanks
+  to Peter Uhnak for the report.
+- :feature:`1831` Grant `~fabric.group.Group` (and subclasses) the ability to
+  take arbitrary keyword arguments and pass them onto the internal
+  `~fabric.connection.Connection` constructors. This allows code such as::
+
+    mygroup = Group('host1', 'host2', 'host3', user='admin')
+
+  which was previously impossible without manually stuffing premade
+  ``Connection`` objects into `Group.from_connections
+  <fabric.group.Group.from_connections>`.
+- :bug:`1762` Fix problem where lower configuration levels' setting of
+  ``connect_kwargs.key_filename`` were being overwritten by the CLI
+  ``--identity`` flag's value...even when that value was the empty list.
+  CLI-given values are supposed to win, but not quite that hard. Reported by
+  ``@garu57``.
+- :support:`1653 backported` Clarify `~fabric.transfer.Transfer` API docs
+  surrounding remote file paths, such as the lack of tilde expansion (a buggy
+  and ultimately unnecessary v1 feature). Thanks to ``@pint12`` for bringing it
+  up.
+- :release:`2.2.1 <2018-07-18>`
+- :bug:`1824` The changes implementing :issue:`1772` failed to properly account
+  for backwards compatibility with Invoke-level task objects. This has been
+  fixed; thanks to ``@ilovezfs`` and others for the report.
+- :release:`2.2.0 <2018-07-13>`
+- :release:`2.1.4 <2018-07-13>`
+- :release:`2.0.3 <2018-07-13>`
+- :bug:`-` The `fabric.testing.fixtures.remote` pytest fixture was found to not
+  be properly executing expectation/sanity tests on teardown; this was an
+  oversight and has been fixed.
+- :support:`-` Updated the minimum required Invoke version to ``1.1``.
+- :feature:`1772` ``@hosts`` is back -- as a `@task <fabric.tasks.task>`/`Task
+  <fabric.tasks.Task>` parameter of the same name. Acts much like a per-task
+  :option:`--hosts`, but can optionally take dicts of
+  `fabric.connection.Connection` kwargs as well as the typical shorthand host
+  strings.
 
-    * Thanks in no particular order to Erwin Bolwidt, Oskari Saarenmaa, Steven
-      Noonan, Vladimir Lazarenko, Lincoln de Sousa, Valentino Volonghi, Olle
-      Lundberg and Github user `@acrish` for providing the original patches to
-      both Fabric and Paramiko.
+  .. note::
+    As of this change, we are now recommending the use of the
+    new-in-this-release Fabric-level `@task <fabric.tasks.task>`/`Task
+    <fabric.tasks.Task>` objects instead of their Invoke counterparts, even if
+    you're not using the ``hosts`` kwarg -- it will help future-proof your code
+    for similar feature-adds later, and generally be less confusing than having
+    mixed Invoke/Fabric imports for these object types.
+
+- :feature:`1766` Reinstate support for use as ``python -m fabric``, which (as
+  in v1) now behaves identically to invoking ``fab``. Thanks to
+  ``@RupeshPatro`` for the original patchset.
+- :bug:`1753` Set one of our test modules to skip user/system SSH config file
+  loading by default, as it was too easy to forget to do so for tests aimed at
+  related functionality. Reported by Chris Rose.
+- :release:`2.1.3 <2018-05-24>`
+- :bug:`-` Our packaging metadata lacked a proper ``MANIFEST.in`` and thus some
+  distributions were not including ancillary directories like tests and
+  documentation. This has been fixed.
+- :bug:`-` Our ``packages=`` argument to ``setuptools.setup`` was too specific
+  and did not allow for subpackages...such as the newly added
+  ``fabric.testing``. Fixed now.
+- :release:`2.1.2 <2018-05-24>`
+- :bug:`-` Minor fix to ``extras_require`` re: having ``fabric[pytest]``
+  encompass the contents of ``fabric[testing]``.
+- :release:`2.1.1 <2018-05-24>`
+- :bug:`-` Somehow neglected to actually add ``extras_require`` to our
+  ``setup.py`` to enable ``pip install fabric[testing]`` et al. This has been
+  fixed. We hope.
+- :release:`2.1.0 <2018-05-24>`
+- :release:`2.0.2 <2018-05-24>`
+- :feature:`-` Exposed our previously internal test helpers for use by
+  downstream test suites, as the :ref:`fabric.testing <testing-subpackage>`
+  subpackage.
 
-* :feature:`684 backported` (also :issue:`569`) Update how
-  `~fabric.decorators.task` wraps task functions to preserve additional
-  metadata; this allows decorated functions to play nice with Sphinx autodoc.
-  Thanks to Jaka Hudoklin for catch & patch.
-* :support:`103` (via :issue:`748`) Long standing Sphinx autodoc issue requiring
-  error-prone duplication of function signatures in our API docs has been
-  fixed. Thanks to Alex Morega for the patch.
-* :bug:`767 major` Fix (and add test for) regression re: having linewise output
-  automatically activate when parallelism is in effect. Thanks to Alexander
-  Fortin and Dustin McQuay for the bug reports.
-* :bug:`736 major` Ensure context managers that build env vars play nice with
-  ``contextlib.nested`` by deferring env var reference to entry time, not call
-  time. Thanks to Matthew Tretter for catch & patch.
-* :feature:`763` Add ``--initial-password-prompt`` to allow prefilling the
-  password cache at the start of a run. Great for sudo-powered parallel runs.
-* :feature:`665` (and #629) Update `~fabric.contrib.files.upload_template` to
-  have a more useful return value, namely that of its internal
-  `~fabric.operations.put` call. Thanks to Miquel Torres for the catch &
-  Rodrigue Alcazar for the patch.
-* :feature:`578` Add ``name`` argument to `~fabric.decorators.task` (:ref:`docs
-  <task-decorator-arguments>`) to allow overriding of the default "function
-  name is task name" behavior. Thanks to Daniel Simmons for catch & patch.
-* :feature:`761` Allow advanced users to parameterize ``fabric.main.main()`` to
-  force loading of specific fabfiles.
-* :bug:`749` Gracefully work around calls to ``fabric.version`` on systems
-  lacking ``/bin/sh`` (which causes an ``OSError`` in ``subprocess.Popen``
-  calls.)
-* :feature:`723` Add the ``group=`` argument to
-  `~fabric.operations.sudo`. Thanks to Antti Kaihola for the pull request.
-* :feature:`725` Updated `~fabric.operations.local` to allow override
-  of which local shell is used. Thanks to Mustafa Khattab.
-* :bug:`704 major` Fix up a bunch of Python 2.x style ``print`` statements to
-  be forwards compatible. Thanks to Francesco Del Degan for the patch.
-* :feature:`491` (also :feature:`385`) IPv6 host string support. Thanks to Max
-  Arnold for the patch.
-* :feature:`699` Allow `name` attribute on file-like objects for get/put. Thanks
-  to Peter Lyons for the pull request.
-* :bug:`711 major` `~fabric.sftp.get` would fail when filenames had % in their
-  path.  Thanks to John Begeman
-* :bug:`702 major` `~fabric.operations.require` failed to test for "empty"
-  values in the env keys it checks (e.g.
-  ``require('a-key-whose-value-is-an-empty-list')`` would register a successful
-  result instead of alerting that the value was in fact empty. This has been
-  fixed, thanks to Rich Schumacher.
-* :bug:`718` ``isinstance(foo, Bar)`` is used in `~fabric.main` instead
-  of ``type(foo) == Bar`` in order to fix some edge cases.
-  Thanks to Mikhail Korobov.
-* :bug:`693` Fixed edge case where ``abort`` driven failures within parallel
-  tasks could result in a top level exception (a ``KeyError``) regarding error
-  handling. Thanks to Marcin Kuźmiński for the report.
-* :support:`681 backported` Fixed outdated docstring for
-  `~fabric.decorators.runs_once` which claimed it would get run multiple times
-  in parallel mode. That behavior was fixed in an earlier release but the docs
-  were not updated. Thanks to Jan Brauer for the catch.
-* :release:`1.4.3 <2012-07-06>`
-* :release:`1.3.8 <2012-07-06>`
-* :feature:`263` Shell environment variable support for
-  `~fabric.operations.run`/`~fabric.operations.sudo` added in the form of the
-  `~fabric.context_managers.shell_env` context manager. Thanks to Oliver
-  Tonnhofer for the original pull request, and to Kamil Kisiel for the final
-  implementation.
-* :feature:`669` Updates to our Windows compatibility to rely more heavily on
-  cross-platform Python stdlib implementations. Thanks to Alexey Diyan for the
-  patch.
-* :bug:`671` :ref:`reject-unknown-hosts` sometimes resulted in a password
-  prompt instead of an abort. This has been fixed. Thanks to Roy Smith for the
-  report.
-* :bug:`659` Update docs to reflect that `~fabric.operations.local` currently
-  honors :ref:`env.path <env-path>`. Thanks to `@floledermann
-  <https://github.com/floledermann>`_ for the catch.
-* :bug:`652` Show available commands when aborting on invalid command names.
-* :support:`651 backported` Added note about nesting ``with`` statements on
-  Python 2.6+.  Thanks to Jens Rantil for the patch.
-* :bug:`649` Don't swallow non-``abort``-driven exceptions in parallel mode.
-  Fabric correctly printed such exceptions, and returned them from
-  `~fabric.tasks.execute`, but did not actually cause the child or parent
-  processes to halt with a nonzero status. This has been fixed.
-  `~fabric.tasks.execute` now also honors :ref:`env.warn_only <warn_only>` so
-  users may still opt to call it by hand and inspect the returned exceptions,
-  instead of encountering a hard stop. Thanks to Matt Robenolt for the catch.
-* :feature:`241` Add the command executed as a ``.command`` attribute to the
-  return value of `~fabric.operations.run`/`~fabric.operations.sudo`. (Also
-  includes a second attribute containing the "real" command executed, including
-  the shell wrapper and any escaping.)
-* :feature:`646` Allow specification of which local streams to use when
-  `~fabric.operations.run`/`~fabric.operations.sudo` print the remote
-  stdout/stderr, via e.g. ``run("command", stderr=sys.stdout)``.
-* :support:`645 backported` Update Sphinx docs to work well when run out of a
-  source tarball as opposed to a Git checkout. Thanks again to `@Arfrever` for
-  the catch.
-* :support:`640 backported` (also :issue:`644`) Update packaging manifest so
-  sdist tarballs include all necessary test & doc files. Thanks to Mike Gilbert
-  and `@Arfrever` for catch & patch.
-* :feature:`627` Added convenient ``quiet`` and ``warn_only`` keyword arguments
-  to `~fabric.operations.run`/`~fabric.operations.sudo` which are aliases for
-  ``settings(hide('everything'), warn_only=True)`` and
-  ``settings(warn_only=True)``, respectively. (Also added corresponding
-  `context <fabric.context_managers.quiet>` `managers
-  <fabric.context_managers.warn_only>`.) Useful for remote program calls which
-  are expected to fail and/or whose output doesn't need to be shown to users.
-* :feature:`633` Allow users to turn off host list deduping by setting
-  :ref:`env.dedupe_hosts <dedupe_hosts>` to ``False``. This enables running the
-  same task multiple times on a single host, which was previously not possible.
-* :support:`634 backported` Clarified that `~fabric.context_managers.lcd` does
-  no special handling re: the user's current working directory, and thus
-  relative paths given to it will be relative to ``os.getcwd()``. Thanks to
-  `@techtonik <https://github.com/techtonik>`_ for the catch.
-* :release:`1.4.2 <2012-05-07>`
-* :release:`1.3.7 <2012-05-07>`
-* :bug:`562` Agent forwarding would error out or freeze when multiple uses of
-  the forwarded agent were used per remote invocation (e.g. a single
-  `~fabric.operations.run` command resulting in multiple Git or SVN checkouts.)
-  This has been fixed thanks to Steven McDonald and GitHub user `@lynxis`.
-* :support:`626 backported` Clarity updates to the tutorial. Thanks to GitHub
-  user `m4z` for the patches.
-* :bug:`625` `~fabric.context_managers.hide`/`~fabric.context_managers.show`
-  did not correctly restore prior display settings if an exception was raised
-  inside the block. This has been fixed.
-* :bug:`624` Login password prompts did not always display the username being
-  authenticated for. This has been fixed. Thanks to Nick Zalutskiy for catch &
-  patch.
-* :bug:`617` Fix the ``clean_revert`` behavior of
-  `~fabric.context_managers.settings` so it doesn't ``KeyError`` for newly
-  created settings keys. Thanks to Chris Streeter for the catch.
-* :feature:`615` Updated `~fabric.operations.sudo` to honor the new setting
-  :ref:`env.sudo_user <sudo_user>` as a default for its ``user`` kwarg.
-* :bug:`616` Add port number to the error message displayed upon connection
-  failures.
-* :bug:`609` (and :issue:`564`) Document and clean up :ref:`env.sudo_prefix
-  <sudo_prefix>` so it can be more easily modified by users facing uncommon
-  use cases. Thanks to GitHub users `3point2` for the cleanup and `SirScott`
-  for the documentation catch.
-* :bug:`610` Change detection of ``env.key_filename``'s type (added as part of
-  SSH config support in 1.4) so it supports arbitrary iterables. Thanks to
-  Brandon Rhodes for the catch.
-* :release:`1.4.1 <2012-04-04>`
-* :release:`1.3.6 <2012-04-04>`
-* :bug:`608` Add ``capture`` kwarg to `~fabric.contrib.project.rsync_project`
-  to aid in debugging rsync problems.
-* :bug:`607` Allow `~fabric.operations.local` to display stdout/stderr when it
-  warns/aborts, if it was capturing them.
-* :bug:`395` Added :ref:`an FAQ entry <init-scripts-pty>` detailing how to
-  handle init scripts which misbehave when a pseudo-tty is allocated.
-* :bug:`568` `~fabric.tasks.execute` allowed too much of its internal state
-  changes (to variables such as ``env.host_string`` and ``env.parallel``) to
-  persist after execution completed; this caused a number of different
-  incorrect behaviors. `~fabric.tasks.execute` has been overhauled to clean up
-  its own state changes -- while preserving any state changes made by the task
-  being executed.
-* :bug:`584` `~fabric.contrib.project.upload_project` did not take explicit
-  remote directory location into account when untarring, and now uses
-  `~fabric.context_managers.cd` to address this. Thanks to Ben Burry for the
-  patch.
-* :bug:`458` `~fabric.decorators.with_settings` did not perfectly match
-  `~fabric.context_managers.settings`, re: ability to inline additional context
-  managers. This has been corrected. Thanks to Rory Geoghegan for the patch.
-* :bug:`499` `contrib.files.first <fabric.contrib.files.first>` used an
-  outdated function signature in its wrapped `~fabric.contrib.files.exists`
-  call. This has been fixed. Thanks to Massimiliano Torromeo for catch & patch.
-* :bug:`551` ``--list`` output now detects terminal window size and truncates
-  (or doesn't truncate) accordingly. Thanks to Horacio G. de Oro for the
-  initial pull request.
-* :bug:`572` Parallel task aborts (as oppposed to unhandled exceptions) now
-  correctly print their abort messages instead of tracebacks, and cause the
-  parent process to exit with the correct (nonzero) return code. Thanks to Ian
-  Langworth for the catch.
-* :bug:`306` Remote paths now use posixpath for a separator. Thanks to Jason
-  Coombs for the patch.
-* :release:`1.4.0 <2012-02-13>`
-* :release:`1.3.5 <2012-02-13>`
-* :release:`1.2.6 <2012-02-13>`
-* :release:`1.1.8 <2012-02-13>`
-* :bug:`495` Fixed documentation example showing how to subclass
-  `~fabric.tasks.Task`. Thanks to Brett Haydon for the catch and Mark Merritt
-  for the patch.
-* :bug:`410` Fixed a bug where using the `~fabric.decorators.task` decorator
-  inside/under another decorator such as `~fabric.decorators.hosts` could cause
-  that task to become invalid when invoked by name (due to how old-style vs
-  new-style tasks are detected.) Thanks to Dan Colish for the initial patch.
-* :feature:`559` `~fabric.contrib.project.rsync_project` now allows users to
-  append extra SSH-specific arguments to ``rsync``'s ``--rsh`` flag.
-* :feature:`138` :ref:`env.port <port>` may now be written to at fabfile module
-  level to set a default nonstandard port number. Previously this value was
-  read-only.
-* :feature:`3` Fabric can now load a subset of SSH config functionality
-  directly from your local ``~/.ssh/config`` if :ref:`env.use_ssh_config
-  <use-ssh-config>` is set to ``True``. See :ref:`ssh-config` for details.
-  Thanks to Kirill Pinchuk for the initial patch.
-* :feature:`12` Added the ability to try connecting multiple times to
-  temporarily-down remote systems, instead of immediately failing. (Default
-  behavior is still to only try once.) See :ref:`env.timeout <timeout>` and
-  :ref:`env.connection_attempts <connection-attempts>` for controlling both
-  connection timeouts and total number of attempts. `~fabric.operations.reboot`
-  has also been overhauled (but practically deprecated -- see its updated
-  docs.)
-* :feature:`474` `~fabric.tasks.execute` now allows you to access the executed
-  task's return values, by itself returning a dictionary whose keys are the
-  host strings executed against.
-* :bug:`487 major` Overhauled the regular expression escaping performed in
-  `~fabric.contrib.files.append` and `~fabric.contrib.files.contains` to try
-  and handle more corner cases. Thanks to Neilen Marais for the patch.
-* :support:`532` Reorganized and cleaned up the output of ``fab --help``.
-* :feature:`8` Added ``--skip-bad-hosts``/:ref:`env.skip_bad_hosts
-  <skip-bad-hosts>` option to allow skipping past temporarily down/unreachable
-  hosts.
-* :feature:`13` Env vars may now be set at runtime via the new ``--set``
-  command-line flag.
-* :feature:`506` A new :ref:`output alias <output-aliases>`, ``commands``, has
-  been added, which allows hiding remote stdout and local "running command X"
-  output lines.
-* :feature:`72` SSH agent forwarding support has made it into Fabric's SSH
-  library, and hooks for using it have been added (disabled by default; use
-  ``-A`` or :ref:`env.forward_agent <forward-agent>` to enable.) Thanks to Ben
-  Davis for porting an existing Paramiko patch to `ssh` and providing the
-  necessary tweak to Fabric.
-* :release:`1.3.4 <2012-01-12>`
-* :bug:`492` `@parallel <fabric.decorators.parallel>` did not automatically
-  trigger :ref:`linewise output <linewise-output>`, as was intended. This has
-  been fixed. Thanks to Brandon Huey for the catch.
-* :bug:`510` Parallel mode is incompatible with user input, such as
-  password/hostname prompts, and was causing cryptic `Operation not supported
-  by device` errors when such prompts needed to be displayed. This behavior has
-  been updated to cleanly and obviously ``abort`` instead.
-* :bug:`494` Fixed regression bug affecting some `env` values such as
-  `env.port` under parallel mode. Symptoms included
-  `~fabric.contrib.project.rsync_project` bailing out due to a None port value
-  when run under `@parallel <fabric.decorators.parallel>`. Thanks to Rob
-  Terhaar for the report.
-* :bug:`339` Don't show imported `~fabric.colors` members in ``--list``
-  output.  Thanks to Nick Trew for the report.
-* :release:`1.3.3 <2011-11-23>`
-* :release:`1.2.5 <2011-11-23>`
-* :release:`1.1.7 <2011-11-23>`
-* :bug:`441` Specifying a task module as a task on the command line no longer
-  blows up but presents the usual "no task by that name" error message instead.
-  Thanks to Mitchell Hashimoto for the catch.
-* :bug:`475` Allow escaping of equals signs in per-task args/kwargs.
-* :bug:`450` Improve traceback display when handling ``ImportError`` for
-  dependencies. Thanks to David Wolever for the patches.
-* :bug:`446` Add QNX to list of secondary-case `~fabric.contrib.files.sed`
-  targets. Thanks to Rodrigo Madruga for the tip.
-* :bug:`443` `~fabric.contrib.files.exists` didn't expand tildes; now it does.
-  Thanks to Riccardo Magliocchetti for the patch.
-* :bug:`437` `~fabric.decorators.with_settings` now correctly preserves the
-  wrapped function's docstring and other attributes. Thanks to Eric Buckley for
-  the catch and Luke Plant for the patch.
-* :bug:`400` Handle corner case of systems where ``pwd.getpwuid`` raises
-  ``KeyError`` for the user's UID instead of returning a valid string. Thanks
-  to Dougal Matthews for the catch.
-* :bug:`397` Some poorly behaved objects in third party modules triggered
-  exceptions during Fabric's "classic or new-style task?" test. A fix has been
-  added which tries to work around these.
-* :bug:`341` `~fabric.contrib.files.append` incorrectly failed to detect that
-  the line(s) given already existed in files hidden to the remote user, and
-  continued appending every time it ran. This has been fixed. Thanks to
-  Dominique Peretti for the catch and Martin Vilcans for the patch.
-* :bug:`342` Combining `~fabric.context_managers.cd` with
-  `~fabric.operations.put` and its ``use_sudo`` keyword caused an unrecoverable
-  error. This has been fixed. Thanks to Egor M for the report.
-* :bug:`482` Parallel mode should imply linewise output; omission of this
-  behavior was an oversight.
-* :bug:`230` Fix regression re: combo of no fabfile & arbitrary command use.
-  Thanks to Ali Saifee for the catch.
-* :release:`1.3.2 <2011-11-07>`
-* :release:`1.2.4 <2011-11-07>`
-* :release:`1.1.6 <2011-11-07>`
-* :support:`459 backported` Update our `setup.py` files to note that PyCrypto
-  released 2.4.1, which fixes the setuptools problems.
-* :support:`467 backported` (also :issue:`468`, :issue:`469`) Handful of
-  documentation clarification tweaks. Thanks to Paul Hoffman for the patches.
-* :release:`1.3.1 <2011-10-24>`
-* :bug:`457` Ensured that Fabric fast-fails parallel tasks if any child
-  processes encountered errors. Previously, multi-task invocations would
-  continue to the 2nd, etc task when failures occurred, which does not fit with
-  how Fabric usually behaves. Thanks to Github user ``sdcooke`` for the report
-  and Morgan Goose for the fix.
-* :release:`1.3.0 <2011-10-23>`
-* :release:`1.2.3 <2011-10-23>`
-* :release:`1.1.5 <2011-10-23>`
-* :release:`1.0.5 <2011-10-23>`
-* :support:`275` To support an edge use case of the features released in
-  :issue:`19`, and to lay the foundation for :issue:`275`, we have forked
-  Paramiko into the `Python 'ssh' library <http://pypi.python.org/pypi/ssh/>`_
-  and changed our dependency to it for Fabric 1.3 and higher. This may have
-  implications for the more uncommon install use cases, and package
-  maintainers, but we hope to iron out any issues as they come up.
-* :bug:`323` `~fabric.operations.put` forgot how to expand leading tildes in
-  the remote file path. This has been corrected. Thanks to Piet Delport for the
-  catch.
-* :feature:`21` It is now possible, using the new `~fabric.tasks.execute` API
-  call, to execute task objects (by reference or by name) from within other
-  tasks or in library mode. `~fabric.tasks.execute` honors the other tasks'
-  `~fabric.decorators.hosts`/`~fabric.decorators.roles` decorators, and also
-  supports passing in explicit host and/or role arguments.
-* :feature:`19` Tasks may now be optionally executed in parallel. Please see
-  the :ref:`parallel execution docs <parallel-execution>` for details. Major
-  thanks to Morgan Goose for the initial implementation.
-* :bug:`182` During display of remote stdout/stderr, Fabric occasionally
-  printed extraneous line prefixes (which in turn sometimes overwrote wrapped
-  text.) This has been fixed.
-* :bug:`430` Tasks decorated with `~fabric.decorators.runs_once` printed
-  extraneous 'Executing...' status lines on subsequent invocations. This is
-  noisy at best and misleading at worst, and has been corrected. Thanks to
-  Jacob Kaplan-Moss for the report.
-* :release:`1.2.2 <2011-09-01>`
-* :release:`1.1.4 <2011-09-01>`
-* :release:`1.0.4 <2011-09-01>`
-* :bug:`252` `~fabric.context_managers.settings` would silently fail to set
-  ``env`` values for keys which did not exist outside the context manager
-  block.  It now works as expected. Thanks to Will Maier for the catch and
-  suggested solution.
-* :support:`393 backported` Fixed a typo in an example code snippet in the task
-  docs.  Thanks to Hugo Garza for the catch.
-* :bug:`396` ``--shortlist`` broke after the addition of ``--list-format`` and
-  no longer displayed the short list format correctly. This has been fixed.
-* :bug:`373` Re-added missing functionality preventing :ref:`host exclusion
-  <excluding-hosts>` from working correctly.
-* :bug:`303` Updated terminal size detection to correctly skip over non-tty
-  stdout, such as when running ``fab taskname | other_command``.
-* :release:`1.2.1 <2011-08-21>`
-* :release:`1.1.3 <2011-08-21>`
-* :release:`1.0.3 <2011-08-21>`
-* :bug:`417` :ref:`abort-on-prompts` would incorrectly abort when set to True,
-  even if both password and host were defined. This has been fixed. Thanks to
-  Valerie Ishida for the report.
-* :support:`416 backported` Updated documentation to reflect move from Redmine
-  to Github.
-* :bug:`389` Fixed/improved error handling when Paramiko import fails. Thanks
-  to Brian Luft for the catch.
-* :release:`1.2.0 <2011-07-12>`
-* :feature:`22` Enhanced `@task <fabric.decorators.task>` to add :ref:`aliasing
-  <task-aliases>`, :ref:`per-module default tasks <default-tasks>`, and
-  :ref:`control over the wrapping task class <task-decorator-and-classes>`.
-  Thanks to Travis Swicegood for the initial work and collaboration.
-* :bug:`380` Improved unicode support when testing objects for being
-  string-like. Thanks to Jiri Barton for catch & patch.
-* :support:`382` Experimental overhaul of changelog formatting & process to
-  make supporting multiple lines of development less of a hassle.
-* :release:`1.1.2 <2011-07-07>`
-* :release:`1.0.2 <2011-06-24>`
+  .. note::
+    As this code requires non-production dependencies, we've also updated our
+    packaging metadata to publish some setuptools "extras", ``fabric[testing]``
+    (base) and ``fabric[pytest]`` (for pytest users).
+
+- :support:`1761 backported` Integration tests were never added to Travis or
+  ported to pytest before 2.0's release; this has been addressed.
+- :support:`1759 backported` Apply the ``black`` code formatter to the codebase
+  and engage it on Travis-CI. Thanks to Chris Rose.
+- :support:`1745 backported` Wrap any imports of ``invoke.vendor.*`` with
+  ``try``/``except`` such that downstream packages which have removed
+  ``invoke.vendor`` are still able to function by using stand-alone
+  dependencies. Patch courtesy of Othmane Madjoudj.
+- :release:`2.0.1 <2018-05-14>`
+- :bug:`1740` A Python 3 wheel was not uploaded during the previous release as
+  expected; it turned out we were lacking the typical 'build universal wheels'
+  setting in our ``setup.cfg`` (due to copying it from the one other project in
+  our family of projects which explicitly cannot build universal wheels!) This
+  has been fixed and a proper universal wheel is now built.
+- :release:`2.0.0 <2018-05-08>`
+- :feature:`-` Rewrite for 2.0! See :ref:`upgrading`.
diff -Nru fabric-1.14.0/sites/www/changelog-v1.rst fabric-2.5.0/sites/www/changelog-v1.rst
--- fabric-1.14.0/sites/www/changelog-v1.rst	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/sites/www/changelog-v1.rst	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,832 @@
+===============
+Changelog (1.x)
+===============
+
+.. note::
+    This is the changelog for the legacy 1.x version of Fabric. For the current
+    (2.0+) changelog, please see :doc:`the main changelog </changelog>`.
+
+* :release:`1.14.1 <2018-11-27>`
+* :bug:`1341` (via :issue:`1586`) Attempt to ``rm -f`` the temporary file used
+  by ``put``'s sudo mode, when exceptions are encountered; previously, the
+  internal ``sudo mv`` call could potentially fail and leave the file around.
+  Thanks to Andrei Sura for the report and Uku Loskit for the fix.
+* :bug:`1242` (via :issue:`1243`) `~fabric.contrib.project.rsync_project`: only
+  supply the ``-p <number>`` option to generated ``rsync`` commands when the
+  port number differs from the default; this allows removing ``--rsh`` entirely
+  most of the time, and thus enables things like using rsync's daemon mode on
+  the remote end. Reported & patched by Arnaud Rocher.
+* :bug:`1227` Remove a bash/zsh-ism from
+  `~fabric.contrib.files.upload_template` when backing up the target file,
+  preventing issues on simpler remote shells. Patch courtesy of Paul
+  Chakravarti.
+* :bug:`983` Move a ``getpass`` import inside a Windows-oriented
+  ``try``/``except ImportError`` so password prompting is less likely to
+  explode on certain systems. Thanks to ``@dongweiming`` for the patch.
+* :support:`- backported` Update packaging metadata so wheel archives include
+  the ``LICENSE`` file.
+* :release:`1.14.0 <2017-08-25>`
+* :feature:`1475` Honor ``env.timeout`` when opening new remote sessions (as
+  opposed to the initial overall connection, which already honored timeout
+  settings.) Thanks to ``@EugeniuZ`` for the report & ``@jrmsgit`` for the
+  first draft of the patch.
+
+  .. note::
+    This feature only works with Paramiko 1.14.3 and above; if your Paramiko
+    version is older, no timeout can be set, and the previous behavior will
+    occur instead.
+
+* :release:`1.13.2 <2017-04-24>`
+* :release:`1.12.2 <2017-04-24>`
+* :bug:`1542` (via :issue:`1543`) Catch Paramiko-level gateway connection
+  errors (``ChannelError``) when raising ``NetworkError``; this prevents an
+  issue where gateway related issues were being treated as authentication
+  errors. Thanks to Charlie Stanley for catch & patch.
+* :bug:`1555` Multiple simultaneous `~fabric.operations.get` and/or
+  `~fabric.operations.put` with ``use_sudo=True`` and for the same remote host
+  and path could fail unnecessarily. Thanks ``@arnimarj`` for the report and
+  Pierce Lopez for the patch.
+* :bug:`1427` (via :issue:`1428`) Locate ``.pyc`` files when searching for
+  fabfiles to load; previously we only used the presence of ``.py`` files to
+  determine whether loading should be attempted. Credit: Ray Chen.
+* :bug:`1294` fix text escaping for `~fabric.contrib.files.contains` and
+  `~fabric.contrib.files.append` which would fail if the text contained e.g.
+  ``>``. Thanks to ``@ecksun`` for report & Pierce Lopez for the patch.
+* :support:`1065 backported` Fix incorrect SSH config reference in the docs for
+  ``env.keepalive``; it corresponds to ``ServerAliveInterval``, not
+  ``ClientAliveInterval``. Credit: Harry Percival.
+* :bug:`1574` `~fabric.contrib.project.upload_project` failed for folder in
+  current directory specified without any path separator. Thanks ``@aidanmelen``
+  for the report and Pierce Lopez for the patch.
+* :support:`1590 backported` Replace a reference to ``fab`` in a test
+  subprocess, to use the ``python -m <package>`` style instead; this allows
+  ``python setup.py test`` to run the test suite without having Fabric already
+  installed. Thanks to ``@BenSturmfels`` for catch & patch.
+* :support:`- backported` Backport :issue:`1462` to 1.12.x (was previously only
+  backported to 1.13.x.)
+* :support:`1416 backported` Add explicit "Python 2 only" note to ``setup.py``
+  trove classifiers to help signal that fact to various info-gathering tools.
+  Patch courtesy of Gavin Bisesi.
+* :bug:`1526` Disable use of PTY and shell for a background command execution
+  within ``contrib.sed``, preventing a small class of issues on some
+  platforms/environments. Thanks to ``@doflink`` for the report and Pierce
+  Lopez for the final patch.
+* :support:`1539 backported` Add documentation for ``env.output_prefix``.
+  Thanks ``@jphalip``.
+* :bug:`1514` Compatibility with Python 2.5 was broken by using the ``format()``
+  method of a string (only in 1.11+). Report by ``@pedrudehuere``.
+* :release:`1.13.1 <2016-12-09>`
+* :bug:`1462` Make a PyCrypto-specific import and method call optional to avoid
+  ``ImportError`` problems under Paramiko 2.x. Thanks to Alex Gaynor for catch
+  & patch!
+* :release:`1.13.0 <2016-12-09>`
+* :support:`1461` Update setup requirements to allow Paramiko 2.x, now that
+  it's stable and been out in the wild for some time. Paramiko 1.x still works
+  like it always did; the only change to Paramiko 2 was the backend moving from
+  PyCrypto to Cryptography.
+
+  .. warning::
+    If you are upgrading an existing environment, the install dependencies have
+    changed; please see Paramiko's installation docs for details:
+    http://www.paramiko.org/installing.html
+
+* :release:`1.12.1 <2016-12-05>`
+* :release:`1.11.3 <2016-12-05>`
+* :release:`1.10.5 <2016-12-05>`
+* :bug:`1470` When using ``fabric.operations.get`` with glob expressions, a lack
+  of matches for the glob would result in an empty file named after the glob
+  expression (in addition to raising an error). This has been fixed so the
+  empty file is no longer generated. Thanks to Georgy Kibardin for the catch &
+  initial patch.
+* :feature:`1495` Update the internals of ``fabric.contrib.files`` so its
+  members work with SSH servers running on Windows. Thanks to Hamdi Sahloul for
+  the patch.
+* :support:`1483 backported` (also re: :issue:`1386`, :issue:`1374`,
+  :issue:`1300`) Add an FAQ about quote problems in remote
+  ``csh`` causing issues with Fabric's shell-wrapping and quote-escaping.
+  Thanks to Michael Radziej for the update.
+* :support:`1379 backported` (also :issue:`1464`) Clean up a lot of unused
+  imports and similar cruft (many found via ``flake8 --select E4``). Thanks to
+  Mathias Ertl for the original patches.
+* :bug:`1458` Detect ``known_hosts``-related instances of
+  ``paramiko.SSHException`` and prevent them from being handled like
+  authentication errors (which is the default behavior). This fixes
+  issues with incorrect password prompts or prompt-related exceptions when
+  using ``reject_unknown_hosts`` and encountering missing or bad
+  ``known_hosts`` entries. Thanks to Lukáš Doktor for catch & patch.
+* :release:`1.12.0 <2016-07-25>`
+* :release:`1.11.2 <2016-07-25>`
+* :release:`1.10.4 <2016-07-25>`
+* :feature:`1491` Implement ``sudo``-specific password caching. This can be
+  used to work around issues where over-eager submission of ``env.password`` at
+  login time causes authentication problems (e.g. during two-factor auth).
+* :bug:`1447` Fix a relative import in ``fabric.network`` to be
+  correctly/consistently absolute instead. Thanks to ``@bildzeitung`` for catch
+  & patch.
+* :release:`1.11.1 <2016-04-09>`
+* :bug:`- (==1.11)` Bumped version to ``1.11.1`` due to apparently accidentally
+  uploading a false ``1.11.0`` to PyPI sometime in the past (PyPI is secure &
+  prevents reusing deleted filenames.) We have no memory of this, but databases
+  don't lie!
+* :release:`1.11.0 <2016-04-09>`
+* :release:`1.10.3 <2016-04-09>`
+* :bug:`1135` (via :issue:`1241`) Modified order of operations in
+  ``fabric.operations.run``/``fabric.operations.sudo`` to apply environment vars
+  before prefixing commands (instead of after). Report by ``@warsamebashir``,
+  patch by Curtis Mattoon.
+* :feature:`1203` (via :issue:`1240`) Add a ``case_sensitive`` kwarg to
+  ``fabric.contrib.files.contains`` (which toggles use of ``egrep -i``). Report
+  by ``@xoul``, patch by Curtis Mattoon.
+* :feature:`800` Add ``capture_buffer_size`` kwarg to
+  ``fabric.operations.run``/``fabric.operations.sudo`` so users can limit memory
+  usage in situations where subprocesses generate very large amounts of
+  stdout/err. Thanks to Jordan Starcher for the report & Omri Bahumi for an
+  early version of the patchset.
+* :feature:`1161` Add ``use_sudo`` kwarg to ``fabric.operations.reboot``.
+  Credit: Bryce Verdier.
+* :support:`943 backported` Tweak ``env.warn_only`` docs to note that it
+  applies to all operations, not just ``run``/``sudo``. Thanks ``@akitada``.
+* :feature:`932` Add a ``temp_dir`` kwarg to
+  ``fabric.contrib.files.upload_template`` which is passed into its inner
+  ``fabric.operations.put`` call. Thanks to ``@nburlett`` for the patch.
+* :support:`1257 backported` Add notes to the usage docs for ``fab`` regarding
+  the program's exit status. Credit: ``@koalaman``.
+* :feature:`1261` Expose Paramiko's Kerberos functionality as Fabric config
+  vars & command-line options. Thanks to Ramanan Sivaranjan for catch & patch,
+  and to Johannes Löthberg & Michael Bennett for additional testing.
+* :feature:`1271` Allow users whose fabfiles use ``fabric.colors`` to disable
+  colorization at runtime by specifying ``FABRIC_DISABLE_COLORS=1`` (or any
+  other non-empty value). Credit: Eric Berg.
+* :feature:`1326` Make ``fabric.contrib.project.rsync_project`` aware of
+  ``env.gateway``, using a ``ProxyCommand`` under the hood. Credit: David
+  Rasch.
+* :support:`1359` Add a more-visible top-level ``CHANGELOG.rst`` pointing users
+  to the actual changelog stored within the Sphinx directory tree. Thanks to
+  Jonathan Vanasco for catch & patch.
+* :feature:`1388` Expose Jinja's ``keep_trailing_newline`` parameter in
+  ``fabric.contrib.files.upload_template`` so users can force template renders
+  to preserve trailing newlines. Thanks to Chen Lei for the patch.
+* :bug:`1389 major` Gently overhaul SSH port derivation so it's less
+  surprising; previously, any non-default value stored in ``env.port`` was
+  overriding all SSH-config derived values. See the API docs for
+  ``fabric.network.normalize`` for details on how it now behaves. Thanks to
+  Harry Weppner for catch & patch.
+* :support:`1454 backported` Remove use of ``:option:`` directives in the
+  changelog, it's currently broken in modern Sphinx & doesn't seem to have
+  actually functioned on Renaissance-era Sphinx either.
+* :bug:`1365` (via :issue:`1372`) Classic-style fabfiles (ones not using
+  ``@task``) erroneously included custom exception subclasses when collecting
+  tasks. This is now fixed thanks to ``@mattvonrocketstein``.
+* :bug:`1348` (via :issue:`1361`) Fix a bug in ``fabric.operations.get`` where
+  remote file paths containing Python string formatting escape codes caused an
+  exception. Thanks to ``@natecode`` for the report and Bradley Spink for the
+  fix.
+* :release:`1.10.2 <2015-06-19>`
+* :support:`1325` Clarify ``fabric.operations.put`` docs re: the ``mode``
+  argument. Thanks to ``@mjmare`` for the catch.
+* :bug:`1318` Update functionality added in :issue:`1213` so abort error
+  messages don't get printed twice (once by us, once by ``sys.exit``) but the
+  annotated exception error message is retained. Thanks to Felix Almeida for
+  the report.
+* :bug:`1305` (also :issue:`1313`) Fix a couple minor issues with the operation
+  of & demo code for the ``JobQueue`` class. Thanks to ``@dioh`` and Horst
+  Gutmann for the report & Cameron Lane for the patch.
+* :bug:`980` (also :issue:`1312`) Redirect output of ``cd`` to ``/dev/null`` so
+  users enabling bash's ``CDPATH`` (or similar features in other shells) don't
+  have polluted output captures. Thanks to Alex North-Keys for the original
+  report & Steve Ivy for the fix.
+* :bug:`1289` Fix "NameError: free variable referenced before assignment in
+  enclosing scope". Thanks to ``@SamuelMarks`` for catch & patch.
+* :bug:`1286` (also :issue:`971`, :issue:`1032`) Recursively unwrap decorators
+  instead of only unwrapping a single decorator level, when obtaining task
+  docstrings. Thanks to Avishai Ish-Shalom for the original report & Max Kovgan
+  for the patch.
+* :bug:`1273` Fix issue with ssh/config not having a cross-platform default
+  path. Thanks to ``@SamuelMarks`` for catch & patch.
+* :feature:`1200` Introduced ``exceptions`` output level, so users don't have to
+  deal with the debug output just to see tracebacks.
+* :support:`1239` Update README to work better under raw docutils so the
+  example code block is highlighted as Python on PyPI (and not just on our
+  Sphinx-driven website). Thanks to Marc Abramowitz.
+* :release:`1.10.1 <2014-12-19>`
+* :release:`1.9.2 <2014-12-19>`
+* :bug:`1201` Don't naively glob all ``fabric.operations.get`` targets - only
+  glob actual directories. This avoids incorrectly yielding permission errors
+  in edge cases where a requested file is within a directory lacking the read
+  permission bit. Thanks to Sassa Nf for the original report.
+* :bug:`1019` (also :issue:`1022`, :issue:`1186`) Fix "is a tty" tests in
+  environments where streams (eg ``sys.stdout``) have been replaced with
+  objects lacking a ``.isatty()`` method. Thanks to Miki Tebeka for the
+  original report, Lele Long for a subsequent patch, and Julien Phalip
+  for the final/merged patch.
+* :support:`1213 backported` Add useful exception message to the implicit
+  ``SystemExit`` raised by Fabric's use of ``sys.exit`` inside the
+  ``fabric.api.abort`` function. This allows client code catching ``SystemExit``
+  to have better introspection into the error. Thanks to Ioannis Panousis.
+* :bug:`1228` Update the ``CommandTimeout`` class so it has a useful ``str``
+  instead of appearing blank when caught by Fabric's top level exception
+  handling. Catch & patch from Tomaz Muraus.
+* :bug:`1180` Fix issue with unicode steam outputs crashing if stream encoding
+  type is None. Thanks to ``@joekiller`` for catch & patch.
+* :support:`958 backported` Remove the Git SHA portion of our version string
+  generation; it was rarely useful & occasionally caused issues for users with
+  non-Git-based source checkouts.
+* :support:`1229 backported` Add some missing API doc hyperlink references.
+  Thanks to Tony Narlock.
+* :bug:`1226` Update ``fabric.operations.get`` to ensure that ``env.user`` has
+  access to tempfiles before changing permissions. Also corrected permissions
+  from 404 to 0400 to match comment. Patch by Curtis Mattoon; original report
+  from Daniel Watkins.
+* :release:`1.10.0 <2014-09-04>`
+* :bug:`1188 major` Update ``fabric.operations.local`` to close non-pipe file
+  descriptors in the child process so subsequent calls to
+  ``fabric.operations.local`` aren't blocked on e.g. already-connected network
+  sockets. Thanks to Tolbkni Kao for catch & patch.
+* :feature:`700` Added ``use_sudo`` and ``temp_dir`` params to
+  ``fabric.operations.get``. This allows downloading files normally not
+  accessible to the user using ``sudo``. Thanks to Jason Coombs for initial
+  report and to Alex Plugaru for the patch (:issue:`1121`).
+* :feature:`1098` Add support for dict style roledefs. Thanks to Jonas
+  Lundberg.
+* :feature:`1090` Add option to skip unknown tasks. Credit goes to Jonas
+  Lundberg.
+* :feature:`975` Fabric can now be invoked via ``python -m fabric`` in addition
+  to the typical use of the ``fab`` entrypoint. Patch courtesy of Jason Coombs.
+
+  .. note:: This functionality is only available under Python 2.7.
+
+* :release:`1.9.1 <2014-08-06>`
+* :release:`1.8.5 <2014-08-06>`
+* :release:`1.7.5 <2014-08-06>`
+* :bug:`1165` Prevent infinite loop condition when a gateway host is enabled &
+  the same host is in the regular target host list. Thanks to ``@CzBiX`` for
+  catch & patch.
+* :bug:`1147` Use ``stat`` instead of ``lstat`` when testing directory-ness in
+  the SFTP module. This allows recursive downloads to avoid recursing into
+  symlinks unexpectedly. Thanks to Igor Kalnitsky for the patch.
+* :bug:`1146` Fix a bug where ``fabric.contrib.files.upload_template`` failed to
+  honor ``lcd`` when ``mirror_local_mode`` is ``True``. Thanks to Laszlo Marai
+  for catch & patch.
+* :bug:`1134` Skip bad hosts when the tasks are executed in parallel. Thanks to
+  Igor Maravić ``@i-maravic``.
+* :bug:`852` Fix to respect ``template_dir`` for non Jinja2 templates in
+  ``fabric.contrib.files.upload_template``. Thanks to Adam Kowalski for the
+  patch and Alex Plugaru for the initial test case.
+* :bug:`1096` Encode Unicode text appropriately for its target stream object to
+  avoid issues on non-ASCII systems. Thanks to Toru Uetani for the original
+  patch.
+* :bug:`1059` Update IPv6 support to work with link-local address formats.
+  Fix courtesy of ``@obormot``.
+* :bug:`1026` Fix a typo preventing quiet operation of
+  ``fabric.contrib.files.is_link``. Caught by ``@dongweiming``.
+* :bug:`600` Clear out connection caches in full when prepping
+  parallel-execution subprocesses. This avoids corner cases causing
+  hangs/freezes due to client/socket reuse. Thanks to Ruslan Lutsenko for the
+  initial report and Romain Chossart for the suggested fix.
+* :bug:`1167` Add Jinja to ``test_requires`` in ``setup.py`` for the couple of
+  newish tests that now require it. Thanks to Kubilay Kocak for the catch.
+* :release:`1.9.0 <2014-06-08>`
+* :feature:`1078` Add ``.command`` and ``.real_command`` attributes to
+  ``local`` return value.  Thanks to Alexander Teves (``@alexanderteves``) and
+  Konrad Hałas (``@konradhalas``).
+* :feature:`938` Add an env var ``env.effective_roles``
+  specifying roles used in the currently executing command. Thanks to
+  Piotr Betkier for the patch.
+* :feature:`1101` Reboot operation now supports custom command. Thanks to Jonas
+  Lejon.
+* :support:`1106` Fix a misleading/ambiguous example snippet in the ``fab``
+  usage docs to be clearer. Thanks to ``@zed``.
+* :release:`1.8.4 <2014-06-08>`
+* :release:`1.7.4 <2014-06-08>`
+* :bug:`898` Treat paths that begin with tilde "~" as absolute paths instead of
+  relative. Thanks to Alex Plugaru for the patch and Dan Craig for the
+  suggestion.
+* :support:`1105 backported` Enhance ``setup.py`` to allow Paramiko 1.13+ under
+  Python 2.6+. Thanks to to ``@Arfrever`` for catch & patch.
+* :release:`1.8.3 <2014-03-21>`
+* :release:`1.7.3 <2014-03-21>`
+* :support:`- backported` Modified packaging data to reflect that Fabric
+  requires Paramiko < 1.13 (which dropped Python 2.5 support.)
+* :feature:`1082` Add ``pty`` passthrough kwarg to
+  ``fabric.contrib.files.upload_template``.
+* :release:`1.8.2 <2014-02-14>`
+* :release:`1.7.2 <2014-02-14>`
+* :bug:`955` Quote directories created as part of ``put``'s recursive directory
+  uploads when ``use_sudo=True`` so directories with shell meta-characters
+  (such as spaces) work correctly. Thanks to John Harris for the catch.
+* :bug:`917` Correct an issue with ``put(use_sudo=True, mode=xxx)`` where the
+  ``chmod`` was trying to apply to the wrong location. Thanks to Remco
+  (``@nl5887``) for catch & patch.
+* :bug:`1046` Fix typo preventing use of ProxyCommand in some situations.
+  Thanks to Keith Yang.
+* :release:`1.8.1 <2013-12-24>`
+* :release:`1.7.1 <2013-12-24>`
+* :release:`1.6.4 <2013-12-24>` 956, 957
+* :release:`1.5.5 <2013-12-24>` 956, 957
+* :bug:`956` Fix pty size detection when running inside Emacs. Thanks to
+  ``@akitada`` for catch & patch.
+* :bug:`957` Fix bug preventing use of ``env.gateway`` with
+  targets requiring password authentication. Thanks to Daniel González,
+  ``@Bengrunt`` and ``@adrianbn`` for their bug reports.
+* :feature:`741` Add ``env.prompts`` dictionary, allowing
+  users to set up custom prompt responses (similar to the built-in sudo prompt
+  auto-responder.) Thanks to Nigel Owens and David Halter for the patch.
+* :bug:`965 major` Tweak IO flushing behavior when in linewise (& thus
+  parallel) mode so interwoven output is less frequent. Thanks to ``@akidata``
+  for catch & patch.
+* :bug:`948` Handle connection failures due to server load and try connecting
+  to hosts a number of times specified in ``env.connection_attempts``.
+* :release:`1.8.0 <2013-09-20>`
+* :feature:`931` Allow overriding of ``abort`` behavior via a custom
+  exception-returning callable set as ``env.abort_exception``.
+  Thanks to Chris Rose for the patch.
+* :support:`984 backported` Make this changelog easier to read! Now with
+  per-release sections, generated automatically from the old timeline source
+  format.
+* :feature:`910` Added a keyword argument to rsync_project to configure the
+  default options. Thanks to ``@moorepants`` for the patch.
+* :release:`1.7.0 <2013-07-26>`
+* :release:`1.6.2 <2013-07-26>`
+* :feature:`925` Added ``contrib.files.is_link``. Thanks to ``@jtangas``
+  for the patch.
+* :feature:`922` Task argument strings are now displayed when using
+  ``fab -d``. Thanks to Kevin Qiu for the patch.
+* :bug:`912` Leaving ``template_dir`` un-specified when using
+  ``upload_template`` in Jinja mode used to cause ``'NoneType' has no attribute
+  'startswith'`` errors. This has been fixed. Thanks to Erick Yellott for catch
+  & to Erick Yellott + Kevin Williams for patches.
+* :feature:`924` Add new env var option ``colorize-errors`` to enable
+  coloring errors and warnings. Thanks to Aaron Meurer for the patch.
+* :bug:`593` Non-ASCII character sets in Jinja templates rendered within
+  ``upload_template`` would cause ``UnicodeDecodeError`` when uploaded. This has
+  been addressed by encoding as ``utf-8`` prior to upload. Thanks to Sébastien
+  Fievet for the catch.
+* :feature:`908` Support loading SSH keys from memory. Thanks to Caleb Groom
+  for the patch.
+* :bug:`171` Added missing cross-references from ``env`` variables documentation
+  to corresponding command-line options. Thanks to Daniel D. Beck for the
+  contribution.
+* :bug:`884` The password cache feature was not working correctly with
+  password-requiring SSH gateway connections. That's fixed now. Thanks to Marco
+  Nenciarini for the catch.
+* :feature:`826` Enable sudo extraction of compressed archive via ``use_sudo``
+  kwarg in ``upload_project``. Thanks to ``@abec`` for the patch.
+* :bug:`694 major` Allow users to work around ownership issues in the default
+  remote login directory: add ``temp_dir`` kwarg for explicit specification of
+  which "bounce" folder to use when calling ``put`` with ``use_sudo=True``.
+  Thanks to Devin Bayer for the report & Dieter Plaetinck / Jesse Myers for
+  suggesting the workaround.
+* :bug:`882` Fix a ``get`` bug regarding spaces in remote working directory
+  names. Thanks to Chris Rose for catch & patch.
+* :release:`1.6.1 <2013-05-23>`
+* :bug:`868` Substantial speedup of parallel tasks by removing an unnecessary
+  blocking timeout in the ``JobQueue`` loop. Thanks to Simo Kinnunen for the
+  patch.
+* :bug:`328` ``lcd`` was no longer being correctly applied to
+  ``upload_template``; this has been fixed. Thanks to Joseph Lawson for the
+  catch.
+* :feature:`812` Add ``use_glob`` option to ``put`` so users trying to upload
+  real filenames containing glob patterns (``*``, ``[`` etc) can disable the
+  default globbing behavior. Thanks to Michael McHugh for the patch.
+* :bug:`864 major` Allow users to disable Fabric's auto-escaping in
+  ``run``/``sudo``.  Thanks to Christian Long and Michael McHugh for the patch.
+* :bug:`870` Changes to shell env var escaping highlighted some extraneous and
+  now damaging whitespace in ``with path():``. This has been removed and
+  a regression test added.
+* :bug:`871` Use of string mode values in ``put(local, remote, mode="NNNN")``
+  would sometimes cause ``Unsupported operand`` errors. This has been
+  fixed.
+* :bug:`84 major` Fixed problem with missing -r flag in Mac OS X sed version.
+  Thanks to Konrad Hałas for the patch.
+* :bug:`861` Gracefully handle situations where users give a single string
+  literal to ``env.hosts``. Thanks to Bill Tucker for catch & patch.
+* :bug:`367` Expand paths with tilde inside (``contrib.files``). Thanks to
+  Konrad Hałas for catch & patch.
+* :feature:`845 backported` Downstream synchronization option implemented for
+  ``fabric.contrib.project.rsync_project``. Thanks to Antonio Barrero for the
+  patch.
+* :release:`1.6.0 <2013-03-01>`
+* :release:`1.5.4 <2013-03-01>`
+* :bug:`844` Account for SSH config overhaul in Paramiko 1.10 by e.g. updating
+  treatment of ``IdentityFile`` to handle multiple values. **This and related
+  SSH config parsing changes are backwards incompatible**; we are including
+  them in this release because they do fix incorrect, off-spec behavior.
+* :bug:`843` Ensure string ``pool_size`` values get run through ``int()``
+  before deriving final result (stdlib ``min()`` has odd behavior here...).
+  Thanks to Chris Kastorff for the catch.
+* :bug:`839` Fix bug in ``fabric.contrib.project.rsync_project`` where IPv6
+  address were not always correctly detected. Thanks to Antonio Barrero for
+  catch & patch.
+* :bug:`587` Warn instead of aborting when ``env.use_ssh_config``
+  is True but the configured SSH conf file doesn't exist.
+  This allows multi-user fabfiles to enable SSH config without causing hard
+  stops for users lacking SSH configs. Thanks to Rodrigo Pimentel for the
+  report.
+* :feature:`821` Add ``fabric.context_managers.remote_tunnel`` to allow reverse
+  SSH tunneling (exposing locally-visible network ports to the remote end).
+  Thanks to Giovanni Bajo for the patch.
+* :feature:`823` Add ``env.remote_interrupt`` which
+  controls whether Ctrl-C is forwarded to the remote end or is captured locally
+  (previously, only the latter behavior was implemented). Thanks to Geert
+  Jansen for the patch.
+* :release:`1.5.3 <2013-01-28>`
+* :bug:`806` Force strings given to ``getpass`` during password prompts to be
+  ASCII, to prevent issues on some platforms when Unicode is encountered.
+  Thanks to Alex Louden for the patch.
+* :bug:`805` Update ``fabric.context_managers.shell_env`` to play nice with
+  Windows (7, at least) systems and ``fabric.operations.local``. Thanks to
+  Fernando Macedo for the patch.
+* :bug:`654` Parallel runs whose sum total of returned data was large (e.g.
+  large return values from the task, or simply a large number of hosts in the
+  host list) were causing frustrating hangs. This has been fixed.
+* :feature:`402` Attempt to detect stale SSH sessions and reconnect when they
+  arise. Thanks to ``@webengineer`` for the patch.
+* :bug:`791` Cast ``fabric.operations.reboot``'s ``wait`` parameter to a numeric
+  type in case the caller submitted a string by mistake. Thanks to Thomas
+  Schreiber for the patch.
+* :bug:`703 major` Add a ``shell`` kwarg to many methods in
+  ``fabric.contrib.files`` to help avoid conflicts with
+  ``fabric.context_managers.cd`` and similar.  Thanks to ``@mikek`` for the patch.
+* :feature:`730` Add ``env.system_known_hosts``/``--system-known-hosts``
+  to allow loading a user-specified system-level SSH
+  ``known_hosts`` file. Thanks to Roy Smith for the patch.
+* :release:`1.5.2 <2013-01-15>`
+* :feature:`818` Added ``env.eagerly_disconnect``
+  option to help prevent pile-up of many open connections.
+* :feature:`706` Added ``env.tasks``, returning list of tasks to
+  be executed by current ``fab`` command.
+* :bug:`766` Use the variable name of a new-style ``fabric.tasks.Task``
+  subclass object when the object name attribute is undefined.  Thanks to
+  ``@todddeluca`` for the patch.
+* :bug:`604` Fixed wrong treatment of backslashes in put operation when uploading
+  directory tree on Windows. Thanks to Jason Coombs for the catch and
+  ``@diresys`` & Oliver Janik for the patch.
+  for the patch.
+* :bug:`792` The newish ``fabric.context_managers.shell_env`` context manager
+  was incorrectly omitted from the ``fabric.api`` import endpoint. This has
+  been remedied. Thanks to Vishal Rana for the catch.
+* :feature:`735` Add ``ok_ret_codes`` option to ``env`` to allow alternate
+  return codes to be treated os "ok". Thanks to Andy Kraut for the pull request.
+* :bug:`775` Shell escaping was incorrectly applied to the value of ``$PATH``
+  updates in our shell environment handling, causing (at the very least)
+  ``fabric.operations.local`` binary paths to become inoperable in certain
+  situations.  This has been fixed.
+* :feature:`787` Utilize new Paramiko feature allowing us to skip the use of
+  temporary local files when using file-like objects in
+  ``fabric.operations.get``/``fabric.operations.put``.
+* :feature:`249` Allow specification of remote command timeout value by
+  setting ``env.command_timeout``. Thanks to Paul
+  McMillan for suggestion & initial patch.
+* Added current host string to prompt abort error messages.
+* :release:`1.5.1 <2012-11-15>`
+* :bug:`776` Fixed serious-but-non-obvious bug in direct-tcpip driven
+  gatewaying (e.g. that triggered by ``-g`` or ``env.gateway``.) Should work
+  correctly now.
+* :bug:`771` Sphinx autodoc helper ``fabric.docs.unwrap_tasks`` didn't play nice
+  with ``@task(name=xxx)`` in some situations. This has been fixed.
+* :release:`1.5.0 <2012-11-06>`
+* :release:`1.4.4 <2012-11-06>`
+* :feature:`38` (also :issue:`698`) Implement both SSH-level and
+  ``ProxyCommand``-based gatewaying for SSH traffic. (This is distinct from
+  tunneling non-SSH traffic over the SSH connection, which is :issue:`78` and
+  not implemented yet.)
+
+    * Thanks in no particular order to Erwin Bolwidt, Oskari Saarenmaa, Steven
+      Noonan, Vladimir Lazarenko, Lincoln de Sousa, Valentino Volonghi, Olle
+      Lundberg and Github user ``@acrish`` for providing the original patches to
+      both Fabric and Paramiko.
+
+* :feature:`684 backported` (also :issue:`569`) Update how
+  ``fabric.decorators.task`` wraps task functions to preserve additional
+  metadata; this allows decorated functions to play nice with Sphinx autodoc.
+  Thanks to Jaka Hudoklin for catch & patch.
+* :support:`103` (via :issue:`748`) Long standing Sphinx autodoc issue requiring
+  error-prone duplication of function signatures in our API docs has been
+  fixed. Thanks to Alex Morega for the patch.
+* :bug:`767 major` Fix (and add test for) regression re: having linewise output
+  automatically activate when parallelism is in effect. Thanks to Alexander
+  Fortin and Dustin McQuay for the bug reports.
+* :bug:`736 major` Ensure context managers that build env vars play nice with
+  ``contextlib.nested`` by deferring env var reference to entry time, not call
+  time. Thanks to Matthew Tretter for catch & patch.
+* :feature:`763` Add ``--initial-password-prompt`` to allow prefilling the
+  password cache at the start of a run. Great for sudo-powered parallel runs.
+* :feature:`665` (and #629) Update ``fabric.contrib.files.upload_template`` to
+  have a more useful return value, namely that of its internal
+  ``fabric.operations.put`` call. Thanks to Miquel Torres for the catch &
+  Rodrigue Alcazar for the patch.
+* :feature:`578` Add ``name`` argument to ``fabric.decorators.task`` to allow
+  overriding of the default "function name is task name" behavior. Thanks to
+  Daniel Simmons for catch & patch.
+* :feature:`761` Allow advanced users to parameterize ``fabric.main.main()`` to
+  force loading of specific fabfiles.
+* :bug:`749` Gracefully work around calls to ``fabric.version`` on systems
+  lacking ``/bin/sh`` (which causes an ``OSError`` in ``subprocess.Popen``
+  calls.)
+* :feature:`723` Add the ``group=`` argument to
+  ``fabric.operations.sudo``. Thanks to Antti Kaihola for the pull request.
+* :feature:`725` Updated ``fabric.operations.local`` to allow override
+  of which local shell is used. Thanks to Mustafa Khattab.
+* :bug:`704 major` Fix up a bunch of Python 2.x style ``print`` statements to
+  be forwards compatible. Thanks to Francesco Del Degan for the patch.
+* :feature:`491` (also :feature:`385`) IPv6 host string support. Thanks to Max
+  Arnold for the patch.
+* :feature:`699` Allow ``name`` attribute on file-like objects for get/put. Thanks
+  to Peter Lyons for the pull request.
+* :bug:`711 major` ``fabric.sftp.get`` would fail when filenames had % in their
+  path.  Thanks to John Begeman
+* :bug:`702 major` ``fabric.operations.require`` failed to test for "empty"
+  values in the env keys it checks (e.g.
+  ``require('a-key-whose-value-is-an-empty-list')`` would register a successful
+  result instead of alerting that the value was in fact empty. This has been
+  fixed, thanks to Rich Schumacher.
+* :bug:`718` ``isinstance(foo, Bar)`` is used in ``fabric.main`` instead
+  of ``type(foo) == Bar`` in order to fix some edge cases.
+  Thanks to Mikhail Korobov.
+* :bug:`693` Fixed edge case where ``abort`` driven failures within parallel
+  tasks could result in a top level exception (a ``KeyError``) regarding error
+  handling. Thanks to Marcin Kuźmiński for the report.
+* :support:`681 backported` Fixed outdated docstring for
+  ``fabric.decorators.runs_once`` which claimed it would get run multiple times
+  in parallel mode. That behavior was fixed in an earlier release but the docs
+  were not updated. Thanks to Jan Brauer for the catch.
+* :release:`1.4.3 <2012-07-06>`
+* :release:`1.3.8 <2012-07-06>`
+* :feature:`263` Shell environment variable support for
+  ``fabric.operations.run``/``fabric.operations.sudo`` added in the form of the
+  ``fabric.context_managers.shell_env`` context manager. Thanks to Oliver
+  Tonnhofer for the original pull request, and to Kamil Kisiel for the final
+  implementation.
+* :feature:`669` Updates to our Windows compatibility to rely more heavily on
+  cross-platform Python stdlib implementations. Thanks to Alexey Diyan for the
+  patch.
+* :bug:`671` ``reject-unknown-hosts`` sometimes resulted in a password
+  prompt instead of an abort. This has been fixed. Thanks to Roy Smith for the
+  report.
+* :bug:`659` Update docs to reflect that ``fabric.operations.local`` currently
+  honors ``env.path``. Thanks to `@floledermann
+  <https://github.com/floledermann>`_ for the catch.
+* :bug:`652` Show available commands when aborting on invalid command names.
+* :support:`651 backported` Added note about nesting ``with`` statements on
+  Python 2.6+.  Thanks to Jens Rantil for the patch.
+* :bug:`649` Don't swallow non-``abort``-driven exceptions in parallel mode.
+  Fabric correctly printed such exceptions, and returned them from
+  ``fabric.tasks.execute``, but did not actually cause the child or parent
+  processes to halt with a nonzero status. This has been fixed.
+  ``fabric.tasks.execute`` now also honors ``env.warn_only`` so
+  users may still opt to call it by hand and inspect the returned exceptions,
+  instead of encountering a hard stop. Thanks to Matt Robenolt for the catch.
+* :feature:`241` Add the command executed as a ``.command`` attribute to the
+  return value of ``fabric.operations.run``/``fabric.operations.sudo``. (Also
+  includes a second attribute containing the "real" command executed, including
+  the shell wrapper and any escaping.)
+* :feature:`646` Allow specification of which local streams to use when
+  ``fabric.operations.run``/``fabric.operations.sudo`` print the remote
+  stdout/stderr, via e.g. ``run("command", stderr=sys.stdout)``.
+* :support:`645 backported` Update Sphinx docs to work well when run out of a
+  source tarball as opposed to a Git checkout. Thanks again to ``@Arfrever`` for
+  the catch.
+* :support:`640 backported` (also :issue:`644`) Update packaging manifest so
+  sdist tarballs include all necessary test & doc files. Thanks to Mike Gilbert
+  and ``@Arfrever`` for catch & patch.
+* :feature:`627` Added convenient ``quiet`` and ``warn_only`` keyword arguments
+  to ``fabric.operations.run``/``fabric.operations.sudo`` which are aliases for
+  ``settings(hide('everything'), warn_only=True)`` and
+  ``settings(warn_only=True)``, respectively. (Also added corresponding
+  context managers.) Useful for remote program calls which
+  are expected to fail and/or whose output doesn't need to be shown to users.
+* :feature:`633` Allow users to turn off host list deduping by setting
+  ``env.dedupe_hosts`` to ``False``. This enables running the
+  same task multiple times on a single host, which was previously not possible.
+* :support:`634 backported` Clarified that ``fabric.context_managers.lcd`` does
+  no special handling re: the user's current working directory, and thus
+  relative paths given to it will be relative to ``os.getcwd()``. Thanks to
+  `@techtonik <https://github.com/techtonik>`_ for the catch.
+* :release:`1.4.2 <2012-05-07>`
+* :release:`1.3.7 <2012-05-07>`
+* :bug:`562` Agent forwarding would error out or freeze when multiple uses of
+  the forwarded agent were used per remote invocation (e.g. a single
+  ``fabric.operations.run`` command resulting in multiple Git or SVN checkouts.)
+  This has been fixed thanks to Steven McDonald and GitHub user ``@lynxis``.
+* :support:`626 backported` Clarity updates to the tutorial. Thanks to GitHub
+  user ``m4z`` for the patches.
+* :bug:`625` ``fabric.context_managers.hide``/``fabric.context_managers.show``
+  did not correctly restore prior display settings if an exception was raised
+  inside the block. This has been fixed.
+* :bug:`624` Login password prompts did not always display the username being
+  authenticated for. This has been fixed. Thanks to Nick Zalutskiy for catch &
+  patch.
+* :bug:`617` Fix the ``clean_revert`` behavior of
+  ``fabric.context_managers.settings`` so it doesn't ``KeyError`` for newly
+  created settings keys. Thanks to Chris Streeter for the catch.
+* :feature:`615` Updated ``fabric.operations.sudo`` to honor the new setting
+  ``env.sudo_user`` as a default for its ``user`` kwarg.
+* :bug:`616` Add port number to the error message displayed upon connection
+  failures.
+* :bug:`609` (and :issue:`564`) Document and clean up ``env.sudo_prefix``
+  so it can be more easily modified by users facing uncommon
+  use cases. Thanks to GitHub users ``3point2`` for the cleanup and ``SirScott``
+  for the documentation catch.
+* :bug:`610` Change detection of ``env.key_filename``'s type (added as part of
+  SSH config support in 1.4) so it supports arbitrary iterables. Thanks to
+  Brandon Rhodes for the catch.
+* :release:`1.4.1 <2012-04-04>`
+* :release:`1.3.6 <2012-04-04>`
+* :bug:`608` Add ``capture`` kwarg to ``fabric.contrib.project.rsync_project``
+  to aid in debugging rsync problems.
+* :bug:`607` Allow ``fabric.operations.local`` to display stdout/stderr when it
+  warns/aborts, if it was capturing them.
+* :bug:`395` Added an FAQ entry detailing how to
+  handle init scripts which misbehave when a pseudo-tty is allocated.
+* :bug:`568` ``fabric.tasks.execute`` allowed too much of its internal state
+  changes (to variables such as ``env.host_string`` and ``env.parallel``) to
+  persist after execution completed; this caused a number of different
+  incorrect behaviors. ``fabric.tasks.execute`` has been overhauled to clean up
+  its own state changes -- while preserving any state changes made by the task
+  being executed.
+* :bug:`584` ``fabric.contrib.project.upload_project`` did not take explicit
+  remote directory location into account when untarring, and now uses
+  ``fabric.context_managers.cd`` to address this. Thanks to Ben Burry for the
+  patch.
+* :bug:`458` ``fabric.decorators.with_settings`` did not perfectly match
+  ``fabric.context_managers.settings``, re: ability to inline additional context
+  managers. This has been corrected. Thanks to Rory Geoghegan for the patch.
+* :bug:`499` ``contrib.files.first`` used an
+  outdated function signature in its wrapped ``fabric.contrib.files.exists``
+  call. This has been fixed. Thanks to Massimiliano Torromeo for catch & patch.
+* :bug:`551` ``--list`` output now detects terminal window size and truncates
+  (or doesn't truncate) accordingly. Thanks to Horacio G. de Oro for the
+  initial pull request.
+* :bug:`572` Parallel task aborts (as oppposed to unhandled exceptions) now
+  correctly print their abort messages instead of tracebacks, and cause the
+  parent process to exit with the correct (nonzero) return code. Thanks to Ian
+  Langworth for the catch.
+* :bug:`306` Remote paths now use posixpath for a separator. Thanks to Jason
+  Coombs for the patch.
+* :release:`1.4.0 <2012-02-13>`
+* :release:`1.3.5 <2012-02-13>`
+* :release:`1.2.6 <2012-02-13>`
+* :release:`1.1.8 <2012-02-13>`
+* :bug:`495` Fixed documentation example showing how to subclass
+  ``fabric.tasks.Task``. Thanks to Brett Haydon for the catch and Mark Merritt
+  for the patch.
+* :bug:`410` Fixed a bug where using the ``fabric.decorators.task`` decorator
+  inside/under another decorator such as ``fabric.decorators.hosts`` could cause
+  that task to become invalid when invoked by name (due to how old-style vs
+  new-style tasks are detected.) Thanks to Dan Colish for the initial patch.
+* :feature:`559` ``fabric.contrib.project.rsync_project`` now allows users to
+  append extra SSH-specific arguments to ``rsync``'s ``--rsh`` flag.
+* :feature:`138` ``env.port`` may now be written to at fabfile module
+  level to set a default nonstandard port number. Previously this value was
+  read-only.
+* :feature:`3` Fabric can now load a subset of SSH config functionality
+  directly from your local ``~/.ssh/config`` if ``env.use_ssh_config``
+  is set to ``True``. See ``ssh-config`` for details.
+  Thanks to Kirill Pinchuk for the initial patch.
+* :feature:`12` Added the ability to try connecting multiple times to
+  temporarily-down remote systems, instead of immediately failing. (Default
+  behavior is still to only try once.) See ``env.timeout`` and
+  ``env.connection_attempts`` for controlling both
+  connection timeouts and total number of attempts. ``fabric.operations.reboot``
+  has also been overhauled (but practically deprecated -- see its updated
+  docs.)
+* :feature:`474` ``fabric.tasks.execute`` now allows you to access the executed
+  task's return values, by itself returning a dictionary whose keys are the
+  host strings executed against.
+* :bug:`487 major` Overhauled the regular expression escaping performed in
+  ``fabric.contrib.files.append`` and ``fabric.contrib.files.contains`` to try
+  and handle more corner cases. Thanks to Neilen Marais for the patch.
+* :support:`532` Reorganized and cleaned up the output of ``fab --help``.
+* :feature:`8` Added ``--skip-bad-hosts``/``env.skip_bad_hosts``
+  option to allow skipping past temporarily down/unreachable hosts.
+* :feature:`13` Env vars may now be set at runtime via the new ``--set``
+  command-line flag.
+* :feature:`506` A new output alias, ``commands``, has been added, which allows
+  hiding remote stdout and local "running command X" output lines.
+* :feature:`72` SSH agent forwarding support has made it into Fabric's SSH
+  library, and hooks for using it have been added (disabled by default; use
+  ``-A`` or ``env.forward_agent`` to enable.) Thanks to Ben
+  Davis for porting an existing Paramiko patch to ``ssh`` and providing the
+  necessary tweak to Fabric.
+* :release:`1.3.4 <2012-01-12>`
+* :bug:`492` ``@parallel`` did not automatically trigger linewise output, as
+  was intended. This has been fixed. Thanks to Brandon Huey for the catch.
+* :bug:`510` Parallel mode is incompatible with user input, such as
+  password/hostname prompts, and was causing cryptic ``Operation not supported
+  by device`` errors when such prompts needed to be displayed. This behavior has
+  been updated to cleanly and obviously ``abort`` instead.
+* :bug:`494` Fixed regression bug affecting some ``env`` values such as
+  ``env.port`` under parallel mode. Symptoms included
+  ``fabric.contrib.project.rsync_project`` bailing out due to a None port value
+  when run under ``@parallel``. Thanks to Rob Terhaar for the report.
+* :bug:`339` Don't show imported ``fabric.colors`` members in ``--list``
+  output.  Thanks to Nick Trew for the report.
+* :release:`1.3.3 <2011-11-23>`
+* :release:`1.2.5 <2011-11-23>`
+* :release:`1.1.7 <2011-11-23>`
+* :bug:`441` Specifying a task module as a task on the command line no longer
+  blows up but presents the usual "no task by that name" error message instead.
+  Thanks to Mitchell Hashimoto for the catch.
+* :bug:`475` Allow escaping of equals signs in per-task args/kwargs.
+* :bug:`450` Improve traceback display when handling ``ImportError`` for
+  dependencies. Thanks to David Wolever for the patches.
+* :bug:`446` Add QNX to list of secondary-case ``fabric.contrib.files.sed``
+  targets. Thanks to Rodrigo Madruga for the tip.
+* :bug:`443` ``fabric.contrib.files.exists`` didn't expand tildes; now it does.
+  Thanks to Riccardo Magliocchetti for the patch.
+* :bug:`437` ``fabric.decorators.with_settings`` now correctly preserves the
+  wrapped function's docstring and other attributes. Thanks to Eric Buckley for
+  the catch and Luke Plant for the patch.
+* :bug:`400` Handle corner case of systems where ``pwd.getpwuid`` raises
+  ``KeyError`` for the user's UID instead of returning a valid string. Thanks
+  to Dougal Matthews for the catch.
+* :bug:`397` Some poorly behaved objects in third party modules triggered
+  exceptions during Fabric's "classic or new-style task?" test. A fix has been
+  added which tries to work around these.
+* :bug:`341` ``fabric.contrib.files.append`` incorrectly failed to detect that
+  the line(s) given already existed in files hidden to the remote user, and
+  continued appending every time it ran. This has been fixed. Thanks to
+  Dominique Peretti for the catch and Martin Vilcans for the patch.
+* :bug:`342` Combining ``fabric.context_managers.cd`` with
+  ``fabric.operations.put`` and its ``use_sudo`` keyword caused an unrecoverable
+  error. This has been fixed. Thanks to Egor M for the report.
+* :bug:`482` Parallel mode should imply linewise output; omission of this
+  behavior was an oversight.
+* :bug:`230` Fix regression re: combo of no fabfile & arbitrary command use.
+  Thanks to Ali Saifee for the catch.
+* :release:`1.3.2 <2011-11-07>`
+* :release:`1.2.4 <2011-11-07>`
+* :release:`1.1.6 <2011-11-07>`
+* :support:`459 backported` Update our ``setup.py`` files to note that PyCrypto
+  released 2.4.1, which fixes the setuptools problems.
+* :support:`467 backported` (also :issue:`468`, :issue:`469`) Handful of
+  documentation clarification tweaks. Thanks to Paul Hoffman for the patches.
+* :release:`1.3.1 <2011-10-24>`
+* :bug:`457` Ensured that Fabric fast-fails parallel tasks if any child
+  processes encountered errors. Previously, multi-task invocations would
+  continue to the 2nd, etc task when failures occurred, which does not fit with
+  how Fabric usually behaves. Thanks to Github user ``sdcooke`` for the report
+  and Morgan Goose for the fix.
+* :release:`1.3.0 <2011-10-23>`
+* :release:`1.2.3 <2011-10-23>`
+* :release:`1.1.5 <2011-10-23>`
+* :release:`1.0.5 <2011-10-23>`
+* :support:`275` To support an edge use case of the features released in
+  :issue:`19`, and to lay the foundation for :issue:`275`, we have forked
+  Paramiko into the `Python 'ssh' library <https://pypi.org/project/ssh/>`_
+  and changed our dependency to it for Fabric 1.3 and higher. This may have
+  implications for the more uncommon install use cases, and package
+  maintainers, but we hope to iron out any issues as they come up.
+* :bug:`323` ``fabric.operations.put`` forgot how to expand leading tildes in
+  the remote file path. This has been corrected. Thanks to Piet Delport for the
+  catch.
+* :feature:`21` It is now possible, using the new ``fabric.tasks.execute`` API
+  call, to execute task objects (by reference or by name) from within other
+  tasks or in library mode. ``fabric.tasks.execute`` honors the other tasks'
+  ``fabric.decorators.hosts``/``fabric.decorators.roles`` decorators, and also
+  supports passing in explicit host and/or role arguments.
+* :feature:`19` Tasks may now be optionally executed in parallel. Please see
+  the parallel execution docs for details. Major
+  thanks to Morgan Goose for the initial implementation.
+* :bug:`182` During display of remote stdout/stderr, Fabric occasionally
+  printed extraneous line prefixes (which in turn sometimes overwrote wrapped
+  text.) This has been fixed.
+* :bug:`430` Tasks decorated with ``fabric.decorators.runs_once`` printed
+  extraneous 'Executing...' status lines on subsequent invocations. This is
+  noisy at best and misleading at worst, and has been corrected. Thanks to
+  Jacob Kaplan-Moss for the report.
+* :release:`1.2.2 <2011-09-01>`
+* :release:`1.1.4 <2011-09-01>`
+* :release:`1.0.4 <2011-09-01>`
+* :bug:`252` ``fabric.context_managers.settings`` would silently fail to set
+  ``env`` values for keys which did not exist outside the context manager
+  block.  It now works as expected. Thanks to Will Maier for the catch and
+  suggested solution.
+* :support:`393 backported` Fixed a typo in an example code snippet in the task
+  docs.  Thanks to Hugo Garza for the catch.
+* :bug:`396` ``--shortlist`` broke after the addition of ``--list-format`` and
+  no longer displayed the short list format correctly. This has been fixed.
+* :bug:`373` Re-added missing functionality preventing host exclusion from
+  working correctly.
+* :bug:`303` Updated terminal size detection to correctly skip over non-tty
+  stdout, such as when running ``fab taskname | other_command``.
+* :release:`1.2.1 <2011-08-21>`
+* :release:`1.1.3 <2011-08-21>`
+* :release:`1.0.3 <2011-08-21>`
+* :bug:`417` ``abort-on-prompts`` would incorrectly abort when set to True,
+  even if both password and host were defined. This has been fixed. Thanks to
+  Valerie Ishida for the report.
+* :support:`416 backported` Updated documentation to reflect move from Redmine
+  to Github.
+* :bug:`389` Fixed/improved error handling when Paramiko import fails. Thanks
+  to Brian Luft for the catch.
+* :release:`1.2.0 <2011-07-12>`
+* :feature:`22` Enhanced ``@task`` to add aliasing, per-module default tasks,
+  and control over the wrapping task class. Thanks to Travis Swicegood for the
+  initial work and collaboration.
+* :bug:`380` Improved unicode support when testing objects for being
+  string-like. Thanks to Jiri Barton for catch & patch.
+* :support:`382` Experimental overhaul of changelog formatting & process to
+  make supporting multiple lines of development less of a hassle.
+* :release:`1.1.2 <2011-07-07>`
+* :release:`1.0.2 <2011-06-24>`
diff -Nru fabric-1.14.0/sites/www/conf.py fabric-2.5.0/sites/www/conf.py
--- fabric-1.14.0/sites/www/conf.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/sites/www/conf.py	2019-08-06 23:57:28.000000000 +0100
@@ -3,26 +3,23 @@
 import os
 from os.path import abspath, join, dirname
 
-sys.path.append(abspath(join(dirname(__file__), '..')))
+sys.path.append(abspath(join(dirname(__file__), "..")))
 from shared_conf import *
 
 
 # Releases changelog extension
-extensions.append('releases')
+extensions.append("releases")
+releases_document_name = ["changelog", "changelog-v1"]
 releases_github_path = "fabric/fabric"
 
 # Intersphinx for referencing API/usage docs
-extensions.append('sphinx.ext.intersphinx')
+extensions.append("sphinx.ext.intersphinx")
 # Default is 'local' building, but reference the public docs site when building
 # under RTD.
-target = join(dirname(__file__), '..', 'docs', '_build')
-if os.environ.get('READTHEDOCS') == 'True':
-    target = 'http://docs.fabfile.org/en/latest/'
-intersphinx_mapping = {
-    'docs': (target, None),
-}
+target = join(dirname(__file__), "..", "docs", "_build")
+if on_rtd:
+    target = "http://docs.fabfile.org/en/latest/"
+intersphinx_mapping.update({"docs": (target, None)})
 
 # Sister-site links to API docs
-html_theme_options['extra_nav_links'] = {
-    "API Docs": 'http://docs.fabfile.org',
-}
+html_theme_options["extra_nav_links"] = {"API Docs": "http://docs.fabfile.org"}
diff -Nru fabric-1.14.0/sites/www/contact.rst fabric-2.5.0/sites/www/contact.rst
--- fabric-1.14.0/sites/www/contact.rst	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/sites/www/contact.rst	2019-08-06 23:57:28.000000000 +0100
@@ -2,11 +2,11 @@
 Contact
 =======
 
-If you've scoured the :ref:`prose <usage-docs>` and :ref:`API <api_docs>`
-documentation and still can't find an answer to your question, below are
-various support resources that should help. We do request that you do at least
-skim the documentation before posting tickets or mailing list questions,
-however!
+If you've scoured the :ref:`conceptual <concepts-docs>` and :ref:`API
+<api-docs>` documentation and still can't find an answer to your question,
+below are various support resources that should help. We do request that you do
+at least skim the documentation before posting tickets or mailing list
+questions, however!
 
 Mailing list
 ------------
@@ -21,9 +21,13 @@
 -------
 
 Fabric has an official Twitter account, `@pyfabric
-<http://twitter.com/pyfabric>`_, which is used for announcements and occasional
+<https://twitter.com/pyfabric>`_, which is used for announcements and occasional
 related news tidbits (e.g. "Hey, check out this neat article on Fabric!").
 
+You may also want to follow the principal developer, `@bitprophet
+<https://twitter.com/bitprophet>`_, for development updates and colorful
+commentary.
+
 .. _bugs:
 
 Bugs/ticket tracker
diff -Nru fabric-1.14.0/sites/www/development.rst fabric-2.5.0/sites/www/development.rst
--- fabric-1.14.0/sites/www/development.rst	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/sites/www/development.rst	2019-08-06 23:57:28.000000000 +0100
@@ -24,13 +24,10 @@
   currently use Fabric and how you want to use it. (Please do try to search the
   `ticket tracker`_ first, though,
   when submitting feature ideas.)
-* **Report bugs or submit feature requests.** We follow `contribution-guide.org`_'s guidelines, so please check them out before
+* **Report bugs or submit feature requests.** We follow `contribution-guide.org
+  <http://contribution-guide.org>`_'s guidelines, so please check them out before
   visiting the `ticket tracker`_.
-* **Fix bugs or implement features!** Again, follow `contribution-guide.org`_
-  for details on this process. Regarding the changelog step, our changelog is
-  stored in ``sites/www/changelog.rst``.
 
-.. _contribution-guide.org: http://contribution-guide.org
 .. _ticket tracker: https://github.com/fabric/fabric/issues
 
 While we may not always reply promptly, we do try to make time eventually to
@@ -41,24 +38,14 @@
 Support of older releases
 =========================
 
-Major and minor releases do not mark the end of the previous line or lines of
-development:
+Major and minor releases do not usually mark the end of the previous line or
+lines of development:
 
-* The two most recent minor release branches will continue to receive critical
-  bugfixes. For example, if 1.1 were the latest minor release, it and 1.0 would
-  get bugfixes, but not 0.9 or earlier; and once 1.2 came out, this window
-  would then only extend back to 1.1.
+* Recent minor release branches typically continue to receive critical
+  bugfixes, often extending back two or three release lines (so e.g. if 2.4 was
+  the currently active release line, 2.3 and perhaps even 2.2 might get
+  patches).
 * Depending on the nature of bugs found and the difficulty in backporting them,
   older release lines may also continue to get bugfixes -- but there's no
-  longer a guarantee of any kind. Thus, if a bug were found in 1.1 that
-  affected 0.9 and could be easily applied, a new 0.9.x version *might* be
-  released.
-* This policy may change in the future to accommodate more branches, depending
-  on development speed.
-
-We hope that this policy will allow us to have a rapid minor release cycle (and
-thus keep new features coming out frequently) without causing users to feel too
-much pressure to upgrade right away. At the same time, the backwards
-compatibility guarantee means that users should still feel comfortable
-upgrading to the next minor release in order to stay within this sliding
-support window.
+  guarantee of any kind. Thus, if a bug were found in 2.4 that affected 2.1 and
+  could be easily applied, a new 2.1.x version *might* be released.
diff -Nru fabric-1.14.0/sites/www/faq.rst fabric-2.5.0/sites/www/faq.rst
--- fabric-1.14.0/sites/www/faq.rst	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/sites/www/faq.rst	2019-08-06 23:57:28.000000000 +0100
@@ -1,86 +1,83 @@
-================================
-Frequently Asked Questions (FAQ)
-================================
+=========================================
+Frequently Asked/Answered Questions (FAQ)
+=========================================
 
 These are some of the most commonly encountered problems or frequently asked
 questions which we receive from users. They aren't intended as a substitute for
-reading the rest of the documentation, especially the :ref:`usage docs
-<usage-docs>`, so please make sure you check those out if your question is not
-answered here.
-
-
-Fabric installs but doesn't run!
-================================
-
-On systems with old versions of ``setuptools`` (notably OS X Mavericks [10.9]
-as well as older Linux distribution versions) users frequently have problems
-running Fabric's binary scripts; this is because these ``setuptools`` are too
-old to deal with the modern distribution formats Fabric and some of its
-dependencies may use.
-
-One method we've used to recreate this error:
-
-* OS X 10.9 using system Python
-* Pip obtained via e.g. ``sudo easy_install pip`` or ``sudo python get-pip.py``
-* ``pip install fabric``
-* ``fab [args]`` then results in the following traceback::
-
-    Traceback (most recent call last):
-      File "/usr/local/bin/fab", line 5, in <module>
-        from pkg_resources import load_entry_point
-      File "/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/pkg_resources.py", line 2603, in <module>
-        working_set.require(__requires__)
-      File "/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/pkg_resources.py", line 666, in require
-        needed = self.resolve(parse_requirements(requirements))
-      File "/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/pkg_resources.py", line 565, in resolve
-        raise DistributionNotFound(req)  # XXX put more info here
-    pkg_resources.DistributionNotFound: paramiko>=1.10
-
-The best solution is to obtain a newer ``setuptools`` (which fixes this bug
-among many others) like so::
-
-    $ sudo pip install -U setuptools
-
-Uninstalling, then reinstalling Fabric after doing so should fix the issue.
-
-Another approach is to tell ``pip`` not to use the ``wheel`` format (make sure
-you've already uninstalled Fabric and Paramiko beforehand)::
-
-    $ sudo pip install fabric --no-use-wheel
-
-Finally, you may also find success by using a different Python
-interpreter/ecosystem, such as that provided by `Homebrew <http://brew.sh>`_
-(`specific Python doc page
-<https://github.com/Homebrew/homebrew/wiki/Homebrew-and-Python>`_).
-
-
-How do I dynamically set host lists?
-====================================
-
-See :ref:`dynamic-hosts`.
-
-
-How can I run something after my task is done on all hosts?
-===========================================================
-
-See :ref:`leveraging-execute-return-value`.
+reading the rest of the documentation, so please make sure you check it out if
+your question is not answered here.
 
+.. note::
+    Most API examples and links are for version 2 and up; FAQs specific to
+    version 1 will typically be marked as such.
 
-.. _init-scripts-pty:
+.. warning::
+    Many questions about shell command execution and task behavior are answered
+    on `Invoke's FAQ page <http://www.pyinvoke.org/faq.html>`_ - please check
+    there also!
+
+
+.. _remote-env-vars-dont-work:
+
+Explicitly set env variables are not being set correctly on the remote end!
+===========================================================================
+
+If your attempts to set environment variables for things like `Connection.run
+<fabric.connection.Connection.run>` appear to silently fail, you're almost
+certainly talking to an SSH server which is setting a highly restrictive
+`AcceptEnv <https://man.openbsd.org/sshd_config#AcceptEnv>`_.
+
+To fix, you can either modify the server's configuration to allow the env vars
+you're setting, or use the ``inline_ssh_env`` `~fabric.connection.Connection`
+parameter (or the :ref:`global config option <default-values>` of the same
+name) to force Fabric to send env vars prefixed before your command strings
+instead.
+
+
+The remote shell environment doesn't match interactive shells!
+==============================================================
+
+You may find environment variables (or the behavior they trigger) differ
+interactively vs scripted via Fabric. For example, a program that's on your
+``$PATH`` when you manually ``ssh`` in might not be visible when using
+`Connection.run <fabric.connection.Connection.run>`; or special per-program env
+vars such as those for Python, pip, Java etc are not taking effect; etc.
+
+The root cause of this is typically because the SSH server runs non-interactive
+commands via a very limited shell call: ``/path/to/shell -c "command"`` (for
+example, `OpenSSH
+<https://github.com/fabric/fabric/issues/1519#issuecomment-411247228>`_). Most
+shells, when run this way, are not considered to be either **interactive** or
+**login** shells; and this then impacts which startup files get loaded.
+
+Users typically only modify shell files related to interactive operation (such
+as ``~/.bash_profile`` or ``/etc/zshrc``); such changes do not take effect when
+the SSH server is running one-off commands.
+
+To work around this, consult your shell's documentation to see if it offers any
+non-login, non-interactive config files; for example, ``zsh`` lets you
+configure ``/etc/zshrc`` or ``~/.zshenv`` for this purpose.
 
-Init scripts don't work!
-========================
+.. note::
+    ``bash`` does not appear to offer standard non-login/non-interactive
+    startup files, even in version 4. However, it may attempt to determine if
+    it's being run by a remote-execution daemon and will apparently source
+    ``~/.bashrc`` if so; check to see if this is the case on your target
+    systems.
 
-Init-style start/stop/restart scripts (e.g. ``/etc/init.d/apache2 start``)
-sometimes don't like Fabric's allocation of a pseudo-tty, which is active by
-default. In almost all cases, explicitly calling the command in question with
-``pty=False`` works correctly::
+.. note::
+    Another workaround for ``bash`` users is to reply on its ``$BASH_ENV``
+    functionality, which names a file path as the startup file to load:
 
-    sudo("/etc/init.d/apache2 restart", pty=False)
+    - configure your SSH server to ``AcceptEnv BASH_ENV``, so that you can
+      actually set that env var for the remote session at the top level (most
+      SSH servers disallow this method by default).
+    - decide which file this should be, though if you're already modifying
+      files like ``~/.bash_profile`` or ``~/.bashrc``, you may want to just
+      point at that exact path.
+    - set the Fabric configuration value ``run.env`` to aim at the above path,
+      e.g. ``{"BASH_ENV": "~/.bash_profile"}``.
 
-If you have no need for interactive behavior and run into this problem
-frequently, you may want to deactivate pty allocation globally by setting
-:ref:`env.always_use_pty <always-use-pty>` to ``False``.
 
 .. _one-shell-per-command:
 
@@ -88,103 +85,56 @@
 ===============================================================
 
 While Fabric can be used for many shell-script-like tasks, there's a slightly
-unintuitive catch: each `~fabric.operations.run` or `~fabric.operations.sudo`
-call has its own distinct shell session. This is required in order for Fabric
-to reliably figure out, after your command has run, what its standard out/error
+unintuitive catch: each `~fabric.connection.Connection.run` or
+`~fabric.connection.Connection.sudo` call (or the ``run``/``sudo`` functions in
+v1) has its own distinct shell session. This is required in order for Fabric to
+reliably figure out, after your command has run, what its standard out/error
 and return codes were.
 
 Unfortunately, it means that code like the following doesn't behave as you
 might assume::
 
-    def deploy():
-        run("cd /path/to/application")
-        run("./update.sh")
-
-If that were a shell script, the second `~fabric.operations.run` call would
-have executed with a current working directory of ``/path/to/application/`` --
-but because both commands are run in their own distinct session over SSH, it
-actually tries to execute ``$HOME/update.sh`` instead (since your remote home
-directory is the default working directory).
+    @task
+    def deploy(c):
+        c.run("cd /path/to/application")
+        c.run("./update.sh")
+
+If that were a shell script, the second `~fabric.connection.Connection.run`
+call would have executed with a current working directory of
+``/path/to/application/`` -- but because both commands are run in their own
+distinct session over SSH, it actually tries to execute ``$HOME/update.sh``
+instead (since your remote home directory is the default working directory).
 
 A simple workaround is to make use of shell logic operations such as ``&&``,
 which link multiple expressions together (provided the left hand side executed
 without error) like so::
 
-    def deploy():
-        run("cd /path/to/application && ./update.sh")
+    def deploy(c):
+        c.run("cd /path/to/application && ./update.sh")
 
-Fabric provides a convenient shortcut for this specific use case, in fact:
-`~fabric.context_managers.cd`. There is also `~fabric.context_managers.prefix`
-for arbitrary prefix commands.
+.. TODO: reinsert mention of 'with cd():' if that is reimplemented
 
 .. note::
     You might also get away with an absolute path and skip directory changing
     altogether::
 
-        def deploy():
-            run("/path/to/application/update.sh")
+        def deploy(c):
+            c.run("/path/to/application/update.sh")
 
     However, this requires that the command in question makes no assumptions
     about your current working directory!
 
 
-How do I use ``su`` to run commands as another user?
-====================================================
-
-This is a special case of :ref:`one-shell-per-command`. As that FAQ explains,
-commands like ``su`` which are 'stateful' do not work well in Fabric, so
-workarounds must be used.
-
-In the case of running commands as a user distinct from the login user, you
-have two options:
-
-#. Use `~fabric.operations.sudo` with its ``user=`` kwarg, e.g.
-   ``sudo("command", user="otheruser")``. If you want to factor the ``user``
-   part out of a bunch of commands, use `~fabric.context_managers.settings` to
-   set ``env.sudo_user``::
-
-       with settings(sudo_user="otheruser"):
-           sudo("command 1")
-           sudo("command 2")
-           ...
-
-#. If your target system cannot use ``sudo`` for some reason, you can still use
-   ``su``, but you need to invoke it in a non-interactive fashion by telling it
-   to run a specific command instead of opening a shell. Typically this is the
-   ``-c`` flag, e.g. ``su otheruser -c "command"``.
-
-   To run multiple commands in the same ``su -c`` "wrapper", you could e.g.
-   write a wrapper function around `~fabric.operations.run`::
-
-       def run_su(command, user="otheruser"):
-           return run('su %s -c "%s"' % (user, command))
+.. TODO:
+    reinstate FAQ about 'su' / running as another user, when sudo grows that
+    back. (Probably in Invoke tho.)
 
 
 Why do I sometimes see ``err: stdin: is not a tty``?
 ====================================================
 
-This message is typically generated by programs such as ``biff`` or ``mesg``
-lurking within your remote user's ``.profile`` or ``.bashrc`` files (or any
-other such files, including system-wide ones.) Fabric's default mode of
-operation involves executing the Bash shell in "login mode", which causes these
-files to be executed.
-
-Because Fabric also doesn't bother asking the remote end for a tty by default
-(as it's not usually necessary) programs fired within your startup files, which
-expect a tty to be present, will complain -- and thus, stderr output about
-"stdin is not a tty" or similar.
-
-There are multiple ways to deal with this problem:
-
-* Find and remove or comment out the offending program call. If the program was
-  not added by you on purpose and is simply a legacy of the operating system,
-  this may be safe to do, and is the simplest approach.
-* Override ``env.shell`` to remove the ``-l`` flag. This should tell Bash not
-  to load your startup files. If you don't depend on the contents of your
-  startup files (such as aliases or whatnot) this may be a good solution.
-* Pass ``pty=True`` to `run` or `sudo`, which will force allocation of a
-  pseudo-tty on the remote end, and hopefully cause the offending program to be
-  less cranky.
+See :ref:`Invoke's FAQ <stdin-not-tty>` for this; even for Fabric v1,
+which is not based on Invoke, the answer is the same.
 
 
 .. _faq-daemonize:
@@ -192,11 +142,11 @@
 Why can't I run programs in the background with ``&``? It makes Fabric hang.
 ============================================================================
 
-Because Fabric executes a shell on the remote end for each invocation of
-``run`` or ``sudo`` (:ref:`see also <one-shell-per-command>`), backgrounding a
-process via the shell will not work as expected. Backgrounded processes may
-still prevent the calling shell from exiting until they stop running, and this
-in turn prevents Fabric from continuing on with its own execution.
+Because SSH executes a new shell session on the remote end for each invocation
+of ``run`` or ``sudo`` (:ref:`see also <one-shell-per-command>`), backgrounded
+processes may prevent the calling shell from exiting until the processes stop
+running, which in turn prevents Fabric from continuing on with its own
+execution.
 
 The key to fixing this is to ensure that your process' standard pipes are all
 disassociated from the calling shell, which may be done in a number of ways
@@ -216,52 +166,8 @@
   running shell; these tools have the benefit of allowing you to reattach to
   the process later on if needed (though they are more ad-hoc than
   ``supervisord``-like tools).
-* You *may* be able to the program under ``nohup`` or similar "in-shell" tools
-  - however we strongly recommend the prior approaches because ``nohup`` has
-  only worked well for a minority of our users.
-
-
-.. _faq-bash:
-
-My remote system doesn't have ``bash`` installed by default, do I need to install ``bash``?
-===========================================================================================
-
-While Fabric is written with ``bash`` in mind, it's not an absolute
-requirement.  Simply change :ref:`env.shell <shell>` to call your desired shell, and
-include an argument similar to ``bash``'s ``-c`` argument, which allows us to
-build shell commands of the form::
-
-    /bin/bash -l -c "<command string here>"
-
-where ``/bin/bash -l -c`` is the default value of :ref:`env.shell <shell>`.
-
-.. note::
-
-    The ``-l`` argument specifies a login shell and is not absolutely
-    required, merely convenient in many situations. Some shells lack the option
-    entirely and it may be safely omitted in such cases.
-
-A relatively safe baseline is to call ``/bin/sh``, which may call the original
-``sh`` binary, or (on some systems) ``csh``, and give it the ``-c``
-argument, like so::
-
-    from fabric.api import env
-
-    env.shell = "/bin/sh -c"
-
-This has been shown to work on FreeBSD and may work on other systems as well.
-
-
-.. _faq-csh:
-
-I use ``csh`` remotely and keep getting errors about ``Unmatched ".``.
-======================================================================
-
-If the remote host uses ``csh`` for your login shell, Fabric requires the shell
-variable ``backslash_quote`` to be set, or else any quote-escaping Fabric does
-will not work. For example, add the following line to ``~/.cshrc``::
-
-    set backslash_quote
+* Run the program under ``nohup`` or similar "in-shell" tools - note that this
+  approach has seen limited success for most users.
 
 
 I'm sometimes incorrectly asked for a passphrase instead of a password.
@@ -279,14 +185,5 @@
 enter passphrase for private key", but the text you enter is actually being
 sent to the remote end's password authentication.
 
-We hope to address this in future releases by modifying a fork of the
+We hope to address this in future releases by contributing to the
 aforementioned SSH library.
-
-
-Is Fabric thread-safe?
-======================
-
-Currently, no, it's not -- the present version of Fabric relies heavily on
-shared state in order to keep the codebase simple. However, there are definite
-plans to update its internals so that Fabric may be either threaded or
-otherwise parallelized so your tasks can run on multiple servers concurrently.
diff -Nru fabric-1.14.0/sites/www/index.rst fabric-2.5.0/sites/www/index.rst
--- fabric-1.14.0/sites/www/index.rst	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/sites/www/index.rst	2019-08-06 23:57:28.000000000 +0100
@@ -1,24 +1,208 @@
 Welcome to Fabric!
 ==================
 
-.. include:: ../../README.rst
+What is Fabric?
+---------------
 
-----
+Fabric is a high level Python (2.7, 3.4+) library designed to execute shell
+commands remotely over SSH, yielding useful Python objects in return:
 
-This website covers project information for Fabric such as the changelog,
-contribution guidelines, development roadmap, news/blog, and so forth.
-Detailed usage and API documentation can be found at our code documentation
-site, `docs.fabfile.org <http://docs.fabfile.org>`_.
+.. testsetup:: opener
+
+    mock = MockRemote()
+    # NOTE: hard to get trailing whitespace in a doctest/snippet block, so we
+    # just leave the 'real' newline off here too. Whatever.
+    mock.expect(out=b"Linux")
+
+.. testcleanup:: opener
+
+    mock.stop()
+
+.. doctest:: opener
+
+    >>> from fabric import Connection
+    >>> result = Connection('web1.example.com').run('uname -s', hide=True)
+    >>> msg = "Ran {0.command!r} on {0.connection.host}, got stdout:\n{0.stdout}"
+    >>> print(msg.format(result))
+    Ran 'uname -s' on web1.example.com, got stdout:
+    Linux
+
+It builds on top of `Invoke <http://pyinvoke.org>`_ (subprocess command
+execution and command-line features) and `Paramiko <http://paramiko.org>`_ (SSH
+protocol implementation), extending their APIs to complement one another and
+provide additional functionality.
+
+.. note::
+    Fabric users may also be interested in two *strictly optional* libraries
+    which implement best-practice user-level code: `Invocations
+    <https://invocations.readthedocs.io>`_ (Invoke-only, locally-focused CLI
+    tasks) and `Patchwork <https://fabric-patchwork.readthedocs.io>`_
+    (remote-friendly, typically shell-command-focused, utility functions).
+
+How is it used?
+---------------
+
+Core use cases for Fabric include (but are not limited to):
+
+* Single commands on individual hosts:
+
+  .. testsetup:: single-command
+  
+      from fabric import Connection
+      mock = MockRemote()
+      mock.expect(out=b"web1")
+  
+  .. testcleanup:: single-command
+  
+      mock.stop()
+  
+  .. doctest:: single-command
+
+      >>> result = Connection('web1').run('hostname')
+      web1
+      >>> result
+      <Result cmd='hostname' exited=0>
+
+* Single commands across multiple hosts (via varying methodologies: serial,
+  parallel, etc):
+
+  .. testsetup:: multiple-hosts
+  
+      from fabric import Connection
+      mock = MockRemote()
+      mock.expect_sessions(
+          Session(host='web1', cmd='hostname', out=b'web1\n'),
+          Session(host='web2', cmd='hostname', out=b'web2\n'),
+      )
+  
+  .. testcleanup:: multiple-hosts
+  
+      mock.stop()
+  
+  .. doctest:: multiple-hosts
+
+      >>> from fabric import SerialGroup     
+      >>> result = SerialGroup('web1', 'web2').run('hostname')
+      web1
+      web2
+      >>> # Sorting for consistency...it's a dict!
+      >>> sorted(result.items())
+      [(<Connection host=web1>, <Result cmd='hostname' exited=0>), ...]
+
+* Python code blocks (functions/methods) targeted at individual connections:
+
+  .. testsetup:: tasks
+  
+      from fabric import Connection
+      mock = MockRemote()
+      mock.expect(commands=[
+          Command("uname -s", out=b"Linux\n"),
+          Command("df -h / | tail -n1 | awk '{print $5}'", out=b'33%\n'),
+      ])
+  
+  .. testcleanup:: tasks
+  
+      mock.stop()
+  
+  .. doctest:: tasks
+
+      >>> def disk_free(c):
+      ...     uname = c.run('uname -s', hide=True)
+      ...     if 'Linux' in uname.stdout:
+      ...         command = "df -h / | tail -n1 | awk '{print $5}'"
+      ...         return c.run(command, hide=True).stdout.strip()
+      ...     err = "No idea how to get disk space on {}!".format(uname)
+      ...     raise Exit(err)
+      ...
+      >>> print(disk_free(Connection('web1')))
+      33%
+
+* Python code blocks on multiple hosts:
+
+  .. testsetup:: tasks-on-multiple-hosts
+  
+      from fabric import Connection, SerialGroup
+      mock = MockRemote()
+      mock.expect_sessions(
+        Session(host='web1', commands=[
+          Command("uname -s", out=b"Linux\n"),
+          Command("df -h / | tail -n1 | awk '{print $5}'", out=b'33%\n'),
+        ]),
+        Session(host='web2', commands=[
+          Command("uname -s", out=b"Linux\n"),
+          Command("df -h / | tail -n1 | awk '{print $5}'", out=b'17%\n'),
+        ]),
+        Session(host='db1', commands=[
+          Command("uname -s", out=b"Linux\n"),
+          Command("df -h / | tail -n1 | awk '{print $5}'", out=b'2%\n'),
+        ]),
+      )
+  
+  .. testcleanup:: tasks-on-multiple-hosts
+  
+      mock.stop()
+  
+  .. doctest:: tasks-on-multiple-hosts
+
+      >>> # NOTE: Same code as above!
+      >>> def disk_free(c):
+      ...     uname = c.run('uname -s', hide=True)
+      ...     if 'Linux' in uname.stdout:
+      ...         command = "df -h / | tail -n1 | awk '{print $5}'"
+      ...         return c.run(command, hide=True).stdout.strip()
+      ...     err = "No idea how to get disk space on {}!".format(uname)
+      ...     raise Exit(err)
+      ...
+      >>> for cxn in SerialGroup('web1', 'web2', 'db1'):
+      ...    print("{}: {}".format(cxn, disk_free(cxn)))
+      <Connection host=web1>: 33%
+      <Connection host=web2>: 17%
+      <Connection host=db1>: 2%
+
+In addition to these library-oriented use cases, Fabric makes it easy to
+integrate with Invoke's command-line task functionality, invoking via a ``fab``
+binary stub:
+
+* Python functions, methods or entire objects can be used as CLI-addressable
+  tasks, e.g. ``fab deploy``;
+* Tasks may indicate other tasks to be run before or after they themselves
+  execute (pre- or post-tasks);
+* Tasks are parameterized via regular GNU-style arguments, e.g. ``fab deploy
+  --env=prod -d``;
+* Multiple tasks may be given in a single CLI session, e.g. ``fab build
+  deploy``;
+* Much more - all other Invoke functionality is supported - see `its
+  documentation <http://docs.pyinvoke.org>`_ for details.
+
+I'm a user of Fabric 1, how do I upgrade?
+-----------------------------------------
+
+We've packaged modern Fabric in a manner that allows installation alongside
+Fabric 1, so you can upgrade at whatever pace your use case requires. There are
+multiple possible approaches -- see our :ref:`detailed upgrade documentation
+<upgrading>` for details.
+
+What is this website?
+---------------------
+
+``www.fabfile.org`` provides project information for Fabric such as the
+changelog, contribution guidelines, development roadmap, news/blog, and so
+forth.
+
+Detailed conceptual and API documentation can be found at our code
+documentation site, `docs.fabfile.org <http://docs.fabfile.org>`_.
 
-Please see the navigation sidebar to the left to begin.
 
 .. toctree::
     :hidden:
 
     changelog
+    changelog-v1
     FAQs <faq>
     installing
-    troubleshooting
+    installing-1.x
+    upgrading
     development
+    troubleshooting
     Roadmap <roadmap>
     contact
diff -Nru fabric-1.14.0/sites/www/installing-1.x.rst fabric-2.5.0/sites/www/installing-1.x.rst
--- fabric-1.14.0/sites/www/installing-1.x.rst	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/sites/www/installing-1.x.rst	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,93 @@
+================
+Installing (1.x)
+================
+
+.. note::
+    Installing Fabric 2.0 or above? Looking for non-PyPI downloads or source
+    code checkout instructions? See :doc:`installing`.
+
+This document includes legacy notes on installing Fabric 1.x. Users are
+strongly encouraged to upgrade to 2.x when possible.
+
+
+Basic installation
+==================
+
+Fabric is best installed via `pip <http://pip-installer.org>`_; to ensure you
+get Fabric 1 instead of the new but incompatible Fabric 2, specify ``<2.0``::
+
+    $ pip install 'fabric<2.0'
+
+All advanced ``pip`` use cases work too, such as installing the latest copy of
+the ``v1`` development branch::
+
+    $ pip install -e 'git+https://github.com/fabric/fabric@v1#egg=fabric'
+
+Or cloning the Git repository and running::
+
+    $ git checkout v1
+    $ pip install -e .
+
+within it.
+
+Your operating system may also have a Fabric package available (though these
+are typically older and harder to support), typically called ``fabric`` or
+``python-fabric``. E.g.::
+
+    $ sudo apt-get install fabric
+
+.. note:: Make sure to confirm which major version is currently packaged!
+
+
+Dependencies
+============
+
+In order for Fabric's installation to succeed, you will need four primary pieces of software:
+
+* the Python programming language;
+* the ``setuptools`` packaging/installation library;
+* the Python `Paramiko <http://paramiko.org>`_ SSH library;
+* and Paramiko's dependency, `Cryptography <https://cryptography.io>`_.
+
+and, if using parallel execution mode,
+
+* the `multiprocessing`_ library.
+
+Please read on for important details on each dependency -- there are a few
+gotchas.
+
+Python
+------
+
+Fabric requires `Python <http://python.org>`_ version 2.5+.
+
+setuptools
+----------
+
+`Setuptools`_ comes with most Python installations by default; if yours
+doesn't, you'll need to grab it. In such situations it's typically packaged as
+``python-setuptools``, ``py26-setuptools`` or similar.
+
+.. _setuptools: https://pypi.org/project/setuptools
+
+``multiprocessing``
+-------------------
+
+An optional dependency, the ``multiprocessing`` library is included in Python's
+standard library in version 2.6 and higher. If you're using Python 2.5 and want
+to make use of Fabric's parallel execution features you'll need to install it
+manually; the recommended route, as usual, is via ``pip``.  Please see the
+`multiprocessing PyPI page <https://pypi.org/project/multiprocessing/>`_ for
+details.
+
+
+.. warning::
+    Early versions of Python 2.6 (in our testing, 2.6.0 through 2.6.2) ship
+    with a buggy ``multiprocessing`` module that appears to cause Fabric to
+    hang at the end of sessions involving large numbers of concurrent hosts.
+    If you encounter this problem, either use ``env.pool_size`` / ``-z`` to
+    limit the amount of concurrency, or upgrade to Python
+    >=2.6.3.
+    
+    Python 2.5 is unaffected, as it requires the PyPI version of
+    ``multiprocessing``, which is newer than that shipped with Python <2.6.3.
diff -Nru fabric-1.14.0/sites/www/installing.rst fabric-2.5.0/sites/www/installing.rst
--- fabric-1.14.0/sites/www/installing.rst	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/sites/www/installing.rst	2019-08-06 23:57:28.000000000 +0100
@@ -2,126 +2,119 @@
 Installing
 ==========
 
-Fabric is best installed via `pip <http://pip-installer.org>`_ (highly
-recommended) or `easy_install
-<http://wiki.python.org/moin/CheeseShopTutorial>`_ (older, but still works
-fine), e.g.::
+.. note::
+    Users looking to install Fabric 1.x should see :doc:`installing-1.x`.
+    However, :doc:`upgrading <upgrading>` to 2.x is strongly recommended.
+
+Fabric is best installed via `pip <http://pip-installer.org>`_::
 
     $ pip install fabric
 
-You may also opt to use your operating system's package manager; the package is
-typically called ``fabric`` or ``python-fabric``. E.g.::
+All advanced ``pip`` use cases work too, such as::
+
+    $ pip install -e git+https://github.com/fabric/fabric
+
+Or cloning the Git repository and running::
+
+    $ pip install -e .
+
+within it.
+
+Your operating system may also have a Fabric package available (though these
+are typically older and harder to support), typically called ``fabric`` or
+``python-fabric``. E.g.::
 
     $ sudo apt-get install fabric
 
-Advanced users wanting to install a development version may use ``pip`` to grab
-the latest master branch (as well as the dev version of the Paramiko
-dependency)::
-
-    $ pip install -e git+https://github.com/paramiko/paramiko/#egg=paramiko
-    $ pip install -e git+https://github.com/fabric/fabric/#egg=fabric
-
-.. warning::
-
-    Development installs of Fabric, regardless of whether they involve source
-    checkouts or direct ``pip`` installs, require the development version of
-    Paramiko to be installed beforehand or Fabric's installation may fail.
+
+.. _installing-as-fabric2:
+
+Installing modern Fabric as ``fabric2``
+=======================================
+
+Users who are migrating from Fabric 1 to Fabric 2+ may find it useful to have
+both versions installed side-by-side. The easiest way to do this is to use the
+handy ``fabric2`` PyPI entry::
+
+    $ pip install fabric2
+
+This upload is generated from the normal Fabric repository, but is tweaked at
+build time so that it installs a ``fabric2`` package instead of a ``fabric``
+one (and a ``fab2`` binary instead of a ``fab`` one.) The codebase is otherwise
+unchanged.
+
+Users working off of the Git repository can enable that same tweak with an
+environment variable, e.g.::
+
+    $ PACKAGE_AS_FABRIC2=yes pip install -e .
+
+.. note::
+    The value of the environment variable doesn't matter, as long as it is not
+    empty.
+
+``fabric`` and ``fabric2`` vs ``fabric3``
+-----------------------------------------
+
+Unfortunately, the ``fabric3`` entry on PyPI is an unauthorized fork of Fabric
+1.x which we do not control. Once modern Fabric gets up to 3.x, 4.x etc, we'll
+likely continue distributing it via both ``fabric`` and ``fabric2`` for
+convenience; there will never be any official ``fabric3``, ``fabric4`` etc.
+
+In other words, ``fabric2`` is purely there to help users of 1.x cross the 2.0
+"major rewrite" barrier; future major versions will *not* be large rewrites and
+will only have small sets of backward incompatibilities.
+
+Inability to ``pip install -e`` both versions
+---------------------------------------------
+
+You may encounter issues if *both* versions of Fabric are installed via ``pip
+install -e``, due to how that functionality works (tl;dr it just adds the
+checkout directories to ``sys.path``, regardless of whether you wanted to
+"install" all packages within them - so Fabric 2+'s ``fabric/`` package still
+ends up visible to the import system alongside ``fabric2/``).
+
+Thus, you may only have one of the local copies of Fabric installed in
+'editable' fashion at a time, and the other must be repeatedly reinstalled via
+``pip install`` (no ``-e``) if you need to make edits to it.
+
+Order of installations
+----------------------
+
+Due to the same pip quirk mentioned above, if either of your Fabric versions
+are installed in 'editable' mode, you **must** install the 'editable' version
+first, and then install the 'static' version second.
+
+For example, if you're migrating from some public release of Fabric 1 to a
+checkout of modern Fabric::
+
+    $ PACKAGE_AS_FABRIC2=yes pip install -e /path/to/fabric2
+    $ pip install fabric==1.14.0
+
+You may see some warnings on that second ``pip install`` (eg ``Not uninstalling
+fabric`` or ``Can't uninstall 'fabric'``) but as long as it exits cleanly and
+says something like ``Successfully installed fabric-1.14.0``, you should be
+okay. Double check with e.g. ``pip list`` and you should have entries for both
+``fabric`` and ``fabric2``.
 
 
 Dependencies
 ============
 
-In order for Fabric's installation to succeed, you will need three primary pieces of software:
+In order for Fabric's installation to succeed, you will need the following:
 
-* the Python programming language;
-* the ``setuptools`` packaging/installation library;
-* and the Python `Paramiko <http://paramiko.org>`_ SSH library. Paramiko's dependencies differ
-  significantly between the 1.x and 2.x releases. See the `Paramiko installation docs
-  <http://www.paramiko.org/installing.html>`_ for more info.
-
-and, if using the :ref:`parallel execution mode <parallel-execution>`:
-
-* the `multiprocessing`_ library.
-
-If you're using Paramiko 1.12 or above, you will also need an additional
-dependency for Paramiko:
-
-* the `ecdsa <https://pypi.python.org/pypi/ecdsa/>`_ library
-
-Please read on for important details on these -- there are a few gotchas.
-
-Python
-------
-
-Fabric requires `Python <http://python.org>`_ version 2.5 - 2.7. Some caveats
-and notes about other Python versions:
-
-* We are not planning on supporting **Python 2.4** given its age and the number
-  of useful tools in Python 2.5 such as context managers and new modules.
-  That said, the actual amount of 2.5-specific functionality is not
-  prohibitively large, and we would link to -- but not support -- a third-party
-  2.4-compatible fork. (No such fork exists at this time, to our knowledge.)
-* Fabric has not yet been tested on **Python 3.x** and is thus likely to be
-  incompatible with that line of development. However, we try to be at least
-  somewhat forward-looking (e.g. using ``print()`` instead of ``print``) and
-  will definitely be porting to 3.x in the future once our dependencies do.
-
-setuptools
-----------
-
-`Setuptools`_ comes with some Python installations by default; if yours doesn't,
-you'll need to grab it. In such situations it's typically packaged as
-``python-setuptools``, ``py25-setuptools`` or similar. Fabric may drop its
-setuptools dependency in the future, or include alternative support for the
-`Distribute`_ project, but for now setuptools is required for installation.
-
-.. _setuptools: http://pypi.python.org/pypi/setuptools
-.. _Distribute: http://pypi.python.org/pypi/distribute
-
-``multiprocessing``
--------------------
-
-An optional dependency, the ``multiprocessing`` library is included in Python's
-standard library in version 2.6 and higher. If you're using Python 2.5 and want
-to make use of Fabric's :ref:`parallel execution features <parallel-execution>`
-you'll need to install it manually; the recommended route, as usual, is via
-``pip``.  Please see the `multiprocessing PyPI page
-<http://pypi.python.org/pypi/multiprocessing/>`_ for details.
-
-
-.. warning::
-    Early versions of Python 2.6 (in our testing, 2.6.0 through 2.6.2) ship
-    with a buggy ``multiprocessing`` module that appears to cause Fabric to
-    hang at the end of sessions involving large numbers of concurrent hosts.
-    If you encounter this problem, either use :ref:`env.pool_size / -z
-    <pool-size>` to limit the amount of concurrency, or upgrade to Python
-    >=2.6.3.
-    
-    Python 2.5 is unaffected, as it requires the PyPI version of
-    ``multiprocessing``, which is newer than that shipped with Python <2.6.3.
+* the Python programming language, versions 2.7 or 3.4+;
+* the `Invoke <http://pyinvoke.org>`_ command-running and task-execution
+  library;
+* and the `Paramiko <http://paramiko.org>`_ SSH library (as well as its own
+  dependencies; see `its install docs <http://paramiko.org/installing.html>`_.)
 
 Development dependencies
 ------------------------
 
 If you are interested in doing development work on Fabric (or even just running
-the test suite), you may also need to install some or all of the following
-packages:
-
-* `git <http://git-scm.com>`_ and `Mercurial`_, in order to obtain some of the
-  other dependencies below;
-* `Nose <https://github.com/nose-devs/nose>`_
-* `Coverage <http://nedbatchelder.com/code/modules/coverage.html>`_
-* `PyLint <http://www.logilab.org/857>`_
-* `Fudge <http://farmdev.com/projects/fudge/index.html>`_
-* `Sphinx <http://sphinx.pocoo.org/>`_
-
-For an up-to-date list of exact testing/development requirements, including
-version numbers, please see the ``requirements.txt`` file included with the
-source distribution. This file is intended to be used with ``pip``, e.g. ``pip
-install -r requirements.txt``.
-
-.. _Mercurial: http://mercurial.selenic.com/wiki/
-
+the test suite), you'll need the libraries listed in the
+``dev-requirements.txt`` (included in the source distribution.) Usually it's
+easy to simply ``pip install -r dev-requirements.txt``.
 
 .. _downloads:
 
@@ -129,9 +122,8 @@
 =========
 
 To obtain a tar.gz or zip archive of the Fabric source code, you may visit
-`Fabric's PyPI page <http://pypi.python.org/pypi/Fabric>`_, which offers manual
-downloads in addition to being the entry point for ``pip`` and
-``easy-install``.
+`Fabric's PyPI page <https://pypi.org/project/fabric>`_, which offers manual
+downloads in addition to being the entry point for ``pip``.
 
 
 .. _source-code-checkouts:
@@ -144,8 +136,8 @@
 downloading official releases, you have the following options:
 
 * Clone the canonical repository straight from `the Fabric organization's
-  repository on Github <https://github.com/fabric/fabric>`_,
-  ``git://github.com/fabric/fabric.git``
+  repository on Github <https://github.com/fabric/fabric>`_ (cloning
+  instructions available on that page).
 * Make your own fork of the Github repository by making a Github account,
   visiting `fabric/fabric <http://github.com/fabric/fabric>`_ and clicking the
   "fork" button.
@@ -153,35 +145,11 @@
 .. note::
 
     If you've obtained the Fabric source via source control and plan on
-    updating your checkout in the future, we highly suggest using ``python
-    setup.py develop`` instead -- it will use symbolic links instead of file
-    copies, ensuring that imports of the library or use of the command-line
-    tool will always refer to your checkout.
+    updating your checkout in the future, we highly suggest using ``pip install
+    -e .`` (or ``python setup.py develop``) instead -- it will use symbolic
+    links instead of file copies, ensuring that imports of the library or use
+    of the command-line tool will always refer to your checkout.
 
 For information on the hows and whys of Fabric development, including which
 branches may be of interest and how you can help out, please see the
 :doc:`development` page.
-
-
-.. _pypm:
-
-ActivePython and PyPM
-=====================
-
-Windows users who already have ActiveState's `ActivePython
-<http://www.activestate.com/activepython/downloads>`_ distribution installed
-may find Fabric is best installed with `its package manager, PyPM
-<http://code.activestate.com/pypm/>`_. Below is example output from an
-installation of Fabric via ``pypm``::
-
-    C:\> pypm install fabric
-    The following packages will be installed into "%APPDATA%\Python" (2.7):
-     paramiko-1.7.8 pycrypto-2.4 fabric-1.3.0
-    Get: [pypm-free.activestate.com] fabric 1.3.0
-    Get: [pypm-free.activestate.com] paramiko 1.7.8
-    Get: [pypm-free.activestate.com] pycrypto 2.4
-    Installing paramiko-1.7.8
-    Installing pycrypto-2.4
-    Installing fabric-1.3.0
-    Fixing script %APPDATA%\Python\Scripts\fab-script.py
-    C:\>
diff -Nru fabric-1.14.0/sites/www/roadmap.rst fabric-2.5.0/sites/www/roadmap.rst
--- fabric-1.14.0/sites/www/roadmap.rst	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/sites/www/roadmap.rst	2019-08-06 23:57:28.000000000 +0100
@@ -1,73 +1,47 @@
+.. _roadmap:
+
 ===================
 Development roadmap
 ===================
 
 This document outlines Fabric's intended development path. Please make sure
 you're reading `the latest version <http://fabfile.org/roadmap.html>`_ of this
-document!
+document, and also see the page about :ref:`upgrading <upgrading>` if you are
+migrating from version 1 to versions 2 or above.
+
+Fabric 2 and above
+==================
+
+Modern Fabric versions (2+) receive active feature and bugfix development:
 
-.. warning::
-    This information is subject to change without warning, and should not be
-    used as a basis for any life- or career-altering decisions!
+- **2.0**: Initial public release, arguably a technology preview and a
+  packaging/upgrade trial. Intent is to act as a jolt for users of 1.x who
+  aren't pinning their dependencies (sorry, folks!), enable installation
+  via PyPI so users don't have to install via Git to start upgrading, and
+  generally get everything above-board and iterating in classic semantic
+  versioning fashion.
+- **2.1, 2.2, 2.3, etc**: Implement the most pressing "missing features",
+  including features which were present in 1.0 (see :ref:`upgrading` for
+  details on these) as well as any brand new features we've been wanting in 2.x
+  for a while (though most of these will come via Invoke and/or Paramiko
+  releases -- see note below for more).
+- **3.0, 4.0, etc**: Subsequent major releases will **not** be full-on rewrites
+  as 2.0 was, but will be *small* (feature-release-sized) releases that just
+  happen to contain one or more backwards incompatible API changes. These will
+  be clearly marked in the changelog and reflected in the upgrading
+  documentation.
+
+.. note::
+    Many features that you may use via Fabric will only need development in the
+    libraries Fabric wraps -- `Invoke <http://pyinvoke.org>`_ and `Paramiko
+    <http://paramiko.org>`_ -- and unless Fabric itself needs changes to match,
+    you can often get new features by upgrading only one of the three. Make
+    sure to check the other projects' changelogs periodically!
 
 Fabric 1.x
 ==========
 
-Fabric 1.x, while not quite yet end-of-life'd, has reached a tipping point
-regarding internal tech debt & ability to make improvements without harming
-backwards compatibility.
-
-As such, future 1.x releases (**1.6** onwards) will emphasize small-to-medium
-features (new features not requiring major overhauls of the internals) and
-bugfixes.
-
-Invoke, Fabric 2.x and Patchwork
-================================
-
-While 1.x moves on as above, we are working on a reimagined 2.x version of the
-tool, and plan to:
-
-* Finish and release `the Invoke tool/library
-  <https://github.com/pyinvoke/invoke>`_ (see also :issue:`565` and `this
-  Invoke FAQ
-  <http://www.pyinvoke.org/faq.html#why-was-invoke-split-off-from-the-fabric-project>`_),
-  which is a revamped and standalone version of Fabric's task running
-  components.
-
-    * As of early 2015, Invoke is already reasonably mature and has a handful of
-      features lacking in Fabric itself, including but not limited to:
-      
-        * a more explicit and powerful namespacing implementation
-        * "regular" style CLI flags, including powerful tab completion
-        * before/after hooks
-        * explicit context management (no shared state)
-        * significantly more powerful configuration mechanisms
-
-    * Invoke is already Python 3 compatible, due to being a new codebase with
-      few dependencies.
-    * As Fabric 2 is developed, Invoke will approach a 1.0 release, and will
-      continue to grow & change to suit Fabric's needs while remaining a high
-      quality standalone task runner.
-
-* Release Fabric 2.0, a mostly-rewritten Fabric core:
-
-    * Leverage Invoke for task running, leaving Fabric itself much more library
-      oriented.
-    * Implement object-oriented hosts/host lists and all the fun stuff that
-      provides (no more hacky host string and unintuitive env var
-      manipulation.)
-    * No more shared state by default (thanks to Invoke's context design.)
-    * Any other core overhauls difficult to do in a backwards compatible
-      fashion.
-    * Test-driven development (Invoke does this as well.)
-
-* Spin off ``fabric.contrib.*`` into a standalone "super-Fabric" (as in, "above
-  Fabric") library, `Patchwork <https://github.com/fabric/patchwork>`_.
-
-    * This lets core "execute commands on hosts" functionality iterate
-      separately from "commonly useful shortcuts using Fabric core".
-    * Lots of preliminary work & prior-art scanning has been done in
-      :issue:`461`.
-    * A public-but-alpha codebase for Patchwork exists as we think about the
-      API, and is currently based on Fabric 1.x. It will likely be Fabric 2.x
-      based by the time it is stable.
+Fabric 1.x has reached a tipping point regarding internal tech debt, lack of
+testability & ability to make improvements without harming backwards
+compatibility. As such, the 1.x line now receives bugfixes only. We
+**strongly** encourage all users to :ref:`upgrade <upgrading>` to Fabric 2.x.
diff -Nru fabric-1.14.0/sites/www/troubleshooting.rst fabric-2.5.0/sites/www/troubleshooting.rst
--- fabric-1.14.0/sites/www/troubleshooting.rst	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/sites/www/troubleshooting.rst	2019-08-06 23:57:28.000000000 +0100
@@ -9,7 +9,7 @@
   version, your problem may have been solved already! Upgrading is always the
   best first step.
 * **Try older versions.** If you're already *on* the latest Fabric, try rolling
-  back a few minor versions (e.g. if on 1.7, try Fabric 1.5 or 1.6) and see if
+  back a few minor versions (e.g. if on 2.3, try Fabric 2.2 or 2.1) and see if
   the problem goes away. This will help the devs narrow down when the problem
   first arose in the commit log.
 * **Try switching up your Paramiko.** Fabric relies heavily on the Paramiko
@@ -24,23 +24,26 @@
   behavior or output of a remote command, try recreating it without Fabric
   involved:
 
-    * Run Fabric with ``--show=debug`` and look for the ``run:`` or ``sudo:``
-      line about the command in question. Try running that exact command,
-      including any ``/bin/bash`` wrapper, remotely and see what happens. This
-      may find problems related to the bash or sudo wrappers.
-    * Execute the command (both the normal version, and the 'unwrapped' version
-      seen via ``--show=debug``) from your local workstation using ``ssh``,
-      e.g.::
-
-          $ ssh -t mytarget "my command"
-
-      The ``-t`` flag matches Fabric's default behavior of enabling a PTY
-      remotely. This helps identify apps that behave poorly when run in a
-      non-shell-spawned PTY.
+    * Find out the exact command Fabric is executing on your behalf:
+
+        - In 2.x and up, activate command echoing via the ``echo=True`` keyword
+          argument, the ``run.echo`` config setting, or the ``-e`` CLI option.
+        - In 1.x, run Fabric with ``--show=debug`` and look for ``run:`` or
+          ``sudo:`` lines.
+
+    * Execute the command in an interactive remote shell first, to make sure it
+      works for a regular human; this will catch issues such as errors in
+      command construction.
+    * If that doesn't find the issue, run the command over a non-shell SSH
+      session, e.g. ``ssh yourserver "your command"``. Depending on your
+      settings and Fabric version, you may want to use ``ssh -T`` (disable PTY)
+      or ``-t`` (enable PTY) to most closely match how Fabric is executing the
+      command.
 
 * **Enable Paramiko-level debug logging.** If your issue is in the lower level
   Paramiko library, it can help us to see the debug output Paramiko prints. At
-  top level in your fabfile, add the following::
+  top level in your fabfile (or in an appropriate module, if not using a
+  fabfile), add the following::
 
       import logging
       logging.basicConfig(level=logging.DEBUG)
diff -Nru fabric-1.14.0/sites/www/upgrading.rst fabric-2.5.0/sites/www/upgrading.rst
--- fabric-1.14.0/sites/www/upgrading.rst	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/sites/www/upgrading.rst	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,1626 @@
+.. _upgrading:
+
+==================
+Upgrading from 1.x
+==================
+
+Modern Fabric (2+) represents a near-total reimplementation & reorganization of
+the software. It's been :ref:`broken in two <invoke-split-from-fabric>`,
+cleaned up, made more explicit, and so forth. In some cases, upgrading requires
+only basic search & replace; in others, more work is needed.
+
+If you read this document carefully, it should guide you in the right direction
+until you're fully upgraded. If any functionality you're using in Fabric 1
+isn't listed here, please file a ticket `on Github
+<https://github.com/fabric/fabric>`_ and we'll update it ASAP.
+
+.. warning::
+    As of the 2.0 release line, Fabric 2 is **not** at 100% feature parity with
+    1.x! Some features have been explicitly dropped, but others simply have not
+    been ported over yet, either due to time constraints or because said
+    features need to be re-examined in a modern context.
+
+    Please review the information below, including the :ref:`upgrade-specifics`
+    section which contains a very detailed list, before filing bug reports!
+
+    Also see :ref:`the roadmap <roadmap>` for additional notes about release
+    versioning.
+
+Why upgrade?
+============
+
+We'd like to call out, in no particular order, some specific improvements in
+modern Fabric that might make upgrading worth your time.
+
+.. note::
+    These are all listed in the rest of the doc too, so if you're already sold,
+    just skip there.
+
+- Python 3 compatibility (specifically, we now support 2.7 and 3.4+);
+- Thread-safe - no more requirement on multiprocessing for concurrency;
+- API reorganized around `fabric.connection.Connection` objects instead of
+  global module state;
+- Command-line parser overhauled to allow for regular GNU/POSIX style flags and
+  options on a per-task basis (no more ``fab mytask:weird=custom,arg=format``);
+- Task organization is more explicit and flexible / has less 'magic';
+- Tasks can declare other tasks to always be run before or after themselves;
+- Configuration massively expanded to allow for multiple config files &
+  formats, env vars, per-user/project/module configs, and much more;
+- SSH config file loading enabled by default & has been fleshed out re:
+  system/user/runtime file selection;
+- Shell command execution API consistent across local and remote method calls -
+  no more differentiation between ``local`` and ``run`` (besides where the
+  command runs, of course!);
+- Shell commands significantly more flexible re: interactive behavior,
+  simultaneous capture & display (now applies to local subprocesses, not just
+  remote), encoding control, and auto-responding;
+- Use of Paramiko's APIs for the SSH layer much more transparent - e.g.
+  `fabric.connection.Connection` allows control over the kwargs given to
+  `SSHClient.connect <paramiko.client.SSHClient.connect>`;
+- Gateway/jump-host functionality offers a ``ProxyJump`` style 'native' (no
+  proxy-command subprocesses) option, which can be nested infinitely;
+
+
+'Sidegrading' to Invoke
+=======================
+
+We linked to a note about this above, but to be explicit: modern Fabric is
+really a few separate libraries, and anything not strictly SSH or network
+related has been :ref:`split out into the Invoke project
+<invoke-split-from-fabric>`.
+
+This means that if you're in the group of users leveraging Fabric solely for
+its task execution or ``local``, and never used ``run``, ``put`` or
+similar - **you don't need to use Fabric itself anymore** and can simply
+**'sidegrade' to Invoke instead**.
+
+You'll still want to read over this document to get a sense of how things have
+changed, but be aware that you can get away with ``pip install invoke`` and
+won't need Fabric, Paramiko, cryptography dependencies, or anything else.
+
+
+Using modern Fabric from within Invoke
+======================================
+
+We intend to enhance modern Fabric until it encompasses the bulk of Fabric 1's
+use cases, such that you can use ``fab`` and fabfiles on their own without
+caring too much about how it's built on top of Invoke.
+
+However, prior to that point -- and very useful on its own for
+intermediate-to-advanced users -- is the fact that modern Fabric is
+designed with library or direct API use in mind. **It's entirely possible, and
+in some cases preferable, to use Invoke for your CLI needs and Fabric as a pure
+API within your Invoke tasks.**
+
+In other words, you can eschew ``fab``/fabfiles entirely unless you find
+yourself strongly needing the conveniences it wraps around ad-hoc sessions,
+such as :option:`--hosts` and the like.
+
+
+Running both Fabric versions simultaneously
+===========================================
+
+To help with gradual upgrades, modern Fabric may be installed under the name
+``fabric2`` (in addition to being made available "normally" as versions 2.0+ of
+``fabric``) and can live alongside installations of version 1.x.
+
+Thus, if you have a large codebase and don't want to make the jump to modern
+versions in one leap, it's possible to have both Fabric 1 (``fabric``, as you
+presumably had it installed previously) and modern Fabric (as ``fabric2``)
+resident in your Python environment simultaneously.
+
+.. note::
+    We strongly recommend that you eventually migrate all code using Fabric 1,
+    to versions 2 or above, so that you can move back to installing and
+    importing under the ``fabric`` name. ``fabric2`` as a distinct package and
+    module is intended to be a stopgap, and there will not be any ``fabric3``
+    or above (not least because some of those names are already taken!)
+
+For details on how to obtain the ``fabric2`` version of the package, see
+:ref:`installing-as-fabric2`.
+
+.. _from-v1:
+
+Creating ``Connection`` and/or ``Config`` objects from v1 settings
+------------------------------------------------------------------
+
+A common tactic when upgrading piecemeal is to generate modern Fabric objects
+whose contents match the current Fabric 1 environment. Whereas Fabric 1 stores
+*all* configuration (including the "current host") in a single place -- the
+``env`` object -- modern Fabric breaks things up into multiple (albeit
+composed) objects: `~fabric.connection.Connection` for per-connection
+parameters, and `~fabric.config.Config` for general settings and defaults.
+
+In most cases, you'll only need to generate a `~fabric.connection.Connection`
+object using the alternate class constructor `Connection.from_v1
+<fabric.connection.Connection.from_v1>`, which should be fed your appropriate
+local ``fabric.api.env`` object; see its API docs for details.
+
+A contrived example::
+
+    from fabric.api import env, run
+    from fabric2 import Connection
+
+    env.host_string = "admin@myserver"
+    run("whoami") # v1
+    cxn = Connection.from_v1(env)
+    cxn.run("whoami") # v2+
+
+By default, this constructor calls another API member -- `Config.from_v1
+<fabric.config.Config.from_v1>` -- internally on your behalf. Users who need
+tighter control over modern-style config options may opt to call that
+classmethod explicitly and hand their modified result into `Connection.from_v1
+<fabric.connection.Connection.from_v1>`, which will cause the latter to skip
+any implicit config creation.
+
+.. _v1-env-var-imports:
+
+Mapping of v1 ``env`` vars to modern API members
+------------------------------------------------
+
+The ``env`` vars and how they map to `~fabric.connection.Connection` arguments
+or `~fabric.config.Config` values (when fed into the ``.from_v1`` constructors
+described above) are listed below.
+
+.. list-table::
+    :header-rows: 1
+
+    * - v1 ``env`` var
+      - v2+ usage (prefixed with the class it ends up in)
+
+    * - ``always_use_pty``
+      - Config: ``run.pty``.
+    * - ``command_timeout``
+      - Config: ``timeouts.command``; timeouts are now their own config
+        subtree, whereas in v1 it was possible for the ambiguous ``timeout``
+        setting -- see below -- to work for either connect OR command timeouts.
+    * - ``forward_agent``
+      - Config: ``connect_kwargs.forward_agent``.
+    * - ``gateway``
+      - Config: ``gateway``.
+    * - ``host_string``
+      - Connection: ``host`` kwarg (which can handle host-string like values,
+        including user/port).
+    * - ``key``
+      - **Not supported**: Fabric 1 performed extra processing on this
+        (trying a bunch of key classes to instantiate) before
+        handing it into Paramiko; modern Fabric prefers to just let you handle
+        Paramiko-level parameters directly.
+
+        If you're filling your Fabric 1 ``key`` data from a file, we recommend
+        switching to ``key_filename`` instead, which is supported.
+
+        If you're loading key data from some other source as a string, you
+        should know what type of key your data is and manually instantiate it
+        instead, then supply it to the ``connect_kwargs`` parameter. For
+        example::
+
+            from io import StringIO  # or 'from StringIO' on Python 2
+            from fabric.state import env
+            from fabric2 import Connection
+            from paramiko import RSAKey
+            from somewhere import load_my_key_string
+
+            pkey = RSAKey.from_private_key(StringIO(load_my_key_string()))
+            cxn = Connection.from_v1(env, connect_kwargs={"pkey": pkey})
+
+    * - ``key_filename``
+      - Config: ``connect_kwargs.key_filename``.
+    * - ``no_agent``
+      - Config: ``connect_kwargs.allow_agent`` (inverted).
+    * - ``password``
+      - Config: ``connect_kwargs.password``, as well as ``sudo.password``
+        **if and only if** the env's ``sudo_password`` (see below) is unset.
+        (This mimics how v1 uses this particular setting - in earlier versions
+        there was no ``sudo_password`` at all.)
+    * - ``port``
+      - Connection: ``port`` kwarg. Is casted to an integer due to Fabric 1's
+        default being a string value (which is not valid in v2).
+
+        .. note::
+            Since v1's ``port`` is used both for a default *and* to store the
+            current connection state, v2 uses it to fill in the Connection
+            only, and not the Config, on assumption that it will typically be
+            the current connection state.
+
+    * - ``ssh_config_path``
+      - Config: ``ssh_config_path``.
+    * - ``sudo_password``
+      - Config: ``sudo.password``.
+    * - ``sudo_prompt``
+      - Config: ``sudo.prompt``.
+    * - ``timeout``
+      - Config: ``timeouts.connection``, for connection timeouts, or
+        ``timeouts.command`` for command timeouts (see above).
+    * - ``use_ssh_config``
+      - Config: ``load_ssh_configs``.
+    * - ``user``
+      - Connection: ``user`` kwarg.
+    * - ``warn_only``
+      - Config: ``run.warn``
+
+
+.. _upgrade-specifics:
+
+Upgrade specifics
+=================
+
+This is (intended to be) an exhaustive list of *all* Fabric 1.x functionality,
+as well as new-to-Invoke-or-Fabric-2 functionality not present in 1.x; it
+specifies whether upgrading is necessary, how to upgrade if so, and tracks
+features which haven't been implemented in modern versions yet.
+
+Most sections are broken down in table form, as follows:
+
+.. list-table::
+
+    * - Fabric 1 feature or behavior
+      - Status, see below for breakdown
+      - Migration notes, removal rationale, etc
+
+Below are the typical values for the 'status' column, though some of them are a
+bit loose - make sure to read the notes column in all cases! Also note that
+things are not ironclad - eg any 'removed' item has some chance of returning if
+enough users request it or use cases are made that workarounds are
+insufficient.
+
+- **Ported**: available already, possibly renamed or moved (frequently, moved
+  into the `Invoke <http://pyinvoke.org>`_ codebase.)
+- **Pending**: would fit, but has not yet been ported, good candidate for a
+  patch. *These entries link to the appropriate Github ticket* - please do
+  not make new ones!
+- **Removed**: explicitly *not* ported (no longer fits with vision, had too
+  poor a maintenance-to-value ratio, etc) and unlikely to be reinstated.
+
+Here's a quick local table of contents for navigation purposes:
+
+.. contents::
+    :local:
+
+.. _upgrading-general:
+
+General / conceptual
+--------------------
+
+- Modern Fabric is fully Python 3 compatible; as a cost, Python 2.5 support (a
+  longstanding feature of Fabric 1) has been dropped - in fact, we've dropped
+  support for anything older than Python 2.7.
+- The CLI task-oriented workflow remains a primary design goal, but the library
+  use case is no longer a second-class citizen; instead, the library
+  functionality has been designed first, with the CLI/task features built on
+  top of it.
+- Additionally, within the CLI use case, version 1 placed too much emphasis on
+  'lazy' interactive prompts for authentication secrets or even connection
+  parameters, driven in part by a lack of strong configuration mechanisms. Over
+  time it became clear this wasn't worth the tradeoffs of having confusing
+  noninteractive behavior and difficult debugging/testing procedures.
+
+  Modern Fabric takes an arguably cleaner approach (based on functionality
+  added to v1 over time) where users are encouraged to leverage the
+  configuration system and/or serve the user prompts for runtime secrets at the
+  *start* of the process; if the system determines it's missing information
+  partway through, it raises exceptions instead of prompting.
+- Invoke's design includes :ref:`explicit user-facing testing functionality
+  <testing-user-code>`; if you didn't find a way to write tests for your
+  Fabric-using code before, it should be much easier now.
+
+    - We recommend trying to write tests early on; they will help clarify the
+      upgrade process for you & also make the process safer!
+
+.. _upgrading-api:
+
+API organization
+----------------
+
+High level code flow and API member concerns.
+
+.. list-table::
+    :widths: 40 10 50
+
+    * - Import everything via ``fabric.api``
+      - Removed
+      - All useful imports are now available at the top level, e.g. ``from
+        fabric import Connection``.
+    * - Configure connection parameters globally (via ``env.host_string``,
+        ``env.host``, ``env.port``, ``env.user``) and call global methods which
+        implicitly reference them (``run``/``sudo``/etc)
+      - Removed
+      - The primary API is now properly OOP: instantiate
+        `fabric.connection.Connection` objects and call their methods. These
+        objects encapsulate all connection state (user, host, gateway, etc) and
+        have their own SSH client instances.
+
+        .. seealso::
+            `Connection.from_v1 <fabric.connection.Connection.from_v1>`
+
+    * - Emphasis on serialized "host strings" as method of setting user, host,
+        port, etc
+      - Ported/Removed
+      - `fabric.connection.Connection` *can* accept a shorthand "host
+        string"-like argument, but the primary API is now explicit user, host,
+        port, etc keyword arguments.
+
+        Additionally, many arguments/settings/etc that expected a host string
+        in v1 will now expect a `fabric.connection.Connection` instance instead.
+    * - Use of "roles" as global named lists of host strings
+      - Ported
+      - This need is now served by `fabric.group.Group` objects (which wrap
+        some number of `fabric.connection.Connection` instances with "do a
+        thing to all members" methods.) Users can create & organize these any
+        way they want.
+
+        See the line items for ``--roles`` (:ref:`upgrading-cli`),
+        ``env.roles`` (:ref:`upgrading-env`) and ``@roles``
+        (:ref:`upgrading-tasks`) for the status of those specifics.
+
+.. _upgrading-tasks:
+
+Task functions & decorators
+---------------------------
+
+.. note::
+    Nearly all task-related functionality is implemented in Invoke; for more
+    details see its :ref:`execution <invoking-tasks>` and :ref:`namespaces
+    <task-namespaces>` documentation.
+
+.. list-table::
+    :widths: 40 10 50
+
+    * - By default, tasks are loaded from a ``fabfile.py`` which is sought up
+        towards filesystem root from the user's current working directory
+      - Ported
+      - This behavior is basically identical today, with minor modifications
+        and enhancements (such as tighter control over the load process, and
+        API hooks for implementing custom loader logic - see
+        :ref:`loading-collections`.)
+    * - "Classic" style implicit task functions lacking a ``@task`` decorator
+      - Removed
+      - These were on the way out even in v1, and arbitrary task/namespace
+        creation is more explicitly documented now, via Invoke's
+        `~invoke.tasks.Task` and `~invoke.collection.Collection`.
+    * - "New" style ``@task``-decorated, module-level task functions
+      - Ported
+      - Largely the same, though now with superpowers - `@task
+        <fabric.tasks.task>` can still be used without any parentheses, but
+        where v1 only had a single ``task_class`` argument, the new version
+        (largely based on Invoke's) has a number of namespace and parser hints,
+        as well as execution related options (such as those formerly served by
+        ``@hosts`` and friends).
+    * - Arbitrary task function arguments (i.e. ``def mytask(any, thing, at,
+        all)``)
+      - Ported
+      - This gets its own line item because: tasks must now take a
+        `~invoke.context.Context` (vanilla Invoke) or
+        `fabric.connection.Connection` (Fabric) object as their first
+        positional argument. The rest of the function signature is, as before,
+        totally up to the user & will get automatically turned into CLI flags.
+
+        This sacrifices a small bit of the "quick DSL" of v1 in exchange for a
+        cleaner, easier to understand/debug, and more user-overrideable API
+        structure.
+
+        As a side effect, it lessens the distinction between "module of
+        functions" and "class of methods"; users can more easily start with the
+        former and migrate to the latter when their needs grow/change.
+    * - Implicit task tree generation via import-crawling
+      - Ported/Removed
+      - Namespace construction is now more explicit; for example, imported
+        modules in your ``fabfile.py`` are no longer auto-scanned and
+        auto-added to the task tree.
+
+        However, the root ``fabfile.py`` *is* automatically loaded (using
+        `Collection.from_module <invoke.collection.Collection.from_module>`),
+        preserving the simple/common case. See :ref:`task-namespaces` for
+        details.
+
+        We may reinstate (in an opt-in fashion) imported module scanning later,
+        since the use of explicit namespace objects still allows users control
+        over the tree that results.
+    * - ``@hosts`` for determining the default host or list of hosts a given
+        task uses
+      - Ported
+      - Reinstated as the ``hosts`` parameter of `@task <fabric.tasks.task>`.
+        Further, it can now handle dicts of `fabric.connection.Connection`
+        kwargs in addition to simple host strings.
+    * - ``@roles`` for determining the default list of group-of-host targets a
+        given task uses
+      - Pending
+      - See :ref:`upgrading-api` for details on the overall 'roles' concept.
+        When it returns, this will probably follow ``@hosts`` and become some
+        ``@task`` argument.
+    * - ``@serial``/``@parallel``/``@runs_once``
+      - Ported/`Pending <https://github.com/pyinvoke/invoke/issues/63>`__
+      - Parallel execution is currently offered at the API level via
+        `fabric.group.Group` subclasses such as `fabric.group.ThreadingGroup`;
+        however, designating entire sessions and/or tasks to run in parallel
+        (or to exempt from parallelism) has not been solved yet.
+
+        The problem needs solving at a higher level than just SSH targets, so
+        this links to an Invoke-level ticket.
+    * - ``execute`` for calling named tasks from other tasks while honoring
+        decorators and other execution mechanics (as opposed to calling them
+        simply as functions)
+      - `Pending <https://github.com/pyinvoke/invoke/issues/170>`__
+      - This is one of the top "missing features" from the rewrite; link is to
+        Invoke's tracker.
+    * - ``Task`` class for programmatic creation of tasks (as opposed to using
+        some function object and the ``@task`` decorator)
+      - Ported
+      - While not sharing many implementation details with v1, modern Fabric
+        (via Invoke) has a publicly exposed `~invoke.tasks.Task` class, which
+        alongside `~invoke.collection.Collection` allow full programmatic
+        creation of task trees, no decorator needed.
+
+.. _upgrading-cli:
+
+CLI arguments, options and behavior
+-----------------------------------
+
+.. list-table::
+    :widths: 40 10 50
+
+    * - Exposure of task arguments as custom colon/comma delimited CLI
+        arguments, e.g. ``fab mytask:posarg,kwarg=val``
+      - Removed
+      - CLI arguments are now proper GNU/POSIX-style long and short flags,
+        including globbing shortflags together, space or equals signs to attach
+        values, optional values, and much more. See :ref:`invoking-tasks`.
+    * - Task definition names are mirrored directly on the command-line, e.g
+        for task ``def journald_logs()``, command line argument is ``fab
+        journald_logs``
+      - Removed
+      - Tasks names now get converted from underscores to hyphens. Eg. task
+        ``def journald_logs()`` now evaluates to ``fab journald-logs`` on the
+        commandline.
+    * - Ability to invoke multiple tasks in a single command line, e.g. ``fab
+        task1 task2``
+      - Ported
+      - Works great!
+    * - ``python -m fabric`` as stand-in for ``fab``
+      - Ported
+      - Ported in 2.2.
+    * - ``-a``/``--no_agent`` for disabling automatic SSH agent key selection
+      - Removed
+      - To disable use of an agent permanently, set config value
+        ``connect_kwargs.allow_agent`` to ``False``; to disable temporarily,
+        unset the ``SSH_AUTH_SOCK`` env var.
+    * - ``-A``/``--forward-agent`` for enabling agent forwarding to the remote
+        end
+      - Removed
+      - The config and kwarg versions of this are ported, but there is
+        currently no CLI flag. Usual "you can set the config value at runtime
+        with a shell env variable" clause is in effect, so this *may* not get
+        ported, depending.
+    * - ``--abort-on-prompts`` to turn interactive prompts into exceptions
+        (helps avoid 'hanging' sessions)
+      - Removed
+      - See the notes about interactive prompts going away in
+        :ref:`upgrading-general`. Without mid-session prompts, there's no need
+        for this option.
+    * - ``-c``/``--config`` for specifying an alternate config file path
+      - Ported
+      - ``--config`` lives on, but the short flag is now ``-f`` (``-c`` now
+        determines which collection module name is sought by the task loader.)
+    * - ``--colorize-errors`` (and ``env.colorize_errors``) to enable ANSI
+        coloring of error output
+      - `Pending <https://github.com/fabric/fabric/issues/101>`__
+      - Very little color work has been done yet and this is one of the
+        potentially missing pieces. We're unsure how often this was used in v1
+        so it's possible it won't show up again, but generally, we like using
+        color as an additional output vector, so...
+    * - ``-d``/``--display`` for showing info on a given command
+      - Ported
+      - This is now the more standard ``-h``/``--help``, and can be given in
+        either "direction": ``fab -h mytask`` or ``fab mytask -h``.
+    * - ``-D``/``--disable-known-hosts`` to turn off Paramiko's automatic
+        loading of user-level ``known_hosts`` files
+      - `Pending <https://github.com/fabric/fabric/issues/1804>`__
+      - Not ported yet, probably will be.
+    * - ``-e``/``--eagerly-disconnect`` (and ``env.eagerly_disconnect``) which
+        tells the execution system to disconnect from hosts as soon as a task
+        is done running
+      - Ported/`Pending <https://github.com/fabric/fabric/issues/1805>`__
+      - There's no explicit connection cache anymore, so eager disconnection
+        should be less necessary. However, investigation and potential feature
+        toggles are still pending.
+    * - ``-f``/``--fabfile`` to select alternate fabfile location
+      - Ported
+      - This is now split up into ``-c``/``--collection`` and
+        ``-r``/``--search-root``; see :ref:`loading-collections`.
+    * - ``-g``/``--gateway`` (and ``env.gateway``) for selecting a global SSH
+        gateway host string
+      - `Pending <https://github.com/fabric/fabric/issues/1806>`__
+      - One can set the global ``gateway`` config option via an
+        environment variable, which at a glance would remove the need for a
+        dedicated CLI option. However, this approach only allows setting
+        string values, which in turn only get used for ``ProxyCommand``
+        style gatewaying, so it *doesn't* replace v1's ``--gateway``
+        (which took a host string and turned it into a ``ProxyJump`` style
+        gateway).
+
+        Thus, if enough users notice the lack, we'll consider a feature-add
+        that largely mimics the v1 behavior: string becomes first argument to
+        `fabric.connection.Connection` and that resulting object is then set as
+        ``gateway``.
+    * - ``--gss-auth``/``--gss-deleg``/``--gss-kex``
+      - Removed
+      - These didn't seem used enough to be worth porting over, especially
+        since they fall under the usual umbrella of "Paramiko-level connect
+        passthrough" covered by the ``connect_kwargs`` config option. (Which,
+        if necessary, can be set at runtime via shell environment variables,
+        like any other config value.)
+    * - ``--hide``/``--show`` for tweaking output display globally
+      - Removed
+      - This is configurable via the config system and env vars.
+    * - ``-H``/``--hosts``
+      - Ported
+      - Works basically the same as before - if given, is shorthand for
+        executing any given tasks once per host.
+    * - ``-i`` for SSH key filename selection
+      - Ported
+      - Works same as v1, including ability to give multiple times to build a
+        list of keys to try.
+    * - ``-I``/``--initial-password-prompt`` for requesting an initial
+        pre-execution password prompt
+      - Ported
+      - It's now :option:`--prompt-for-login-password`,
+        :ref:`--prompt-for-sudo-password <prompt-for-sudo-password>` or
+        :option:`--prompt-for-passphrase`, depending on whether you were using
+        the former to fill in passwords or key passphrases (or both.)
+    * - ``--initial-sudo-password-prompt`` for requesting an initial
+        pre-execution sudo password prompt
+      - Ported
+      - This is now :option:`--prompt-for-sudo-password`. Still a bit of a
+        mouthful but still 4 characters shorter!
+    * - ``-k``/``--no-keys`` which prevents Paramiko's automatic loading of key
+        files such as ``~/.ssh/id_rsa``
+      - Removed
+      - Use environment variables to set the ``connect_kwargs.look_for_keys``
+        config value to ``False``.
+    * - ``--keepalive`` for setting network keepalive
+      - `Pending <https://github.com/fabric/fabric/issues/1807>`__
+      - Not ported yet.
+    * - ``-l``/``--list`` for listing tasks, plus ``-F``/``--list-format`` for
+        tweaking list display format
+      - Ported
+      - Now with bonus JSON list-format! Which incidentally replaces ``-F
+        short``/``--shortlist``.
+    * - ``--linewise`` for buffering output line by line instead of roughly
+        byte by byte
+      - Removed
+      - This doesn't really fit with the way modern command execution code
+        views the world, so it's gone.
+    * - ``-n``/``--connection-attempts`` controlling multiple connect retries
+      - `Pending <https://github.com/fabric/fabric/issues/1808>`__
+      - Not ported yet.
+    * - ``--no-pty`` to disable automatic PTY allocation in ``run``, etc
+      - Ported
+      - Is now ``-p``/``--pty`` as the default behavior was switched around.
+    * - ``--password``/``--sudo-password`` for specifying login/sudo password
+        values
+      - Removed
+      - This is typically not very secure to begin with, and there are now many
+        other avenues for setting the related configuration values, so
+        they're gone at least for now.
+    * - ``-P``/``--parallel`` for activating global parallelism
+      - `Pending <https://github.com/pyinvoke/invoke/issues/63>`__
+      - See the notes around ``@parallel`` in :ref:`upgrading-tasks`.
+    * - ``--port`` to set default SSH port
+      - Removed
+      - Our gut says this is best left up to the configuration system's env var
+        layer, or use of the ``port`` kwarg on `fabric.connection.Connection`;
+        however it may find its way back.
+    * - ``r``/``--reject-unknown-hosts`` to modify Paramiko known host behavior
+      - `Pending <https://github.com/fabric/fabric/issues/1804>`__
+      - Not ported yet.
+    * - ``-R``/``--roles`` for global list-of-hosts target selection
+      - `Pending <https://github.com/fabric/fabric/issues/1594>`__
+      - As noted under :ref:`upgrading-api`, role lists are only partially
+        applicable to the new API and we're still feeling out whether/how they
+        would work at a global or CLI level.
+    * - ``--set key=value`` for setting ``fabric.state.env`` vars at runtime
+      - Removed
+      - This is largely obviated by the new support for shell environment
+        variables (just do ``INVOKE_KEY=value fab mytask`` or similar), though
+        it's remotely possible a CLI flag method of setting config values will
+        reappear later.
+    * - ``-s``/``--shell`` to override default shell path
+      - Removed
+      - Use the configuration system for this.
+    * - ``--shortlist`` for short/computer-friendly list output
+      - Ported
+      - See ``--list``/``--list-format`` - there's now a JSON format instead.
+        No point reinventing the wheel.
+    * - ``--skip-bad-hosts`` (and ``env.skip_bad_hosts``) to bypass problematic
+        hosts
+      - `Pending <https://github.com/fabric/fabric/issues/1809>`__
+      - Not ported yet.
+    * - ``--skip-unknown-tasks`` and ``env.skip_unknown_tasks`` for silently
+        skipping past bogus task names on CLI invocation
+      - Removed
+      - This felt mostly like bloat to us and could require nontrivial parser
+        changes to reimplement, so it's out for now.
+    * - ``--ssh-config-path`` and ``env.ssh_config_path`` for selecting an SSH
+        config file
+      - Ported
+      - This is now ``-S``/``--ssh-config``.
+    * - ``--system-known-hosts`` to trigger loading systemwide ``known_hosts``
+        files
+      - `Pending <https://github.com/fabric/fabric/issues/1804>`__/Removed
+      - This isn't super likely to come back as its own CLI flag but it may
+        well return as a configuration value.
+    * - ``-t``/``--timeout`` controlling connection timeout
+      - Ported
+      - It's now ``-t``/``--connect-timeout`` as ``--timeout`` was technically
+        ambiguous re: connect vs command timeout.
+    * - ``-T``/``--command-timeout``
+      - Ported
+      - Implemented in Invoke and preserved in ``fab`` under the same name.
+    * - ``-u``/``--user`` to set global default username
+      - Removed
+      - Most of the time, configuration (env vars for true runtime, or eg
+        user/project level config files as appropriate) should be used for
+        this, but it may return.
+    * - ``-w``/``--warn-only`` to toggle warn-vs-abort behavior
+      - Ported
+      - Ported as-is, no changes.
+    * - ``-x``/``--exclude-hosts`` (and ``env.exclude_hosts``) for excluding
+        otherwise selected targets
+      - `Pending <https://github.com/fabric/fabric/issues/1594>`__
+      - Not ported yet, is pending an in depth rework of global (vs
+        hand-instantiated) connection/group selection.
+    * - ``-z``/``--pool-size`` for setting parallel-mode job queue pool size
+      - Removed
+      - There's no job queue anymore, or at least at present. Whatever replaces
+        it (besides the already-implemented threading model) is likely to look
+        pretty different.
+
+.. _upgrading-commands:
+
+Shell command execution (``local``/``run``/``sudo``)
+----------------------------------------------------
+
+General
+~~~~~~~
+
+Behaviors shared across either ``run``/``sudo``, or all of
+``run``/``sudo``/``local``. Subsequent sections go into per-function
+differences.
+
+.. list-table::
+    :widths: 40 10 50
+
+    * - ``local`` and ``run``/``sudo`` have wildly differing APIs and
+        implementations
+      - Removed
+      - All command execution is now unified; all three functions (now
+        methods on `fabric.connection.Connection`, though ``local`` is also
+        available as `invoke.run` for standalone use) have the same underlying
+        protocol and logic (the `~invoke.runners.Runner` class hierarchy), with
+        only low-level details like process creation and pipe consumption
+        differing.
+
+        For example, in v1 ``local`` required you to choose between displaying
+        and capturing subprocess output; modern ``local`` is like ``run`` and
+        does both at the same time.
+    * - Prompt auto-response, via ``env.prompts`` and/or ``sudo``'s internals
+      - Ported
+      - The ``env.prompts`` functionality has been significantly fleshed out,
+        into a framework of :ref:`Watchers <autoresponding>` which operate on
+        any (local or remote!) running command's input and output streams.
+
+        In addition, ``sudo`` has been rewritten to use that framework; while
+        still useful enough to offer an implementation in core, it no longer
+        does anything users cannot do themselves using public APIs.
+    * - ``fabric.context_managers.cd``/``lcd`` (and ``prefix``) allow scoped
+        mutation of executed comments
+      - Ported/`Pending <https://github.com/fabric/fabric/issues/1752>`__
+      - These are now methods on `~invoke.context.Context` (`Context.cd
+        <invoke.context.Context.cd>`, `Context.prefix
+        <invoke.context.Context.prefix>`) but need work in its subclass
+        `fabric.connection.Connection` (quite possibly including recreating
+        ``lcd``) so that local vs remote state are separated.
+    * - ``fabric.context_managers.shell_env`` and its specific expression
+        ``path`` (plus ``env.shell_env``, ``env.path`` and
+        ``env.path_behavior``), for modifying remote environment variables
+        (locally, one would just modify `os.environ`.)
+      - Ported
+      - The context managers were the only way to set environment variables at
+        any scope; in modern Fabric, subprocess shell environment is
+        controllable per-call (directly in `fabric.connection.Connection.run`
+        and siblings via an ``env`` kwarg) *and* across multiple calls (by
+        manipulating the configuration system, statically or at runtime.)
+    * - Controlling subprocess output & other activity display text by
+        manipulating ``fabric.state.output`` (directly or via
+        ``fabric.context_managers.hide``, ``show`` or ``quiet`` as well as the
+        ``quiet`` kwarg to ``run``/``sudo``; plus
+        ``utils.puts``/``fastprint``)
+      - Ported/`Pending <https://github.com/pyinvoke/invoke/issues/15>`__
+      - The core concept of "output levels" is gone, likely to be replaced in
+        the near term by a logging module (stdlib or other) which output levels
+        poorly reimplemented.
+
+        Command execution methods like `~invoke.runners.Runner.run` retain a
+        ``hide`` kwarg controlling which subprocess streams are copied to your
+        terminal, and an ``echo`` kwarg controlling whether commands are
+        printed before execution. All of these also honor the configuration
+        system.
+    * - ``timeout`` kwarg and the ``CommandTimeout`` exception raised when said
+        command-runtime timeout was violated
+      - Ported
+      - Primarily lives at the Invoke layer now, but applies to all command
+        execution, local or remote; see the ``timeout`` argument to
+        `~invoke.runners.Runner.run` and its related configuration value and
+        CLI flag.
+    * - ``pty`` kwarg and ``env.always_use_pty``, controlling whether commands
+        run in a pseudo-terminal or are invoked directly
+      - Ported
+      - This has been thoroughly ported (and its behavior often improved)
+        including preservation of the ``pty`` kwarg and updating the config
+        value to be simply ``run.pty``. However, a major change is that pty
+        allocation is now ``False`` by default instead of ``True``.
+
+        Fabric 0.x and 1.x already changed this value around; during Fabric 1's
+        long lifetime it became clear that neither default works for all or
+        even most users, so we opted to return the default to ``False`` as it's
+        cleaner and less wasteful.
+    * - ``combine_stderr`` (kwarg and ``env.combine_stderr``) controlling
+        whether Paramiko weaves remote stdout and stderr into the stdout stream
+      - Removed
+      - This wasn't terrifically useful, and often caused conceptual problems
+        in tandem with ``pty`` (as pseudo-terminals by their nature always
+        combine the two streams.)
+
+        We recommend users who really need both streams to be merged, either
+        use shell redirection in their command, or set ``pty=True``.
+    * - ``warn_only`` kwarg for preventing automatic abort on non-zero return
+        codes
+      - Ported
+      - This is now just ``warn``, both kwarg and config value. It continues to
+        default to ``False``.
+    * - ``stdout`` and ``stderr`` kwargs for reassigning default stdout/err
+        mirroring targets, which otherwise default to the appropriate `sys`
+        members
+      - Ported
+      - These are now ``out_stream`` and ``err_stream`` but otherwise remain
+        similar in nature. They are also accompanied by the new, rather obvious
+        in hindsight ``in_stream``.
+    * - ``capture_buffer_size`` arg & use of a ring buffer for storing captured
+        stdout/stderr to limit total size
+      - `Pending <https://github.com/pyinvoke/invoke/issues/344>`__
+      - Existing `~invoke.runners.Runner` implementation uses regular lists for
+        capture buffers, but we fully expect to upgrade this to a ring buffer
+        or similar at some point.
+    * - Return values are string-like objects with extra attributes like
+        ``succeeded`` and ``return_code`` sprinkled on top
+      - Ported
+      - Return values are no longer string-a-likes with a semi-private API, but
+        are full fledged regular objects of type `~invoke.runners.Result`. They
+        expose all of the same info as the old "attribute strings", and only
+        really differ in that they don't pretend to be strings themselves.
+
+        They do, however, still behave as booleans - just ones reflecting the
+        exit code's relation to zero instead of whether there was any stdout.
+    * - ``open_shell`` for obtaining interactive-friendly remote shell sessions
+        (something that ``run`` historically was bad at )
+      - Ported
+      - Technically "removed", but only because the new version of
+        ``run`` is vastly improved and can deal with interactive sessions at
+        least as well as the old ``open_shell`` did, if not moreso.
+        ``c.run("/my/favorite/shell", pty=True)`` should be all you need.
+
+``run``
+~~~~~~~
+
+.. list-table::
+    :widths: 40 10 50
+
+    * - ``shell`` / ``env.use_shell`` designating whether or not to wrap
+        commands within an explicit call to e.g. ``/bin/sh -c "real command"``;
+        plus their attendant options like ``shell_escape``
+      - Removed
+      - Non-``sudo`` remote execution never truly required an explicit shell
+        wrapper: the remote SSH daemon hands your command string off to the
+        connecting user's login shell in almost all cases. Since wrapping is
+        otherwise extremely error-prone and requires frustrating escaping
+        rules, we dropped it for this use case.
+
+        See the matching line items for ``local`` and ``sudo`` as their
+        situations differ. (For now, because they all share the same
+        underpinnings, `fabric.connection.Connection.run` does accept a
+        ``shell`` kwarg - it just doesn't do anything with it.)
+
+``sudo``
+~~~~~~~~
+
+Unless otherwise noted, all common ``run``+``sudo`` args/functionality (e.g.
+``pty``, ``warn_only`` etc) are covered above in the section on ``run``; the
+below are ``sudo`` specific.
+
+.. list-table::
+    :widths: 40 10 50
+
+    * - ``shell`` / ``env.use_shell`` designating whether or not to wrap
+        commands within an explicit call to e.g. ``/bin/sh -c "real command"``
+      - `Pending <https://github.com/pyinvoke/invoke/issues/344>`__/Removed
+      - See the note above under ``run`` for details on shell wrapping
+        as a general strategy; unfortunately for ``sudo``, some sort of manual
+        wrapping is still necessary for nontrivial commands (i.e. anything
+        using actual shell syntax as opposed to a single program's argv) due to
+        how the command string is handed off to the ``sudo`` program.
+
+        We hope to upgrade ``sudo`` soon so it can perform a common-best-case,
+        no-escaping-required shell wrapping on your behalf; see the 'Pending'
+        link.
+    * - ``user`` argument (and ``env.sudo_user``) allowing invocation via
+        ``sudo -u <user>`` (instead of defaulting to root)
+      - Ported
+      - This is still here, and still called ``user``.
+    * - ``group`` argument controlling the effective group of the sudo'd
+        command
+      - `Pending <https://github.com/pyinvoke/invoke/issues/540>`__
+      - This has not been ported yet.
+
+``local``
+~~~~~~~~~
+
+See the 'general' notes at top of this section for most details about the new
+``local``. A few specific extras are below.
+
+.. list-table::
+    :widths: 40 10 50
+
+    * - ``shell`` kwarg designating which shell to ask `subprocess.Popen` to
+        use
+      - Ported
+      - Basically the same as in v1, though there are now situations where
+        `os.execve` (or similar) is used instead of `subprocess.Popen`.
+        Behavior is much the same: no shell wrapping (as in legacy ``run``),
+        just informing the operating system what actual program to run.
+
+.. _upgrading-utility:
+
+Utilities
+---------
+
+.. list-table::
+    :widths: 40 10 50
+
+    * - Error handling via ``abort`` and ``warn``
+      - Ported
+      - The old functionality leaned too far in the "everything is a DSL"
+        direction & didn't offer enough value to offset how it gets in the way
+        of experienced Pythonistas.
+
+        These functions have been removed in favor of "just raise an exception"
+        (with one useful option being Invoke's `~invoke.exceptions.Exit`) as
+        exception handling feels more Pythonic than thin wrappers around
+        ``sys.exit`` or having to ``except SystemExit:`` and hope it was a
+        `SystemExit` your own code raised!
+    * - ANSI color helpers in ``fabric.colors`` allowed users to easily print
+        ANSI colored text without a standalone library
+      - Removed
+      - There seemed no point to poorly replicating one of the many fine
+        terminal-massaging libraries out there (such as those listed in the
+        description of `#101 <https://github.com/fabric/fabric/issues/101>`_)
+        in the rewrite, so we didn't.
+
+        That said, it seems highly plausible we'll end up vendoring such a
+        library in the future to offer internal color support, at which point
+        "baked-in" color helpers would again be within easy reach.
+    * - ``with char_buffered`` context manager for forcing a local stream to be
+        character buffered
+      - Ported
+      - This is now `~invoke.terminals.character_buffered`.
+    * - ``docs.unwrap_tasks`` for extracting docstrings from wrapped task
+        functions
+      - Ported
+      - v1 required using a Fabric-specific 'unwrap_tasks' helper function
+        somewhere in your Sphinx build pipeline; now you can instead just
+        enable the new `invocations.autodoc
+        <http://invocations.readthedocs.io/en/latest/api/autodoc.html>`_ Sphinx
+        mini-plugin in your extensions list; see link for details.
+    * - ``network.normalize``, ``denormalize`` and ``parse_host_string``,
+        ostensibly internals but sometimes exposed to users for dealing with
+        host strings
+      - Removed
+      - As with other host-string-related tools, these are gone and serve no
+        purpose. `fabric.connection.Connection` is now the primary API focus
+        and has individual attributes for all "host string" components.
+    * - ``utils.indent`` for indenting/wrapping text (uncommonly used)
+      - Pending
+      - Not ported yet; ideally we'll just vendor a third party lib in Invoke.
+    * - ``reboot`` for rebooting and reconnecting to a remote system
+      - Removed
+      - No equivalent has been written for modern Fabric; now that the
+        connection/client objects are made explicit, one can simply
+        instantiate a new object with the same parameters (potentially with
+        sufficient timeout parameters to get past the reboot, if one doesn't
+        want to manually call something like `time.sleep`.)
+
+        There is a small chance it will return if there appears to be enough
+        need; if so, it's likely to be a more generic reconnection related
+        `fabric.connection.Connection` method, where the user is responsible
+        for issuing the restart shell command via ``sudo`` themselves.
+    * - ``require`` for ensuring certain key(s) in ``env`` have values set,
+        optionally by noting they can be ``provided_by=`` a list of setup tasks
+      - Removed
+      - This has not been ported, in part because the maintainers never used it
+        themselves, and is unlikely to be directly reimplemented. However, its
+        core use case of "require certain data to be available to run a given
+        task" may return within the upcoming dependency framework.
+    * - ``prompt`` for prompting the user & storing the entered data
+        (optionally with validation) directly into ``env``
+      - Removed
+      - Like ``require``, this seemed like a less-used feature (especially
+        compared to its sibling ``confirm``) and was not ported. If it returns
+        it's likely to be via ``invocations``, which is where ``confirm`` ended
+        up.
+
+.. _upgrading-networking:
+
+Networking
+----------
+
+.. list-table::
+    :widths: 40 10 50
+
+    * - ``env.gateway`` for setting an SSH jump gateway
+      - Ported
+      - This is now the ``gateway`` kwarg to `fabric.connection.Connection`,
+        and -- for the newly supported ``ProxyJump`` style gateways, which can
+        be nested indefinitely! -- should be another
+        `fabric.connection.Connection` object instead of a host string.
+
+        (You may specify a runtime, non-SSH-config-driven
+        ``ProxyCommand``-style string as the ``gateway`` kwarg instead, which
+        will act just like a regular ``ProxyCommand``.)
+    * - ``ssh_config``-driven ``ProxyCommand`` support
+      - Ported
+      - This continues to work as it did in v1.
+    * - ``with remote_tunnel(...):`` port forwarding
+      - Ported
+      - This is now `fabric.connection.Connection.forward_local`, since it's
+        used to *forward* a *local* port to the remote end. (Newly added is the
+        logical inverse, `fabric.connection.Connection.forward_remote`.)
+    * - ``NetworkError`` raised on some network related errors
+      - Removed
+      - In v1 this was simply a (partially implemented) stepping-back from the
+        original "just sys.exit on any error!" behavior. Modern Fabric is
+        significantly more exception-friendly; situations that would raise
+        ``NetworkError`` in v1 now simply become the real underlying
+        exceptions, typically from Paramiko or the stdlib.
+    * - ``env.keepalive`` for setting network keepalive value
+      - `Pending <https://github.com/fabric/fabric/issues/1807>`__
+      - Not ported yet.
+    * - ``env.connection_attempts`` for setting connection retries
+      - `Pending <https://github.com/fabric/fabric/issues/1808>`__
+      - Not ported yet.
+    * - ``env.timeout`` for controlling connection (and sometimes command
+        execution) timeout
+      - Ported
+      - Connection timeout is now controllable both via the configuration
+        system (as ``timeouts.connect``) and a direct kwarg on
+        `fabric.connection.Connection`. Command execution timeout is its own
+        setting now, ``timeouts.command`` and a ``timeout`` kwarg to ``run``
+        and friends.
+
+Authentication
+--------------
+
+.. note::
+    Some ``env`` keys from v1 were simply passthroughs to Paramiko's
+    `SSHClient.connect <paramiko.client.SSHClient.connect>` method. Modern
+    Fabric gives you explicit control over the arguments it passes to that
+    method, via the ``connect_kwargs`` :ref:`configuration <fab-configuration>`
+    subtree, and the below table will frequently refer you to that approach.
+
+.. list-table::
+    :widths: 40 10 50
+
+    * - ``env.key_filename``
+      - Ported
+      - Use ``connect_kwargs``.
+    * - ``env.password``
+      - Ported
+      - Use ``connect_kwargs``.
+
+        Also note that this used to perform double duty as connection *and*
+        sudo password; the latter is now found in the ``sudo.password``
+        setting.
+    * - ``env.gss_(auth|deleg|kex)``
+      - Ported
+      - Use ``connect_kwargs``.
+    * - ``env.key``, a string or file object holding private key data, whose
+        specific type is auto-determined and instantiated for use as the
+        ``pkey`` connect kwarg
+      - Removed
+      - This has been dropped as unnecessary (& bug-prone) obfuscation of
+        Paramiko-level APIs; users should already know which type of key
+        they're dealing with and instantiate a ``PKey`` subclass themselves,
+        placing the result in ``connect_kwargs.pkey``.
+    * - ``env.no_agent``, which is a renaming/inversion of Paramiko's
+        ``allow_agent`` connect kwarg
+      - Ported
+      - Users who were setting this to ``True`` should now simply set
+        ``connect_kwargs.allow_agent`` to ``False`` instead.
+    * - ``env.no_keys``, similar to ``no_agent``, just an inversion of the
+        ``look_for_keys`` connect kwarg
+      - Ported
+      - Use ``connect_kwargs.look_for_keys`` instead (setting it to ``False``
+        to disable Paramiko's default key-finding behavior.)
+    * - ``env.passwords`` (and ``env.sudo_passwords``) stores connection/sudo
+        passwords in a dict keyed by host strings
+      - Ported/`Pending <https://github.com/fabric/fabric/issues/4>`__
+      - Each `fabric.connection.Connection` object may be configured with its
+        own ``connect_kwargs`` given at instantiation time, allowing for
+        per-host password configuration already.
+
+        However, we expect users may want a simpler way to set configuration
+        values that are turned into implicit `fabric.connection.Connection`
+        objects automatically; such a feature is still pending.
+    * - Configuring ``IdentityFile`` in one's ``ssh_config``
+      - Ported
+      - Still honored, along with a bunch of newly honored ``ssh_config``
+        settings; see :ref:`ssh-config`.
+
+.. _upgrading-transfers:
+
+File transfer
+-------------
+
+The below feature breakdown applies to the ``put`` and/or ``get`` "operation"
+functions from v1.
+
+.. list-table::
+    :widths: 40 10 50
+
+    * - Transferring individual files owned by the local and remote user
+      - Ported
+      - Basic file transfer in either direction works and is offered as
+        `fabric.connection.Connection.get`/`fabric.connection.Connection.put`
+        (though the code is split out into a separate-responsibility class,
+        `fabric.transfer.Transfer`.)
+
+        The signature of these methods has been cleaned up compared to v1,
+        though their positional-argument essence (``get(remote, local)`` and
+        ``put(local, remote)`` remains the same.
+    * - Omit the 'destination' argument for implicit 'relative to local
+        context' behavior (e.g. ``put("local.txt")`` implicitly uploading to
+        remote ``$HOME/local.txt``.)
+      - Ported
+      - You should probably still be explicit, because this is Python.
+    * - Use either file paths *or* file-like objects on either side of
+        the transfer operation (e.g. uploading a ``StringIO`` instead of an
+        on-disk file)
+      - Ported
+      - This was a useful enough and simple enough trick to keep around.
+    * - Preservation of source file mode at destination (e.g. ensuring an
+        executable bit that would otherwise be dropped by the destination's
+        umask, is re-added.)
+      - Ported
+      - Not only was this ported, but it is now the default behavior. It may be
+        disabled via kwarg if desired.
+    * - Bundled ``sudo`` operations as part of file transfer
+      - Removed
+      - This was one of the absolute buggiest parts of v1 and never truly did
+        anything users could not do themselves with a followup call to
+        ``sudo``, so we opted not to port it.
+
+        Should enough users pine for its loss, we *may* reconsider, but if we
+        do it will be with a serious eye towards simplification and/or an
+        approach not involving intermediate files.
+    * - Recursive multi-file transfer (e.g. ``put(a_directory)`` uploads entire
+        directory and all its contents)
+      - Removed
+      - This was *another* one of the buggiest parts of v1, and over time it
+        became clear that its maintenance burden far outweighed the fact that
+        it was poorly reinventing ``rsync`` and/or the use of archival file
+        tools like ye olde ``tar``+``gzip``.
+
+        For one potential workaround, see the ``rsync`` function in `patchwork
+        <https://github.com/fabric/patchwork>`_.
+    * - Remote file path tilde expansion
+      - Removed
+      - This behavior is ultimately unnecessary (one can simply leave the
+        tilde off for the same result) and had a few pernicious bugs of its
+        own, so it's gone.
+    * - Naming downloaded files after some aspect of the remote destination, to
+        avoid overwriting during multi-server actions
+      - `Pending <https://github.com/fabric/fabric/issues/1868>`__
+      - This falls under the `~fabric.group.Group` family, which still needs
+        some work in this regard.
+
+
+.. _upgrading-configuration:
+
+Configuration
+-------------
+
+In general, configuration has been massively improved over the old ``fabricrc``
+files; most config logic comes from :ref:`Invoke's configuration system
+<configuration>`, which offers a full-fledged configuration hierarchy (in-code
+config, multiple config file locations, environment variables, CLI flags, and
+more) and multiple file formats. Nearly all configuration avenues in Fabric 1
+become, in modern Fabric, manipulation of whatever part of the config hierarchy
+is most appropriate for your needs.
+
+Modern versions of Fabric only make minor modifications to (or
+parameterizations of) Invoke's setup; see :ref:`our locally-specific config doc
+page <fab-configuration>` for details.
+
+.. note::
+    Make sure to look elsewhere in this document for details on any given v1
+    ``env`` setting, as many have moved outside the configuration system into
+    object or method keyword arguments.
+
+.. list-table::
+    :widths: 40 10 50
+
+    * - Modifying ``fabric.(api.)env`` directly
+      - Ported
+      - To effect truly global-scale config changes, use config files,
+        task-collection-level config data, or the invoking shell's environment
+        variables.
+    * - Making locally scoped ``fabric.env`` changes via ``with
+        settings(...):`` or its decorator equivalent, ``@with_settings``
+      - Ported/Pending
+      - Most of the use cases surrounding ``settings`` are now served by
+        the fact that `fabric.connection.Connection` objects keep
+        per-host/connection state - the pattern of switching the implicit
+        global context around was a design antipattern which is now gone.
+
+        The remaining such use cases have been turned into context-manager
+        methods of `fabric.connection.Connection` (or its parent class), or
+        have such methods pending.
+    * - SSH config file loading (off by default, limited to ``~/.ssh/config``
+        only unless configured to a different, single path)
+      - Ported
+      - Much improved: SSH config file loading is **on** by default (which
+        :ref:`can be changed <disabling-ssh-config>`), multiple sources are
+        loaded and merged just like OpenSSH, and more besides; see
+        :ref:`ssh-config`.
+
+        In addition, we've added support for some ``ssh_config`` directives
+        which were ignored by v1, such as ``ConnectTimeout`` and
+        ``ProxyCommand``, and going forwards we intend to support as much of
+        ``ssh_config`` as is reasonably possible.
+
+.. _upgrading-contrib:
+
+``contrib``
+-----------
+
+The old ``contrib`` module represented "best practice" functions that did not,
+themselves, require core support from the rest of Fabric but were built using
+the same primitives available to users.
+
+In modern Fabric, that responsibility has been removed from the core library
+into other standalone libraries which have their own identity & release
+process, typically either `invocations
+<https://github.com/pyinvoke/invocations>`_ (local-oriented code that does not
+use SSH) or `patchwork <https://github.com/fabric/patchwork>`_ (primarily
+remote-oriented code, though anything not explicitly dealing with both ends of
+the connection will work just as well locally.)
+
+Those libraries are still a work in progress, not least because we still need
+to identify the best way to bridge the gap between them (as many operations are
+not intrinsically local-or-remote but can work on either end.)
+
+Since they are by definition built on the core APIs available to all users,
+they currently get less development focus; users can always implement their own
+versions without sacrificing much (something less true for the core libraries.)
+We expect to put more work into curating these collections once the core APIs
+have settled down.
+
+Details about what happened to each individual chunk of ``fabric.contrib`` are
+in the below table:
+
+.. list-table::
+    :widths: 40 10 50
+
+    * - ``console.confirm`` for easy bool-returning confirmation prompts
+      - Ported
+      - Moved to ``invocations.console.confirm``, with minor signature tweaks.
+    * - ``django.*``, supporting integration with a local Django project re:
+        importing and using Django models and other code
+      - Removed
+      - We aren't even sure if this is useful a decade after it was written,
+        given how much Django has surely changed since then. If you're reading
+        and are sad that this is gone, let us know!
+    * - ``files.*`` (e.g. ``exists``, ``append``, ``contains`` etc) for
+        interrogating and modifying remote files
+      - Ported/Pending
+      - Many of the more useful functions in this file have been ported to
+        ``patchwork.files`` but are still in an essentially alpha state.
+
+        Others, such as ``is_link``, ``comment``/``uncomment``, etc have not
+        been ported yet. If they are, the are likely to end up in the same
+        place.
+    * - ``project.rsync_project`` for rsyncing the entire host project remotely
+      - Ported
+      - Now ``patchwork.transfers.rsync``, with some modifications.
+    * - ``project.rsync_project`` for uploading host project via archive file
+        and scp
+      - Removed
+      - This did not seem worth porting; the overall pattern of "copy my local
+        bits remotely" is already arguably an antipattern (vs repeatable
+        deploys of artifacts, or at least remote checkout of a VCS tag) and if
+        one is going down that road anyways, rsync is a much smarter choice.
+
+.. _upgrading-env:
+
+``fabric.env`` reference
+------------------------
+
+Many/most of the members in v1's ``fabric.env`` are covered in the above
+per-topic sections; any that are *not* covered elsewhere, live here. All are
+explicitly noted as ``env.<name>`` for ease of searching in your browser or
+viewer.
+
+A small handful of env vars were never publicly documented & were thus
+implicitly private; those are not represented here.
+
+.. list-table::
+    :widths: 40 10 50
+
+    * - ``env.abort_exception`` for setting which exception is used to abort
+      - Removed
+      - Aborting as a concept is gone, just raise whatever exception seems most
+        reasonable to surface to an end user, or use `~invoke.exceptions.Exit`.
+        See also :ref:`upgrading-utility`.
+    * - ``env.all_hosts`` and ``env.tasks`` listing execution targets
+      - Ported/`Pending <https://github.com/pyinvoke/invoke/issues/443>`__
+      - Fabric's `~invoke.executor.Executor` subclass stores references to all
+        CLI parsing results (including the value of :option:`--hosts`, the
+        tasks requested and their args, etc) and the intent is for users to
+        have access to that information.
+
+        However, the details for that API (e.g. exposing the executor via a
+        task's `~invoke.context.Context`/`fabric.connection.Connection`) are
+        still in flux.
+    * - ``env.command`` noting currently executing task name (in hindsight,
+        quite the misnomer...)
+      - Ported/`Pending <https://github.com/pyinvoke/invoke/issues/443>`__
+      - See the notes for ``env.all_hosts`` above - same applies here re: user
+        visibility into CLI parsing results.
+    * - ``env.command_prefixes`` for visibility into (arguably also mutation
+        of) the shell command prefixes to be applied to ``run``/``sudo``
+      - Ported
+      - This is now `~invoke.context.Context.command_prefixes`.
+    * - ``env.cwd`` noting current intended working directory
+      - Ported
+      - This is now `~invoke.context.Context.command_cwds` (a list, not a
+        single string, to more properly model the intended
+        contextmanager-driven use case.)
+
+        Note that remote-vs-local context for this data isn't yet set up; see
+        the notes about ``with cd`` under :ref:`upgrading-commands`.
+    * - ``env.dedupe_hosts`` controlling whether duplicate hosts in merged host
+        lists get deduplicated or not
+      - `Pending <https://github.com/fabric/fabric/issues/1594>`__
+      - Not ported yet, will probably get tackled as part of roles/host lists
+        overhaul.
+    * - ``env.echo_stdin`` (undocumented) for turning off the default echoing
+        of standard input
+      - Ported
+      - Is now a config option under the ``run`` tree, with much the same
+        behavior.
+    * - ``env.local_user`` for read-only access to the discovered local
+        username
+      - Removed
+      - We're not entirely sure why v1 felt this was worth caching in the
+        config; if you need this info, just import and call
+        `fabric.util.get_local_user`.
+    * - ``env.output_prefix`` determining whether or not line-by-line
+        host-string prefixes are displayed
+      - `Pending <https://github.com/pyinvoke/invoke/issues/15>`__
+      - Differentiating parallel stdout/err is still a work in progress; we may
+        end up reusing line-by-line logging and prefixing (ideally via actual
+        logging) or we may try for something cleaner such as streaming to
+        per-connection log files.
+    * - ``env.prompts`` controlling prompt auto-response
+      - Ported
+      - Prompt auto-response is now publicly implemented as the
+        `~invoke.watchers.StreamWatcher` and `~invoke.watchers.Responder` class
+        hierarchy, instances of which can be handed to ``run`` via kwarg or
+        stored globally in the config as ``run.watchers``.
+    * - ``env.real_fabfile`` storing read-only fabfile path which was loaded by
+        the CLI machinery
+      - Ported
+      - The loaded task `~invoke.collection.Collection` is stored on both the
+        top level `~invoke.program.Program` object as well as the
+        `~invoke.executor.Executor` which calls tasks; and
+        `~invoke.collection.Collection` has a ``loaded_from`` attribute with
+        this information.
+    * - ``env.remote_interrupt`` controlling how interrupts (i.e. a local
+        `KeyboardInterrupt` are caught, forwarded or other
+      - Ported/Removed
+      - Invoke's interrupt capture behavior is currently "always just send the
+        interrupt character to the subprocess and continue", allowing
+        subprocesses to handle ``^C`` however they need to, which is an
+        improvement over Fabric 1 and roughly equivalent to setting
+        ``env.remote_interrupt = True``.
+
+        Allowing users to change this behavior via config is not yet
+        implemented, and may not be, depending on whether anybody needs it - it
+        was added as an option in v1 for backwards compat reasons.
+
+        It is also technically possible to change interrupt behavior by
+        subclassing and overriding `invoke.runners.Runner.send_interrupt`.
+    * - ``env.roles``, ``env.roledefs`` and ``env.effective_roles``
+        controlling/exposing what roles are available or currently in play
+      - `Pending <https://github.com/fabric/fabric/issues/1594>`__
+      - As noted in :ref:`upgrading-api`, roles as a concept were ported to
+        `fabric.group.Group`, but there's no central clearinghouse in which to
+        store them.
+
+        We *may* delegate this to userland forever, but seems likely a
+        common-best-practice option (such as creating `Groups
+        <fabric.group.Group>` from some configuration subtree and storing them
+        as a `~invoke.context.Context` attribute) will appear in early 2.x.
+    * - ``env.ok_ret_codes`` for overriding the default "0 good, non-0 bad"
+        error detection for subprocess commands
+      - `Pending <https://github.com/pyinvoke/invoke/issues/541>`__
+      - Not ported yet, but should involve some presumably minor updates to
+        `invoke.runners.Runner.generate_result` and `~invoke.runners.Result`.
+    * - ``env.sudo_prefix`` determining the sudo binary name + its flags used
+        when creating ``sudo`` command strings
+      - `Pending <https://github.com/pyinvoke/invoke/issues/540>`__
+      - Sudo command construction does not currently look at the config for
+        anything but the actual sudo prompt.
+    * - ``env.sudo_prompt`` for setting the prompt string handed to ``sudo``
+        (and then expected in return for auto-replying with a configured
+        password)
+      - Ported
+      - Is now ``sudo.prompt`` in the configuration system.
+    * - ``env.use_exceptions_for`` to note which actions raise exceptions
+      - Removed
+      - As with most other functionality surrounding Fabric 1's "jump straight
+        to `sys.exit`" design antipattern, this is gone - modern Fabric will
+        not be hiding any exceptions from user-level code.
+    * - ``env.use_ssh_config`` to enable off-by-default SSH config loading
+      - Ported
+      - SSH config loading is now on by default, but an option remains to
+        disable it. See :ref:`upgrading-configuration` for more.
+    * - ``env.version`` exposing current Fabric version number
+      - Removed
+      - Just ``import fabric`` and reference ``fabric.__version__`` (string) or
+        ``fabric.__version_info__`` (tuple).
+
+
+Example upgrade process
+=======================
+
+This section goes over upgrading a small but nontrivial Fabric 1 fabfile to
+work with modern Fabric. It's not meant to be exhaustive, merely illustrative;
+for a full list of how to upgrade individual features or concepts, see
+:ref:`upgrade-specifics`.
+
+Sample original fabfile
+-----------------------
+
+Here's a (slightly modified to concur with 'modern' Fabric 1 best practices)
+copy of Fabric 1's final tutorial snippet, which we will use as our test case
+for upgrading::
+
+    from fabric.api import abort, env, local, run, settings, task
+    from fabric.contrib.console import confirm
+
+    env.hosts = ["my-server"]
+
+    @task
+    def test():
+        with settings(warn_only=True):
+            result = local("./manage.py test my_app", capture=True)
+        if result.failed and not confirm("Tests failed. Continue anyway?"):
+            abort("Aborting at user request.")
+
+    @task
+    def commit():
+        local("git add -p && git commit")
+
+    @task
+    def push():
+        local("git push")
+
+    @task
+    def prepare_deploy():
+        test()
+        commit()
+        push()
+
+    @task
+    def deploy():
+        code_dir = "/srv/django/myproject"
+        with settings(warn_only=True):
+            if run("test -d {}".format(code_dir)).failed:
+                cmd = "git clone user@vcshost:/path/to/repo/.git {}"
+                run(cmd.format(code_dir))
+        with cd(code_dir):
+            run("git pull")
+            run("touch app.wsgi")
+
+We'll port this directly, meaning the result will still be ``fabfile.py``,
+though we'd like to note that writing your code in a more library-oriented
+fashion - even just as functions not wrapped in ``@task`` - can make testing
+and reusing code easier.
+
+Imports
+-------
+
+In modern Fabric, we don't need to import nearly as many functions, due to the
+emphasis on object methods instead of global functions. We only need the
+following:
+
+- `~invoke.exceptions.Exit`, a friendlier way of requesting a `sys.exit`;
+- `@task <invoke.tasks.task>`, as before, but coming from Invoke as it's not
+  SSH-specific;
+- ``confirm``, which now comes from the Invocations library (also not
+  SSH-specific; though Invocations is one of the descendants of
+  ``fabric.contrib``, which no longer exists);
+
+::
+
+    from fabric import task
+    from invoke import Exit
+    from invocations.console import confirm
+
+Host list
+---------
+
+The idea of a predefined *global* host list is gone; there is currently no
+direct replacement. In general, users can set up their own execution context,
+creating explicit `fabric.connection.Connection` and/or `fabric.group.Group`
+objects as needed; core Fabric is in the process of building convenience
+helpers on top of this, but "create your own Connections" will always be there
+as a backstop.
+
+Speaking of convenience helpers: most of the functionality of ``fab --hosts``
+and ``@hosts`` has been ported over -- the former directly (see
+:option:`--hosts`), the latter as a `@task <fabric.tasks.task>` keyword
+argument. Thus, for now our example will be turning the global ``env.hosts``
+into a lightweight module-level variable declaration, intended for use in the
+subsequent calls to ``@task``::
+
+    my_hosts = ["my-server"]
+
+.. note::
+    This is an area under active development, so feedback is welcomed.
+
+.. TODO:
+    - pre-task example
+    - true baked-in default example (requires some sort of config hook)
+
+Test task
+---------
+
+The first task in the fabfile uses a good spread of the API. We'll outline the
+changes here (though again, all details are in :ref:`upgrade-specifics`):
+
+- Declaring a function as a task is nearly the same as before: use a ``@task``
+  decorator (which, in modern Fabric, can take more optional keyword arguments
+  than its predecessor, including some which replace some of v1's decorators).
+- ``@task``-wrapped functions must now take an explicit initial context
+  argument, whose value will be a `fabric.connection.Connection` object at
+  runtime.
+- The use of ``with settings(warn_only=True)`` can be replaced by a simple
+  kwarg to the ``local`` call.
+- That ``local`` call is now a method call on the
+  `fabric.connection.Connection`, `fabric.connection.Connection.local`.
+- ``capture`` is no longer a useful argument; we can now capture and display at
+  the same time, locally or remotely. If you don't actually *want* a local
+  subprocess to mirror its stdout/err while it runs, you can simply say
+  ``hide=True`` (or ``hide="stdout"`` or etc.)
+- Result objects are pretty similar between versions; modern Fabric's results
+  no longer pretend to "be" strings, but instead act more like booleans, acting
+  truthy if the command exited cleanly, and falsey otherwise. In terms of
+  attributes exhibited, most of the same info is available, and more besides.
+- ``abort`` is gone; you should use whatever exceptions you feel are
+  appropriate, or `~invoke.exceptions.Exit` for a `sys.exit` equivalent. (Or
+  just call `sys.exit` if you want a no-questions-asked immediate exit that
+  even our CLI machinery won't touch.)
+
+The result::
+
+    @task
+    def test(c):
+        result = c.local("./manage.py test my_app", warn=True)
+        if not result and not confirm("Tests failed. Continue anyway?"):
+            raise Exit("Aborting at user request.")
+
+Other simple tasks
+------------------
+
+The next two tasks are simple one-liners, and you've already seen what replaced
+the global ``local`` function::
+
+    @task
+    def commit(c):
+        c.local("git add -p && git commit")
+
+    @task
+    def push(c):
+        c.local("git push")
+
+Calling tasks from other tasks
+------------------------------
+
+This is another area that is in flux at the Invoke level, but for now, we can
+simply call the other tasks as functions, just as was done in v1. The main
+difference is that we want to pass along our context object to preserve the
+configuration context (such as loaded config files or CLI flags)::
+
+    @task
+    def prepare_deploy(c):
+        test(c)
+        commit(c)
+        push(c)
+
+Actual remote steps
+-------------------
+
+Note that up to this point, nothing truly Fabric-related has been in play -
+`fabric.connection.Connection.local` is just a rebinding of `Context.run
+<invoke.context.Context.run>`, Invoke's local subprocess execution method. Now
+we get to the actual deploy step, which invokes
+`fabric.connection.Connection.run` instead, executing remotely (on whichever
+host the `fabric.connection.Connection` has been bound to).
+
+``with cd`` is not fully implemented for the remote side of things, but we
+expect it will be soon. For now we fall back to command chaining with ``&&``.
+And, notably, now that we care about selecting host targets, we refer to our
+earlier definition of a default host list -- ``my_hosts`` -- when declaring the
+default host list for this task.
+
+::
+
+    @task(hosts=my_hosts)
+    def deploy(c):
+        code_dir = "/srv/django/myproject"
+        if not c.run("test -d {}".format(code_dir), warn=True):
+            cmd = "git clone user@vcshost:/path/to/repo/.git {}"
+            c.run(cmd.format(code_dir))
+        c.run("cd {} && git pull".format(code_dir))
+        c.run("cd {} && touch app.wsgi".format(code_dir))
+
+The whole thing
+---------------
+
+Now we have the entire, upgraded fabfile that will work with modern Fabric::
+
+    from invoke import Exit
+    from invocations.console import confirm
+
+    from fabric import task
+
+    my_hosts = ["my-server"]
+
+    @task
+    def test(c):
+        result = c.local("./manage.py test my_app", warn=True)
+        if not result and not confirm("Tests failed. Continue anyway?"):
+            raise Exit("Aborting at user request.")
+
+    @task
+    def commit(c):
+        c.local("git add -p && git commit")
+
+    @task
+    def push(c):
+        c.local("git push")
+
+    @task
+    def prepare_deploy(c):
+        test(c)
+        commit(c)
+        push(c)
+
+    @task(hosts=my_hosts)
+    def deploy(c):
+        code_dir = "/srv/django/myproject"
+        if not c.run("test -d {}".format(code_dir), warn=True):
+            cmd = "git clone user@vcshost:/path/to/repo/.git {}"
+            c.run(cmd.format(code_dir))
+        c.run("cd {} && git pull".format(code_dir))
+        c.run("cd {} && touch app.wsgi".format(code_dir))
diff -Nru fabric-1.14.0/tasks.py fabric-2.5.0/tasks.py
--- fabric-1.14.0/tasks.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/tasks.py	2019-08-06 23:57:28.000000000 +0100
@@ -1,16 +1,161 @@
-from invocations.docs import docs, www
+from functools import partial
+from os import environ, getcwd
+import sys
+
+from invocations import travis
+from invocations.checks import blacken
+from invocations.docs import docs, www, sites, watch_docs
+from invocations.pytest import test, integration as integration_, coverage
 from invocations.packaging import release
+from invocations.util import tmpdir
+
+from invoke import Collection, task
+from invoke.util import LOG_FORMAT
+
+
+# Neuter the normal release.publish task to prevent accidents, then reinstate
+# it as a custom task that does dual fabric-xxx and fabric2-xxx releases.
+# TODO: tweak this once release.all_ actually works right...sigh
+# TODO: if possible, try phrasing as a custom build that builds x2, and then
+# convince the vanilla publish() to use that custom build instead of its local
+# build?
+# NOTE: this skips the dual_wheels, alt_python bits the upstream task has,
+# which are at the moment purely for Invoke's sake (as it must publish explicit
+# py2 vs py3 wheels due to some vendored dependencies)
+@task
+def publish(
+    c,
+    sdist=True,
+    wheel=False,
+    index=None,
+    sign=False,
+    dry_run=False,
+    directory=None,
+    check_desc=False,
+):
+    # TODO: better pattern for merging kwargs + config
+    config = c.config.get("packaging", {})
+    index = config.get("index", index)
+    sign = config.get("sign", sign)
+    check_desc = config.get("check_desc", check_desc)
+    # Initial sanity check, if needed. Will die usefully.
+    # TODO: this could also get factored out harder in invocations. shrug. it's
+    # like 3 lines total...
+    if check_desc:
+        c.run("python setup.py check -r -s")
+    with tmpdir(skip_cleanup=dry_run, explicit=directory) as directory:
+        # Doesn't reeeeally need to be a partial, but if we start having to add
+        # a kwarg to one call or the other, it's nice
+        builder = partial(
+            release.build, c, sdist=sdist, wheel=wheel, directory=directory
+        )
+        # Vanilla build
+        builder()
+        # Fabric 2 build
+        environ["PACKAGE_AS_FABRIC2"] = "yes"
+        builder()
+        # Upload
+        release.upload(c, directory, index, sign, dry_run)
+
+
+@task
+def sanity_test_from_v1(c):
+    """
+    Run some very quick in-process sanity tests on a dual fabric1-v-2 env.
+
+    Assumes Fabric 2+ is already installed as 'fabric2'.
+    """
+    # This cannot, by definition, work under Python 3 as Fabric 1 is not Python
+    # 3 compatible.
+    PYTHON = environ.get("TRAVIS_PYTHON_VERSION", "")
+    if PYTHON.startswith("3") or PYTHON == "pypy3":
+        return
+    c.run("pip install 'fabric<2'")
+    # Make darn sure the two copies of fabric are coming from install root, not
+    # local directory - which would result in 'fabric' always being v2!
+    for serious in (getcwd(), ""):
+        if serious in sys.path:  # because why would .remove be idempotent?!
+            sys.path.remove(serious)
+
+    from fabric.api import env
+    from fabric2 import Connection
+
+    env.gateway = "some-gateway"
+    env.no_agent = True
+    env.password = "sikrit"
+    env.user = "admin"
+    env.host_string = "localghost"
+    env.port = "2222"
+    cxn = Connection.from_v1(env)
+    config = cxn.config
+    assert config.run.pty is True
+    assert config.gateway == "some-gateway"
+    assert config.connect_kwargs.password == "sikrit"
+    assert config.sudo.password == "sikrit"
+    assert cxn.host == "localghost"
+    assert cxn.user == "admin"
+    assert cxn.port == 2222
+
+
+# TODO: as usual, this just wants a good pattern for "that other task, with a
+# tweaked default arg value"
+@task
+def integration(
+    c,
+    opts=None,
+    pty=True,
+    x=False,
+    k=None,
+    verbose=True,
+    color=True,
+    capture="no",
+    module=None,
+):
+    return integration_(c, opts, pty, x, k, verbose, color, capture, module)
 
-from invoke import Collection
 
+# Better than nothing, since we haven't solved "pretend I have some other
+# task's signature" yet...
+publish.__doc__ = release.publish.__doc__
+my_release = Collection(
+    "release", release.build, release.status, publish, release.prepare
+)
 
-ns = Collection(docs, www, release)
-ns.configure({
-    'packaging': {
-        'sign': True,
-        'wheel': True,
-        'changelog_file': 'sites/www/changelog.rst',
-        'package': 'fabric',
-        'version_module': 'version',
-    },
-})
+ns = Collection(
+    blacken,
+    coverage,
+    docs,
+    integration,
+    my_release,
+    sites,
+    test,
+    travis,
+    watch_docs,
+    www,
+    sanity_test_from_v1,
+)
+ns.configure(
+    {
+        "tests": {
+            # TODO: have pytest tasks honor these?
+            "package": "fabric",
+            "logformat": LOG_FORMAT,
+        },
+        "packaging": {
+            # NOTE: this is currently for identifying the source directory.
+            # Should it get used for actual releasing, needs changing.
+            "package": "fabric",
+            "sign": True,
+            "wheel": True,
+            "check_desc": True,
+            "changelog_file": "sites/www/changelog.rst",
+        },
+        # TODO: perhaps move this into a tertiary, non automatically loaded,
+        # conf file so that both this & the code under test can reference it?
+        # Meh.
+        "travis": {
+            "sudo": {"user": "sudouser", "password": "mypass"},
+            "black": {"version": "18.6b4"},
+        },
+    }
+)
diff -Nru fabric-1.14.0/tests/client.key fabric-2.5.0/tests/client.key
--- fabric-1.14.0/tests/client.key	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/tests/client.key	1970-01-01 01:00:00.000000000 +0100
@@ -1,30 +0,0 @@
------BEGIN RSA PRIVATE KEY-----
-Proc-Type: 4,ENCRYPTED
-DEK-Info: DES-EDE3-CBC,F1AFE040F412E6D1
-
-cIBbwu1/PD9vjtyFn+xbpc2X9Uv9sllCRooLwkOv9rkBxDRItT8D5UiGHGIGIAvj
-eq9sUze8bXQeXs9zpJwMRH1kjdmCmnmRX0iXcsxSgnioL3aEGLTbXqxkUOnSgj4Y
-cJ1trT51XVRSBGlRHYPmF1IhYYW/RPZlFUPMJDE5s1moROU29DfnaboTREf8shJ9
-A/jHvKoivn4GgM1U6VcwwtijvmgrrB5KzqpRfTLf6Rxe6St3e4WjQusYWVP4BOmz
-ImQyaATcPwn5iMWPfvXohPQR/ajuoU9jzMM3DqzcrH7Q4VmpSTrmkdG7Ra5GfSE1
-O5WEiqNwUkfjAYIjbxo11gVtIH8ddsMuF5odsh2LVXYocHeZzRlZvsip2AePKiKX
-xMkZItP4xqFBfi0jnqCVkQGUdtRYhHomDUO8U0JtB3BFNT/L+LC+dsrj8G/FaQiD
-n8an2sDf1CrYXqfz3V3rGzuPDq/CKwPD8HeTpjZUT7bPUNsTNMVx58LiYShRV2uB
-zUn83diKX12xS+gyS5PfuujwQP93ZQXOP9agKSa2UlY2ojUxtpc1vxiEzcFcU9Zg
-2uLEbsRKW1qe2jLDTmRyty14rJmi7ocbjPUuEuw9Aj1v46jzhBXBPE7cWHGm1o2/
-/e0lGfLTtm3Q2SponTLTcHTrBvrDBRlDAN5sChhbaoEoUCHjTKo8aj6whDKfAw4Q
-KNHrOkkXyDyvd90c1loen5u5iaol+l5W+7LG3Sr5uRHMHAsF0MH9cZd/RQXMSY/U
-sQLWumskx/iSrbjFztW0La0bBCB6vHBYLervC3lrrmvnhfYrNBrZM8eH1hTSZUsT
-VFeKgm+KVkwEG/uXoI/XOge01b1oOHzKNKGT7Q5ogbV6w67LtOrSeTH0FCjHsN8z
-2LCQHWuII4h3b1U/Pg8N5Pz59+qraSrMZAHOROYc19r0HSS5gg7m1yD3IPXO73fI
-gLO0/44f/KYqVP2+FKgQo9enUSLI5GuMAfhWaTpeOpJNd10egSOB3SaJ7nn20/Pm
-vSBSL0KsSeXY4/Df43MuHu46PvYzRwKvZB7GJJJPi2XjdFqCxuoCuEqfaZxf1lnI
-ZhZFmsZE1rd7kgBYyn0VXn1AvrLjaLuvmsOKaFdO4TAbQpE3Pps6AdQ8EpJ62Gei
-0yZlXgh2+zZp5lRMfO5JFtr7/pVpIqnRKfaDk1XawWP7i1/0PnVXsR2G6yu6kbEg
-R/v2LKnp49TUldfNmVW8QHElw/LrCBW08iA+44vlGYdCU8nAW9Sy+y4plW+X32z8
-Viw82ISUcoJSHmRfzXOWaj24AftbSOzo2bRmCO+xkBkXFrhTI83Aqbu7TN/yejB8
-hDb04AVxzEkBTw/B0pLkJUt5lpcr9fZMvACHsL0gTRc5OPb4/zhG7y9npWgq5Snb
-ZnUAOi+ndnW8IL4y9YI6U7LBSyMvE7L7+QCnLJxVnO2NxjDCJVDDe6fLR9pRBCCC
-Sh3X/FNsu1YQzNIOvf75ri1zzqKmv4x6ETmmgs+vMGRl62s8SQcgWFEGAVrAP+uR
-ocx0chW3BWEQalRat2vBWpj1gyH2aHd8tgamb8XXFLK35iTk2/oCqQ==
------END RSA PRIVATE KEY-----
diff -Nru fabric-1.14.0/tests/client.key.pub fabric-2.5.0/tests/client.key.pub
--- fabric-1.14.0/tests/client.key.pub	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/tests/client.key.pub	1970-01-01 01:00:00.000000000 +0100
@@ -1 +0,0 @@
-ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEA2FxgXlTZGk/JZMacwgMPC6LEd3efYgIdgK0RXGRMNs06aSyeEUwTKqmelNnElsRsUW68Ybosox0LoHGfTUj0gtSOqG+pb0QJQ5yslPBwBlL+WUC65HDzHdBrUf/bFR+rc02i2Ciraan4elvuLW07UfO5ceCOeJSYyNmrhN/vboHr3Pcv2QG717sEy/9pSAVzrriCqYFd6IFg9o6UhuSB7hvW4bzKXDHtz6OeXrC6U/FWxx3rYZg3h9K2SBGXLavqiJSkFgeSzn3geSbyAjTgowaZ8kNq4+Mc1hsAMtLZBKMBZUTuMjHpQR31nWloUUfuz5QhaORk1pJBmE90MqShiw== jforcier@ytram
diff -Nru fabric-1.14.0/tests/config.py fabric-2.5.0/tests/config.py
--- fabric-1.14.0/tests/config.py	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/tests/config.py	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,316 @@
+import errno
+from os.path import join, expanduser
+
+from paramiko.config import SSHConfig
+from invoke.vendor.lexicon import Lexicon
+
+from fabric import Config
+from fabric.util import get_local_user
+
+from mock import patch, call
+
+from _util import support, faux_v1_env
+
+
+class Config_:
+    def defaults_to_merger_of_global_defaults(self):
+        # I.e. our global_defaults + Invoke's global_defaults
+        c = Config()
+        # From invoke's global_defaults
+        assert c.run.warn is False
+        # From ours
+        assert c.port == 22
+
+    def our_global_defaults_can_override_invokes(self):
+        "our global_defaults can override Invoke's key-by-key"
+        with patch.object(
+            Config,
+            "global_defaults",
+            return_value={
+                "run": {"warn": "nope lol"},
+                # NOTE: Config requires these to be present to instantiate
+                # happily
+                "load_ssh_configs": True,
+                "ssh_config_path": None,
+            },
+        ):
+            # If our global_defaults didn't win, this would still
+            # resolve to False.
+            assert Config().run.warn == "nope lol"
+
+    def has_various_Fabric_specific_default_keys(self):
+        c = Config()
+        assert c.port == 22
+        assert c.user == get_local_user()
+        assert c.forward_agent is False
+        assert c.connect_kwargs == {}
+        assert c.timeouts.connect is None
+        assert c.ssh_config_path is None
+        assert c.inline_ssh_env is False
+
+    def overrides_some_Invoke_defaults(self):
+        config = Config()
+        # This value defaults to False in Invoke proper.
+        assert config.run.replace_env is True
+        assert config.tasks.collection_name == "fabfile"
+
+    def uses_Fabric_prefix(self):
+        # NOTE: see also the integration-esque tests in tests/main.py; this
+        # just tests the underlying data/attribute driving the behavior.
+        assert Config().prefix == "fabric"
+
+    class from_v1:
+        def setup(self):
+            self.env = faux_v1_env()
+
+        def _conf(self, **kwargs):
+            self.env.update(kwargs)
+            return Config.from_v1(self.env)
+
+        def must_be_given_explicit_env_arg(self):
+            config = Config.from_v1(
+                env=Lexicon(self.env, sudo_password="sikrit")
+            )
+            assert config.sudo.password == "sikrit"
+
+        class additional_kwargs:
+            def forwards_arbitrary_kwargs_to_init(self):
+                config = Config.from_v1(
+                    self.env,
+                    # Vanilla Invoke
+                    overrides={"some": "value"},
+                    # Fabric
+                    system_ssh_path="/what/ever",
+                )
+                assert config.some == "value"
+                assert config._system_ssh_path == "/what/ever"
+
+            def subservient_to_runtime_overrides(self):
+                env = self.env
+                env.sudo_password = "from-v1"
+                config = Config.from_v1(
+                    env, overrides={"sudo": {"password": "runtime"}}
+                )
+                assert config.sudo.password == "runtime"
+
+            def connect_kwargs_also_merged_with_imported_values(self):
+                self.env["key_filename"] = "whatever"
+                conf = Config.from_v1(
+                    self.env, overrides={"connect_kwargs": {"meh": "effort"}}
+                )
+                assert conf.connect_kwargs["key_filename"] == "whatever"
+                assert conf.connect_kwargs["meh"] == "effort"
+
+        class var_mappings:
+            def always_use_pty(self):
+                # Testing both due to v1-didn't-use-None-default issues
+                config = self._conf(always_use_pty=True)
+                assert config.run.pty is True
+                config = self._conf(always_use_pty=False)
+                assert config.run.pty is False
+
+            def forward_agent(self):
+                config = self._conf(forward_agent=True)
+                assert config.forward_agent is True
+
+            def gateway(self):
+                config = self._conf(gateway="bastion.host")
+                assert config.gateway == "bastion.host"
+
+            class key_filename:
+                def base(self):
+                    config = self._conf(key_filename="/some/path")
+                    assert (
+                        config.connect_kwargs["key_filename"] == "/some/path"
+                    )
+
+                def is_not_set_if_None(self):
+                    config = self._conf(key_filename=None)
+                    assert "key_filename" not in config.connect_kwargs
+
+            def no_agent(self):
+                config = self._conf()
+                assert config.connect_kwargs.allow_agent is True
+                config = self._conf(no_agent=True)
+                assert config.connect_kwargs.allow_agent is False
+
+            class password:
+                def set_just_to_connect_kwargs_if_sudo_password_set(self):
+                    # NOTE: default faux env has sudo_password set already...
+                    config = self._conf(password="screaming-firehawks")
+                    passwd = config.connect_kwargs.password
+                    assert passwd == "screaming-firehawks"
+
+                def set_to_both_password_fields_if_necessary(self):
+                    config = self._conf(password="sikrit", sudo_password=None)
+                    assert config.connect_kwargs.password == "sikrit"
+                    assert config.sudo.password == "sikrit"
+
+            def ssh_config_path(self):
+                self.env.ssh_config_path = "/where/ever"
+                config = Config.from_v1(self.env, lazy=True)
+                assert config.ssh_config_path == "/where/ever"
+
+            def sudo_password(self):
+                config = self._conf(sudo_password="sikrit")
+                assert config.sudo.password == "sikrit"
+
+            def sudo_prompt(self):
+                config = self._conf(sudo_prompt="password???")
+                assert config.sudo.prompt == "password???"
+
+            def timeout(self):
+                config = self._conf(timeout=15)
+                assert config.timeouts.connect == 15
+
+            def use_ssh_config(self):
+                # Testing both due to v1-didn't-use-None-default issues
+                config = self._conf(use_ssh_config=True)
+                assert config.load_ssh_configs is True
+                config = self._conf(use_ssh_config=False)
+                assert config.load_ssh_configs is False
+
+            def warn_only(self):
+                # Testing both due to v1-didn't-use-None-default issues
+                config = self._conf(warn_only=True)
+                assert config.run.warn is True
+                config = self._conf(warn_only=False)
+                assert config.run.warn is False
+
+
+class ssh_config_loading:
+    "ssh_config loading"
+
+    # NOTE: actual _behavior_ of loaded SSH configs is tested in Connection's
+    # tests; these tests just prove that the loading itself works & the data is
+    # correctly available.
+
+    _system_path = join(support, "ssh_config", "system.conf")
+    _user_path = join(support, "ssh_config", "user.conf")
+    _runtime_path = join(support, "ssh_config", "runtime.conf")
+    _empty_kwargs = dict(
+        system_ssh_path="nope/nope/nope", user_ssh_path="nope/noway/nuhuh"
+    )
+
+    def defaults_to_empty_sshconfig_obj_if_no_files_found(self):
+        c = Config(**self._empty_kwargs)
+        # TODO: Currently no great public API that lets us figure out if
+        # one of these is 'empty' or not. So for now, expect an empty inner
+        # SSHConfig._config from an un-.parse()d such object. (AFAIK, such
+        # objects work fine re: .lookup, .get_hostnames etc.)
+        assert type(c.base_ssh_config) is SSHConfig
+        assert c.base_ssh_config._config == []
+
+    def object_can_be_given_explicitly_via_ssh_config_kwarg(self):
+        sc = SSHConfig()
+        assert Config(ssh_config=sc).base_ssh_config is sc
+
+    @patch.object(Config, "_load_ssh_file")
+    def when_config_obj_given_default_paths_are_not_sought(self, method):
+        sc = SSHConfig()
+        Config(ssh_config=sc)
+        assert not method.called
+
+    @patch.object(Config, "_load_ssh_file")
+    def config_obj_prevents_loading_runtime_path_too(self, method):
+        sc = SSHConfig()
+        Config(ssh_config=sc, runtime_ssh_path=self._system_path)
+        assert not method.called
+
+    @patch.object(Config, "_load_ssh_file")
+    def when_runtime_path_given_other_paths_are_not_sought(self, method):
+        Config(runtime_ssh_path=self._runtime_path)
+        method.assert_called_once_with(self._runtime_path)
+
+    @patch.object(Config, "_load_ssh_file")
+    def runtime_path_can_be_given_via_config_itself(self, method):
+        Config(overrides={"ssh_config_path": self._runtime_path})
+        method.assert_called_once_with(self._runtime_path)
+
+    def runtime_path_does_not_die_silently(self):
+        try:
+            Config(runtime_ssh_path="sure/thing/boss/whatever/you/say")
+        except IOError as e:
+            assert "No such file or directory" in str(e)
+            assert e.errno == errno.ENOENT
+        else:
+            assert False, "Bad runtime path didn't raise IOError!"
+
+    # TODO: skip on windows
+    @patch.object(Config, "_load_ssh_file")
+    def default_file_paths_match_openssh(self, method):
+        Config()
+        method.assert_has_calls(
+            [call(expanduser("~/.ssh/config")), call("/etc/ssh/ssh_config")]
+        )
+
+    def system_path_loads_ok(self):
+        c = Config(
+            **dict(self._empty_kwargs, system_ssh_path=self._system_path)
+        )
+        names = c.base_ssh_config.get_hostnames()
+        assert names == {"system", "shared", "*"}
+
+    def user_path_loads_ok(self):
+        c = Config(**dict(self._empty_kwargs, user_ssh_path=self._user_path))
+        names = c.base_ssh_config.get_hostnames()
+        assert names == {"user", "shared", "*"}
+
+    def both_paths_loaded_if_both_exist_with_user_winning(self):
+        c = Config(
+            user_ssh_path=self._user_path, system_ssh_path=self._system_path
+        )
+        names = c.base_ssh_config.get_hostnames()
+        expected = {"user", "system", "shared", "*"}
+        assert names == expected
+        # Expect the user value (321), not the system one (123)
+        assert c.base_ssh_config.lookup("shared")["port"] == "321"
+
+    @patch.object(Config, "_load_ssh_file")
+    @patch("fabric.config.os.path.exists", lambda x: True)
+    def runtime_path_subject_to_user_expansion(self, method):
+        # TODO: other expansion types? no real need for abspath...
+        tilded = "~/probably/not/real/tho"
+        Config(runtime_ssh_path=tilded)
+        method.assert_called_once_with(expanduser(tilded))
+
+    @patch.object(Config, "_load_ssh_file")
+    def user_path_subject_to_user_expansion(self, method):
+        # TODO: other expansion types? no real need for abspath...
+        tilded = "~/probably/not/real/tho"
+        Config(user_ssh_path=tilded)
+        method.assert_any_call(expanduser(tilded))
+
+    class core_ssh_load_option_allows_skipping_ssh_config_loading:
+        @patch.object(Config, "_load_ssh_file")
+        def skips_default_paths(self, method):
+            Config(overrides={"load_ssh_configs": False})
+            assert not method.called
+
+        @patch.object(Config, "_load_ssh_file")
+        def does_not_affect_explicit_object(self, method):
+            sc = SSHConfig()
+            c = Config(ssh_config=sc, overrides={"load_ssh_configs": False})
+            # Implicit loading still doesn't happen...sanity check
+            assert not method.called
+            # Real test: the obj we passed in is present as usual
+            assert c.base_ssh_config is sc
+
+        @patch.object(Config, "_load_ssh_file")
+        def does_not_skip_loading_runtime_path(self, method):
+            Config(
+                runtime_ssh_path=self._runtime_path,
+                overrides={"load_ssh_configs": False},
+            )
+            # Expect that loader method did still run (and, as usual, that
+            # it did not load any other files)
+            method.assert_called_once_with(self._runtime_path)
+
+    class lazy_loading_and_explicit_methods:
+        @patch.object(Config, "_load_ssh_file")
+        def may_use_lazy_plus_explicit_methods_to_control_flow(self, method):
+            c = Config(lazy=True)
+            assert not method.called
+            c.set_runtime_ssh_path(self._runtime_path)
+            c.load_ssh_config()
+            method.assert_called_once_with(self._runtime_path)
diff -Nru fabric-1.14.0/tests/conftest.py fabric-2.5.0/tests/conftest.py
--- fabric-1.14.0/tests/conftest.py	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/tests/conftest.py	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,2 @@
+# flake8: noqa
+from fabric.testing.fixtures import client, remote, sftp, sftp_objs, transfer
diff -Nru fabric-1.14.0/tests/connection.py fabric-2.5.0/tests/connection.py
--- fabric-1.14.0/tests/connection.py	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/tests/connection.py	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,1259 @@
+from itertools import chain, repeat
+
+try:
+    from invoke.vendor.six import b
+except ImportError:
+    from six import b
+import errno
+from os.path import join
+import socket
+import time
+
+from mock import patch, Mock, call, ANY
+from paramiko.client import SSHClient, AutoAddPolicy
+from paramiko import SSHConfig
+import pytest  # for mark
+from pytest import skip, param
+from pytest_relaxed import raises
+from invoke.vendor.lexicon import Lexicon
+
+from invoke.config import Config as InvokeConfig
+from invoke.exceptions import ThreadException
+
+from fabric import Config as Config_
+from fabric.exceptions import InvalidV1Env
+from fabric.util import get_local_user
+
+from _util import support, Connection, Config, faux_v1_env
+
+
+# Remote is woven in as a config default, so must be patched there
+remote_path = "fabric.config.Remote"
+
+
+def _select_result(obj):
+    """
+    Return iterator/generator suitable for mocking a select.select() call.
+
+    Specifically one that has a single initial return value of ``obj``, and
+    then empty results thereafter.
+
+    If ``obj`` is an exception, it will be used as the sole initial
+    ``side_effect`` (as opposed to a return value among tuples).
+    """
+    # select.select() returns three N-tuples. Have it just act like a single
+    # read event happened, then quiet after. So chain a single-item iterable to
+    # a repeat(). (Mock has no built-in way to do this apparently.)
+    initial = [(obj,), tuple(), tuple()]
+    if isinstance(obj, Exception) or (
+        isinstance(obj, type) and issubclass(obj, Exception)
+    ):
+        initial = obj
+    return chain([initial], repeat([tuple(), tuple(), tuple()]))
+
+
+class Connection_:
+    class basic_attributes:
+        def is_connected_defaults_to_False(self):
+            assert Connection("host").is_connected is False
+
+        def client_defaults_to_a_new_SSHClient(self):
+            c = Connection("host").client
+            assert isinstance(c, SSHClient)
+            assert c.get_transport() is None
+
+    class known_hosts_behavior:
+        def defaults_to_auto_add(self):
+            # TODO: change Paramiko API so this isn't a private access
+            # TODO: maybe just merge with the __init__ test that is similar
+            assert isinstance(Connection("host").client._policy, AutoAddPolicy)
+
+    class init:
+        "__init__"
+
+        class host:
+            @raises(TypeError)
+            def is_required(self):
+                Connection()
+
+            def is_exposed_as_attribute(self):
+                assert Connection("host").host == "host"  # buffalo buffalo
+
+            def may_contain_user_shorthand(self):
+                c = Connection("user@host")
+                assert c.host == "host"
+                assert c.user == "user"
+
+            def may_contain_port_shorthand(self):
+                c = Connection("host:123")
+                assert c.host == "host"
+                assert c.port == 123
+
+            def may_contain_user_and_port_shorthand(self):
+                c = Connection("user@host:123")
+                assert c.host == "host"
+                assert c.user == "user"
+                assert c.port == 123
+
+            def ipv6_addresses_work_ok_but_avoid_port_shorthand(self):
+                for addr in ("2001:DB8:0:0:0:0:0:1", "2001:DB8::1", "::1"):
+                    c = Connection(addr, port=123)
+                    assert c.user == get_local_user()
+                    assert c.host == addr
+                    assert c.port == 123
+                    c2 = Connection("somebody@{}".format(addr), port=123)
+                    assert c2.user == "somebody"
+                    assert c2.host == addr
+                    assert c2.port == 123
+
+        class user:
+            def defaults_to_local_user_with_no_config(self):
+                # Tautology-tastic!
+                assert Connection("host").user == get_local_user()
+
+            def accepts_config_user_option(self):
+                config = Config(overrides={"user": "nobody"})
+                assert Connection("host", config=config).user == "nobody"
+
+            def may_be_given_as_kwarg(self):
+                assert Connection("host", user="somebody").user == "somebody"
+
+            @raises(ValueError)
+            def errors_when_given_as_both_kwarg_and_shorthand(self):
+                Connection("user@host", user="otheruser")
+
+            def kwarg_wins_over_config(self):
+                config = Config(overrides={"user": "nobody"})
+                cxn = Connection("host", user="somebody", config=config)
+                assert cxn.user == "somebody"
+
+            def shorthand_wins_over_config(self):
+                config = Config(overrides={"user": "nobody"})
+                cxn = Connection("somebody@host", config=config)
+                assert cxn.user == "somebody"
+
+        class port:
+            def defaults_to_22_because_yup(self):
+                assert Connection("host").port == 22
+
+            def accepts_configuration_port(self):
+                config = Config(overrides={"port": 2222})
+                assert Connection("host", config=config).port == 2222
+
+            def may_be_given_as_kwarg(self):
+                assert Connection("host", port=2202).port == 2202
+
+            @raises(ValueError)
+            def errors_when_given_as_both_kwarg_and_shorthand(self):
+                Connection("host:123", port=321)
+
+            def kwarg_wins_over_config(self):
+                config = Config(overrides={"port": 2222})
+                cxn = Connection("host", port=123, config=config)
+                assert cxn.port == 123
+
+            def shorthand_wins_over_config(self):
+                config = Config(overrides={"port": 2222})
+                cxn = Connection("host:123", config=config)
+                assert cxn.port == 123
+
+        class forward_agent:
+            def defaults_to_False(self):
+                assert Connection("host").forward_agent is False
+
+            def accepts_configuration_value(self):
+                config = Config(overrides={"forward_agent": True})
+                assert Connection("host", config=config).forward_agent is True
+
+            def may_be_given_as_kwarg(self):
+                cxn = Connection("host", forward_agent=True)
+                assert cxn.forward_agent is True
+
+            def kwarg_wins_over_config(self):
+                config = Config(overrides={"forward_agent": True})
+                cxn = Connection("host", forward_agent=False, config=config)
+                assert cxn.forward_agent is False
+
+        class connect_timeout:
+            def defaults_to_None(self):
+                assert Connection("host").connect_timeout is None
+
+            def accepts_configuration_value(self):
+                config = Config(overrides={"timeouts": {"connect": 10}})
+                assert Connection("host", config=config).connect_timeout == 10
+
+            def may_be_given_as_kwarg(self):
+                cxn = Connection("host", connect_timeout=15)
+                assert cxn.connect_timeout == 15
+
+            def kwarg_wins_over_config(self):
+                config = Config(overrides={"timeouts": {"connect": 20}})
+                cxn = Connection("host", connect_timeout=100, config=config)
+                assert cxn.connect_timeout == 100
+
+        class config:
+            # NOTE: behavior local to Config itself is tested in its own test
+            # module; below is solely about Connection's config kwarg and its
+            # handling of that value
+
+            def is_not_required(self):
+                assert Connection("host").config.__class__ == Config
+
+            def can_be_specified(self):
+                c = Config(overrides={"user": "me", "custom": "option"})
+                config = Connection("host", config=c).config
+                assert c is config
+                assert config["user"] == "me"
+                assert config["custom"] == "option"
+
+            def if_given_an_invoke_Config_we_upgrade_to_our_own_Config(self):
+                # Scenario: user has Fabric-level data present at vanilla
+                # Invoke config level, and is then creating Connection objects
+                # with those vanilla invoke Configs.
+                # (Could also _not_ have any Fabric-level data, but then that's
+                # just a base case...)
+                # TODO: adjust this if we ever switch to all our settings being
+                # namespaced...
+                vanilla = InvokeConfig(overrides={"forward_agent": True})
+                cxn = Connection("host", config=vanilla)
+                assert cxn.forward_agent is True  # not False, which is default
+
+        class gateway:
+            def is_optional_and_defaults_to_None(self):
+                c = Connection(host="host")
+                assert c.gateway is None
+
+            def takes_a_Connection(self):
+                c = Connection("host", gateway=Connection("otherhost"))
+                assert isinstance(c.gateway, Connection)
+                assert c.gateway.host == "otherhost"
+
+            def takes_a_string(self):
+                c = Connection("host", gateway="meh")
+                assert c.gateway == "meh"
+
+            def accepts_configuration_value(self):
+                gw = Connection("jumpbox")
+                config = Config(overrides={"gateway": gw})
+                # TODO: the fact that they will be eq, but _not_ necessarily be
+                # the same object, could be problematic in some cases...
+                cxn = Connection("host", config=config)
+                assert cxn.gateway == gw
+
+        class initializes_client:
+            @patch("fabric.connection.SSHClient")
+            def instantiates_empty_SSHClient(self, Client):
+                Connection("host")
+                Client.assert_called_once_with()
+
+            @patch("fabric.connection.AutoAddPolicy")
+            def sets_missing_host_key_policy(self, Policy, client):
+                # TODO: should make the policy configurable early on
+                sentinel = Mock()
+                Policy.return_value = sentinel
+                Connection("host")
+                set_policy = client.set_missing_host_key_policy
+                set_policy.assert_called_once_with(sentinel)
+
+            def is_made_available_as_client_attr(self, client):
+                # NOTE: client is SSHClient.return_value
+                assert Connection("host").client is client
+
+        class ssh_config:
+            def _runtime_config(self, overrides=None, basename="runtime"):
+                confname = "{}.conf".format(basename)
+                runtime_path = join(support, "ssh_config", confname)
+                if overrides is None:
+                    overrides = {}
+                return Config_(
+                    runtime_ssh_path=runtime_path, overrides=overrides
+                )
+
+            def _runtime_cxn(self, **kwargs):
+                config = self._runtime_config(**kwargs)
+                return Connection("runtime", config=config)
+
+            def effectively_blank_when_no_loaded_config(self):
+                c = Config_(ssh_config=SSHConfig())
+                cxn = Connection("host", config=c)
+                # NOTE: paramiko always injects this even if you look up a host
+                # that has no rules, even wildcard ones.
+                assert cxn.ssh_config == {"hostname": "host"}
+
+            def shows_result_of_lookup_when_loaded_config(self):
+                conf = self._runtime_cxn().ssh_config
+                expected = {
+                    "connecttimeout": "15",
+                    "forwardagent": "yes",
+                    "hostname": "runtime",
+                    "identityfile": ["whatever.key", "some-other.key"],
+                    "port": "666",
+                    "proxycommand": "my gateway",
+                    "user": "abaddon",
+                }
+                assert conf == expected
+
+            class hostname:
+                def original_host_always_set(self):
+                    cxn = Connection("somehost")
+                    assert cxn.original_host == "somehost"
+                    assert cxn.host == "somehost"
+
+                def hostname_directive_overrides_host_attr(self):
+                    # TODO: not 100% convinced this is the absolute most
+                    # obvious API for 'translation' of given hostname to
+                    # ssh-configured hostname, but it feels okay for now.
+                    path = join(
+                        support, "ssh_config", "overridden_hostname.conf"
+                    )
+                    config = Config_(runtime_ssh_path=path)
+                    cxn = Connection("aliasname", config=config)
+                    assert cxn.host == "realname"
+                    assert cxn.original_host == "aliasname"
+                    assert cxn.port == 2222
+
+            class user:
+                def wins_over_default(self):
+                    assert self._runtime_cxn().user == "abaddon"
+
+                def wins_over_configuration(self):
+                    cxn = self._runtime_cxn(overrides={"user": "baal"})
+                    assert cxn.user == "abaddon"
+
+                def loses_to_explicit(self):
+                    # Would be 'abaddon', as above
+                    config = self._runtime_config()
+                    cxn = Connection("runtime", config=config, user="set")
+                    assert cxn.user == "set"
+
+            class port:
+                def wins_over_default(self):
+                    assert self._runtime_cxn().port == 666
+
+                def wins_over_configuration(self):
+                    cxn = self._runtime_cxn(overrides={"port": 777})
+                    assert cxn.port == 666
+
+                def loses_to_explicit(self):
+                    config = self._runtime_config()  # Would be 666, as above
+                    cxn = Connection("runtime", config=config, port=777)
+                    assert cxn.port == 777
+
+            class forward_agent:
+                def wins_over_default(self):
+                    assert self._runtime_cxn().forward_agent is True
+
+                def wins_over_configuration(self):
+                    # Of course, this "config override" is also the same as the
+                    # default. Meh.
+                    cxn = self._runtime_cxn(overrides={"forward_agent": False})
+                    assert cxn.forward_agent is True
+
+                def loses_to_explicit(self):
+                    # Would be True, as above
+                    config = self._runtime_config()
+                    cxn = Connection(
+                        "runtime", config=config, forward_agent=False
+                    )
+                    assert cxn.forward_agent is False
+
+            class proxy_command:
+                def wins_over_default(self):
+                    assert self._runtime_cxn().gateway == "my gateway"
+
+                def wins_over_configuration(self):
+                    cxn = self._runtime_cxn(overrides={"gateway": "meh gw"})
+                    assert cxn.gateway == "my gateway"
+
+                def loses_to_explicit(self):
+                    # Would be "my gateway", as above
+                    config = self._runtime_config()
+                    cxn = Connection(
+                        "runtime", config=config, gateway="other gateway"
+                    )
+                    assert cxn.gateway == "other gateway"
+
+                def explicit_False_turns_off_feature(self):
+                    # This isn't as necessary for things like user/port, which
+                    # _may not_ be None in the end - this setting could be.
+                    config = self._runtime_config()
+                    cxn = Connection("runtime", config=config, gateway=False)
+                    assert cxn.gateway is False
+
+            class proxy_jump:
+                def setup(self):
+                    self._expected_gw = Connection("jumpuser@jumphost:373")
+
+                def wins_over_default(self):
+                    cxn = self._runtime_cxn(basename="proxyjump")
+                    assert cxn.gateway == self._expected_gw
+
+                def wins_over_configuration(self):
+                    cxn = self._runtime_cxn(
+                        basename="proxyjump", overrides={"gateway": "meh gw"}
+                    )
+                    assert cxn.gateway == self._expected_gw
+
+                def loses_to_explicit(self):
+                    # Would be a Connection equal to self._expected_gw, as
+                    # above
+                    config = self._runtime_config(basename="proxyjump")
+                    cxn = Connection(
+                        "runtime", config=config, gateway="other gateway"
+                    )
+                    assert cxn.gateway == "other gateway"
+
+                def explicit_False_turns_off_feature(self):
+                    config = self._runtime_config(basename="proxyjump")
+                    cxn = Connection("runtime", config=config, gateway=False)
+                    assert cxn.gateway is False
+
+                def wins_over_proxycommand(self):
+                    cxn = self._runtime_cxn(basename="both_proxies")
+                    assert cxn.gateway == Connection("winner@everything:777")
+
+                def multi_hop_works_ok(self):
+                    cxn = self._runtime_cxn(basename="proxyjump_multi")
+                    innermost = cxn.gateway.gateway.gateway
+                    middle = cxn.gateway.gateway
+                    outermost = cxn.gateway
+                    assert innermost == Connection("jumpuser3@jumphost3:411")
+                    assert middle == Connection("jumpuser2@jumphost2:872")
+                    assert outermost == Connection("jumpuser@jumphost:373")
+
+                def wildcards_do_not_trigger_recursion(self):
+                    # When #1850 is present, this will RecursionError.
+                    conf = self._runtime_config(basename="proxyjump_recursive")
+                    cxn = Connection("runtime.tld", config=conf)
+                    assert cxn.gateway == Connection("bastion.tld")
+                    assert cxn.gateway.gateway is None
+
+                def multihop_plus_wildcards_still_no_recursion(self):
+                    conf = self._runtime_config(
+                        basename="proxyjump_multi_recursive"
+                    )
+                    cxn = Connection("runtime.tld", config=conf)
+                    outer = cxn.gateway
+                    inner = cxn.gateway.gateway
+                    assert outer == Connection("bastion1.tld")
+                    assert inner == Connection("bastion2.tld")
+                    assert inner.gateway is None
+
+                def gateway_Connections_get_parent_connection_configs(self):
+                    conf = self._runtime_config(
+                        basename="proxyjump",
+                        overrides={"some_random_option": "a-value"},
+                    )
+                    cxn = Connection("runtime", config=conf)
+                    # Sanity
+                    assert cxn.config is conf
+                    assert cxn.gateway == self._expected_gw
+                    # Real check
+                    assert cxn.gateway.config.some_random_option == "a-value"
+                    # Prove copy not reference
+                    # TODO: would we ever WANT a reference? can't imagine...
+                    assert cxn.gateway.config is not conf
+
+            class connect_timeout:
+                def wins_over_default(self):
+                    assert self._runtime_cxn().connect_timeout == 15
+
+                def wins_over_configuration(self):
+                    cxn = self._runtime_cxn(
+                        overrides={"timeouts": {"connect": 17}}
+                    )
+                    assert cxn.connect_timeout == 15
+
+                def loses_to_explicit(self):
+                    config = self._runtime_config()
+                    cxn = Connection(
+                        "runtime", config=config, connect_timeout=23
+                    )
+                    assert cxn.connect_timeout == 23
+
+            class identity_file:
+                # NOTE: ssh_config value gets merged w/ (instead of overridden
+                # by) config and kwarg values; that is tested in the tests for
+                # open().
+                def basic_loading_of_value(self):
+                    # By default, key_filename will be empty, and the data from
+                    # the runtime ssh config will be all that appears.
+                    value = self._runtime_cxn().connect_kwargs["key_filename"]
+                    assert value == ["whatever.key", "some-other.key"]
+
+        class connect_kwargs:
+            def defaults_to_empty_dict(self):
+                assert Connection("host").connect_kwargs == {}
+
+            def may_be_given_explicitly(self):
+                cxn = Connection("host", connect_kwargs={"foo": "bar"})
+                assert cxn.connect_kwargs == {"foo": "bar"}
+
+            def may_be_configured(self):
+                c = Config(overrides={"connect_kwargs": {"origin": "config"}})
+                cxn = Connection("host", config=c)
+                assert cxn.connect_kwargs == {"origin": "config"}
+
+            def kwarg_wins_over_config(self):
+                # TODO: should this be more of a merge-down?
+                c = Config(overrides={"connect_kwargs": {"origin": "config"}})
+                cxn = Connection(
+                    "host", connect_kwargs={"origin": "kwarg"}, config=c
+                )
+                assert cxn.connect_kwargs == {"origin": "kwarg"}
+
+        class inline_ssh_env:
+            def defaults_to_config_value(self):
+                assert Connection("host").inline_ssh_env is False
+                config = Config({"inline_ssh_env": True})
+                assert Connection("host", config=config).inline_ssh_env is True
+
+            def may_be_given(self):
+                assert Connection("host").inline_ssh_env is False
+                cxn = Connection("host", inline_ssh_env=True)
+                assert cxn.inline_ssh_env is True
+
+    class from_v1:
+        def setup(self):
+            self.env = faux_v1_env()
+
+        def _cxn(self, **kwargs):
+            self.env.update(kwargs)
+            return Connection.from_v1(self.env)
+
+        def must_be_given_explicit_env_arg(self):
+            cxn = Connection.from_v1(self.env)
+            assert cxn.host == "localghost"
+
+        class obtaining_config:
+            @patch("fabric.connection.Config.from_v1")
+            def defaults_to_calling_Config_from_v1(self, Config_from_v1):
+                Connection.from_v1(self.env)
+                Config_from_v1.assert_called_once_with(self.env)
+
+            @patch("fabric.connection.Config.from_v1")
+            def may_be_given_config_explicitly(self, Config_from_v1):
+                # Arguably a dupe of regular Connection constructor behavior,
+                # but whatever.
+                Connection.from_v1(env=self.env, config=Config())
+                assert not Config_from_v1.called
+
+        class additional_kwargs:
+            # I.e. as opposed to what happens to the 'env' kwarg...
+            def forwards_arbitrary_kwargs_to_init(self):
+                cxn = Connection.from_v1(
+                    self.env,
+                    connect_kwargs={"foo": "bar"},
+                    inline_ssh_env=True,
+                    connect_timeout=15,
+                )
+                assert cxn.connect_kwargs["foo"] == "bar"
+                assert cxn.inline_ssh_env is True
+                assert cxn.connect_timeout == 15
+
+            def conflicting_kwargs_win_over_v1_env_values(self):
+                env = Lexicon(self.env)
+                cxn = Connection.from_v1(
+                    env, host="not-localghost", port=2222, user="remoteuser"
+                )
+                assert cxn.host == "not-localghost"
+                assert cxn.user == "remoteuser"
+                assert cxn.port == 2222
+
+        class var_mappings:
+            def host_string(self):
+                cxn = self._cxn()  # default is 'localghost'
+                assert cxn.host == "localghost"
+
+            @raises(InvalidV1Env)
+            def None_host_string_errors_usefully(self):
+                self._cxn(host_string=None)
+
+            def user(self):
+                cxn = self._cxn(user="space")
+                assert cxn.user == "space"
+
+            class port:
+                def basic(self):
+                    cxn = self._cxn(port=2222)
+                    assert cxn.port == 2222
+
+                def casted_to_int(self):
+                    cxn = self._cxn(port="2222")
+                    assert cxn.port == 2222
+
+                def not_supplied_if_given_in_host_string(self):
+                    cxn = self._cxn(host_string="localghost:3737", port=2222)
+                    assert cxn.port == 3737
+
+    class string_representation:
+        "string representations"
+
+        def str_displays_repr(self):
+            c = Connection("meh")
+            assert str(c) == "<Connection host=meh>"
+
+        def displays_core_params(self):
+            c = Connection(user="me", host="there", port=123)
+            template = "<Connection host=there user=me port=123>"
+            assert repr(c) == template
+
+        def omits_default_param_values(self):
+            c = Connection("justhost")
+            assert repr(c) == "<Connection host=justhost>"
+
+        def param_comparison_uses_config(self):
+            conf = Config(overrides={"user": "zerocool"})
+            c = Connection(
+                user="zerocool", host="myhost", port=123, config=conf
+            )
+            template = "<Connection host=myhost port=123>"
+            assert repr(c) == template
+
+        def proxyjump_gateway_shows_type(self):
+            c = Connection(host="myhost", gateway=Connection("jump"))
+            template = "<Connection host=myhost gw=proxyjump>"
+            assert repr(c) == template
+
+        def proxycommand_gateway_shows_type(self):
+            c = Connection(host="myhost", gateway="netcat is cool")
+            template = "<Connection host=myhost gw=proxycommand>"
+            assert repr(c) == template
+
+    class comparison_and_hashing:
+        def comparison_uses_host_user_and_port(self):
+            # Just host
+            assert Connection("host") == Connection("host")
+            # Host + user
+            c1 = Connection("host", user="foo")
+            c2 = Connection("host", user="foo")
+            assert c1 == c2
+            # Host + user + port
+            c1 = Connection("host", user="foo", port=123)
+            c2 = Connection("host", user="foo", port=123)
+            assert c1 == c2
+
+        def comparison_to_non_Connections_is_False(self):
+            assert Connection("host") != 15
+
+        def hashing_works(self):
+            assert hash(Connection("host")) == hash(Connection("host"))
+
+        def sorting_works(self):
+            # Hostname...
+            assert Connection("a-host") < Connection("b-host")
+            # User...
+            assert Connection("a-host", user="a-user") < Connection(
+                "a-host", user="b-user"
+            )
+            # then port...
+            assert Connection("a-host", port=1) < Connection("a-host", port=2)
+
+    class open:
+        def has_no_required_args_and_returns_None(self, client):
+            assert Connection("host").open() is None
+
+        def calls_SSHClient_connect(self, client):
+            "calls paramiko.SSHClient.connect() with correct args"
+            Connection("host").open()
+            client.connect.assert_called_with(
+                username=get_local_user(), hostname="host", port=22
+            )
+
+        def passes_through_connect_kwargs(self, client):
+            Connection("host", connect_kwargs={"foobar": "bizbaz"}).open()
+            client.connect.assert_called_with(
+                username=get_local_user(),
+                hostname="host",
+                port=22,
+                foobar="bizbaz",
+            )
+
+        def refuses_to_overwrite_connect_kwargs_with_others(self, client):
+            for key, value, kwargs in (
+                # Core connection args should definitely not get overwritten!
+                # NOTE: recall that these keys are the SSHClient.connect()
+                # kwarg names, NOT our own config/kwarg names!
+                ("hostname", "nothost", {}),
+                ("port", 17, {}),
+                ("username", "zerocool", {}),
+                # These might arguably still be allowed to work, but let's head
+                # off confusion anyways.
+                ("timeout", 100, {"connect_timeout": 25}),
+            ):
+                try:
+                    Connection(
+                        "host", connect_kwargs={key: value}, **kwargs
+                    ).open()
+                except ValueError as e:
+                    err = "Refusing to be ambiguous: connect() kwarg '{}' was given both via regular arg and via connect_kwargs!"  # noqa
+                    assert str(e) == err.format(key)
+                else:
+                    assert False, "Did not raise ValueError!"
+
+        def connect_kwargs_protection_not_tripped_by_defaults(self, client):
+            Connection("host", connect_kwargs={"timeout": 300}).open()
+            client.connect.assert_called_with(
+                username=get_local_user(),
+                hostname="host",
+                port=22,
+                timeout=300,
+            )
+
+        def submits_connect_timeout(self, client):
+            Connection("host", connect_timeout=27).open()
+            client.connect.assert_called_with(
+                username=get_local_user(), hostname="host", port=22, timeout=27
+            )
+
+        def is_connected_True_when_successful(self, client):
+            c = Connection("host")
+            c.open()
+            assert c.is_connected is True
+
+        def short_circuits_if_already_connected(self, client):
+            cxn = Connection("host")
+            # First call will set self.transport to fixture's mock
+            cxn.open()
+            # Second call will check .is_connected which will see active==True,
+            # and short circuit
+            cxn.open()
+            assert client.connect.call_count == 1
+
+        def is_connected_still_False_when_connect_fails(self, client):
+            client.connect.side_effect = socket.error
+            cxn = Connection("host")
+            try:
+                cxn.open()
+            except socket.error:
+                pass
+            assert cxn.is_connected is False
+
+        def uses_configured_user_host_and_port(self, client):
+            Connection(user="myuser", host="myhost", port=9001).open()
+            client.connect.assert_called_once_with(
+                username="myuser", hostname="myhost", port=9001
+            )
+
+        # NOTE: does more involved stuff so can't use "client" fixture
+        @patch("fabric.connection.SSHClient")
+        def uses_gateway_channel_as_sock_for_SSHClient_connect(self, Client):
+            "uses Connection gateway as 'sock' arg to SSHClient.connect"
+            # Setup
+            mock_gw = Mock()
+            mock_main = Mock()
+            Client.side_effect = [mock_gw, mock_main]
+            gw = Connection("otherhost")
+            gw.open = Mock(wraps=gw.open)
+            main = Connection("host", gateway=gw)
+            main.open()
+            # Expect gateway is also open()'d
+            gw.open.assert_called_once_with()
+            # Expect direct-tcpip channel open on 1st client
+            open_channel = mock_gw.get_transport.return_value.open_channel
+            kwargs = open_channel.call_args[1]
+            assert kwargs["kind"] == "direct-tcpip"
+            assert kwargs["dest_addr"], "host" == 22
+            # Expect result of that channel open as sock arg to connect()
+            sock_arg = mock_main.connect.call_args[1]["sock"]
+            assert sock_arg is open_channel.return_value
+
+        @patch("fabric.connection.ProxyCommand")
+        def uses_proxycommand_as_sock_for_Client_connect(self, moxy, client):
+            "uses ProxyCommand from gateway as 'sock' arg to SSHClient.connect"
+            # Setup
+            main = Connection("host", gateway="net catty %h %p")
+            main.open()
+            # Expect ProxyCommand instantiation
+            moxy.assert_called_once_with("net catty host 22")
+            # Expect result of that as sock arg to connect()
+            sock_arg = client.connect.call_args[1]["sock"]
+            assert sock_arg is moxy.return_value
+
+        # TODO: all the various connect-time options such as agent forwarding,
+        # host acceptance policies, how to auth, etc etc. These are all aspects
+        # of a given session and not necessarily the same for entire lifetime
+        # of a Connection object, should it ever disconnect/reconnect.
+        # TODO: though some/all of those things might want to be set to
+        # defaults at initialization time...
+
+    class connect_kwargs_key_filename:
+        "connect_kwargs(key_filename=...)"
+
+        # TODO: it'd be nice to truly separate CLI from regular (non override
+        # level) invoke config; as it is, invoke config comes first in expected
+        # outputs since otherwise there's no way for --identity to "come
+        # first".
+        @pytest.mark.parametrize(
+            "ssh, invoke, kwarg, expected",
+            [
+                param(
+                    True,
+                    True,
+                    True,
+                    [
+                        "configured.key",
+                        "kwarg.key",
+                        "ssh-config-B.key",
+                        "ssh-config-A.key",
+                    ],
+                    id="All sources",
+                ),
+                param(False, False, False, [], id="No sources"),
+                param(
+                    True,
+                    False,
+                    False,
+                    ["ssh-config-B.key", "ssh-config-A.key"],
+                    id="ssh_config only",
+                ),
+                param(
+                    False,
+                    True,
+                    False,
+                    ["configured.key"],
+                    id="Invoke-level config only",
+                ),
+                param(
+                    False,
+                    False,
+                    True,
+                    ["kwarg.key"],
+                    id="Connection kwarg only",
+                ),
+                param(
+                    True,
+                    True,
+                    False,
+                    ["configured.key", "ssh-config-B.key", "ssh-config-A.key"],
+                    id="ssh_config + invoke config, no kwarg",
+                ),
+                param(
+                    True,
+                    False,
+                    True,
+                    ["kwarg.key", "ssh-config-B.key", "ssh-config-A.key"],
+                    id="ssh_config + kwarg, no Invoke-level config",
+                ),
+                param(
+                    False,
+                    True,
+                    True,
+                    ["configured.key", "kwarg.key"],
+                    id="Invoke-level config + kwarg, no ssh_config",
+                ),
+            ],
+        )
+        def merges_sources(self, client, ssh, invoke, kwarg, expected):
+            config_kwargs = {}
+            if ssh:
+                # SSH config with 2x IdentityFile directives.
+                config_kwargs["runtime_ssh_path"] = join(
+                    support, "ssh_config", "runtime_identity.conf"
+                )
+            if invoke:
+                # Use overrides config level to mimic --identity use NOTE: (the
+                # fact that --identity is an override, and thus overrides eg
+                # invoke config file values is part of invoke's config test
+                # suite)
+                config_kwargs["overrides"] = {
+                    "connect_kwargs": {"key_filename": ["configured.key"]}
+                }
+            conf = Config_(**config_kwargs)
+            connect_kwargs = {}
+            if kwarg:
+                # Stitch in connect_kwargs value
+                connect_kwargs = {"key_filename": ["kwarg.key"]}
+            # Tie in all sources that were configured & open()
+            Connection(
+                "runtime", config=conf, connect_kwargs=connect_kwargs
+            ).open()
+            # Ensure we got the expected list of keys
+            kwargs = client.connect.call_args[1]
+            if expected:
+                assert kwargs["key_filename"] == expected
+            else:
+                # No key filenames -> it's not even passed in as connect_kwargs
+                # is gonna be a blank dict
+                assert "key_filename" not in kwargs
+
+    class close:
+        def has_no_required_args_and_returns_None(self, client):
+            c = Connection("host")
+            c.open()
+            assert c.close() is None
+
+        def calls_SSHClient_close(self, client):
+            "calls paramiko.SSHClient.close()"
+            c = Connection("host")
+            c.open()
+            c.close()
+            client.close.assert_called_with()
+
+        @patch("fabric.connection.AgentRequestHandler")
+        def calls_agent_handler_close_if_enabled(self, Handler, client):
+            c = Connection("host", forward_agent=True)
+            c.create_session()
+            c.close()
+            # NOTE: this will need to change if, for w/e reason, we ever want
+            # to run multiple handlers at once
+            Handler.return_value.close.assert_called_once_with()
+
+        def short_circuits_if_not_connected(self, client):
+            c = Connection("host")
+            # Won't trigger close() on client because it'll already think it's
+            # closed (due to no .transport & the behavior of .is_connected)
+            c.close()
+            assert not client.close.called
+
+        def class_works_as_a_closing_contextmanager(self, client):
+            with Connection("host") as c:
+                c.open()
+            client.close.assert_called_once_with()
+
+    class create_session:
+        def calls_open_for_you(self, client):
+            c = Connection("host")
+            c.open = Mock()
+            c.transport = Mock()  # so create_session no asplode
+            c.create_session()
+            assert c.open.called
+
+        @patch("fabric.connection.AgentRequestHandler")
+        def activates_paramiko_agent_forwarding_if_configured(
+            self, Handler, client
+        ):
+            c = Connection("host", forward_agent=True)
+            chan = c.create_session()
+            Handler.assert_called_once_with(chan)
+
+    class run:
+        # NOTE: most actual run related tests live in the runners module's
+        # tests. Here we are just testing the outer interface a bit.
+
+        @patch(remote_path)
+        def calls_open_for_you(self, Remote, client):
+            c = Connection("host")
+            c.open = Mock()
+            c.run("command")
+            assert c.open.called
+
+        @patch(remote_path)
+        def passes_inline_env_to_Remote(self, Remote, client):
+            Connection("host").run("command")
+            assert Remote.call_args[1]["inline_env"] is False
+            Connection("host", inline_ssh_env=True).run("command")
+            assert Remote.call_args[1]["inline_env"] is True
+
+        @patch(remote_path)
+        def calls_Remote_run_with_command_and_kwargs_and_returns_its_result(
+            self, Remote, client
+        ):
+            remote = Remote.return_value
+            sentinel = object()
+            remote.run.return_value = sentinel
+            c = Connection("host")
+            r1 = c.run("command")
+            r2 = c.run("command", warn=True, hide="stderr")
+            # NOTE: somehow, .call_args & the methods built on it (like
+            # .assert_called_with()) stopped working, apparently triggered by
+            # our code...somehow...after commit (roughly) 80906c7.
+            # And yet, .call_args_list and its brethren work fine. Wha?
+            Remote.assert_any_call(c, inline_env=False)
+            remote.run.assert_has_calls(
+                [call("command"), call("command", warn=True, hide="stderr")]
+            )
+            for r in (r1, r2):
+                assert r is sentinel
+
+    class local:
+        # NOTE: most tests for this functionality live in Invoke's runner
+        # tests.
+        @patch("invoke.config.Local")
+        def calls_invoke_Local_run(self, Local):
+            Connection("host").local("foo")
+            # NOTE: yet another casualty of the bizarre mock issues
+            assert call().run("foo") in Local.mock_calls
+
+    class sudo:
+        @patch(remote_path)
+        def calls_open_for_you(self, Remote, client):
+            c = Connection("host")
+            c.open = Mock()
+            c.sudo("command")
+            assert c.open.called
+
+        @patch(remote_path)
+        def passes_inline_env_to_Remote(self, Remote, client):
+            Connection("host").sudo("command")
+            assert Remote.call_args[1]["inline_env"] is False
+            Connection("host", inline_ssh_env=True).sudo("command")
+            assert Remote.call_args[1]["inline_env"] is True
+
+        @patch(remote_path)
+        def basic_invocation(self, Remote, client):
+            # Technically duplicates Invoke-level tests, but ensures things
+            # still work correctly at our level.
+            cxn = Connection("host")
+            cxn.sudo("foo")
+            cmd = "sudo -S -p '{}' foo".format(cxn.config.sudo.prompt)
+            # NOTE: this is another spot where Mock.call_args is inexplicably
+            # None despite call_args_list being populated. WTF. (Also,
+            # Remote.return_value is two different Mocks now, despite Remote's
+            # own Mock having the same ID here and in code under test. WTF!!)
+            expected = [
+                call(cxn, inline_env=False),
+                call().run(cmd, watchers=ANY),
+            ]
+            assert Remote.mock_calls == expected
+            # NOTE: we used to have a "sudo return value is literally the same
+            # return value from Remote.run()" sanity check here, which is
+            # completely impossible now thanks to the above issue.
+
+        def per_host_password_works_as_expected(self):
+            # TODO: needs clearly defined "per-host" config API, if a distinct
+            # one is necessary besides "the config obj handed in when
+            # instantiating the Connection".
+            # E.g. generate a Connection pulling in a sudo.password value from
+            # what would be a generic conf file or similar, *and* one more
+            # specific to that particular Connection (perhaps simply the
+            # 'override' level?), w/ test asserting the more-specific value is
+            # what's submitted.
+            skip()
+
+    class sftp:
+        def returns_result_of_client_open_sftp(self, client):
+            "returns result of client.open_sftp()"
+            sentinel = object()
+            client.open_sftp.return_value = sentinel
+            assert Connection("host").sftp() == sentinel
+            client.open_sftp.assert_called_with()
+
+        def lazily_caches_result(self, client):
+            sentinel1, sentinel2 = object(), object()
+            client.open_sftp.side_effect = [sentinel1, sentinel2]
+            cxn = Connection("host")
+            first = cxn.sftp()
+            # TODO: why aren't we just asserting about calls of open_sftp???
+            err = "{0!r} wasn't the sentinel object()!"
+            assert first is sentinel1, err.format(first)
+            second = cxn.sftp()
+            assert second is sentinel1, err.format(second)
+
+    class get:
+        @patch("fabric.connection.Transfer")
+        def calls_Transfer_get(self, Transfer):
+            "calls Transfer.get()"
+            c = Connection("host")
+            c.get("meh")
+            Transfer.assert_called_with(c)
+            Transfer.return_value.get.assert_called_with("meh")
+
+    class put:
+        @patch("fabric.connection.Transfer")
+        def calls_Transfer_put(self, Transfer):
+            "calls Transfer.put()"
+            c = Connection("host")
+            c.put("meh")
+            Transfer.assert_called_with(c)
+            Transfer.return_value.put.assert_called_with("meh")
+
+    class forward_local:
+        @patch("fabric.tunnels.select")
+        @patch("fabric.tunnels.socket.socket")
+        @patch("fabric.connection.SSHClient")
+        def _forward_local(self, kwargs, Client, mocket, select):
+            # Tease out bits of kwargs for use in the mocking/expecting.
+            # But leave it alone for raw passthru to the API call itself.
+            # TODO: unhappy with how much this apes the real code & its sig...
+            local_port = kwargs["local_port"]
+            remote_port = kwargs.get("remote_port", local_port)
+            local_host = kwargs.get("local_host", "localhost")
+            remote_host = kwargs.get("remote_host", "localhost")
+            # These aren't part of the real sig, but this is easier than trying
+            # to reconcile the mock decorators + optional-value kwargs. meh.
+            tunnel_exception = kwargs.pop("tunnel_exception", None)
+            listener_exception = kwargs.pop("listener_exception", False)
+            # Mock setup
+            client = Client.return_value
+            listener_sock = Mock(name="listener_sock")
+            if listener_exception:
+                listener_sock.bind.side_effect = listener_exception
+            data = b("Some data")
+            tunnel_sock = Mock(name="tunnel_sock", recv=lambda n: data)
+            local_addr = Mock()
+            transport = client.get_transport.return_value
+            channel = transport.open_channel.return_value
+            # socket.socket is only called once directly
+            mocket.return_value = listener_sock
+            # The 2nd socket is obtained via an accept() (which should only
+            # fire once & raise EAGAIN after)
+            listener_sock.accept.side_effect = chain(
+                [(tunnel_sock, local_addr)],
+                repeat(socket.error(errno.EAGAIN, "nothing yet")),
+            )
+            obj = tunnel_sock if tunnel_exception is None else tunnel_exception
+            select.select.side_effect = _select_result(obj)
+            with Connection("host").forward_local(**kwargs):
+                # Make sure we give listener thread enough time to boot up :(
+                # Otherwise we might assert before it does things. (NOTE:
+                # doesn't need to be much, even at 0.01s, 0/100 trials failed
+                # (vs 45/100 with no sleep)
+                time.sleep(0.015)
+                assert client.connect.call_args[1]["hostname"] == "host"
+                listener_sock.setsockopt.assert_called_once_with(
+                    socket.SOL_SOCKET, socket.SO_REUSEADDR, 1
+                )
+                listener_sock.setblocking.assert_called_once_with(0)
+                listener_sock.bind.assert_called_once_with(
+                    (local_host, local_port)
+                )
+                if not listener_exception:
+                    listener_sock.listen.assert_called_once_with(1)
+                    transport.open_channel.assert_called_once_with(
+                        "direct-tcpip", (remote_host, remote_port), local_addr
+                    )
+                # Local write to tunnel_sock is implied by its mocked-out
+                # recv() call above...
+                # NOTE: don't assert if explodey; we want to mimic "the only
+                # error that occurred was within the thread" behavior being
+                # tested by thread-exception-handling tests
+                if not (tunnel_exception or listener_exception):
+                    channel.sendall.assert_called_once_with(data)
+            # Shutdown, with another sleep because threads.
+            time.sleep(0.015)
+            if not listener_exception:
+                tunnel_sock.close.assert_called_once_with()
+                channel.close.assert_called_once_with()
+                listener_sock.close.assert_called_once_with()
+
+        def forwards_local_port_to_remote_end(self):
+            self._forward_local({"local_port": 1234})
+
+        def distinct_remote_port(self):
+            self._forward_local({"local_port": 1234, "remote_port": 4321})
+
+        def non_localhost_listener(self):
+            self._forward_local(
+                {"local_port": 1234, "local_host": "nearby_local_host"}
+            )
+
+        def non_remote_localhost_connection(self):
+            self._forward_local(
+                {"local_port": 1234, "remote_host": "nearby_remote_host"}
+            )
+
+        def _thread_error(self, which):
+            class Sentinel(Exception):
+                pass
+
+            try:
+                self._forward_local(
+                    {
+                        "local_port": 1234,
+                        "{}_exception".format(which): Sentinel,
+                    }
+                )
+            except ThreadException as e:
+                # NOTE: ensures that we're getting what we expected and not
+                # some deeper, test-bug related error
+                assert len(e.exceptions) == 1
+                inner = e.exceptions[0]
+                err = "Expected wrapped exception to be Sentinel, was {}"
+                assert inner.type is Sentinel, err.format(inner.type.__name__)
+            else:
+                # no exception happened :( implies the thread went boom but
+                # nobody noticed
+                err = "Failed to get ThreadException on {} error"
+                assert False, err.format(which)
+
+        def tunnel_errors_bubble_up(self):
+            self._thread_error("tunnel")
+
+        def tunnel_manager_errors_bubble_up(self):
+            self._thread_error("listener")
+
+        # TODO: these require additional refactoring of _forward_local to be
+        # more like the decorators in _util
+        def multiple_tunnels_can_be_open_at_once(self):
+            skip()
+
+    class forward_remote:
+        @patch("fabric.connection.socket.socket")
+        @patch("fabric.tunnels.select")
+        @patch("fabric.connection.SSHClient")
+        def _forward_remote(self, kwargs, Client, select, mocket):
+            # TODO: unhappy with how much this duplicates of the code under
+            # test, re: sig/default vals
+            # Set up parameter values/defaults
+            remote_port = kwargs["remote_port"]
+            remote_host = kwargs.get("remote_host", "127.0.0.1")
+            local_port = kwargs.get("local_port", remote_port)
+            local_host = kwargs.get("local_host", "localhost")
+            # Mock/etc setup, anything that can be prepped before the forward
+            # occurs (which is most things)
+            tun_socket = mocket.return_value
+            cxn = Connection("host")
+            # Channel that will yield data when read from
+            chan = Mock()
+            chan.recv.return_value = "data"
+            # And make select() yield it as being ready once, when called
+            select.select.side_effect = _select_result(chan)
+            with cxn.forward_remote(**kwargs):
+                # At this point Connection.open() has run and generated a
+                # Transport mock for us (because SSHClient is mocked). Let's
+                # first make sure we asked it for the port forward...
+                # NOTE: this feels like it's too limited/tautological a test,
+                # until you realize that it's functionally impossible to mock
+                # out everything required for Paramiko's inner guts to run
+                # _parse_channel_open() and suchlike :(
+                call = cxn.transport.request_port_forward.call_args_list[0]
+                assert call[1]["address"] == remote_host
+                assert call[1]["port"] == remote_port
+                # Pretend the Transport called our callback with mock Channel
+                call[1]["handler"](chan, tuple(), tuple())
+                # Then have to sleep a bit to make sure we give the tunnel
+                # created by that callback to spin up; otherwise ~5% of the
+                # time we exit the contextmanager so fast, the tunnel's "you're
+                # done!" flag is set before it even gets a chance to select()
+                # once.
+                time.sleep(0.01)
+                # And make sure we hooked up to the local socket OK
+                tup = (local_host, local_port)
+                tun_socket.connect.assert_called_once_with(tup)
+            # Expect that our socket got written to by the tunnel (due to the
+            # above-setup select() and channel mocking). Need to do this after
+            # tunnel shutdown or we risk thread ordering issues.
+            tun_socket.sendall.assert_called_once_with("data")
+            # Ensure we closed down the mock socket
+            mocket.return_value.close.assert_called_once_with()
+            # And that the transport canceled the port forward on the remote
+            # end.
+            assert cxn.transport.cancel_port_forward.call_count == 1
+
+        def forwards_remote_port_to_local_end(self):
+            self._forward_remote({"remote_port": 1234})
+
+        def distinct_local_port(self):
+            self._forward_remote({"remote_port": 1234, "local_port": 4321})
+
+        def non_localhost_connections(self):
+            self._forward_remote(
+                {"remote_port": 1234, "local_host": "nearby_local_host"}
+            )
+
+        def remote_non_localhost_listener(self):
+            self._forward_remote(
+                {"remote_port": 1234, "remote_host": "192.168.1.254"}
+            )
+
+        # TODO: these require additional refactoring of _forward_remote to be
+        # more like the decorators in _util
+        def multiple_tunnels_can_be_open_at_once(self):
+            skip()
+
+        def tunnel_errors_bubble_up(self):
+            skip()
+
+        def listener_errors_bubble_up(self):
+            skip()
diff -Nru fabric-1.14.0/tests/executor.py fabric-2.5.0/tests/executor.py
--- fabric-1.14.0/tests/executor.py	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/tests/executor.py	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,136 @@
+from invoke import Collection, Context, Call, Task as InvokeTask
+from invoke.parser import ParseResult, ParserContext, Argument
+from fabric import Executor, Task, Connection
+from fabric.executor import ConnectionCall
+from fabric.exceptions import NothingToDo
+
+from mock import Mock
+from pytest import skip, raises  # noqa
+
+
+def _get_executor(hosts_flag=None, hosts_kwarg=None, post=None, remainder=""):
+    post_tasks = []
+    if post is not None:
+        post_tasks.append(post)
+    hosts = Argument(name="hosts")
+    if hosts_flag is not None:
+        hosts.value = hosts_flag
+    core_args = ParseResult([ParserContext(args=[hosts])])
+    core_args.remainder = remainder
+    body = Mock(pre=[], post=[])
+    task = Task(body, post=post_tasks, hosts=hosts_kwarg)
+    coll = Collection(mytask=task)
+    return body, Executor(coll, core=core_args)
+
+
+def _execute(**kwargs):
+    invocation = kwargs.pop("invocation", ["mytask"])
+    task, executor = _get_executor(**kwargs)
+    executor.execute(*invocation)
+    return task
+
+
+class Executor_:
+    class expand_calls:
+        class hosts_flag_empty:
+            def no_parameterization_is_done(self):
+                task = _execute()
+                assert task.call_count == 1
+                assert isinstance(task.call_args[0][0], Context)
+
+        class hosts_flag_set:
+            def parameterization_per_host(self):
+                task = _execute(hosts_flag="host1,host2,host3")
+                assert task.call_count == 3
+                assert isinstance(task.call_args[0][0], Connection)
+
+            def post_tasks_happen_once_only(self):
+                post = Mock()
+                task = _execute(
+                    hosts_flag="host1,host2,host3", post=Task(post)
+                )
+                assert task.call_count == 3
+                assert post.call_count == 1
+
+        class hosts_attribute_on_task_objects:
+            def parameterization_per_host(self):
+                task = _execute(hosts_kwarg=["host1", "host2", "host3"])
+                assert task.call_count == 3
+                assert isinstance(task.call_args[0][0], Connection)
+
+            def post_tasks_happen_once_only(self):
+                post = Mock()
+                task = _execute(
+                    hosts_kwarg=["host1", "host2", "host3"], post=Task(post)
+                )
+                assert task.call_count == 3
+                assert post.call_count == 1
+
+            def may_give_Connection_kwargs_as_values(self):
+                task = _execute(
+                    hosts_kwarg=[
+                        {"host": "host1"},
+                        {"host": "host2", "user": "doge"},
+                    ]
+                )
+                assert task.call_count == 2
+                expected = [
+                    Connection("host1"),
+                    Connection("host2", user="doge"),
+                ]
+                assert [x[0][0] for x in task.call_args_list] == expected
+
+        class Invoke_task_objects_without_hosts_attribute_still_work:
+            def execution_happens_normally_without_parameterization(self):
+                body = Mock(pre=[], post=[])
+                coll = Collection(mytask=InvokeTask(body))
+                hosts = Argument(name="hosts")
+                core_args = ParseResult([ParserContext(args=[hosts])])
+                # When #1824 present, this just blows up because no .hosts attr
+                Executor(coll, core=core_args).execute("mytask")
+                assert body.call_count == 1
+
+            def hosts_flag_still_triggers_parameterization(self):
+                body = Mock(pre=[], post=[])
+                coll = Collection(mytask=InvokeTask(body))
+                hosts = Argument(name="hosts")
+                hosts.value = "host1,host2,host3"
+                core_args = ParseResult([ParserContext(args=[hosts])])
+                Executor(coll, core=core_args).execute("mytask")
+                assert body.call_count == 3
+
+        class hosts_flag_vs_attributes:
+            def flag_wins(self):
+                task = _execute(
+                    hosts_flag="via-flag", hosts_kwarg=["via-kwarg"]
+                )
+                assert task.call_count == 1
+                assert task.call_args[0][0] == Connection(host="via-flag")
+
+        class remainder:
+            def raises_NothingToDo_when_no_hosts(self):
+                with raises(NothingToDo):
+                    _execute(remainder="whatever")
+
+            def creates_anonymous_call_per_host(self):
+                # TODO: annoying to do w/o mucking around w/ our Executor class
+                # more, and that stuff wants to change semi soon anyways when
+                # we grow past --hosts; punting.
+                skip()
+
+        class dedupe:
+            def deduplication_not_performed(self):
+                task = _execute(invocation=["mytask", "mytask"])
+                assert task.call_count == 2  # not 1
+
+        class parameterize:
+            def always_generates_ConnectionCall_with_host_attr(self):
+                task, executor = _get_executor(hosts_flag="host1,host2,host3")
+                calls = executor.expand_calls(calls=[Call(task)])
+                assert len(calls) == 3
+                assert all(isinstance(x, ConnectionCall) for x in calls)
+                assert [x.init_kwargs["host"] for x in calls] == [
+                    "host1",
+                    "host2",
+                    "host3",
+                ]
diff -Nru fabric-1.14.0/tests/fake_filesystem.py fabric-2.5.0/tests/fake_filesystem.py
--- fabric-1.14.0/tests/fake_filesystem.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/tests/fake_filesystem.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,71 +0,0 @@
-import os
-import stat
-from StringIO import StringIO
-from types import StringTypes
-
-from fabric.network import ssh
-
-
-class FakeFile(StringIO):
-
-    def __init__(self, value=None, path=None):
-        init = lambda x: StringIO.__init__(self, x)
-        if value is None:
-            init("")
-            ftype = 'dir'
-            size = 4096
-        else:
-            init(value)
-            ftype = 'file'
-            size = len(value)
-        attr = ssh.SFTPAttributes()
-        attr.st_mode = {'file': stat.S_IFREG, 'dir': stat.S_IFDIR}[ftype]
-        attr.st_size = size
-        attr.filename = os.path.basename(path)
-        self.attributes = attr
-
-    def __str__(self):
-        return self.getvalue()
-
-    def write(self, value):
-        StringIO.write(self, value)
-        self.attributes.st_size = len(self.getvalue())
-
-    def close(self):
-        """
-        Always hold fake files open.
-        """
-        pass
-
-    def __cmp__(self, other):
-        me = str(self) if isinstance(other, StringTypes) else self
-        return cmp(me, other)
-
-
-class FakeFilesystem(dict):
-    def __init__(self, d=None):
-        # Replicate input dictionary using our custom __setitem__
-        d = d or {}
-        for key, value in d.iteritems():
-            self[key] = value
-
-    def __setitem__(self, key, value):
-        if isinstance(value, StringTypes) or value is None:
-            value = FakeFile(value, key)
-        super(FakeFilesystem, self).__setitem__(key, value)
-
-    def normalize(self, path):
-        """
-        Normalize relative paths.
-
-        In our case, the "home" directory is just the root, /.
-
-        I expect real servers do this as well but with the user's home
-        directory.
-        """
-        if not path.startswith(os.path.sep):
-            path = os.path.join(os.path.sep, path)
-        return path
-
-    def __getitem__(self, key):
-        return super(FakeFilesystem, self).__getitem__(self.normalize(key))
diff -Nru fabric-1.14.0/tests/group.py fabric-2.5.0/tests/group.py
--- fabric-1.14.0/tests/group.py	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/tests/group.py	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,247 @@
+from mock import Mock, patch, call
+from pytest_relaxed import raises
+
+from fabric import Connection, Group, SerialGroup, ThreadingGroup, GroupResult
+from fabric.group import thread_worker
+from fabric.exceptions import GroupException
+
+
+class Group_:
+    class init:
+        "__init__"
+
+        def may_be_empty(self):
+            assert len(Group()) == 0
+
+        def takes_splat_arg_of_host_strings(self):
+            g = Group("foo", "bar")
+            assert g[0].host == "foo"
+            assert g[1].host == "bar"
+
+        def takes_splat_kwargs_and_passes_them_to_Connections(self):
+            g = Group("foo", "bar", user="admin", forward_agent=True)
+            assert g[0].host == "foo"
+            assert g[0].user == "admin"
+            assert g[0].forward_agent is True
+            assert g[1].host == "bar"
+            assert g[1].user == "admin"
+            assert g[1].forward_agent is True
+
+    class from_connections:
+        def inits_from_iterable_of_Connections(self):
+            g = Group.from_connections((Connection("foo"), Connection("bar")))
+            assert len(g) == 2
+            assert g[1].host == "bar"
+
+    def acts_like_an_iterable_of_Connections(self):
+        g = Group("foo", "bar", "biz")
+        assert g[0].host == "foo"
+        assert g[-1].host == "biz"
+        assert len(g) == 3
+        for c in g:
+            assert isinstance(c, Connection)
+
+    class run:
+        @raises(NotImplementedError)
+        def not_implemented_in_base_class(self):
+            Group().run()
+
+    class close_and_contextmanager_behavior:
+        def close_closes_all_member_connections(self):
+            cxns = [Mock(name=x) for x in ("foo", "bar", "biz")]
+            g = Group.from_connections(cxns)
+            g.close()
+            for c in cxns:
+                c.close.assert_called_once_with()
+
+        def contextmanager_behavior_works_like_Connection(self):
+            cxns = [Mock(name=x) for x in ("foo", "bar", "biz")]
+            g = Group.from_connections(cxns)
+            with g as my_g:
+                assert my_g is g
+            for c in cxns:
+                c.close.assert_called_once_with()
+
+
+def _make_serial_tester(cxns, index, args, kwargs):
+    args = args[:]
+    kwargs = kwargs.copy()
+
+    def tester(*a, **k):  # Don't care about doing anything with our own args.
+        car, cdr = index, index + 1
+        predecessors = cxns[:car]
+        successors = cxns[cdr:]
+        for predecessor in predecessors:
+            predecessor.run.assert_called_with(*args, **kwargs)
+        for successor in successors:
+            assert not successor.run.called
+
+    return tester
+
+
+class SerialGroup_:
+    class run:
+        def executes_arguments_on_contents_run_serially(self):
+            "executes arguments on contents' run() serially"
+            cxns = [Connection(x) for x in ("host1", "host2", "host3")]
+            args = ("command",)
+            kwargs = {"hide": True, "warn": True}
+            for index, cxn in enumerate(cxns):
+                side_effect = _make_serial_tester(cxns, index, args, kwargs)
+                cxn.run = Mock(side_effect=side_effect)
+            g = SerialGroup.from_connections(cxns)
+            g.run(*args, **kwargs)
+            # Sanity check, e.g. in case none of them were actually run
+            for cxn in cxns:
+                cxn.run.assert_called_with(*args, **kwargs)
+
+        def errors_in_execution_capture_and_continue_til_end(self):
+            cxns = [Mock(name=x) for x in ("host1", "host2", "host3")]
+
+            class OhNoz(Exception):
+                pass
+
+            onoz = OhNoz()
+            cxns[1].run.side_effect = onoz
+            g = SerialGroup.from_connections(cxns)
+            try:
+                g.run("whatever", hide=True)
+            except GroupException as e:
+                result = e.result
+            else:
+                assert False, "Did not raise GroupException!"
+            succeeded = {
+                cxns[0]: cxns[0].run.return_value,
+                cxns[2]: cxns[2].run.return_value,
+            }
+            failed = {cxns[1]: onoz}
+            expected = succeeded.copy()
+            expected.update(failed)
+            assert result == expected
+            assert result.succeeded == succeeded
+            assert result.failed == failed
+
+        def returns_results_mapping(self):
+            cxns = [Mock(name=x) for x in ("host1", "host2", "host3")]
+            g = SerialGroup.from_connections(cxns)
+            result = g.run("whatever", hide=True)
+            assert isinstance(result, GroupResult)
+            expected = {x: x.run.return_value for x in cxns}
+            assert result == expected
+            assert result.succeeded == expected
+            assert result.failed == {}
+
+
+class ThreadingGroup_:
+    def setup(self):
+        self.cxns = [Connection(x) for x in ("host1", "host2", "host3")]
+        self.args = ("command",)
+        self.kwargs = {"hide": True, "warn": True}
+
+    class run:
+        @patch("fabric.group.Queue")
+        @patch("fabric.group.ExceptionHandlingThread")
+        def executes_arguments_on_contents_run_via_threading(
+            self, Thread, Queue
+        ):
+            queue = Queue.return_value
+            g = ThreadingGroup.from_connections(self.cxns)
+            # Make sure .exception() doesn't yield truthy Mocks. Otherwise we
+            # end up with 'exceptions' that cause errors due to all being the
+            # same.
+            Thread.return_value.exception.return_value = None
+            g.run(*self.args, **self.kwargs)
+            # Testing that threads were used the way we expect is mediocre but
+            # I honestly can't think of another good way to assert "threading
+            # was used & concurrency occurred"...
+            instantiations = [
+                call(
+                    target=thread_worker,
+                    kwargs=dict(
+                        cxn=cxn,
+                        queue=queue,
+                        args=self.args,
+                        kwargs=self.kwargs,
+                    ),
+                )
+                for cxn in self.cxns
+            ]
+            Thread.assert_has_calls(instantiations, any_order=True)
+            # These ought to work as by default a Mock.return_value is a
+            # singleton mock object
+            expected = len(self.cxns)
+            for name, got in (
+                ("start", Thread.return_value.start.call_count),
+                ("join", Thread.return_value.join.call_count),
+            ):
+                err = (
+                    "Expected {} calls to ExceptionHandlingThread.{}, got {}"
+                )  # noqa
+                err = err.format(expected, name, got)
+                assert expected, got == err
+
+        @patch("fabric.group.Queue")
+        def queue_used_to_return_results(self, Queue):
+            # Regular, explicit, mocks for Connections
+            cxns = [Mock(host=x) for x in ("host1", "host2", "host3")]
+            # Set up Queue with enough behavior to work / assert
+            queue = Queue.return_value
+            # Ending w/ a True will terminate a while-not-empty loop
+            queue.empty.side_effect = (False, False, False, True)
+            fakes = [(x, x.run.return_value) for x in cxns]
+            queue.get.side_effect = fakes[:]
+            # Execute & inspect results
+            g = ThreadingGroup.from_connections(cxns)
+            results = g.run(*self.args, **self.kwargs)
+            expected = {x: x.run.return_value for x in cxns}
+            assert results == expected
+            # Make sure queue was used as expected within worker &
+            # ThreadingGroup.run()
+            puts = [call(x) for x in fakes]
+            queue.put.assert_has_calls(puts, any_order=True)
+            assert queue.empty.called
+            gets = [call(block=False) for _ in cxns]
+            queue.get.assert_has_calls(gets)
+
+        def bubbles_up_errors_within_threads(self):
+            # TODO: I feel like this is the first spot where a raw
+            # ThreadException might need tweaks, at least presentation-wise,
+            # since we're no longer dealing with truly background threads (IO
+            # workers and tunnels), but "middle-ground" threads the user is
+            # kind of expecting (and which they might expect to encounter
+            # failures).
+            cxns = [Mock(host=x) for x in ("host1", "host2", "host3")]
+
+            class OhNoz(Exception):
+                pass
+
+            onoz = OhNoz()
+            cxns[1].run.side_effect = onoz
+            g = ThreadingGroup.from_connections(cxns)
+            try:
+                g.run(*self.args, **self.kwargs)
+            except GroupException as e:
+                result = e.result
+            else:
+                assert False, "Did not raise GroupException!"
+            succeeded = {
+                cxns[0]: cxns[0].run.return_value,
+                cxns[2]: cxns[2].run.return_value,
+            }
+            failed = {cxns[1]: onoz}
+            expected = succeeded.copy()
+            expected.update(failed)
+            assert result == expected
+            assert result.succeeded == succeeded
+            assert result.failed == failed
+
+        def returns_results_mapping(self):
+            # TODO: update if/when we implement ResultSet
+            cxns = [Mock(name=x) for x in ("host1", "host2", "host3")]
+            g = ThreadingGroup.from_connections(cxns)
+            result = g.run("whatever", hide=True)
+            assert isinstance(result, GroupResult)
+            expected = {x: x.run.return_value for x in cxns}
+            assert result == expected
+            assert result.succeeded == expected
+            assert result.failed == {}
diff -Nru fabric-1.14.0/tests/init.py fabric-2.5.0/tests/init.py
--- fabric-1.14.0/tests/init.py	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/tests/init.py	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,43 @@
+import fabric
+from fabric import _version, connection, runners, group, tasks, executor
+
+
+class init:
+    "__init__"
+
+    def version_and_version_info(self):
+        for name in ("__version_info__", "__version__"):
+            assert getattr(_version, name) == getattr(fabric, name)
+
+    def Connection(self):
+        assert fabric.Connection is connection.Connection
+
+    def Remote(self):
+        assert fabric.Remote is runners.Remote
+
+    def Result(self):
+        assert fabric.Result is runners.Result
+
+    def Config(self):
+        assert fabric.Config is connection.Config
+
+    def Group(self):
+        assert fabric.Group is group.Group
+
+    def SerialGroup(self):
+        assert fabric.SerialGroup is group.SerialGroup
+
+    def ThreadingGroup(self):
+        assert fabric.ThreadingGroup is group.ThreadingGroup
+
+    def GroupResult(self):
+        assert fabric.GroupResult is group.GroupResult
+
+    def task(self):
+        assert fabric.task is tasks.task
+
+    def Task(self):
+        assert fabric.Task is tasks.Task
+
+    def Executor(self):
+        assert fabric.Executor is executor.Executor
diff -Nru fabric-1.14.0/tests/main.py fabric-2.5.0/tests/main.py
--- fabric-1.14.0/tests/main.py	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/tests/main.py	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,362 @@
+"""
+Tests concerned with the ``fab`` tool & how it overrides Invoke defaults.
+"""
+
+import os
+import sys
+import re
+
+from invoke import run
+from invoke.util import cd
+from mock import patch
+import pytest  # because WHY would you expose @skip normally? -_-
+from pytest_relaxed import raises
+
+from fabric.config import Config
+from fabric.main import make_program
+from fabric.exceptions import NothingToDo
+
+from fabric.testing.base import Session
+from _util import expect, support, config_file, trap
+
+
+# Designate a runtime config file intended for the test environment; it does
+# things like automatically mute stdin so test harnesses that care about stdin
+# don't get upset.
+# NOTE: this requires the test environment to have Invoke 1.1.0 or above; for
+# now this is fine as we don't do a big serious matrix, we typically use Invoke
+# master to allow testing in-dev changes.
+# TODO: if that _changes_ then we may have to rethink this so that it goes back
+# to being testable on Invoke >=1.0 instead of >=1.1...
+os.environ["INVOKE_RUNTIME_CONFIG"] = config_file
+
+
+class Fab_:
+    class core_program_behavior:
+        def version_output_contains_our_name_plus_deps(self):
+            expect(
+                "--version",
+                r"""
+Fabric .+
+Paramiko .+
+Invoke .+
+""".strip(),
+                test="regex",
+            )
+
+        def help_output_says_fab(self):
+            expect("--help", "Usage: fab", test="contains")
+
+        def exposes_hosts_flag_in_help(self):
+            expect("--help", "-H STRING, --hosts=STRING", test="contains")
+
+        def executes_remainder_as_anonymous_task(self, remote):
+            remote.expect(host="myhost", cmd="whoami")
+            make_program().run("fab -H myhost -- whoami", exit=False)
+
+        def uses_FABRIC_env_prefix(self, environ):
+            environ["FABRIC_RUN_ECHO"] = "1"
+            with cd(support):
+                make_program().run("fab expect-from-env")
+
+        def basic_pre_and_post_tasks_still_work(self):
+            with cd(support):
+                # Sanity
+                expect("first", "First!\n")
+                expect("third", "Third!\n")
+                # Real test
+                expect("second", "First!\nSecond!\nThird!\n")
+
+    class filenames:
+        def loads_fabfile_not_tasks(self):
+            "Loads fabfile.py, not tasks.py"
+            with cd(support):
+                expect(
+                    "--list",
+                    """
+Available tasks:
+
+  basic-run
+  build
+  deploy
+  expect-connect-timeout
+  expect-from-env
+  expect-identities
+  expect-identity
+  expect-mutation
+  expect-mutation-to-fail
+  expect-vanilla-Context
+  first
+  hosts-are-host-stringlike
+  hosts-are-init-kwargs
+  hosts-are-mixed-values
+  hosts-are-myhost
+  mutate
+  second
+  third
+  two-hosts
+  vanilla-Task-works-ok
+
+""".lstrip(),
+                )
+
+        def loads_fabric_config_files_not_invoke_ones(self):
+            for type_ in ("yaml", "yml", "json", "py"):
+                with cd(os.path.join(support, "{}_conf".format(type_))):
+                    # This task, in each subdir, expects data present in a
+                    # fabric.<ext> nearby to show up in the config.
+                    make_program().run("fab expect-conf-value")
+
+    class runtime_ssh_config_path:
+        def _run(
+            self,
+            flag="-S",
+            file_="ssh_config/runtime.conf",
+            tasks="runtime-ssh-config",
+        ):
+            with cd(support):
+                # Relies on asserts within the task, which will bubble up as
+                # it's executed in-process
+                cmd = "fab -c runtime_fabfile {} {} -H runtime {}"
+                make_program().run(cmd.format(flag, file_, tasks))
+
+        def capital_F_flag_specifies_runtime_ssh_config_file(self):
+            self._run(flag="-S")
+
+        def long_form_flag_also_works(self):
+            self._run(flag="--ssh-config")
+
+        @raises(IOError)
+        def IOErrors_if_given_missing_file(self):
+            self._run(file_="nope/nothere.conf")
+
+        @patch.object(Config, "_load_ssh_file")
+        def config_only_loaded_once_per_session(self, method):
+            # Task that doesn't make assertions about the config (since the
+            # _actual_ config it gets is empty as we had to mock out the loader
+            # method...sigh)
+            self._run(tasks="dummy dummy")
+            # Called only once (initial __init__) with runtime conf, instead of
+            # that plus a few more pairs of calls against the default files
+            # (which is what happens when clone() isn't preserving the
+            # already-parsed/loaded SSHConfig)
+            method.assert_called_once_with("ssh_config/runtime.conf")
+
+    class hosts_flag_parameterizes_tasks:
+        # NOTE: many of these just rely on MockRemote's builtin
+        # "channel.exec_command called with given command string" asserts.
+
+        def single_string_is_single_host_and_single_exec(self, remote):
+            remote.expect(host="myhost", cmd="nope")
+            # In addition to just testing a base case, this checks for a really
+            # dumb bug where one appends to, instead of replacing, the task
+            # list during parameterization/expansion XD
+            with cd(support):
+                make_program().run("fab -H myhost basic-run")
+
+        def comma_separated_string_is_multiple_hosts(self, remote):
+            remote.expect_sessions(
+                Session("host1", cmd="nope"), Session("host2", cmd="nope")
+            )
+            with cd(support):
+                make_program().run("fab -H host1,host2 basic-run")
+
+        def multiple_hosts_works_with_remainder_too(self, remote):
+            remote.expect_sessions(
+                Session("host1", cmd="whoami"), Session("host2", cmd="whoami")
+            )
+            make_program().run("fab -H host1,host2 -- whoami")
+
+        def host_string_shorthand_is_passed_through(self, remote):
+            remote.expect(host="host1", port=1234, user="someuser")
+            make_program().run("fab -H someuser@host1:1234 -- whoami")
+
+        # NOTE: no mocking because no actual run() under test, only
+        # parameterization
+        # TODO: avoiding for now because implementing this requires more work
+        # at the Invoke level re: deciding when to _not_ pass in the
+        # session-global config object (Executor's self.config). At the moment,
+        # our threading-concurrency API is oriented around Group, and we're not
+        # using it for --hosts, so it's not broken...yet.
+        @pytest.mark.skip
+        def config_mutation_not_preserved(self):
+            with cd(support):
+                make_program().run(
+                    "fab -H host1,host2 expect-mutation-to-fail"
+                )
+
+        @trap
+        def pre_post_tasks_are_not_parameterized_across_hosts(self):
+            with cd(support):
+                make_program().run(
+                    "fab -H hostA,hostB,hostC second --show-host"
+                )
+                output = sys.stdout.getvalue()
+                # Expect pre once, 3x main, post once, as opposed to e.g. both
+                # pre and main task
+                expected = """
+First!
+Second: hostA
+Second: hostB
+Second: hostC
+Third!
+""".lstrip()
+                assert output == expected
+
+    class hosts_task_arg_parameterizes_tasks:
+        # NOTE: many of these just rely on MockRemote's builtin
+        # "channel.exec_command called with given command string" asserts.
+
+        def single_string_is_single_exec(self, remote):
+            remote.expect(host="myhost", cmd="nope")
+            with cd(support):
+                make_program().run("fab hosts-are-myhost")
+
+        def multiple_strings_is_multiple_host_args(self, remote):
+            remote.expect_sessions(
+                Session("host1", cmd="nope"), Session("host2", cmd="nope")
+            )
+            with cd(support):
+                make_program().run("fab two-hosts")
+
+        def host_string_shorthand_works_ok(self, remote):
+            remote.expect(host="host1", port=1234, user="someuser")
+            with cd(support):
+                make_program().run("fab hosts-are-host-stringlike")
+
+        def may_give_Connection_init_kwarg_dicts(self, remote):
+            remote.expect_sessions(
+                Session("host1", user="admin", cmd="nope"),
+                Session("host2", cmd="nope"),
+            )
+            with cd(support):
+                make_program().run("fab hosts-are-init-kwargs")
+
+        def may_give_mixed_value_types(self, remote):
+            remote.expect_sessions(
+                Session("host1", user="admin", cmd="nope"),
+                Session("host2", cmd="nope"),
+            )
+            with cd(support):
+                make_program().run("fab hosts-are-mixed-values")
+
+    class no_hosts_flag_or_task_arg:
+        def calls_task_once_with_invoke_context(self):
+            with cd(support):
+                make_program().run("fab expect-vanilla-Context")
+
+        def vanilla_Invoke_task_works_too(self):
+            with cd(support):
+                make_program().run("fab vanilla-Task-works-ok")
+
+        @raises(NothingToDo)
+        def generates_exception_if_combined_with_remainder(self):
+            make_program().run("fab -- nope")
+
+        def invokelike_multitask_invocation_preserves_config_mutation(self):
+            # Mostly a guard against Executor subclass tweaks breaking Invoke
+            # behavior added in pyinvoke/invoke#309
+            with cd(support):
+                make_program().run("fab mutate expect-mutation")
+
+    class connect_timeout:
+        def dash_t_supplies_default_connect_timeout(self):
+            with cd(support):
+                make_program().run("fab -t 5 expect-connect-timeout")
+
+        def double_dash_connect_timeout_also_works(self):
+            with cd(support):
+                make_program().run(
+                    "fab --connect-timeout 5 expect-connect-timeout"
+                )
+
+    class runtime_identity_file:
+        def dash_i_supplies_default_connect_kwarg_key_filename(self):
+            # NOTE: the expect-identity task in tests/_support/fabfile.py
+            # performs asserts about its context's .connect_kwargs value,
+            # relying on other tests to prove connect_kwargs makes its way into
+            # that context.
+            with cd(support):
+                make_program().run("fab -i identity.key expect-identity")
+
+        def double_dash_identity_also_works(self):
+            with cd(support):
+                make_program().run(
+                    "fab --identity identity.key expect-identity"
+                )
+
+        def may_be_given_multiple_times(self):
+            with cd(support):
+                make_program().run(
+                    "fab -i identity.key -i identity2.key expect-identities"
+                )
+
+    class secrets_prompts:
+        @patch("fabric.main.getpass.getpass")
+        def _expect_prompt(self, getpass, flag, key, value, prompt):
+            getpass.return_value = value
+            with cd(support):
+                # Expect that the given key was found in the context.
+                cmd = "fab -c prompting {} expect-connect-kwarg --key {} --val {}"  # noqa
+                make_program().run(cmd.format(flag, key, value))
+            # Then we also expect that getpass was called w/ expected prompt
+            getpass.assert_called_once_with(prompt)
+
+        def password_prompt_updates_connect_kwargs(self):
+            self._expect_prompt(
+                flag="--prompt-for-login-password",
+                key="password",
+                value="mypassword",
+                prompt="Enter login password for use with SSH auth: ",
+            )
+
+        def passphrase_prompt_updates_connect_kwargs(self):
+            self._expect_prompt(
+                flag="--prompt-for-passphrase",
+                key="passphrase",
+                value="mypassphrase",
+                prompt="Enter passphrase for use unlocking SSH keys: ",
+            )
+
+    class configuration_updating_and_merging:
+        def key_filename_can_be_set_via_non_override_config_levels(self):
+            # Proves/protects against #1762, where eg key_filenames gets
+            # 'reset' to an empty list. Arbitrarily uses the 'yml' level of
+            # test fixtures, which has a fabric.yml w/ a
+            # connect_kwargs.key_filename value of [private.key, other.key].
+            with cd(os.path.join(support, "yml_conf")):
+                make_program().run("fab expect-conf-key-filename")
+
+        def cli_identity_still_overrides_when_non_empty(self):
+            with cd(os.path.join(support, "yml_conf")):
+                make_program().run("fab -i cli.key expect-cli-key-filename")
+
+    class completion:
+        # NOTE: most completion tests are in Invoke too; this is just an
+        # irritating corner case driven by Fabric's 'remainder' functionality.
+        @trap
+        def complete_flag_does_not_trigger_remainder_only_behavior(self):
+            # When bug present, 'fab --complete -- fab' fails to load any
+            # collections because it thinks it's in remainder-only,
+            # work-without-a-collection mode.
+            with cd(support):
+                make_program().run("fab --complete -- fab", exit=False)
+            # Cherry-picked sanity checks looking for tasks from fixture
+            # fabfile
+            output = sys.stdout.getvalue()
+            for name in ("build", "deploy", "expect-from-env"):
+                assert name in output
+
+
+class main:
+    "__main__"
+
+    def python_dash_m_acts_like_fab(self, capsys):
+        # Rehash of version output test, but using 'python -m fabric'
+        expected_output = r"""
+Fabric .+
+Paramiko .+
+Invoke .+
+""".strip()
+        output = run("python -m fabric --version", hide=True, in_stream=False)
+        assert re.match(expected_output, output.stdout)
diff -Nru fabric-1.14.0/tests/mock_streams.py fabric-2.5.0/tests/mock_streams.py
--- fabric-1.14.0/tests/mock_streams.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/tests/mock_streams.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,85 +0,0 @@
-"""
-Stand-alone stream mocking decorator for easier imports.
-"""
-from functools import wraps
-import sys
-from StringIO import StringIO  # No need for cStringIO at this time
-
-
-class CarbonCopy(StringIO):
-    """
-    A StringIO capable of multiplexing its writes to other buffer objects.
-    """
-
-    def __init__(self, buffer='', cc=None):
-        """
-        If ``cc`` is given and is a file-like object or an iterable of same,
-        it/they will be written to whenever this StringIO instance is written
-        to.
-        """
-        StringIO.__init__(self, buffer)
-        if cc is None:
-            cc = []
-        elif hasattr(cc, 'write'):
-            cc = [cc]
-        self.cc = cc
-
-    def write(self, s):
-        StringIO.write(self, s)
-        for writer in self.cc:
-            writer.write(s)
-
-
-def mock_streams(which):
-    """
-    Replaces a stream with a ``StringIO`` during the test, then restores after.
-
-    Must specify which stream (stdout, stderr, etc) via string args, e.g.::
-
-        @mock_streams('stdout')
-        def func():
-            pass
-
-        @mock_streams('stderr')
-        def func():
-            pass
-
-        @mock_streams('both')
-        def func()
-            pass
-
-    If ``'both'`` is specified, not only will both streams be replaced with
-    StringIOs, but a new combined-streams output (another StringIO) will appear
-    at ``sys.stdall``. This StringIO will resemble what a user sees at a
-    terminal, i.e. both streams intermingled.
-    """
-    both = (which == 'both')
-    stdout = (which == 'stdout') or both
-    stderr = (which == 'stderr') or both
-
-    def mocked_streams_decorator(func):
-        @wraps(func)
-        def inner_wrapper(*args, **kwargs):
-            if both:
-                sys.stdall = StringIO()
-                fake_stdout = CarbonCopy(cc=sys.stdall)
-                fake_stderr = CarbonCopy(cc=sys.stdall)
-            else:
-                fake_stdout, fake_stderr = StringIO(), StringIO()
-            if stdout:
-                my_stdout, sys.stdout = sys.stdout, fake_stdout
-            if stderr:
-                my_stderr, sys.stderr = sys.stderr, fake_stderr
-            try:
-                func(*args, **kwargs)
-            finally:
-                if stdout:
-                    sys.stdout = my_stdout
-                if stderr:
-                    sys.stderr = my_stderr
-                if both:
-                    del sys.stdall
-        return inner_wrapper
-    return mocked_streams_decorator
-
-
diff -Nru fabric-1.14.0/tests/private.key fabric-2.5.0/tests/private.key
--- fabric-1.14.0/tests/private.key	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/tests/private.key	1970-01-01 01:00:00.000000000 +0100
@@ -1,15 +0,0 @@
------BEGIN RSA PRIVATE KEY-----
-MIICWgIBAAKBgQDTj1bqB4WmayWNPB+8jVSYpZYk80Ujvj680pOTh2bORBjbIAyz
-oWGW+GUjzKxTiiPvVmxFgx5wdsFvF03v34lEVVhMpouqPAYQ15N37K/ir5XY+9m/
-d8ufMCkjeXsQkKqFbAlQcnWMCRnOoPHS3I4vi6hmnDDeeYTSRvfLbW0fhwIBIwKB
-gBIiOqZYaoqbeD9OS9z2K9KR2atlTxGxOJPXiP4ESqP3NVScWNwyZ3NXHpyrJLa0
-EbVtzsQhLn6rF+TzXnOlcipFvjsem3iYzCpuChfGQ6SovTcOjHV9z+hnpXvQ/fon
-soVRZY65wKnF7IAoUwTmJS9opqgrN6kRgCd3DASAMd1bAkEA96SBVWFt/fJBNJ9H
-tYnBKZGw0VeHOYmVYbvMSstssn8un+pQpUm9vlG/bp7Oxd/m+b9KWEh2xPfv6zqU
-avNwHwJBANqzGZa/EpzF4J8pGti7oIAPUIDGMtfIcmqNXVMckrmzQ2vTfqtkEZsA
-4rE1IERRyiJQx6EJsz21wJmGV9WJQ5kCQQDwkS0uXqVdFzgHO6S++tjmjYcxwr3g
-H0CoFYSgbddOT6miqRskOQF3DZVkJT3kyuBgU2zKygz52ukQZMqxCb1fAkASvuTv
-qfpH87Qq5kQhNKdbbwbmd2NxlNabazPijWuphGTdW0VfJdWfklyS2Kr+iqrs/5wV
-HhathJt636Eg7oIjAkA8ht3MQ+XSl9yIJIS8gVpbPxSw5OMfw0PjVE7tBdQruiSc
-nvuQES5C9BMHjF39LZiGH1iLQy7FgdHyoP+eodI7
------END RSA PRIVATE KEY-----
diff -Nru fabric-1.14.0/tests/Python26SocketServer.py fabric-2.5.0/tests/Python26SocketServer.py
--- fabric-1.14.0/tests/Python26SocketServer.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/tests/Python26SocketServer.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,697 +0,0 @@
-"""Generic socket server classes.
-
-This module tries to capture the various aspects of defining a server:
-
-For socket-based servers:
-
-- address family:
-        - AF_INET{,6}: IP (Internet Protocol) sockets (default)
-        - AF_UNIX: Unix domain sockets
-        - others, e.g. AF_DECNET are conceivable (see <socket.h>
-- socket type:
-        - SOCK_STREAM (reliable stream, e.g. TCP)
-        - SOCK_DGRAM (datagrams, e.g. UDP)
-
-For request-based servers (including socket-based):
-
-- client address verification before further looking at the request
-        (This is actually a hook for any processing that needs to look
-         at the request before anything else, e.g. logging)
-- how to handle multiple requests:
-        - synchronous (one request is handled at a time)
-        - forking (each request is handled by a new process)
-        - threading (each request is handled by a new thread)
-
-The classes in this module favor the server type that is simplest to
-write: a synchronous TCP/IP server.  This is bad class design, but
-save some typing.  (There's also the issue that a deep class hierarchy
-slows down method lookups.)
-
-There are five classes in an inheritance diagram, four of which represent
-synchronous servers of four types:
-
-        +------------+
-        | BaseServer |
-        +------------+
-              |
-              v
-        +-----------+        +------------------+
-        | TCPServer |------->| UnixStreamServer |
-        +-----------+        +------------------+
-              |
-              v
-        +-----------+        +--------------------+
-        | UDPServer |------->| UnixDatagramServer |
-        +-----------+        +--------------------+
-
-Note that UnixDatagramServer derives from UDPServer, not from
-UnixStreamServer -- the only difference between an IP and a Unix
-stream server is the address family, which is simply repeated in both
-unix server classes.
-
-Forking and threading versions of each type of server can be created
-using the ForkingMixIn and ThreadingMixIn mix-in classes.  For
-instance, a threading UDP server class is created as follows:
-
-        class ThreadingUDPServer(ThreadingMixIn, UDPServer): pass
-
-The Mix-in class must come first, since it overrides a method defined
-in UDPServer! Setting the various member variables also changes
-the behavior of the underlying server mechanism.
-
-To implement a service, you must derive a class from
-BaseRequestHandler and redefine its handle() method.  You can then run
-various versions of the service by combining one of the server classes
-with your request handler class.
-
-The request handler class must be different for datagram or stream
-services.  This can be hidden by using the request handler
-subclasses StreamRequestHandler or DatagramRequestHandler.
-
-Of course, you still have to use your head!
-
-For instance, it makes no sense to use a forking server if the service
-contains state in memory that can be modified by requests (since the
-modifications in the child process would never reach the initial state
-kept in the parent process and passed to each child).  In this case,
-you can use a threading server, but you will probably have to use
-locks to avoid two requests that come in nearly simultaneous to apply
-conflicting changes to the server state.
-
-On the other hand, if you are building e.g. an HTTP server, where all
-data is stored externally (e.g. in the file system), a synchronous
-class will essentially render the service "deaf" while one request is
-being handled -- which may be for a very long time if a client is slow
-to reqd all the data it has requested.  Here a threading or forking
-server is appropriate.
-
-In some cases, it may be appropriate to process part of a request
-synchronously, but to finish processing in a forked child depending on
-the request data.  This can be implemented by using a synchronous
-server and doing an explicit fork in the request handler class
-handle() method.
-
-Another approach to handling multiple simultaneous requests in an
-environment that supports neither threads nor fork (or where these are
-too expensive or inappropriate for the service) is to maintain an
-explicit table of partially finished requests and to use select() to
-decide which request to work on next (or whether to handle a new
-incoming request).  This is particularly important for stream services
-where each client can potentially be connected for a long time (if
-threads or subprocesses cannot be used).
-
-Future work:
-- Standard classes for Sun RPC (which uses either UDP or TCP)
-- Standard mix-in classes to implement various authentication
-  and encryption schemes
-- Standard framework for select-based multiplexing
-
-XXX Open problems:
-- What to do with out-of-band data?
-
-BaseServer:
-- split generic "request" functionality out into BaseServer class.
-  Copyright (C) 2000  Luke Kenneth Casson Leighton <lkcl@samba.org>
-
-  example: read entries from a SQL database (requires overriding
-  get_request() to return a table entry from the database).
-  entry is processed by a RequestHandlerClass.
-
-"""
-# This file copyright (c) 2001-2015 Python Software Foundation; All Rights Reserved
-
-# Author of the BaseServer patch: Luke Kenneth Casson Leighton
-
-# XXX Warning!
-# There is a test suite for this module, but it cannot be run by the
-# standard regression test.
-# To run it manually, run Lib/test/test_socketserver.py.
-
-__version__ = "0.4"
-
-import socket
-import select
-import sys
-import os
-try:
-    import threading
-except ImportError:
-    import dummy_threading as threading
-
-__all__ = ["TCPServer", "UDPServer", "ForkingUDPServer", "ForkingTCPServer",
-           "ThreadingUDPServer", "ThreadingTCPServer", "BaseRequestHandler",
-           "StreamRequestHandler", "DatagramRequestHandler",
-           "ThreadingMixIn", "ForkingMixIn"]
-if hasattr(socket, "AF_UNIX"):
-    __all__.extend(["UnixStreamServer", "UnixDatagramServer",
-                    "ThreadingUnixStreamServer",
-                    "ThreadingUnixDatagramServer"])
-
-
-class BaseServer:
-    """Base class for server classes.
-
-    Methods for the caller:
-
-    - __init__(server_address, RequestHandlerClass)
-    - serve_forever(poll_interval=0.5)
-    - shutdown()
-    - handle_request()  # if you do not use serve_forever()
-    - fileno() -> int   # for select()
-
-    Methods that may be overridden:
-
-    - server_bind()
-    - server_activate()
-    - get_request() -> request, client_address
-    - handle_timeout()
-    - verify_request(request, client_address)
-    - server_close()
-    - process_request(request, client_address)
-    - close_request(request)
-    - handle_error()
-
-    Methods for derived classes:
-
-    - finish_request(request, client_address)
-
-    Class variables that may be overridden by derived classes or
-    instances:
-
-    - timeout
-    - address_family
-    - socket_type
-    - allow_reuse_address
-
-    Instance variables:
-
-    - RequestHandlerClass
-    - socket
-
-    """
-
-    timeout = None
-
-    def __init__(self, server_address, RequestHandlerClass):
-        """Constructor.  May be extended, do not override."""
-        self.server_address = server_address
-        self.RequestHandlerClass = RequestHandlerClass
-        self.__is_shut_down = threading.Event()
-        self.__serving = False
-
-    def server_activate(self):
-        """Called by constructor to activate the server.
-
-        May be overridden.
-
-        """
-        pass
-
-    def serve_forever(self, poll_interval=0.5):
-        """Handle one request at a time until shutdown.
-
-        Polls for shutdown every poll_interval seconds. Ignores
-        self.timeout. If you need to do periodic tasks, do them in
-        another thread.
-        """
-        self.__serving = True
-        self.__is_shut_down.clear()
-        while self.__serving:
-            # XXX: Consider using another file descriptor or
-            # connecting to the socket to wake this up instead of
-            # polling. Polling reduces our responsiveness to a
-            # shutdown request and wastes cpu at all other times.
-            r, w, e = select.select([self], [], [], poll_interval)
-            if r:
-                self._handle_request_noblock()
-        self.__is_shut_down.set()
-
-    def shutdown(self):
-        """Stops the serve_forever loop.
-
-        Blocks until the loop has finished. This must be called while
-        serve_forever() is running in another thread, or it will
-        deadlock.
-        """
-        self.__serving = False
-        self.__is_shut_down.wait()
-
-    # The distinction between handling, getting, processing and
-    # finishing a request is fairly arbitrary.  Remember:
-    #
-    # - handle_request() is the top-level call.  It calls
-    #   select, get_request(), verify_request() and process_request()
-    # - get_request() is different for stream or datagram sockets
-    # - process_request() is the place that may fork a new process
-    #   or create a new thread to finish the request
-    # - finish_request() instantiates the request handler class;
-    #   this constructor will handle the request all by itself
-
-    def handle_request(self):
-        """Handle one request, possibly blocking.
-
-        Respects self.timeout.
-        """
-        # Support people who used socket.settimeout() to escape
-        # handle_request before self.timeout was available.
-        timeout = self.socket.gettimeout()
-        if timeout is None:
-            timeout = self.timeout
-        elif self.timeout is not None:
-            timeout = min(timeout, self.timeout)
-        fd_sets = select.select([self], [], [], timeout)
-        if not fd_sets[0]:
-            self.handle_timeout()
-            return
-        self._handle_request_noblock()
-
-    def _handle_request_noblock(self):
-        """Handle one request, without blocking.
-
-        I assume that select.select has returned that the socket is
-        readable before this function was called, so there should be
-        no risk of blocking in get_request().
-        """
-        try:
-            request, client_address = self.get_request()
-        except socket.error:
-            return
-        if self.verify_request(request, client_address):
-            try:
-                self.process_request(request, client_address)
-            except:
-                self.handle_error(request, client_address)
-                self.close_request(request)
-
-    def handle_timeout(self):
-        """Called if no new request arrives within self.timeout.
-
-        Overridden by ForkingMixIn.
-        """
-        pass
-
-    def verify_request(self, request, client_address):
-        """Verify the request.  May be overridden.
-
-        Return True if we should proceed with this request.
-
-        """
-        return True
-
-    def process_request(self, request, client_address):
-        """Call finish_request.
-
-        Overridden by ForkingMixIn and ThreadingMixIn.
-
-        """
-        self.finish_request(request, client_address)
-        self.close_request(request)
-
-    def server_close(self):
-        """Called to clean-up the server.
-
-        May be overridden.
-
-        """
-        pass
-
-    def finish_request(self, request, client_address):
-        """Finish one request by instantiating RequestHandlerClass."""
-        self.RequestHandlerClass(request, client_address, self)
-
-    def close_request(self, request):
-        """Called to clean up an individual request."""
-        pass
-
-    def handle_error(self, request, client_address):
-        """Handle an error gracefully.  May be overridden.
-
-        The default is to print a traceback and continue.
-
-        """
-        print('-' * 40)
-        print('Exception happened during processing of request from %s' % (client_address,))
-        import traceback
-        traceback.print_exc()  # XXX But this goes to stderr!
-        print('-' * 40)
-
-
-class TCPServer(BaseServer):
-
-    """Base class for various socket-based server classes.
-
-    Defaults to synchronous IP stream (i.e., TCP).
-
-    Methods for the caller:
-
-    - __init__(server_address, RequestHandlerClass, bind_and_activate=True)
-    - serve_forever(poll_interval=0.5)
-    - shutdown()
-    - handle_request()  # if you don't use serve_forever()
-    - fileno() -> int   # for select()
-
-    Methods that may be overridden:
-
-    - server_bind()
-    - server_activate()
-    - get_request() -> request, client_address
-    - handle_timeout()
-    - verify_request(request, client_address)
-    - process_request(request, client_address)
-    - close_request(request)
-    - handle_error()
-
-    Methods for derived classes:
-
-    - finish_request(request, client_address)
-
-    Class variables that may be overridden by derived classes or
-    instances:
-
-    - timeout
-    - address_family
-    - socket_type
-    - request_queue_size (only for stream sockets)
-    - allow_reuse_address
-
-    Instance variables:
-
-    - server_address
-    - RequestHandlerClass
-    - socket
-
-    """
-
-    address_family = socket.AF_INET
-
-    socket_type = socket.SOCK_STREAM
-
-    request_queue_size = 5
-
-    allow_reuse_address = False
-
-    def __init__(self, server_address, RequestHandlerClass,
-                 bind_and_activate=True):
-        """Constructor.  May be extended, do not override."""
-        BaseServer.__init__(self, server_address, RequestHandlerClass)
-        self.socket = socket.socket(self.address_family,
-                                    self.socket_type)
-        if bind_and_activate:
-            self.server_bind()
-            self.server_activate()
-
-    def server_bind(self):
-        """Called by constructor to bind the socket.
-
-        May be overridden.
-
-        """
-        if self.allow_reuse_address:
-            self.socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
-        self.socket.bind(self.server_address)
-        self.server_address = self.socket.getsockname()
-
-    def server_activate(self):
-        """Called by constructor to activate the server.
-
-        May be overridden.
-
-        """
-        self.socket.listen(self.request_queue_size)
-
-    def server_close(self):
-        """Called to clean-up the server.
-
-        May be overridden.
-
-        """
-        self.socket.close()
-
-    def fileno(self):
-        """Return socket file number.
-
-        Interface required by select().
-
-        """
-        return self.socket.fileno()
-
-    def get_request(self):
-        """Get the request and client address from the socket.
-
-        May be overridden.
-
-        """
-        return self.socket.accept()
-
-    def close_request(self, request):
-        """Called to clean up an individual request."""
-        request.close()
-
-
-class UDPServer(TCPServer):
-
-    """UDP server class."""
-
-    allow_reuse_address = False
-
-    socket_type = socket.SOCK_DGRAM
-
-    max_packet_size = 8192
-
-    def get_request(self):
-        data, client_addr = self.socket.recvfrom(self.max_packet_size)
-        return (data, self.socket), client_addr
-
-    def server_activate(self):
-        # No need to call listen() for UDP.
-        pass
-
-    def close_request(self, request):
-        # No need to close anything.
-        pass
-
-
-class ForkingMixIn:
-    """Mix-in class to handle each request in a new process."""
-
-    timeout = 300
-    active_children = None
-    max_children = 40
-
-    def collect_children(self):
-        """Internal routine to wait for children that have exited."""
-        if self.active_children is None:
-            return
-        while len(self.active_children) >= self.max_children:
-            # XXX: This will wait for any child process, not just ones
-            # spawned by this library. This could confuse other
-            # libraries that expect to be able to wait for their own
-            # children.
-            try:
-                pid, status = os.waitpid(0, 0)
-            except os.error:
-                pid = None
-            if pid not in self.active_children:
-                continue
-            self.active_children.remove(pid)
-
-        # XXX: This loop runs more system calls than it ought
-        # to. There should be a way to put the active_children into a
-        # process group and then use os.waitpid(-pgid) to wait for any
-        # of that set, but I couldn't find a way to allocate pgids
-        # that couldn't collide.
-        for child in self.active_children:
-            try:
-                pid, status = os.waitpid(child, os.WNOHANG)
-            except os.error:
-                pid = None
-            if not pid:
-                continue
-            try:
-                self.active_children.remove(pid)
-            except ValueError, e:
-                raise ValueError('%s. x=%d and list=%r' % \
-                                    (e.message, pid, self.active_children))
-
-    def handle_timeout(self):
-        """Wait for zombies after self.timeout seconds of inactivity.
-
-        May be extended, do not override.
-        """
-        self.collect_children()
-
-    def process_request(self, request, client_address):
-        """Fork a new subprocess to process the request."""
-        self.collect_children()
-        pid = os.fork()
-        if pid:
-            # Parent process
-            if self.active_children is None:
-                self.active_children = []
-            self.active_children.append(pid)
-            self.close_request(request)
-            return
-        else:
-            # Child process.
-            # This must never return, hence os._exit()!
-            try:
-                self.finish_request(request, client_address)
-                os._exit(0)
-            except:
-                try:
-                    self.handle_error(request, client_address)
-                finally:
-                    os._exit(1)
-
-
-class ThreadingMixIn:
-    """Mix-in class to handle each request in a new thread."""
-
-    # Decides how threads will act upon termination of the
-    # main process
-    daemon_threads = False
-
-    def process_request_thread(self, request, client_address):
-        """Same as in BaseServer but as a thread.
-
-        In addition, exception handling is done here.
-
-        """
-        try:
-            self.finish_request(request, client_address)
-            self.close_request(request)
-        except:
-            self.handle_error(request, client_address)
-            self.close_request(request)
-
-    def process_request(self, request, client_address):
-        """Start a new thread to process the request."""
-        t = threading.Thread(target=self.process_request_thread,
-                             args=(request, client_address))
-        if self.daemon_threads:
-            t.setDaemon(1)
-        t.start()
-
-
-class ForkingUDPServer(ForkingMixIn, UDPServer):
-    pass
-
-
-class ForkingTCPServer(ForkingMixIn, TCPServer):
-    pass
-
-
-class ThreadingUDPServer(ThreadingMixIn, UDPServer):
-    pass
-
-
-class ThreadingTCPServer(ThreadingMixIn, TCPServer):
-    pass
-
-
-if hasattr(socket, 'AF_UNIX'):
-
-    class UnixStreamServer(TCPServer):
-        address_family = socket.AF_UNIX
-
-    class UnixDatagramServer(UDPServer):
-        address_family = socket.AF_UNIX
-
-    class ThreadingUnixStreamServer(ThreadingMixIn, UnixStreamServer):
-        pass
-
-    class ThreadingUnixDatagramServer(ThreadingMixIn, UnixDatagramServer):
-        pass
-
-
-class BaseRequestHandler:
-
-    """Base class for request handler classes.
-
-    This class is instantiated for each request to be handled.  The
-    constructor sets the instance variables request, client_address
-    and server, and then calls the handle() method.  To implement a
-    specific service, all you need to do is to derive a class which
-    defines a handle() method.
-
-    The handle() method can find the request as self.request, the
-    client address as self.client_address, and the server (in case it
-    needs access to per-server information) as self.server.  Since a
-    separate instance is created for each request, the handle() method
-    can define arbitrary other instance variariables.
-
-    """
-
-    def __init__(self, request, client_address, server):
-        self.request = request
-        self.client_address = client_address
-        self.server = server
-        try:
-            self.setup()
-            self.handle()
-            self.finish()
-        finally:
-            sys.exc_traceback = None    # Help garbage collection
-
-    def setup(self):
-        pass
-
-    def handle(self):
-        pass
-
-    def finish(self):
-        pass
-
-
-# The following two classes make it possible to use the same service
-# class for stream or datagram servers.
-# Each class sets up these instance variables:
-# - rfile: a file object from which receives the request is read
-# - wfile: a file object to which the reply is written
-# When the handle() method returns, wfile is flushed properly
-
-
-class StreamRequestHandler(BaseRequestHandler):
-
-    """Define self.rfile and self.wfile for stream sockets."""
-
-    # Default buffer sizes for rfile, wfile.
-    # We default rfile to buffered because otherwise it could be
-    # really slow for large data (a getc() call per byte); we make
-    # wfile unbuffered because (a) often after a write() we want to
-    # read and we need to flush the line; (b) big writes to unbuffered
-    # files are typically optimized by stdio even when big reads
-    # aren't.
-    rbufsize = -1
-    wbufsize = 0
-
-    def setup(self):
-        self.connection = self.request
-        self.rfile = self.connection.makefile('rb', self.rbufsize)
-        self.wfile = self.connection.makefile('wb', self.wbufsize)
-
-    def finish(self):
-        if not self.wfile.closed:
-            self.wfile.flush()
-        self.wfile.close()
-        self.rfile.close()
-
-
-class DatagramRequestHandler(BaseRequestHandler):
-
-    # XXX Regrettably, I cannot get this working on Linux;
-    # s.recvfrom() doesn't return a meaningful client address.
-
-    """Define self.rfile and self.wfile for datagram sockets."""
-
-    def setup(self):
-        try:
-            from cStringIO import StringIO
-        except ImportError:
-            from StringIO import StringIO
-        self.packet, self.socket = self.request
-        self.rfile = StringIO(self.packet)
-        self.wfile = StringIO()
-
-    def finish(self):
-        self.socket.sendto(self.wfile.getvalue(), self.client_address)
diff -Nru fabric-1.14.0/tests/runners.py fabric-2.5.0/tests/runners.py
--- fabric-1.14.0/tests/runners.py	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/tests/runners.py	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,140 @@
+try:
+    from invoke.vendor.six import StringIO
+except ImportError:
+    from six import StringIO
+
+from mock import Mock
+from pytest import skip  # noqa
+
+from invoke import pty_size, Result
+
+from fabric import Config, Connection, Remote
+
+
+# On most systems this will explode if actually executed as a shell command;
+# this lets us detect holes in our network mocking.
+CMD = "nope"
+
+# TODO: see TODO in tests/main.py above _run_fab(), this is the same thing.
+def _Connection(*args, **kwargs):
+    kwargs["config"] = Config({"run": {"in_stream": False}})
+    return Connection(*args, **kwargs)
+
+
+def _runner():
+    return Remote(context=_Connection("host"))
+
+
+class Remote_:
+    def needs_handle_on_a_Connection(self):
+        c = _Connection("host")
+        assert Remote(context=c).context is c
+
+    class run:
+        def calls_expected_paramiko_bits(self, remote):
+            # remote mocking makes generic sanity checks like "were
+            # get_transport and open_session called", but we also want to make
+            # sure that exec_command got run with our arg to run().
+            remote.expect(cmd=CMD)
+            _runner().run(CMD)
+
+        def writes_remote_streams_to_local_streams(self, remote):
+            remote.expect(out=b"hello yes this is dog")
+            fakeout = StringIO()
+            _runner().run(CMD, out_stream=fakeout)
+            assert fakeout.getvalue() == "hello yes this is dog"
+
+        def pty_True_uses_paramiko_get_pty(self, remote):
+            chan = remote.expect()
+            _runner().run(CMD, pty=True)
+            cols, rows = pty_size()
+            chan.get_pty.assert_called_with(width=cols, height=rows)
+
+        def return_value_is_Result_subclass_exposing_cxn_used(self, remote):
+            c = _Connection("host")
+            result = Remote(context=c).run(CMD)
+            assert isinstance(result, Result)
+            # Mild sanity test for other Result superclass bits
+            assert result.ok is True
+            assert result.exited == 0
+            # Test the attr our own subclass adds
+            assert result.connection is c
+
+        def channel_is_closed_normally(self, remote):
+            chan = remote.expect()
+            # I.e. Remote.stop() closes the channel automatically
+            _runner().run(CMD)
+            chan.close.assert_called_once_with()
+
+        def channel_is_closed_on_body_exceptions(self, remote):
+            chan = remote.expect()
+
+            # I.e. Remote.stop() is called within a try/finally.
+            # Technically is just testing invoke.Runner, but meh.
+            class Oops(Exception):
+                pass
+
+            class _OopsRemote(Remote):
+                def wait(self):
+                    raise Oops()
+
+            r = _OopsRemote(context=_Connection("host"))
+            try:
+                r.run(CMD)
+            except Oops:
+                chan.close.assert_called_once_with()
+            else:
+                assert False, "Runner failed to raise exception!"
+
+        def channel_close_skipped_when_channel_not_even_made(self):
+            # I.e. if obtaining self.channel doesn't even happen (i.e. if
+            # Connection.create_session() dies), we need to account for that
+            # case...
+            class Oops(Exception):
+                pass
+
+            def oops():
+                raise Oops
+
+            cxn = _Connection("host")
+            cxn.create_session = oops
+            # When bug present, this will result in AttributeError because
+            # Remote has no 'channel'
+            try:
+                Remote(context=cxn).run(CMD)
+            except Oops:
+                pass
+            else:
+                assert False, "Weird, Oops never got raised..."
+
+        # TODO: how much of Invoke's tests re: the upper level run() (re:
+        # things like returning Result, behavior of Result, etc) to
+        # duplicate here? Ideally none or very few core ones.
+
+        # TODO: only test guts of our stuff, Invoke's Runner tests should
+        # handle all the normal shit like stdout/err print and capture.
+        # Implies we want a way to import & run those tests ourselves, though,
+        # with the Runner instead being a Remote. Or do we just replicate the
+        # basics?
+
+        # TODO: all other run() tests from fab1...
+
+    class start:
+        def sends_env_to_paramiko_update_environment_by_default(self, remote):
+            chan = remote.expect()
+            _runner().run(CMD, env={"FOO": "bar"})
+            chan.update_environment.assert_called_once_with({"FOO": "bar"})
+
+        def uses_export_prefixing_when_inline_env_is_True(self, remote):
+            chan = remote.expect(
+                cmd="export DEBUG=1 PATH=/opt/bin && {}".format(CMD)
+            )
+            r = Remote(context=_Connection("host"), inline_env=True)
+            r.run(CMD, env={"PATH": "/opt/bin", "DEBUG": "1"})
+            assert not chan.update_environment.called
+
+    def kill_closes_the_channel(self):
+        runner = _runner()
+        runner.channel = Mock()
+        runner.kill()
+        runner.channel.close.assert_called_once_with()
diff -Nru fabric-1.14.0/tests/server.py fabric-2.5.0/tests/server.py
--- fabric-1.14.0/tests/server.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/tests/server.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,489 +0,0 @@
-from __future__ import with_statement
-
-import os
-import re
-import socket
-import threading
-import time
-import types
-from functools import wraps
-from Python26SocketServer import BaseRequestHandler, ThreadingMixIn, TCPServer
-
-from fabric.operations import _sudo_prefix
-from fabric.api import env, hide
-from fabric.thread_handling import ThreadHandler
-from fabric.network import disconnect_all, ssh
-
-from fake_filesystem import FakeFilesystem, FakeFile
-
-#
-# Debugging
-#
-
-import logging
-logging.basicConfig(filename='/tmp/fab.log', level=logging.DEBUG)
-logger = logging.getLogger('server.py')
-
-
-#
-# Constants
-#
-
-HOST = '127.0.0.1'
-PORT = 2200
-USER = 'username'
-HOME = '/'
-RESPONSES = {
-    "ls /simple": "some output",
-    "ls /": """AUTHORS
-FAQ
-Fabric.egg-info
-INSTALL
-LICENSE
-MANIFEST
-README
-build
-docs
-fabfile.py
-fabfile.pyc
-fabric
-requirements.txt
-setup.py
-tests""",
-    "both_streams": [
-        "stdout",
-        "stderr"
-    ],
-}
-FILES = FakeFilesystem({
-    '/file.txt': 'contents',
-    '/file2.txt': 'contents2',
-    '/folder/file3.txt': 'contents3',
-    '/empty_folder': None,
-    '/tree/file1.txt': 'x',
-    '/tree/file2.txt': 'y',
-    '/tree/subfolder/file3.txt': 'z',
-    '/etc/apache2/apache2.conf': 'Include other.conf',
-    HOME: None  # So $HOME is a directory
-})
-PASSWORDS = {
-    'root': 'root',
-    USER: 'password'
-}
-
-
-def _local_file(filename):
-    return os.path.join(os.path.dirname(__file__), filename)
-
-SERVER_PRIVKEY = _local_file('private.key')
-CLIENT_PUBKEY = _local_file('client.key.pub')
-CLIENT_PRIVKEY = _local_file('client.key')
-CLIENT_PRIVKEY_PASSPHRASE = "passphrase"
-
-
-def _equalize(lists, fillval=None):
-    """
-    Pad all given list items in ``lists`` to be the same length.
-    """
-    lists = map(list, lists)
-    upper = max(len(x) for x in lists)
-    for lst in lists:
-        diff = upper - len(lst)
-        if diff:
-            lst.extend([fillval] * diff)
-    return lists
-
-
-class TestServer(ssh.ServerInterface):
-    """
-    Test server implementing the 'ssh' lib's server interface parent class.
-
-    Mostly just handles the bare minimum necessary to handle SSH-level things
-    such as honoring authentication types and exec/shell/etc requests.
-
-    The bulk of the actual server side logic is handled in the
-    ``serve_responses`` function and its ``SSHHandler`` class.
-    """
-    def __init__(self, passwords, home, pubkeys, files):
-        self.event = threading.Event()
-        self.passwords = passwords
-        self.pubkeys = pubkeys
-        self.files = FakeFilesystem(files)
-        self.home = home
-        self.command = None
-
-    def check_channel_request(self, kind, chanid):
-        if kind == 'session':
-            return ssh.OPEN_SUCCEEDED
-        return ssh.OPEN_FAILED_ADMINISTRATIVELY_PROHIBITED
-
-    def check_channel_exec_request(self, channel, command):
-        self.command = command
-        self.event.set()
-        return True
-
-    def check_channel_pty_request(self, *args):
-        return True
-
-    def check_channel_shell_request(self, channel):
-        self.event.set()
-        return True
-
-    def check_auth_password(self, username, password):
-        self.username = username
-        passed = self.passwords.get(username) == password
-        return ssh.AUTH_SUCCESSFUL if passed else ssh.AUTH_FAILED
-
-    def check_auth_publickey(self, username, key):
-        self.username = username
-        return ssh.AUTH_SUCCESSFUL if self.pubkeys else ssh.AUTH_FAILED
-
-    def get_allowed_auths(self, username):
-        return 'password,publickey'
-
-
-class SSHServer(ThreadingMixIn, TCPServer):
-    """
-    Threading TCPServer subclass.
-    """
-    def _socket_info(self, addr_tup):
-        """
-        Clone of the very top of Paramiko 1.7.6 SSHClient.connect().
-
-        We must use this in order to make sure that our address family matches
-        up with the client side (which we cannot control, and which varies
-        depending on individual computers and their network settings).
-        """
-        hostname, port = addr_tup
-        addr_info = socket.getaddrinfo(hostname, port, socket.AF_UNSPEC,
-            socket.SOCK_STREAM)
-        for (family, socktype, proto, canonname, sockaddr) in addr_info:
-            if socktype == socket.SOCK_STREAM:
-                af = family
-                addr = sockaddr
-                break
-        else:
-            # some OS like AIX don't indicate SOCK_STREAM support, so just
-            # guess. :(
-            af, _, _, _, addr = socket.getaddrinfo(hostname, port,
-                socket.AF_UNSPEC, socket.SOCK_STREAM)
-        return af, addr
-
-    def __init__(
-        self, server_address, RequestHandlerClass, bind_and_activate=True
-    ):
-        # Prevent "address already in use" errors when running tests 2x in a
-        # row.
-        self.allow_reuse_address = True
-
-        # Handle network family/host addr (see docstring for _socket_info)
-        family, addr = self._socket_info(server_address)
-        self.address_family = family
-        TCPServer.__init__(self, addr, RequestHandlerClass,
-            bind_and_activate)
-
-
-class FakeSFTPHandle(ssh.SFTPHandle):
-    """
-    Extremely basic way to get SFTPHandle working with our fake setup.
-    """
-    def chattr(self, attr):
-        self.readfile.attributes = attr
-        return ssh.SFTP_OK
-
-    def stat(self):
-        return self.readfile.attributes
-
-
-class PrependList(list):
-    def prepend(self, val):
-        self.insert(0, val)
-
-
-def expand(path):
-    """
-    '/foo/bar/biz' => ('/', 'foo', 'bar', 'biz')
-    'relative/path' => ('relative', 'path')
-    """
-    # Base case
-    if path in ['', os.path.sep]:
-        return [path]
-    ret = PrependList()
-    directory, filename = os.path.split(path)
-    while directory and directory != os.path.sep:
-        ret.prepend(filename)
-        directory, filename = os.path.split(directory)
-    ret.prepend(filename)
-    # Handle absolute vs relative paths
-    ret.prepend(directory if directory == os.path.sep else '')
-    return ret
-
-
-def contains(folder, path):
-    """
-    contains(('a', 'b', 'c'), ('a', 'b')) => True
-    contains('a', 'b', 'c'), ('f',)) => False
-    """
-    return False if len(path) >= len(folder) else folder[:len(path)] == path
-
-
-def missing_folders(paths):
-    """
-    missing_folders(['a/b/c']) => ['a', 'a/b', 'a/b/c']
-    """
-    ret = []
-    pool = set(paths)
-    for path in paths:
-        expanded = expand(path)
-        for i in range(len(expanded)):
-            folder = os.path.join(*expanded[:len(expanded) - i])
-            if folder and folder not in pool:
-                pool.add(folder)
-                ret.append(folder)
-    return ret
-
-
-def canonicalize(path, home):
-    ret = path
-    if not os.path.isabs(path):
-        ret = os.path.normpath(os.path.join(home, path))
-    return ret
-
-
-class FakeSFTPServer(ssh.SFTPServerInterface):
-    def __init__(self, server, *args, **kwargs):
-        self.server = server
-        files = self.server.files
-        # Expand such that omitted, implied folders get added explicitly
-        for folder in missing_folders(files.keys()):
-            files[folder] = None
-        self.files = files
-
-    def canonicalize(self, path):
-        """
-        Make non-absolute paths relative to $HOME.
-        """
-        return canonicalize(path, self.server.home)
-
-    def list_folder(self, path):
-        path = self.files.normalize(path)
-        expanded_files = map(expand, self.files)
-        expanded_path = expand(path)
-        candidates = [x for x in expanded_files if contains(x, expanded_path)]
-        children = []
-        for candidate in candidates:
-            cut = candidate[:len(expanded_path) + 1]
-            if cut not in children:
-                children.append(cut)
-        results = [self.stat(os.path.join(*x)) for x in children]
-        bad = not results or any(x == ssh.SFTP_NO_SUCH_FILE for x in results)
-        return ssh.SFTP_NO_SUCH_FILE if bad else results
-
-    def open(self, path, flags, attr):
-        path = self.files.normalize(path)
-        try:
-            fobj = self.files[path]
-        except KeyError:
-            if flags & os.O_WRONLY:
-                # Only allow writes to files in existing directories.
-                if os.path.dirname(path) not in self.files:
-                    return ssh.SFTP_NO_SUCH_FILE
-                self.files[path] = fobj = FakeFile("", path)
-            # No write flag means a read, which means they tried to read a
-            # nonexistent file.
-            else:
-                return ssh.SFTP_NO_SUCH_FILE
-        f = FakeSFTPHandle()
-        f.readfile = f.writefile = fobj
-        return f
-
-    def stat(self, path):
-        path = self.files.normalize(path)
-        try:
-            fobj = self.files[path]
-        except KeyError:
-            return ssh.SFTP_NO_SUCH_FILE
-        return fobj.attributes
-
-    # Don't care about links right now
-    lstat = stat
-
-    def chattr(self, path, attr):
-        path = self.files.normalize(path)
-        if path not in self.files:
-            return ssh.SFTP_NO_SUCH_FILE
-        # Attempt to gracefully update instead of overwrite, since things like
-        # chmod will call us with an SFTPAttributes object that only exhibits
-        # e.g. st_mode, and we don't want to lose our filename or size...
-        for which in "size uid gid mode atime mtime".split():
-            attname = "st_" + which
-            incoming = getattr(attr, attname)
-            if incoming is not None:
-                setattr(self.files[path].attributes, attname, incoming)
-        return ssh.SFTP_OK
-
-    def mkdir(self, path, attr):
-        self.files[path] = None
-        return ssh.SFTP_OK
-
-
-def serve_responses(responses, files, passwords, home, pubkeys, port):
-    """
-    Return a threading TCP based SocketServer listening on ``port``.
-
-    Used as a fake SSH server which will respond to commands given in
-    ``responses`` and allow connections for users listed in ``passwords``.
-    ``home`` is used as the remote $HOME (mostly for SFTP purposes).
-
-    ``pubkeys`` is a Boolean value determining whether the server will allow
-    pubkey auth or not.
-    """
-    # Define handler class inline so it can access serve_responses' args
-    class SSHHandler(BaseRequestHandler):
-        def handle(self):
-            try:
-                self.init_transport()
-                self.waiting_for_command = False
-                while not self.server.all_done.isSet():
-                    # Don't overwrite channel if we're waiting for a command.
-                    if not self.waiting_for_command:
-                        self.channel = self.transport.accept(1)
-                        if not self.channel:
-                            continue
-                    self.ssh_server.event.wait(10)
-                    if self.ssh_server.command:
-                        self.command = self.ssh_server.command
-                        # Set self.sudo_prompt, update self.command
-                        self.split_sudo_prompt()
-                        if self.command in responses:
-                            self.stdout, self.stderr, self.status = \
-                                self.response()
-                            if self.sudo_prompt and not self.sudo_password():
-                                self.channel.send(
-                                    "sudo: 3 incorrect password attempts\n"
-                                )
-                                break
-                            self.respond()
-                        else:
-                            self.channel.send_stderr(
-                                "Sorry, I don't recognize that command.\n"
-                            )
-                            self.channel.send_exit_status(1)
-                        # Close up shop
-                        self.command = self.ssh_server.command = None
-                        self.waiting_for_command = False
-                        time.sleep(0.5)
-                        self.channel.close()
-                    else:
-                        # If we're here, self.command was False or None,
-                        # but we do have a valid Channel object. Thus we're
-                        # waiting for the command to show up.
-                        self.waiting_for_command = True
-
-            finally:
-                self.transport.close()
-
-        def init_transport(self):
-            transport = ssh.Transport(self.request)
-            transport.add_server_key(ssh.RSAKey(filename=SERVER_PRIVKEY))
-            transport.set_subsystem_handler('sftp', ssh.SFTPServer,
-                sftp_si=FakeSFTPServer)
-            server = TestServer(passwords, home, pubkeys, files)
-            transport.start_server(server=server)
-            self.ssh_server = server
-            self.transport = transport
-
-        def split_sudo_prompt(self):
-            prefix = re.escape(_sudo_prefix(None, None).rstrip()) + ' +'
-            result = re.findall(r'^(%s)?(.*)$' % prefix, self.command)[0]
-            self.sudo_prompt, self.command = result
-
-        def response(self):
-            result = responses[self.command]
-            stderr = ""
-            status = 0
-            sleep = 0
-            if isinstance(result, types.StringTypes):
-                stdout = result
-            else:
-                size = len(result)
-                if size == 1:
-                    stdout = result[0]
-                elif size == 2:
-                    stdout, stderr = result
-                elif size == 3:
-                    stdout, stderr, status = result
-                elif size == 4:
-                    stdout, stderr, status, sleep = result
-            stdout, stderr = _equalize((stdout, stderr))
-            time.sleep(sleep)
-            return stdout, stderr, status
-
-        def sudo_password(self):
-            # Give user 3 tries, as is typical
-            passed = False
-            for x in range(3):
-                self.channel.send(env.sudo_prompt)
-                password = self.channel.recv(65535).strip()
-                # Spit back newline to fake the echo of user's
-                # newline
-                self.channel.send('\n')
-                # Test password
-                if password == passwords[self.ssh_server.username]:
-                    passed = True
-                    break
-                # If here, password was bad.
-                self.channel.send("Sorry, try again.\n")
-            return passed
-
-        def respond(self):
-            for out, err in zip(self.stdout, self.stderr):
-                if out is not None:
-                    self.channel.send(out)
-                if err is not None:
-                    self.channel.send_stderr(err)
-            self.channel.send_exit_status(self.status)
-
-    return SSHServer((HOST, port), SSHHandler)
-
-
-def server(
-        responses=RESPONSES,
-        files=FILES,
-        passwords=PASSWORDS,
-        home=HOME,
-        pubkeys=False,
-        port=PORT
-    ):
-    """
-    Returns a decorator that runs an SSH server during function execution.
-
-    Direct passthrough to ``serve_responses``.
-    """
-    def run_server(func):
-        @wraps(func)
-        def inner(*args, **kwargs):
-            # Start server
-            _server = serve_responses(responses, files, passwords, home,
-                pubkeys, port)
-            _server.all_done = threading.Event()
-            worker = ThreadHandler('server', _server.serve_forever)
-            # Execute function
-            try:
-                return func(*args, **kwargs)
-            finally:
-                # Clean up client side connections
-                with hide('status'):
-                    disconnect_all()
-                # Stop server
-                _server.all_done.set()
-                _server.shutdown()
-                # Why this is not called in shutdown() is beyond me.
-                _server.server_close()
-                worker.thread.join()
-                # Handle subthread exceptions
-                e = worker.exception
-                if e:
-                    raise e[0], e[1], e[2]
-        return inner
-    return run_server
diff -Nru fabric-1.14.0/tests/_support/config.yml fabric-2.5.0/tests/_support/config.yml
--- fabric-1.14.0/tests/_support/config.yml	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/tests/_support/config.yml	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,9 @@
+#
+# Settings overrides for test-executed Invoke code. Test code typically tries
+# specifying this via the -f CLI flag or the runtime arguments to Config().
+#
+
+run:
+  # Disable all stdin mirroring by default. Otherwise, pytest's capture plugin
+  # gets all upset. It looks difficult to change that, too.
+  in_stream: false
diff -Nru fabric-1.14.0/tests/_support/fabfile.py fabric-2.5.0/tests/_support/fabfile.py
--- fabric-1.14.0/tests/_support/fabfile.py	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/tests/_support/fabfile.py	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,114 @@
+from invoke import Context, task as invtask
+from fabric import task, Connection
+
+
+@task
+def build(c):
+    pass
+
+
+@task
+def deploy(c):
+    pass
+
+
+@task
+def basic_run(c):
+    c.run("nope")
+
+
+@task
+def expect_vanilla_Context(c):
+    assert isinstance(c, Context)
+    assert not isinstance(c, Connection)
+
+
+@task
+def expect_from_env(c):
+    assert c.config.run.echo is True
+
+
+@task
+def expect_mutation_to_fail(c):
+    # If user level config changes are preserved between parameterized per-host
+    # task calls, this would assert on subsequent invocations...
+    assert "foo" not in c.config
+    # ... because of this:
+    c.config.foo = "bar"
+
+
+@task
+def mutate(c):
+    c.foo = "bar"
+
+
+@task
+def expect_mutation(c):
+    assert c.foo == "bar"
+
+
+@task
+def expect_identity(c):
+    assert c.config.connect_kwargs["key_filename"] == ["identity.key"]
+
+
+@task
+def expect_identities(c):
+    assert c.config.connect_kwargs["key_filename"] == [
+        "identity.key",
+        "identity2.key",
+    ]
+
+
+@task
+def expect_connect_timeout(c):
+    assert c.config.connect_kwargs["timeout"] == 5
+
+
+@task
+def first(c):
+    print("First!")
+
+
+@task
+def third(c):
+    print("Third!")
+
+
+@task(pre=[first], post=[third])
+def second(c, show_host=False):
+    if show_host:
+        print("Second: {}".format(c.host))
+    else:
+        print("Second!")
+
+
+@task(hosts=["myhost"])
+def hosts_are_myhost(c):
+    c.run("nope")
+
+
+@task(hosts=["host1", "host2"])
+def two_hosts(c):
+    c.run("nope")
+
+
+@task(hosts=["someuser@host1:1234"])
+def hosts_are_host_stringlike(c):
+    c.run("nope")
+
+
+@task(hosts=["admin@host1", {"host": "host2"}])
+def hosts_are_mixed_values(c):
+    c.run("nope")
+
+
+@task(hosts=[{"host": "host1", "user": "admin"}, {"host": "host2"}])
+def hosts_are_init_kwargs(c):
+    c.run("nope")
+
+
+@invtask
+def vanilla_Task_works_ok(c):
+    assert isinstance(c, Context)
+    assert not isinstance(c, Connection)
diff -Nru fabric-1.14.0/tests/_support/json_conf/fabfile.py fabric-2.5.0/tests/_support/json_conf/fabfile.py
--- fabric-1.14.0/tests/_support/json_conf/fabfile.py	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/tests/_support/json_conf/fabfile.py	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,6 @@
+from fabric import task
+
+
+@task
+def expect_conf_value(c):
+    assert c.it_came_from == "json"
diff -Nru fabric-1.14.0/tests/_support/json_conf/fabric.json fabric-2.5.0/tests/_support/json_conf/fabric.json
--- fabric-1.14.0/tests/_support/json_conf/fabric.json	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/tests/_support/json_conf/fabric.json	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,3 @@
+{
+    "it_came_from": "json"
+}
diff -Nru fabric-1.14.0/tests/_support/prompting.py fabric-2.5.0/tests/_support/prompting.py
--- fabric-1.14.0/tests/_support/prompting.py	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/tests/_support/prompting.py	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,6 @@
+from fabric import task
+
+
+@task
+def expect_connect_kwarg(c, key, val):
+    assert c.config.connect_kwargs[key] == val
diff -Nru fabric-1.14.0/tests/_support/py_conf/fabfile.py fabric-2.5.0/tests/_support/py_conf/fabfile.py
--- fabric-1.14.0/tests/_support/py_conf/fabfile.py	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/tests/_support/py_conf/fabfile.py	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,6 @@
+from fabric import task
+
+
+@task
+def expect_conf_value(c):
+    assert c.it_came_from == "py"
diff -Nru fabric-1.14.0/tests/_support/py_conf/fabric.py fabric-2.5.0/tests/_support/py_conf/fabric.py
--- fabric-1.14.0/tests/_support/py_conf/fabric.py	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/tests/_support/py_conf/fabric.py	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1 @@
+it_came_from = "py"
diff -Nru fabric-1.14.0/tests/_support/runtime_fabfile.py fabric-2.5.0/tests/_support/runtime_fabfile.py
--- fabric-1.14.0/tests/_support/runtime_fabfile.py	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/tests/_support/runtime_fabfile.py	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,15 @@
+from fabric import task
+
+
+@task
+def runtime_ssh_config(c):
+    # NOTE: assumes it's run with host='runtime' + ssh_configs/runtime.conf
+    # TODO: SSHConfig should really learn to turn certain things into ints
+    # automatically...
+    assert c.ssh_config["port"] == "666"
+    assert c.port == 666
+
+
+@task
+def dummy(c):
+    pass
diff -Nru fabric-1.14.0/tests/_support/ssh_config/both_proxies.conf fabric-2.5.0/tests/_support/ssh_config/both_proxies.conf
--- fabric-1.14.0/tests/_support/ssh_config/both_proxies.conf	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/tests/_support/ssh_config/both_proxies.conf	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,3 @@
+Host runtime
+    ProxyCommand "netcat I guess"
+    ProxyJump winner@everything:777
diff -Nru fabric-1.14.0/tests/_support/ssh_config/overridden_hostname.conf fabric-2.5.0/tests/_support/ssh_config/overridden_hostname.conf
--- fabric-1.14.0/tests/_support/ssh_config/overridden_hostname.conf	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/tests/_support/ssh_config/overridden_hostname.conf	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,3 @@
+Host aliasname
+    Hostname realname
+    Port 2222
diff -Nru fabric-1.14.0/tests/_support/ssh_config/proxyjump.conf fabric-2.5.0/tests/_support/ssh_config/proxyjump.conf
--- fabric-1.14.0/tests/_support/ssh_config/proxyjump.conf	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/tests/_support/ssh_config/proxyjump.conf	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,2 @@
+Host runtime
+    ProxyJump jumpuser@jumphost:373
diff -Nru fabric-1.14.0/tests/_support/ssh_config/proxyjump_multi.conf fabric-2.5.0/tests/_support/ssh_config/proxyjump_multi.conf
--- fabric-1.14.0/tests/_support/ssh_config/proxyjump_multi.conf	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/tests/_support/ssh_config/proxyjump_multi.conf	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,2 @@
+Host runtime
+    ProxyJump jumpuser@jumphost:373,jumpuser2@jumphost2:872,jumpuser3@jumphost3:411
diff -Nru fabric-1.14.0/tests/_support/ssh_config/proxyjump_multi_recursive.conf fabric-2.5.0/tests/_support/ssh_config/proxyjump_multi_recursive.conf
--- fabric-1.14.0/tests/_support/ssh_config/proxyjump_multi_recursive.conf	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/tests/_support/ssh_config/proxyjump_multi_recursive.conf	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,2 @@
+Host *.tld 
+    ProxyJump bastion1.tld,bastion2.tld
diff -Nru fabric-1.14.0/tests/_support/ssh_config/proxyjump_recursive.conf fabric-2.5.0/tests/_support/ssh_config/proxyjump_recursive.conf
--- fabric-1.14.0/tests/_support/ssh_config/proxyjump_recursive.conf	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/tests/_support/ssh_config/proxyjump_recursive.conf	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,2 @@
+Host *.tld
+    ProxyJump bastion.tld
diff -Nru fabric-1.14.0/tests/_support/ssh_config/runtime.conf fabric-2.5.0/tests/_support/ssh_config/runtime.conf
--- fabric-1.14.0/tests/_support/ssh_config/runtime.conf	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/tests/_support/ssh_config/runtime.conf	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,8 @@
+Host runtime
+    User abaddon
+    Port 666
+    ForwardAgent yes
+    ProxyCommand "my gateway"
+    ConnectTimeout 15
+    IdentityFile whatever.key
+    IdentityFile some-other.key
diff -Nru fabric-1.14.0/tests/_support/ssh_config/runtime_identity.conf fabric-2.5.0/tests/_support/ssh_config/runtime_identity.conf
--- fabric-1.14.0/tests/_support/ssh_config/runtime_identity.conf	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/tests/_support/ssh_config/runtime_identity.conf	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,5 @@
+Host runtime
+    # Named 'backwards' to sanity test ordering (though that's truly a
+    # Paramiko level thing.)
+    IdentityFile ssh-config-B.key
+    IdentityFile ssh-config-A.key
diff -Nru fabric-1.14.0/tests/_support/ssh_config/system.conf fabric-2.5.0/tests/_support/ssh_config/system.conf
--- fabric-1.14.0/tests/_support/ssh_config/system.conf	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/tests/_support/ssh_config/system.conf	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,5 @@
+Host system
+    Port 123
+
+Host shared
+    Port 123
diff -Nru fabric-1.14.0/tests/_support/ssh_config/user.conf fabric-2.5.0/tests/_support/ssh_config/user.conf
--- fabric-1.14.0/tests/_support/ssh_config/user.conf	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/tests/_support/ssh_config/user.conf	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,5 @@
+Host user
+    Port 321
+
+Host shared
+    Port 321
diff -Nru fabric-1.14.0/tests/_support/yaml_conf/fabfile.py fabric-2.5.0/tests/_support/yaml_conf/fabfile.py
--- fabric-1.14.0/tests/_support/yaml_conf/fabfile.py	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/tests/_support/yaml_conf/fabfile.py	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,6 @@
+from fabric import task
+
+
+@task
+def expect_conf_value(c):
+    assert c.it_came_from == "yaml"
diff -Nru fabric-1.14.0/tests/_support/yaml_conf/fabric.yaml fabric-2.5.0/tests/_support/yaml_conf/fabric.yaml
--- fabric-1.14.0/tests/_support/yaml_conf/fabric.yaml	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/tests/_support/yaml_conf/fabric.yaml	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1 @@
+it_came_from: yaml
diff -Nru fabric-1.14.0/tests/_support/yml_conf/fabfile.py fabric-2.5.0/tests/_support/yml_conf/fabfile.py
--- fabric-1.14.0/tests/_support/yml_conf/fabfile.py	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/tests/_support/yml_conf/fabfile.py	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,20 @@
+from fabric import task
+
+
+@task
+def expect_conf_value(c):
+    assert c.it_came_from == "yml"
+
+
+@task
+def expect_conf_key_filename(c):
+    expected = ["private.key", "other.key"]
+    got = c.connect_kwargs.key_filename
+    assert got == expected, "{!r} != {!r}".format(got, expected)
+
+
+@task
+def expect_cli_key_filename(c):
+    expected = ["cli.key"]
+    got = c.connect_kwargs.key_filename
+    assert got == expected, "{!r} != {!r}".format(got, expected)
diff -Nru fabric-1.14.0/tests/_support/yml_conf/fabric.yml fabric-2.5.0/tests/_support/yml_conf/fabric.yml
--- fabric-1.14.0/tests/_support/yml_conf/fabric.yml	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/tests/_support/yml_conf/fabric.yml	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,5 @@
+it_came_from: yml
+connect_kwargs:
+  key_filename:
+    - private.key
+    - other.key
diff -Nru fabric-1.14.0/tests/support/aborts.py fabric-2.5.0/tests/support/aborts.py
--- fabric-1.14.0/tests/support/aborts.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/tests/support/aborts.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,5 +0,0 @@
-from fabric.api import task, abort
-
-@task
-def kaboom():
-    abort("It burns!")
diff -Nru fabric-1.14.0/tests/support/classbased_task_fabfile.py fabric-2.5.0/tests/support/classbased_task_fabfile.py
--- fabric-1.14.0/tests/support/classbased_task_fabfile.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/tests/support/classbased_task_fabfile.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,7 +0,0 @@
-from fabric import tasks
-
-class ClassBasedTask(tasks.Task):
-    def run(self, *args, **kwargs):
-        pass
-
-foo = ClassBasedTask()
diff -Nru fabric-1.14.0/tests/support/decorated_fabfile.py fabric-2.5.0/tests/support/decorated_fabfile.py
--- fabric-1.14.0/tests/support/decorated_fabfile.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/tests/support/decorated_fabfile.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,8 +0,0 @@
-from fabric.decorators import task
-
-@task
-def foo():
-    pass
-
-def bar():
-    pass
diff -Nru fabric-1.14.0/tests/support/decorated_fabfile_with_classbased_task.py fabric-2.5.0/tests/support/decorated_fabfile_with_classbased_task.py
--- fabric-1.14.0/tests/support/decorated_fabfile_with_classbased_task.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/tests/support/decorated_fabfile_with_classbased_task.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,12 +0,0 @@
-from fabric import tasks
-from fabric.decorators import task
-
-class ClassBasedTask(tasks.Task):
-    def __init__(self):
-        self.name = "foo"
-        self.use_decorated = True
-
-    def run(self, *args, **kwargs):
-        pass
-
-foo = ClassBasedTask()
diff -Nru fabric-1.14.0/tests/support/decorated_fabfile_with_modules.py fabric-2.5.0/tests/support/decorated_fabfile_with_modules.py
--- fabric-1.14.0/tests/support/decorated_fabfile_with_modules.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/tests/support/decorated_fabfile_with_modules.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,9 +0,0 @@
-from fabric.decorators import task
-import module_fabtasks as tasks
-
-@task
-def foo():
-    pass
-
-def bar():
-    pass
diff -Nru fabric-1.14.0/tests/support/decorator_order.py fabric-2.5.0/tests/support/decorator_order.py
--- fabric-1.14.0/tests/support/decorator_order.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/tests/support/decorator_order.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,13 +0,0 @@
-from fabric.api import task, hosts, roles
-
-
-@hosts('whatever')
-@task
-def foo():
-    pass
-
-# There must be at least one unmolested new-style task for the decorator order
-# problem to appear.
-@task
-def caller():
-    pass
diff -Nru fabric-1.14.0/tests/support/deep.py fabric-2.5.0/tests/support/deep.py
--- fabric-1.14.0/tests/support/deep.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/tests/support/deep.py	1970-01-01 01:00:00.000000000 +0100
@@ -1 +0,0 @@
-import submodule
diff -Nru fabric-1.14.0/tests/support/default_tasks.py fabric-2.5.0/tests/support/default_tasks.py
--- fabric-1.14.0/tests/support/default_tasks.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/tests/support/default_tasks.py	1970-01-01 01:00:00.000000000 +0100
@@ -1 +0,0 @@
-import default_task_submodule as mymodule
diff -Nru fabric-1.14.0/tests/support/default_task_submodule.py fabric-2.5.0/tests/support/default_task_submodule.py
--- fabric-1.14.0/tests/support/default_task_submodule.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/tests/support/default_task_submodule.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,5 +0,0 @@
-from fabric.api import task
-
-@task(default=True)
-def long_task_name():
-    pass
diff -Nru fabric-1.14.0/tests/support/docstring.py fabric-2.5.0/tests/support/docstring.py
--- fabric-1.14.0/tests/support/docstring.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/tests/support/docstring.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,8 +0,0 @@
-from fabric.decorators import task
-
-@task
-def foo():
-    """
-    Foos!
-    """
-    pass
diff -Nru fabric-1.14.0/tests/support/exceptions_fabfile.py fabric-2.5.0/tests/support/exceptions_fabfile.py
--- fabric-1.14.0/tests/support/exceptions_fabfile.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/tests/support/exceptions_fabfile.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,5 +0,0 @@
-class NotATask(Exception):
-    pass
-
-def some_task():
-    pass
diff -Nru fabric-1.14.0/tests/support/explicit_fabfile.py fabric-2.5.0/tests/support/explicit_fabfile.py
--- fabric-1.14.0/tests/support/explicit_fabfile.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/tests/support/explicit_fabfile.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,7 +0,0 @@
-__all__ = ['foo']
-
-def foo():
-    pass
-
-def bar():
-    pass
diff -Nru fabric-1.14.0/tests/support/flat_aliases.py fabric-2.5.0/tests/support/flat_aliases.py
--- fabric-1.14.0/tests/support/flat_aliases.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/tests/support/flat_aliases.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,5 +0,0 @@
-from fabric.api import task
-
-@task(aliases=["foo_aliased", "foo_aliased_two"])
-def foo():
-    pass
diff -Nru fabric-1.14.0/tests/support/flat_alias.py fabric-2.5.0/tests/support/flat_alias.py
--- fabric-1.14.0/tests/support/flat_alias.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/tests/support/flat_alias.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,5 +0,0 @@
-from fabric.api import task
-
-@task(alias="foo_aliased")
-def foo():
-    pass
diff -Nru fabric-1.14.0/tests/support/implicit_fabfile.py fabric-2.5.0/tests/support/implicit_fabfile.py
--- fabric-1.14.0/tests/support/implicit_fabfile.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/tests/support/implicit_fabfile.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,5 +0,0 @@
-def foo():
-    pass
-
-def bar():
-    pass
diff -Nru fabric-1.14.0/tests/support/mapping.py fabric-2.5.0/tests/support/mapping.py
--- fabric-1.14.0/tests/support/mapping.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/tests/support/mapping.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,8 +0,0 @@
-from fabric.tasks import Task
-
-class MappingTask(dict, Task):
-    def run(self):
-        pass
-
-mapping_task = MappingTask()
-mapping_task.name = "mapping_task"
diff -Nru fabric-1.14.0/tests/support/module_fabtasks.py fabric-2.5.0/tests/support/module_fabtasks.py
--- fabric-1.14.0/tests/support/module_fabtasks.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/tests/support/module_fabtasks.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,6 +0,0 @@
-def hello():
-    print("hello")
-
-
-def world():
-    print("world")
diff -Nru fabric-1.14.0/tests/support/nested_aliases.py fabric-2.5.0/tests/support/nested_aliases.py
--- fabric-1.14.0/tests/support/nested_aliases.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/tests/support/nested_aliases.py	1970-01-01 01:00:00.000000000 +0100
@@ -1 +0,0 @@
-import flat_aliases as nested
diff -Nru fabric-1.14.0/tests/support/nested_alias.py fabric-2.5.0/tests/support/nested_alias.py
--- fabric-1.14.0/tests/support/nested_alias.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/tests/support/nested_alias.py	1970-01-01 01:00:00.000000000 +0100
@@ -1 +0,0 @@
-import flat_alias as nested
diff -Nru fabric-1.14.0/tests/support/ssh_config fabric-2.5.0/tests/support/ssh_config
--- fabric-1.14.0/tests/support/ssh_config	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/tests/support/ssh_config	1970-01-01 01:00:00.000000000 +0100
@@ -1,12 +0,0 @@
-Host myhost
-    User neighbor
-    Port 664
-    IdentityFile neighbor.pub
-
-Host myalias
-    HostName otherhost
-
-Host *
-    User satan
-    Port 666
-    IdentityFile foobar.pub
diff -Nru fabric-1.14.0/tests/support/submodule/__init__.py fabric-2.5.0/tests/support/submodule/__init__.py
--- fabric-1.14.0/tests/support/submodule/__init__.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/tests/support/submodule/__init__.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,4 +0,0 @@
-import subsubmodule
-
-def classic_task():
-    pass
diff -Nru fabric-1.14.0/tests/support/submodule/subsubmodule/__init__.py fabric-2.5.0/tests/support/submodule/subsubmodule/__init__.py
--- fabric-1.14.0/tests/support/submodule/subsubmodule/__init__.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/tests/support/submodule/subsubmodule/__init__.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,5 +0,0 @@
-from fabric.api import task
-
-@task
-def deeptask():
-    pass
diff -Nru fabric-1.14.0/tests/support/testserver_ssh_config fabric-2.5.0/tests/support/testserver_ssh_config
--- fabric-1.14.0/tests/support/testserver_ssh_config	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/tests/support/testserver_ssh_config	1970-01-01 01:00:00.000000000 +0100
@@ -1,5 +0,0 @@
-Host testserver
-    # TODO: get these pulling from server.py. Meh.
-    HostName 127.0.0.1
-    Port 2200
-    User username
diff -Nru fabric-1.14.0/tests/support/tree/db.py fabric-2.5.0/tests/support/tree/db.py
--- fabric-1.14.0/tests/support/tree/db.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/tests/support/tree/db.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,6 +0,0 @@
-from fabric.api import task
-
-
-@task
-def migrate():
-    pass
diff -Nru fabric-1.14.0/tests/support/tree/__init__.py fabric-2.5.0/tests/support/tree/__init__.py
--- fabric-1.14.0/tests/support/tree/__init__.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/tests/support/tree/__init__.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,12 +0,0 @@
-from fabric.api import task
-
-import system, db
-
-
-@task
-def deploy():
-    pass
-
-@task
-def build_docs():
-    pass
diff -Nru fabric-1.14.0/tests/support/tree/system/debian.py fabric-2.5.0/tests/support/tree/system/debian.py
--- fabric-1.14.0/tests/support/tree/system/debian.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/tests/support/tree/system/debian.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,6 +0,0 @@
-from fabric.api import task
-
-
-@task
-def update_apt():
-    pass
diff -Nru fabric-1.14.0/tests/support/tree/system/__init__.py fabric-2.5.0/tests/support/tree/system/__init__.py
--- fabric-1.14.0/tests/support/tree/system/__init__.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/tests/support/tree/system/__init__.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,7 +0,0 @@
-from fabric.api import task
-
-import debian
-
-@task
-def install_package():
-    pass
diff -Nru fabric-1.14.0/tests/task.py fabric-2.5.0/tests/task.py
--- fabric-1.14.0/tests/task.py	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/tests/task.py	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,140 @@
+# NOTE: named task.py, not tasks.py, to avoid some occasional pytest weirdness
+
+from mock import Mock
+from pytest import skip  # noqa
+
+import fabric
+from fabric.tasks import ConnectionCall
+
+
+class Task_:
+    def accepts_Invoke_level_init_kwargs(self):
+        # Arbitrarily selected list of invoke-level kwargs...
+        def body(c, parts):
+            "I am a docstring"
+            pass
+
+        t = fabric.Task(
+            body=body,
+            name="dadbod",
+            aliases=["heavenly", "check", "shop"],
+            default=True,
+            help={"parts": "See: the sum of"},
+            iterable=["parts"],
+        )
+        assert t.body is body
+        assert t.__doc__ == "I am a docstring"
+        assert t.name == "dadbod"
+        assert "heavenly" in t.aliases
+        assert t.is_default
+        assert "parts" in t.help
+        assert "parts" in t.iterable
+
+    def allows_hosts_kwarg(self):
+        # NOTE: most tests are below, in @task tests
+        assert fabric.Task(Mock(), hosts=["user@host"]).hosts == ["user@host"]
+
+
+class task_:
+    def accepts_Invoke_level_kwargs(self):
+        # Arbitrarily selected list of invoke-level kwargs...
+        def body(c, parts):
+            "I am a docstring"
+            pass
+
+        # Faux @task()
+        t = fabric.task(
+            name="dadbod",
+            aliases=["heavenly", "check", "shop"],
+            default=True,
+            help={"parts": "See: the sum of"},
+            iterable=["parts"],
+        )(body)
+        assert t.body is body
+        assert t.__doc__ == "I am a docstring"
+        assert t.name == "dadbod"
+        assert "heavenly" in t.aliases
+        assert t.is_default
+        assert "parts" in t.help
+        assert "parts" in t.iterable
+
+    def returns_Fabric_level_Task_instance(self):
+        assert isinstance(fabric.task(Mock()), fabric.Task)
+
+    def does_not_touch_klass_kwarg_if_explicitly_given(self):
+        # Otherwise sub-subclassers would be screwed, yea?
+        class SubFabTask(fabric.Task):
+            pass
+
+        assert isinstance(fabric.task(klass=SubFabTask)(Mock()), SubFabTask)
+
+    class hosts_kwarg:
+        # NOTE: these don't currently test anything besides "the value given is
+        # attached as .hosts" but they guard against regressions and ensures
+        # things work as documented, even if Executor is what really cares.
+        def _run(self, hosts):
+            @fabric.task(hosts=hosts)
+            def mytask(c):
+                pass
+
+            assert mytask.hosts == hosts
+
+        def values_may_be_connection_first_posarg_strings(self):
+            self._run(["host1", "user@host2", "host3:2222"])
+
+        def values_may_be_Connection_constructor_kwarg_dicts(self):
+            self._run(
+                [
+                    {"host": "host1"},
+                    {"host": "host2", "user": "user"},
+                    {"host": "host3", "port": 2222},
+                ]
+            )
+
+        def values_may_be_mixed(self):
+            self._run([{"host": "host1"}, "user@host2"])
+
+
+def _dummy(c):
+    pass
+
+
+class ConnectionCall_:
+    class init:
+        "__init__"
+
+        def inherits_regular_kwargs(self):
+            t = fabric.Task(_dummy)
+            call = ConnectionCall(
+                task=t,
+                called_as="meh",
+                args=["5"],
+                kwargs={"kwarg": "val"},
+                init_kwargs={},  # whatever
+            )
+            assert call.task is t
+            assert call.called_as == "meh"
+            assert call.args == ["5"]
+            assert call.kwargs["kwarg"] == "val"
+
+        def extends_with_init_kwargs_kwarg(self):
+            call = ConnectionCall(
+                task=fabric.Task(_dummy),
+                init_kwargs={"host": "server", "port": 2222},
+            )
+            assert call.init_kwargs["port"] == 2222
+
+    class str:
+        "___str__"
+
+        def includes_init_kwargs_host_value(self):
+            call = ConnectionCall(
+                fabric.Task(body=_dummy),
+                init_kwargs=dict(host="host", user="user"),
+            )
+            # TODO: worth using some subset of real Connection repr() in here?
+            # For now, just stick with hostname.
+            expected = (
+                "<ConnectionCall '_dummy', args: (), kwargs: {}, host='host'>"
+            )  # noqa
+            assert str(call) == expected
diff -Nru fabric-1.14.0/tests/test_context_managers.py fabric-2.5.0/tests/test_context_managers.py
--- fabric-1.14.0/tests/test_context_managers.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/tests/test_context_managers.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,319 +0,0 @@
-from __future__ import with_statement
-
-import os
-import sys
-from StringIO import StringIO
-
-from nose.tools import eq_, ok_
-
-from fabric.state import env, output
-from fabric.context_managers import (cd, settings, lcd, hide, shell_env, quiet,
-    warn_only, prefix, path)
-from fabric.operations import run, local, _prefix_commands
-from utils import mock_streams, FabricTest
-from server import server
-
-
-#
-# cd()
-#
-
-def test_error_handling():
-    """
-    cd cleans up after itself even in case of an exception
-    """
-
-    class TestException(Exception):
-        pass
-
-    try:
-        with cd('somewhere'):
-            raise TestException('Houston, we have a problem.')
-    except TestException:
-        pass
-    finally:
-        with cd('else'):
-            eq_(env.cwd, 'else')
-
-
-def test_cwd_with_absolute_paths():
-    """
-    cd() should append arg if non-absolute or overwrite otherwise
-    """
-    existing = '/some/existing/path'
-    additional = 'another'
-    absolute = '/absolute/path'
-    with settings(cwd=existing):
-        with cd(absolute):
-            eq_(env.cwd, absolute)
-        with cd(additional):
-            eq_(env.cwd, existing + '/' + additional)
-
-
-def test_cd_home_dir():
-    """
-    cd() should work with home directories
-    """
-    homepath = "~/somepath"
-    with cd(homepath):
-        eq_(env.cwd, homepath)
-
-
-def test_cd_nested_home_abs_dirs():
-    """
-    cd() should work with nested user homedir (starting with ~) paths.
-
-    It should always take the last path if the new path begins with `/` or `~`
-    """
-
-    home_path = "~/somepath"
-    abs_path = "/some/random/path"
-    relative_path = "some/random/path"
-
-    # 2 nested homedir paths
-    with cd(home_path):
-        eq_(env.cwd, home_path)
-        another_path = home_path + "/another/path"
-        with cd(another_path):
-            eq_(env.cwd, another_path)
-
-    # first absolute path, then a homedir path
-    with cd(abs_path):
-        eq_(env.cwd, abs_path)
-        with cd(home_path):
-            eq_(env.cwd, home_path)
-
-    # first relative path, then a homedir path
-    with cd(relative_path):
-        eq_(env.cwd, relative_path)
-        with cd(home_path):
-            eq_(env.cwd, home_path)
-
-    # first home path, then a a relative path
-    with cd(home_path):
-        eq_(env.cwd, home_path)
-        with cd(relative_path):
-            eq_(env.cwd, home_path + "/" + relative_path)
-
-
-#
-#  prefix
-#
-
-def test_nested_prefix():
-    """
-    prefix context managers can be created outside of the with block and nested
-    """
-    cm1 = prefix('1')
-    cm2 = prefix('2')
-    with cm1:
-        with cm2:
-            eq_(env.command_prefixes, ['1', '2'])
-
-#
-# cd prefix with dev/null
-#
-
-def test_cd_prefix():
-    """
-    cd prefix should direct output to /dev/null in case of CDPATH
-    """
-    some_path = "~/somepath"
-
-    with cd(some_path):
-        command_out = _prefix_commands('foo', "remote")
-        eq_(command_out, 'cd %s >/dev/null && foo' % some_path)
-
-
-# def test_cd_prefix_on_win32():
-#     """
-#     cd prefix should NOT direct output to /dev/null on win32
-#     """
-#     some_path = "~/somepath"
-
-#     import fabric
-#     try:
-#         fabric.state.win32 = True
-#         with cd(some_path):
-#             command_out = _prefix_commands('foo', "remote")
-#             eq_(command_out, 'cd %s && foo' % some_path)
-#     finally:
-#         fabric.state.win32 = False
-
-#
-# hide/show
-#
-
-def test_hide_show_exception_handling():
-    """
-    hide()/show() should clean up OK if exceptions are raised
-    """
-    try:
-        with hide('stderr'):
-            # now it's False, while the default is True
-            eq_(output.stderr, False)
-            raise Exception
-    except Exception:
-        # Here it should be True again.
-        # If it's False, this means hide() didn't clean up OK.
-        eq_(output.stderr, True)
-
-
-#
-# settings()
-#
-
-def test_setting_new_env_dict_key_should_work():
-    """
-    Using settings() with a previously nonexistent key should work correctly
-    """
-    key = 'thisshouldnevereverexistseriouslynow'
-    value = 'a winner is you'
-    with settings(**{key: value}):
-        ok_(key in env)
-    ok_(key not in env)
-
-
-def test_settings():
-    """
-    settings() should temporarily override env dict with given key/value pair
-    """
-    env.testval = "outer value"
-    with settings(testval="inner value"):
-        eq_(env.testval, "inner value")
-    eq_(env.testval, "outer value")
-
-
-def test_settings_with_multiple_kwargs():
-    """
-    settings() should temporarily override env dict with given key/value pairS
-    """
-    env.testval1 = "outer 1"
-    env.testval2 = "outer 2"
-    with settings(testval1="inner 1", testval2="inner 2"):
-        eq_(env.testval1, "inner 1")
-        eq_(env.testval2, "inner 2")
-    eq_(env.testval1, "outer 1")
-    eq_(env.testval2, "outer 2")
-
-
-def test_settings_with_other_context_managers():
-    """
-    settings() should take other context managers, and use them with other overrided
-    key/value pairs.
-    """
-    env.testval1 = "outer 1"
-    prev_lcwd = env.lcwd
-
-    with settings(lcd("here"), testval1="inner 1"):
-        eq_(env.testval1, "inner 1")
-        ok_(env.lcwd.endswith("here")) # Should be the side-effect of adding cd to settings
-
-    ok_(env.testval1, "outer 1")
-    eq_(env.lcwd, prev_lcwd)
-
-
-def test_settings_clean_revert():
-    """
-    settings(clean_revert=True) should only revert values matching input values
-    """
-    env.modified = "outer"
-    env.notmodified = "outer"
-    with settings(
-        modified="inner",
-        notmodified="inner",
-        inner_only="only",
-        clean_revert=True
-    ):
-        eq_(env.modified, "inner")
-        eq_(env.notmodified, "inner")
-        eq_(env.inner_only, "only")
-        env.modified = "modified internally"
-    eq_(env.modified, "modified internally")
-    ok_("inner_only" not in env)
-
-
-#
-# shell_env()
-#
-
-def test_shell_env():
-    """
-    shell_env() sets the shell_env attribute in the env dict
-    """
-    with shell_env(KEY="value"):
-        eq_(env.shell_env['KEY'], 'value')
-
-    eq_(env.shell_env, {})
-
-
-class TestQuietAndWarnOnly(FabricTest):
-    @server()
-    @mock_streams('both')
-    def test_quiet_hides_all_output(self):
-        # Sanity test - normally this is not empty
-        run("ls /simple")
-        ok_(sys.stdout.getvalue())
-        # Reset
-        sys.stdout = StringIO()
-        # Real test
-        with quiet():
-            run("ls /simple")
-        # Empty output
-        ok_(not sys.stdout.getvalue())
-        # Reset
-        sys.stdout = StringIO()
-        # Kwarg test
-        run("ls /simple", quiet=True)
-        ok_(not sys.stdout.getvalue())
-
-    @server(responses={'barf': [
-        "this is my stdout",
-        "this is my stderr",
-        1
-    ]})
-    def test_quiet_sets_warn_only_to_true(self):
-        # Sanity test to ensure environment
-        with settings(warn_only=False):
-            with quiet():
-                eq_(run("barf").return_code, 1)
-            # Kwarg test
-            eq_(run("barf", quiet=True).return_code, 1)
-
-    @server(responses={'hrm': ["", "", 1]})
-    @mock_streams('both')
-    def test_warn_only_is_same_as_settings_warn_only(self):
-        with warn_only():
-            eq_(run("hrm").failed, True)
-
-    @server()
-    @mock_streams('both')
-    def test_warn_only_does_not_imply_hide_everything(self):
-        with warn_only():
-            run("ls /simple")
-            assert sys.stdout.getvalue().strip() != ""
-
-
-# path() (distinct from shell_env)
-
-class TestPathManager(FabricTest):
-    def setup(self):
-        super(TestPathManager, self).setup()
-        self.real = os.environ.get('PATH')
-
-    def via_local(self):
-        with hide('everything'):
-            return local("echo $PATH", capture=True)
-
-    def test_lack_of_path_has_default_local_path(self):
-        """
-        No use of 'with path' == default local $PATH
-        """
-        eq_(self.real, self.via_local())
-
-    def test_use_of_path_appends_by_default(self):
-        """
-        'with path' appends by default
-        """
-        with path('foo'):
-            eq_(self.via_local(), self.real + ":foo")
diff -Nru fabric-1.14.0/tests/test_contrib.py fabric-2.5.0/tests/test_contrib.py
--- fabric-1.14.0/tests/test_contrib.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/tests/test_contrib.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,156 +0,0 @@
-# -*- coding: utf-8 -*-
-from __future__ import with_statement
-import os
-
-from fabric.api import hide, get
-from fabric.contrib.files import upload_template, contains
-from fabric.context_managers import lcd
-
-from utils import FabricTest, eq_contents
-from server import server
-
-
-class TestContrib(FabricTest):
-    # Make sure it knows / is a directory.
-    # This is in lieu of starting down the "actual honest to god fake operating
-    # system" road...:(
-    @server(responses={'test -d "$(echo /)"': ""})
-    def test_upload_template_uses_correct_remote_filename(self):
-        """
-        upload_template() shouldn't munge final remote filename
-        """
-        template = self.mkfile('template.txt', 'text')
-        with hide('everything'):
-            upload_template(template, '/')
-            assert self.exists_remotely('/template.txt')
-
-    @server()
-    def test_upload_template_handles_file_destination(self):
-        """
-        upload_template() should work OK with file and directory destinations
-        """
-        template = self.mkfile('template.txt', '%(varname)s')
-        local = self.path('result.txt')
-        remote = '/configfile.txt'
-        var = 'foobar'
-        with hide('everything'):
-            upload_template(template, remote, {'varname': var})
-            get(remote, local)
-        eq_contents(local, var)
-
-    @server()
-    def test_upload_template_handles_template_dir(self):
-        """
-        upload_template() should work OK with template dir
-        """
-        template = self.mkfile('template.txt', '%(varname)s')
-        template_dir = os.path.dirname(template)
-        local = self.path('result.txt')
-        remote = '/configfile.txt'
-        var = 'foobar'
-        with hide('everything'):
-            upload_template(
-                'template.txt', remote, {'varname': var},
-                template_dir=template_dir
-            )
-            get(remote, local)
-        eq_contents(local, var)
-
-
-    @server(responses={
-        'egrep "text" "/file.txt"': (
-            "sudo: unable to resolve host fabric",
-            "",
-            1
-        )}
-    )
-    def test_contains_checks_only_succeeded_flag(self):
-        """
-        contains() should return False on bad grep even if stdout isn't empty
-        """
-        with hide('everything'):
-            result = contains('/file.txt', 'text', use_sudo=True)
-            assert result == False
-
-    @server(responses={
-        r'egrep "Include other\\.conf" "$(echo /etc/apache2/apache2.conf)"': "Include other.conf"
-    })
-    def test_contains_performs_case_sensitive_search(self):
-        """
-        contains() should perform a case-sensitive search by default.
-        """
-        with hide('everything'):
-            result = contains('/etc/apache2/apache2.conf', 'Include other.conf',
-                              use_sudo=True)
-            assert result == True
-
-    @server(responses={
-        r'egrep -i "include Other\\.CONF" "$(echo /etc/apache2/apache2.conf)"': "Include other.conf"
-    })
-    def test_contains_performs_case_insensitive_search(self):
-        """
-        contains() should perform a case-insensitive search when passed `case_sensitive=False`
-        """
-        with hide('everything'):
-            result = contains('/etc/apache2/apache2.conf',
-                              'include Other.CONF',
-                              use_sudo=True,
-                              case_sensitive=False)
-            assert result == True
-
-    @server()
-    def test_upload_template_handles_jinja_template(self):
-        """
-        upload_template() should work OK with Jinja2 template
-        """
-        template = self.mkfile('template_jinja2.txt', '{{ first_name }}')
-        template_name = os.path.basename(template)
-        template_dir = os.path.dirname(template)
-        local = self.path('result.txt')
-        remote = '/configfile.txt'
-        first_name = u'S\u00E9bastien'
-        with hide('everything'):
-            upload_template(template_name, remote, {'first_name': first_name},
-                use_jinja=True, template_dir=template_dir)
-            get(remote, local)
-        eq_contents(local, first_name.encode('utf-8'))
-
-    @server()
-    def test_upload_template_jinja_and_no_template_dir(self):
-        # Crummy doesn't-die test
-        fname = "foo.tpl"
-        try:
-            with hide('everything'):
-                with open(fname, 'w+') as fd:
-                    fd.write('whatever')
-                upload_template(fname, '/configfile.txt', {}, use_jinja=True)
-        finally:
-            os.remove(fname)
-
-
-    def test_upload_template_obeys_lcd(self):
-        for jinja in (True, False):
-            for mirror in (True, False):
-                self._upload_template_obeys_lcd(jinja=jinja, mirror=mirror)
-
-    @server()
-    def _upload_template_obeys_lcd(self, jinja, mirror):
-        template_content = {True: '{{ varname }}s', False: '%(varname)s'}
-
-        template_dir = 'template_dir'
-        template_name = 'template.txt'
-        if not self.exists_locally(self.path(template_dir)):
-            os.mkdir(self.path(template_dir))
-
-        self.mkfile(
-            os.path.join(template_dir, template_name), template_content[jinja]
-        )
-
-        remote = '/configfile.txt'
-        var = 'foobar'
-        with hide('everything'):
-            with lcd(self.path(template_dir)):
-                upload_template(
-                    template_name, remote, {'varname': var},
-                    mirror_local_mode=mirror
-                )
diff -Nru fabric-1.14.0/tests/test_decorators.py fabric-2.5.0/tests/test_decorators.py
--- fabric-1.14.0/tests/test_decorators.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/tests/test_decorators.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,293 +0,0 @@
-from __future__ import with_statement
-
-import random
-import sys
-
-from nose.tools import eq_, ok_, assert_true, assert_false, assert_equal
-import fudge
-from fudge import Fake, with_fakes, patched_context
-
-from fabric import decorators, tasks
-from fabric.state import env
-import fabric # for patching fabric.state.xxx
-from fabric.tasks import _parallel_tasks, requires_parallel, execute
-from fabric.context_managers import lcd, settings, hide
-
-from utils import mock_streams
-
-
-#
-# Support
-#
-
-def fake_function(*args, **kwargs):
-    """
-    Returns a ``fudge.Fake`` exhibiting function-like attributes.
-
-    Passes in all args/kwargs to the ``fudge.Fake`` constructor. However, if
-    ``callable`` or ``expect_call`` kwargs are not given, ``callable`` will be
-    set to True by default.
-    """
-    # Must define __name__ to be compatible with function wrapping mechanisms
-    # like @wraps().
-    if 'callable' not in kwargs and 'expect_call' not in kwargs:
-        kwargs['callable'] = True
-    return Fake(*args, **kwargs).has_attr(__name__='fake')
-
-
-
-#
-# @task
-#
-
-def test_task_returns_an_instance_of_wrappedfunctask_object():
-    def foo():
-        pass
-    task = decorators.task(foo)
-    ok_(isinstance(task, tasks.WrappedCallableTask))
-
-
-def test_task_will_invoke_provided_class():
-    def foo(): pass
-    fake = Fake()
-    fake.expects("__init__").with_args(foo)
-    fudge.clear_calls()
-    fudge.clear_expectations()
-
-    foo = decorators.task(foo, task_class=fake)
-
-    fudge.verify()
-
-
-def test_task_passes_args_to_the_task_class():
-    random_vars = ("some text", random.randint(100, 200))
-    def foo(): pass
-
-    fake = Fake()
-    fake.expects("__init__").with_args(foo, *random_vars)
-    fudge.clear_calls()
-    fudge.clear_expectations()
-
-    foo = decorators.task(foo, task_class=fake, *random_vars)
-    fudge.verify()
-
-
-def test_passes_kwargs_to_the_task_class():
-    random_vars = {
-        "msg": "some text",
-        "number": random.randint(100, 200),
-    }
-    def foo(): pass
-
-    fake = Fake()
-    fake.expects("__init__").with_args(foo, **random_vars)
-    fudge.clear_calls()
-    fudge.clear_expectations()
-
-    foo = decorators.task(foo, task_class=fake, **random_vars)
-    fudge.verify()
-
-
-def test_integration_tests_for_invoked_decorator_with_no_args():
-    r = random.randint(100, 200)
-    @decorators.task()
-    def foo():
-        return r
-
-    eq_(r, foo())
-
-
-def test_integration_tests_for_decorator():
-    r = random.randint(100, 200)
-    @decorators.task(task_class=tasks.WrappedCallableTask)
-    def foo():
-        return r
-
-    eq_(r, foo())
-
-
-def test_original_non_invoked_style_task():
-    r = random.randint(100, 200)
-    @decorators.task
-    def foo():
-        return r
-
-    eq_(r, foo())
-
-
-
-#
-# @runs_once
-#
-
-@with_fakes
-def test_runs_once_runs_only_once():
-    """
-    @runs_once prevents decorated func from running >1 time
-    """
-    func = fake_function(expect_call=True).times_called(1)
-    task = decorators.runs_once(func)
-    for i in range(2):
-        task()
-
-
-def test_runs_once_returns_same_value_each_run():
-    """
-    @runs_once memoizes return value of decorated func
-    """
-    return_value = "foo"
-    task = decorators.runs_once(fake_function().returns(return_value))
-    for i in range(2):
-        eq_(task(), return_value)
-
-
-@decorators.runs_once
-def single_run():
-    pass
-
-def test_runs_once():
-    assert_false(hasattr(single_run, 'return_value'))
-    single_run()
-    assert_true(hasattr(single_run, 'return_value'))
-    assert_equal(None, single_run())
-
-
-
-#
-# @serial / @parallel
-#
-
-
-@decorators.serial
-def serial():
-    pass
-
-@decorators.serial
-@decorators.parallel
-def serial2():
-    pass
-
-@decorators.parallel
-@decorators.serial
-def serial3():
-    pass
-
-@decorators.parallel
-def parallel():
-    pass
-
-@decorators.parallel(pool_size=20)
-def parallel2():
-    pass
-
-fake_tasks = {
-    'serial': serial,
-    'serial2': serial2,
-    'serial3': serial3,
-    'parallel': parallel,
-    'parallel2': parallel2,
-}
-
-def parallel_task_helper(actual_tasks, expected):
-    commands_to_run = map(lambda x: [x], actual_tasks)
-    with patched_context(fabric.state, 'commands', fake_tasks):
-        eq_(_parallel_tasks(commands_to_run), expected)
-
-def test_parallel_tasks():
-    for desc, task_names, expected in (
-        ("One @serial-decorated task == no parallelism",
-            ['serial'], False),
-        ("One @parallel-decorated task == parallelism",
-            ['parallel'], True),
-        ("One @parallel-decorated and one @serial-decorated task == paralellism",
-            ['parallel', 'serial'], True),
-        ("Tasks decorated with both @serial and @parallel count as @parallel",
-            ['serial2', 'serial3'], True)
-    ):
-        parallel_task_helper.description = desc
-        yield parallel_task_helper, task_names, expected
-        del parallel_task_helper.description
-
-def test_parallel_wins_vs_serial():
-    """
-    @parallel takes precedence over @serial when both are used on one task
-    """
-    ok_(requires_parallel(serial2))
-    ok_(requires_parallel(serial3))
-
-@mock_streams('stdout')
-def test_global_parallel_honors_runs_once():
-    """
-    fab -P (or env.parallel) should honor @runs_once
-    """
-    @decorators.runs_once
-    def mytask():
-        print("yolo") # 'Carpe diem' for stupid people!
-    with settings(hide('everything'), parallel=True):
-        execute(mytask, hosts=['localhost', '127.0.0.1'])
-    result = sys.stdout.getvalue()
-    eq_(result, "yolo\n")
-    assert result != "yolo\nyolo\n"
-
-
-#
-# @roles
-#
-
-@decorators.roles('test')
-def use_roles():
-    pass
-
-def test_roles():
-    assert_true(hasattr(use_roles, 'roles'))
-    assert_equal(use_roles.roles, ['test'])
-
-
-
-#
-# @hosts
-#
-
-@decorators.hosts('test')
-def use_hosts():
-    pass
-
-def test_hosts():
-    assert_true(hasattr(use_hosts, 'hosts'))
-    assert_equal(use_hosts.hosts, ['test'])
-
-
-
-#
-# @with_settings
-#
-
-def test_with_settings_passes_env_vars_into_decorated_function():
-    env.value = True
-    random_return = random.randint(1000, 2000)
-    def some_task():
-        return env.value
-    decorated_task = decorators.with_settings(value=random_return)(some_task)
-    ok_(some_task(), msg="sanity check")
-    eq_(random_return, decorated_task())
-
-def test_with_settings_with_other_context_managers():
-    """
-    with_settings() should take other context managers, and use them with other
-    overrided key/value pairs.
-    """
-    env.testval1 = "outer 1"
-    prev_lcwd = env.lcwd
-
-    def some_task():
-        eq_(env.testval1, "inner 1")
-        ok_(env.lcwd.endswith("here")) # Should be the side-effect of adding cd to settings
-
-    decorated_task = decorators.with_settings(
-        lcd("here"),
-        testval1="inner 1"
-    )(some_task)
-    decorated_task()
-
-    ok_(env.testval1, "outer 1")
-    eq_(env.lcwd, prev_lcwd)
diff -Nru fabric-1.14.0/tests/test_io.py fabric-2.5.0/tests/test_io.py
--- fabric-1.14.0/tests/test_io.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/tests/test_io.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,28 +0,0 @@
-from __future__ import with_statement
-
-from nose.tools import eq_
-
-from fabric.io import OutputLooper
-from fabric.context_managers import settings
-
-
-def test_request_prompts():
-    """
-    Test valid responses from prompts
-    """
-    def run(txt, prompts):
-        with settings(prompts=prompts):
-            # try to fulfil the OutputLooper interface, only want to test
-            # _get_prompt_response. (str has a method upper)
-            ol = OutputLooper(str, 'upper', None, list(txt), None)
-            return ol._get_prompt_response()
-
-    prompts = {"prompt2": "response2",
-               "prompt1": "response1",
-               "prompt": "response"
-               }
-
-    eq_(run("this is a prompt for prompt1", prompts), ("prompt1", "response1"))
-    eq_(run("this is a prompt for prompt2", prompts), ("prompt2", "response2"))
-    eq_(run("this is a prompt for promptx:", prompts), (None, None))
-    eq_(run("prompt for promp", prompts), (None, None))
diff -Nru fabric-1.14.0/tests/test_main.py fabric-2.5.0/tests/test_main.py
--- fabric-1.14.0/tests/test_main.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/tests/test_main.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,716 +0,0 @@
-from __future__ import with_statement
-
-import copy
-from functools import partial
-from operator import isMappingType
-import os.path
-import sys
-
-from fudge import Fake, patched_context
-from nose.tools import ok_, eq_
-
-from fabric.decorators import hosts, roles, task
-from fabric.context_managers import settings
-from fabric.main import (parse_arguments, _escape_split, find_fabfile,
-        load_fabfile as _load_fabfile, list_commands, _task_names,
-        COMMANDS_HEADER, NESTED_REMINDER)
-import fabric.state
-from fabric.tasks import Task, WrappedCallableTask
-from fabric.task_utils import _crawl, crawl, merge
-
-from utils import FabricTest, fabfile, path_prefix, aborts
-
-
-# Stupid load_fabfile wrapper to hide newly added return value.
-# WTB more free time to rewrite all this with objects :)
-def load_fabfile(*args, **kwargs):
-    return _load_fabfile(*args, **kwargs)[:2]
-
-
-#
-# Basic CLI stuff
-#
-
-def test_argument_parsing():
-    for args, output in [
-        # Basic
-        ('abc', ('abc', [], {}, [], [], [])),
-        # Arg
-        ('ab:c', ('ab', ['c'], {}, [], [], [])),
-        # Kwarg
-        ('a:b=c', ('a', [], {'b':'c'}, [], [], [])),
-        # Arg and kwarg
-        ('a:b=c,d', ('a', ['d'], {'b':'c'}, [], [], [])),
-        # Multiple kwargs
-        ('a:b=c,d=e', ('a', [], {'b':'c','d':'e'}, [], [], [])),
-        # Host
-        ('abc:host=foo', ('abc', [], {}, ['foo'], [], [])),
-        # Hosts with single host
-        ('abc:hosts=foo', ('abc', [], {}, ['foo'], [], [])),
-        # Hosts with multiple hosts
-        # Note: in a real shell, one would need to quote or escape "foo;bar".
-        # But in pure-Python that would get interpreted literally, so we don't.
-        ('abc:hosts=foo;bar', ('abc', [], {}, ['foo', 'bar'], [], [])),
-
-        # Exclude hosts
-        ('abc:hosts=foo;bar,exclude_hosts=foo', ('abc', [], {}, ['foo', 'bar'], [], ['foo'])),
-        ('abc:hosts=foo;bar,exclude_hosts=foo;bar', ('abc', [], {}, ['foo', 'bar'], [], ['foo','bar'])),
-       # Empty string args
-        ("task:x=y,z=", ('task', [], {'x': 'y', 'z': ''}, [], [], [])),
-        ("task:foo,,x=y", ('task', ['foo', ''], {'x': 'y'}, [], [], [])),
-    ]:
-        yield eq_, parse_arguments([args]), [output]
-
-
-def test_escaped_task_arg_split():
-    """
-    Allow backslashes to escape the task argument separator character
-    """
-    argstr = r"foo,bar\,biz\,baz,what comes after baz?"
-    eq_(
-        _escape_split(',', argstr),
-        ['foo', 'bar,biz,baz', 'what comes after baz?']
-    )
-
-
-def test_escaped_task_kwarg_split():
-    """
-    Allow backslashes to escape the = in x=y task kwargs
-    """
-    argstr = r"cmd:arg,escaped\,arg,nota\=kwarg,regular=kwarg,escaped=regular\=kwarg"
-    args = ['arg', 'escaped,arg', 'nota=kwarg']
-    kwargs = {'regular': 'kwarg', 'escaped': 'regular=kwarg'}
-    eq_(
-        parse_arguments([argstr])[0],
-        ('cmd', args, kwargs, [], [], []),
-    )
-
-
-
-#
-# Host/role decorators
-#
-
-# Allow calling Task.get_hosts as function instead (meh.)
-def get_hosts_and_effective_roles(command, *args):
-    return WrappedCallableTask(command).get_hosts_and_effective_roles(*args)
-
-def eq_hosts(command, expected_hosts, cli_hosts=None, excluded_hosts=None, env=None, func=set):
-    eq_(func(get_hosts_and_effective_roles(command, cli_hosts or [], [], excluded_hosts or [], env)[0]),
-        func(expected_hosts))
-
-def eq_effective_roles(command, expected_effective_roles, cli_roles=None, env=None, func=set):
-    eq_(func(get_hosts_and_effective_roles(command, [], cli_roles or [], [], env)[1]),
-        func(expected_effective_roles))
-
-true_eq_hosts = partial(eq_hosts, func=lambda x: x)
-
-def test_hosts_decorator_by_itself():
-    """
-    Use of @hosts only
-    """
-    host_list = ['a', 'b']
-
-    @hosts(*host_list)
-    def command():
-        pass
-
-    eq_hosts(command, host_list)
-
-
-fake_roles = {
-    'r1': ['a', 'b'],
-    'r2': ['b', 'c']
-}
-
-def test_roles_decorator_by_itself():
-    """
-    Use of @roles only
-    """
-    @roles('r1')
-    def command():
-        pass
-    eq_hosts(command, ['a', 'b'], env={'roledefs': fake_roles})
-    eq_effective_roles(command, ['r1'], env={'roledefs': fake_roles})
-
-def test_roles_decorator_overrides_env_roles():
-    """
-    If @roles is used it replaces any env.roles value
-    """
-    @roles('r1')
-    def command():
-        pass
-    eq_effective_roles(command, ['r1'], env={'roledefs': fake_roles,
-                                             'roles': ['r2']})
-
-def test_cli_roles_override_decorator_roles():
-    """
-    If CLI roles are provided they replace roles defined in @roles.
-    """
-    @roles('r1')
-    def command():
-        pass
-    eq_effective_roles(command, ['r2'], cli_roles=['r2'], env={'roledefs': fake_roles})
-
-
-def test_hosts_and_roles_together():
-    """
-    Use of @roles and @hosts together results in union of both
-    """
-    @roles('r1', 'r2')
-    @hosts('d')
-    def command():
-        pass
-    eq_hosts(command, ['a', 'b', 'c', 'd'], env={'roledefs': fake_roles})
-    eq_effective_roles(command, ['r1', 'r2'], env={'roledefs': fake_roles})
-
-def test_host_role_merge_deduping():
-    """
-    Use of @roles and @hosts dedupes when merging
-    """
-    @roles('r1', 'r2')
-    @hosts('a')
-    def command():
-        pass
-    # Not ['a', 'a', 'b', 'c'] or etc
-    true_eq_hosts(command, ['a', 'b', 'c'], env={'roledefs': fake_roles})
-
-def test_host_role_merge_deduping_off():
-    """
-    Allow turning deduping off
-    """
-    @roles('r1', 'r2')
-    @hosts('a')
-    def command():
-        pass
-    with settings(dedupe_hosts=False):
-        true_eq_hosts(
-            command,
-            # 'a' 1x host 1x role
-            # 'b' 1x r1 1x r2
-            ['a', 'a', 'b', 'b', 'c'],
-            env={'roledefs': fake_roles}
-        )
-
-
-tuple_roles = {
-    'r1': ('a', 'b'),
-    'r2': ('b', 'c'),
-}
-
-def test_roles_as_tuples():
-    """
-    Test that a list of roles as a tuple succeeds
-    """
-    @roles('r1')
-    def command():
-        pass
-    eq_hosts(command, ['a', 'b'], env={'roledefs': tuple_roles})
-    eq_effective_roles(command, ['r1'], env={'roledefs': fake_roles})
-
-
-def test_hosts_as_tuples():
-    """
-    Test that a list of hosts as a tuple succeeds
-    """
-    def command():
-        pass
-    eq_hosts(command, ['foo', 'bar'], env={'hosts': ('foo', 'bar')})
-
-
-def test_hosts_decorator_overrides_env_hosts():
-    """
-    If @hosts is used it replaces any env.hosts value
-    """
-    @hosts('bar')
-    def command():
-        pass
-    eq_hosts(command, ['bar'], env={'hosts': ['foo']})
-
-def test_hosts_decorator_overrides_env_hosts_with_task_decorator_first():
-    """
-    If @hosts is used it replaces any env.hosts value even with @task
-    """
-    @task
-    @hosts('bar')
-    def command():
-        pass
-    eq_hosts(command, ['bar'], env={'hosts': ['foo']})
-
-def test_hosts_decorator_overrides_env_hosts_with_task_decorator_last():
-    @hosts('bar')
-    @task
-    def command():
-        pass
-    eq_hosts(command, ['bar'], env={'hosts': ['foo']})
-
-def test_hosts_stripped_env_hosts():
-    """
-    Make sure hosts defined in env.hosts are cleaned of extra spaces
-    """
-    def command():
-        pass
-    myenv = {'hosts': [' foo ', 'bar '], 'roles': [], 'exclude_hosts': []}
-    eq_hosts(command, ['foo', 'bar'], env=myenv)
-
-
-spaced_roles = {
-    'r1': [' a ', ' b '],
-    'r2': ['b', 'c'],
-}
-
-def test_roles_stripped_env_hosts():
-    """
-    Make sure hosts defined in env.roles are cleaned of extra spaces
-    """
-    @roles('r1')
-    def command():
-        pass
-    eq_hosts(command, ['a', 'b'], env={'roledefs': spaced_roles})
-
-
-dict_roles = {
-    'r1': {'hosts': ['a', 'b']},
-    'r2': ['b', 'c'],
-}
-
-def test_hosts_in_role_dict():
-    """
-    Make sure hosts defined in env.roles are cleaned of extra spaces
-    """
-    @roles('r1')
-    def command():
-        pass
-    eq_hosts(command, ['a', 'b'], env={'roledefs': dict_roles})
-
-
-def test_hosts_decorator_expands_single_iterable():
-    """
-    @hosts(iterable) should behave like @hosts(*iterable)
-    """
-    host_list = ['foo', 'bar']
-
-    @hosts(host_list)
-    def command():
-        pass
-
-    eq_(command.hosts, host_list)
-
-def test_roles_decorator_expands_single_iterable():
-    """
-    @roles(iterable) should behave like @roles(*iterable)
-    """
-    role_list = ['foo', 'bar']
-
-    @roles(role_list)
-    def command():
-        pass
-
-    eq_(command.roles, role_list)
-
-
-#
-# Host exclusion
-#
-
-def dummy(): pass
-
-def test_get_hosts_excludes_cli_exclude_hosts_from_cli_hosts():
-    eq_hosts(dummy, ['bar'], cli_hosts=['foo', 'bar'], excluded_hosts=['foo'])
-
-def test_get_hosts_excludes_cli_exclude_hosts_from_decorator_hosts():
-    @hosts('foo', 'bar')
-    def command():
-        pass
-    eq_hosts(command, ['bar'], excluded_hosts=['foo'])
-
-def test_get_hosts_excludes_global_exclude_hosts_from_global_hosts():
-    fake_env = {'hosts': ['foo', 'bar'], 'exclude_hosts': ['foo']}
-    eq_hosts(dummy, ['bar'], env=fake_env)
-
-
-
-#
-# Basic role behavior
-#
-
-@aborts
-def test_aborts_on_nonexistent_roles():
-    """
-    Aborts if any given roles aren't found
-    """
-    merge([], ['badrole'], [], {})
-
-def test_accepts_non_list_hosts():
-    """
-    Coerces given host string to a one-item list
-    """
-    assert merge('badhosts', [], [], {}) == ['badhosts']
-
-
-lazy_role = {'r1': lambda: ['a', 'b']}
-
-def test_lazy_roles():
-    """
-    Roles may be callables returning lists, as well as regular lists
-    """
-    @roles('r1')
-    def command():
-        pass
-    eq_hosts(command, ['a', 'b'], env={'roledefs': lazy_role})
-
-
-#
-# Fabfile finding
-#
-
-class TestFindFabfile(FabricTest):
-    """Test Fabric's fabfile discovery mechanism."""
-    def test_find_fabfile_can_discovery_package(self):
-        """Fabric should be capable of loading a normal package."""
-        path = self.mkfile("__init__.py", "")
-        name = os.path.dirname(path)
-        assert find_fabfile([name,]) is not None
-
-    def test_find_fabfile_can_discovery_package_with_pyc_only(self):
-        """
-        Fabric should be capable of loading a package with __init__.pyc only.
-        """
-        path = self.mkfile("__init__.pyc", "")
-        name = os.path.dirname(path)
-        assert find_fabfile([name,]) is not None
-
-    def test_find_fabfile_should_refuse_fake_package(self):
-        """Fabric should refuse to load a non-package directory."""
-        path = self.mkfile("foo.py", "")
-        name = os.path.dirname(path)
-        assert find_fabfile([name,]) is None
-
-
-#
-# Fabfile loading
-#
-
-def run_load_fabfile(path, sys_path):
-    # Module-esque object
-    fake_module = Fake().has_attr(__dict__={})
-    # Fake __import__
-    importer = Fake(callable=True).returns(fake_module)
-    # Snapshot sys.path for restore
-    orig_path = copy.copy(sys.path)
-    # Update with fake path
-    sys.path = sys_path
-    # Test for side effects
-    load_fabfile(path, importer=importer)
-    eq_(sys.path, sys_path)
-    # Restore
-    sys.path = orig_path
-
-def test_load_fabfile_should_not_remove_real_path_elements():
-    for fabfile_path, sys_dot_path in (
-        # Directory not in path
-        ('subdir/fabfile.py', ['not_subdir']),
-        ('fabfile.py', ['nope']),
-        # Directory in path, but not at front
-        ('subdir/fabfile.py', ['not_subdir', 'subdir']),
-        ('fabfile.py', ['not_subdir', '']),
-        ('fabfile.py', ['not_subdir', '', 'also_not_subdir']),
-        # Directory in path, and at front already
-        ('subdir/fabfile.py', ['subdir']),
-        ('subdir/fabfile.py', ['subdir', 'not_subdir']),
-        ('fabfile.py', ['', 'some_dir', 'some_other_dir']),
-    ):
-            yield run_load_fabfile, fabfile_path, sys_dot_path
-
-
-#
-# Namespacing and new-style tasks
-#
-
-class TestTaskAliases(FabricTest):
-    def test_flat_alias(self):
-        f = fabfile("flat_alias.py")
-        with path_prefix(f):
-            docs, funcs = load_fabfile(f)
-            eq_(len(funcs), 2)
-            ok_("foo" in funcs)
-            ok_("foo_aliased" in funcs)
-
-    def test_nested_alias(self):
-        f = fabfile("nested_alias.py")
-        with path_prefix(f):
-            docs, funcs = load_fabfile(f)
-            ok_("nested" in funcs)
-            eq_(len(funcs["nested"]), 2)
-            ok_("foo" in funcs["nested"])
-            ok_("foo_aliased" in funcs["nested"])
-
-    def test_flat_aliases(self):
-        f = fabfile("flat_aliases.py")
-        with path_prefix(f):
-            docs, funcs = load_fabfile(f)
-            eq_(len(funcs), 3)
-            ok_("foo" in funcs)
-            ok_("foo_aliased" in funcs)
-            ok_("foo_aliased_two" in funcs)
-
-    def test_nested_aliases(self):
-        f = fabfile("nested_aliases.py")
-        with path_prefix(f):
-            docs, funcs = load_fabfile(f)
-            ok_("nested" in funcs)
-            eq_(len(funcs["nested"]), 3)
-            ok_("foo" in funcs["nested"])
-            ok_("foo_aliased" in funcs["nested"])
-            ok_("foo_aliased_two" in funcs["nested"])
-
-
-class TestNamespaces(FabricTest):
-    def setup(self):
-        # Parent class preserves current env
-        super(TestNamespaces, self).setup()
-        # Reset new-style-tests flag so running tests via Fab itself doesn't
-        # muck with it.
-        import fabric.state
-        if 'new_style_tasks' in fabric.state.env:
-            del fabric.state.env['new_style_tasks']
-
-    def test_implicit_discovery(self):
-        """
-        Default to automatically collecting all tasks in a fabfile module
-        """
-        implicit = fabfile("implicit_fabfile.py")
-        with path_prefix(implicit):
-            docs, funcs = load_fabfile(implicit)
-            eq_(len(funcs), 2)
-            ok_("foo" in funcs)
-            ok_("bar" in funcs)
-
-    def test_exception_exclusion(self):
-        """
-        Exception subclasses should not be considered as tasks
-        """
-        exceptions = fabfile("exceptions_fabfile.py")
-        with path_prefix(exceptions):
-            docs, funcs = load_fabfile(exceptions)
-            ok_("some_task" in funcs)
-            ok_("NotATask" not in funcs)
-
-    def test_explicit_discovery(self):
-        """
-        If __all__ is present, only collect the tasks it specifies
-        """
-        explicit = fabfile("explicit_fabfile.py")
-        with path_prefix(explicit):
-            docs, funcs = load_fabfile(explicit)
-            eq_(len(funcs), 1)
-            ok_("foo" in funcs)
-            ok_("bar" not in funcs)
-
-    def test_should_load_decorated_tasks_only_if_one_is_found(self):
-        """
-        If any new-style tasks are found, *only* new-style tasks should load
-        """
-        module = fabfile('decorated_fabfile.py')
-        with path_prefix(module):
-            docs, funcs = load_fabfile(module)
-            eq_(len(funcs), 1)
-            ok_('foo' in funcs)
-
-    def test_class_based_tasks_are_found_with_proper_name(self):
-        """
-        Wrapped new-style tasks should preserve their function names
-        """
-        module = fabfile('decorated_fabfile_with_classbased_task.py')
-        with path_prefix(module):
-            docs, funcs = load_fabfile(module)
-            eq_(len(funcs), 1)
-            ok_('foo' in funcs)
-
-    def test_class_based_tasks_are_found_with_variable_name(self):
-        """
-        A new-style tasks with undefined name attribute should use the instance
-        variable name.
-        """
-        module = fabfile('classbased_task_fabfile.py')
-        with path_prefix(module):
-            docs, funcs = load_fabfile(module)
-            eq_(len(funcs), 1)
-            ok_('foo' in funcs)
-            eq_(funcs['foo'].name, 'foo')
-
-    def test_recursion_steps_into_nontask_modules(self):
-        """
-        Recursive loading will continue through modules with no tasks
-        """
-        module = fabfile('deep')
-        with path_prefix(module):
-            docs, funcs = load_fabfile(module)
-            eq_(len(funcs), 1)
-            ok_('submodule.subsubmodule.deeptask' in _task_names(funcs))
-
-    def test_newstyle_task_presence_skips_classic_task_modules(self):
-        """
-        Classic-task-only modules shouldn't add tasks if any new-style tasks exist
-        """
-        module = fabfile('deep')
-        with path_prefix(module):
-            docs, funcs = load_fabfile(module)
-            eq_(len(funcs), 1)
-            ok_('submodule.classic_task' not in _task_names(funcs))
-
-    def test_task_decorator_plays_well_with_others(self):
-        """
-        @task, when inside @hosts/@roles, should not hide the decorated task.
-        """
-        module = fabfile('decorator_order')
-        with path_prefix(module):
-            docs, funcs = load_fabfile(module)
-            # When broken, crawl() finds None for 'foo' instead.
-            eq_(crawl('foo', funcs), funcs['foo'])
-
-
-#
-# --list output
-#
-
-def eq_output(docstring, format_, expected):
-    return eq_(
-        "\n".join(list_commands(docstring, format_)),
-        expected
-    )
-
-def list_output(module, format_, expected):
-    module = fabfile(module)
-    with path_prefix(module):
-        docstring, tasks = load_fabfile(module)
-        with patched_context(fabric.state, 'commands', tasks):
-            eq_output(docstring, format_, expected)
-
-def test_list_output():
-    lead = ":\n\n    "
-    normal_head = COMMANDS_HEADER + lead
-    nested_head = COMMANDS_HEADER + NESTED_REMINDER + lead
-    for desc, module, format_, expected in (
-        ("shorthand (& with namespacing)", 'deep', 'short', "submodule.subsubmodule.deeptask"),
-        ("normal (& with namespacing)", 'deep', 'normal', normal_head + "submodule.subsubmodule.deeptask"),
-        ("normal (with docstring)", 'docstring', 'normal', normal_head + "foo  Foos!"),
-        ("nested (leaf only)", 'deep', 'nested', nested_head + """submodule:
-        subsubmodule:
-            deeptask"""),
-        ("nested (full)", 'tree', 'nested', nested_head + """build_docs
-    deploy
-    db:
-        migrate
-    system:
-        install_package
-        debian:
-            update_apt"""),
-    ):
-        list_output.description = "--list output: %s" % desc
-        yield list_output, module, format_, expected
-        del list_output.description
-
-
-def name_to_task(name):
-    t = Task()
-    t.name = name
-    return t
-
-def strings_to_tasks(d):
-    ret = {}
-    for key, value in d.iteritems():
-        if isMappingType(value):
-            val = strings_to_tasks(value)
-        else:
-            val = name_to_task(value)
-        ret[key] = val
-    return ret
-
-def test_task_names():
-    for desc, input_, output in (
-        ('top level (single)', {'a': 5}, ['a']),
-        ('top level (multiple, sorting)', {'a': 5, 'b': 6}, ['a', 'b']),
-        ('just nested', {'a': {'b': 5}}, ['a.b']),
-        ('mixed', {'a': 5, 'b': {'c': 6}}, ['a', 'b.c']),
-        ('top level comes before nested', {'z': 5, 'b': {'c': 6}}, ['z', 'b.c']),
-        ('peers sorted equally', {'z': 5, 'b': {'c': 6}, 'd': {'e': 7}}, ['z', 'b.c', 'd.e']),
-        (
-            'complex tree',
-            {
-                'z': 5,
-                'b': {
-                    'c': 6,
-                    'd': {
-                        'e': {
-                            'f': '7'
-                        }
-                    },
-                    'g': 8
-                },
-                'h': 9,
-                'w': {
-                    'y': 10
-                }
-            },
-            ['h', 'z', 'b.c', 'b.g', 'b.d.e.f', 'w.y']
-        ),
-    ):
-        eq_.description = "task name flattening: %s" % desc
-        yield eq_, _task_names(strings_to_tasks(input_)), output
-        del eq_.description
-
-
-def test_crawl():
-    for desc, name, mapping, output in (
-        ("base case", 'a', {'a': 5}, 5),
-        ("one level", 'a.b', {'a': {'b': 5}}, 5),
-        ("deep", 'a.b.c.d.e', {'a': {'b': {'c': {'d': {'e': 5}}}}}, 5),
-        ("full tree", 'a.b.c', {'a': {'b': {'c': 5}, 'd': 6}, 'z': 7}, 5)
-    ):
-        eq_.description = "crawling dotted names: %s" % desc
-        yield eq_, _crawl(name, mapping), output
-        del eq_.description
-
-
-def test_mapping_task_classes():
-    """
-    Task classes implementing the mapping interface shouldn't break --list
-    """
-    list_output('mapping', 'normal', COMMANDS_HEADER + """:\n
-    mapping_task""")
-
-
-def test_default_task_listings():
-    """
-    @task(default=True) should cause task to also load under module's name
-    """
-    for format_, expected in (
-        ('short', """mymodule
-mymodule.long_task_name"""),
-        ('normal', COMMANDS_HEADER + """:\n
-    mymodule
-    mymodule.long_task_name"""),
-        ('nested', COMMANDS_HEADER + NESTED_REMINDER + """:\n
-    mymodule:
-        long_task_name""")
-    ):
-        list_output.description = "Default task --list output: %s" % format_
-        yield list_output, 'default_tasks', format_, expected
-        del list_output.description
-
-
-def test_default_task_loading():
-    """
-    crawl() should return default tasks where found, instead of module objs
-    """
-    docs, tasks = load_fabfile(fabfile('default_tasks'))
-    ok_(isinstance(crawl('mymodule', tasks), Task))
-
-
-def test_aliases_appear_in_fab_list():
-    """
-    --list should include aliases
-    """
-    list_output('nested_alias', 'short', """nested.foo
-nested.foo_aliased""")
diff -Nru fabric-1.14.0/tests/test_network.py fabric-2.5.0/tests/test_network.py
--- fabric-1.14.0/tests/test_network.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/tests/test_network.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,741 +0,0 @@
-from __future__ import with_statement
-
-import sys
-
-from nose.tools import ok_, raises
-from fudge import (Fake, patch_object, with_patched_object, patched_context,
-                   with_fakes)
-
-from fabric.context_managers import settings, hide, show
-from fabric.network import (HostConnectionCache, join_host_strings, normalize,
-                            denormalize, key_filenames, ssh, NetworkError, connect)
-from fabric.state import env, output, _get_system_username
-from fabric.operations import run, sudo, prompt
-from fabric.tasks import execute
-from fabric.api import parallel
-from fabric import utils # for patching
-
-from mock_streams import mock_streams
-from server import (server, RESPONSES, PASSWORDS, CLIENT_PRIVKEY, USER,
-                    CLIENT_PRIVKEY_PASSPHRASE)
-from utils import (FabricTest, aborts, assert_contains, eq_, password_response,
-                   patched_input, support)
-
-
-#
-# Subroutines, e.g. host string normalization
-#
-
-
-class TestNetwork(FabricTest):
-    def test_host_string_normalization(self):
-        username = _get_system_username()
-        for description, input, output_ in (
-            ("Sanity check: equal strings remain equal",
-                'localhost', 'localhost'),
-            ("Empty username is same as get_system_username",
-                'localhost', username + '@localhost'),
-            ("Empty port is same as port 22",
-                'localhost', 'localhost:22'),
-            ("Both username and port tested at once, for kicks",
-                'localhost', username + '@localhost:22'),
-        ):
-            eq_.description = "Host-string normalization: %s" % description
-            yield eq_, normalize(input), normalize(output_)
-            del eq_.description
-
-    def test_normalization_for_ipv6(self):
-        """
-        normalize() will accept IPv6 notation and can separate host and port
-        """
-        username = _get_system_username()
-        for description, input, output_ in (
-            ("Full IPv6 address",
-                '2001:DB8:0:0:0:0:0:1', (username, '2001:DB8:0:0:0:0:0:1', '22')),
-            ("IPv6 address in short form",
-                '2001:DB8::1', (username, '2001:DB8::1', '22')),
-            ("IPv6 localhost",
-                '::1', (username, '::1', '22')),
-            ("Square brackets are required to separate non-standard port from IPv6 address",
-                '[2001:DB8::1]:1222', (username, '2001:DB8::1', '1222')),
-            ("Username and IPv6 address",
-                'user@2001:DB8::1', ('user', '2001:DB8::1', '22')),
-            ("Username and IPv6 address with non-standard port",
-                'user@[2001:DB8::1]:1222', ('user', '2001:DB8::1', '1222')),
-        ):
-            eq_.description = "Host-string IPv6 normalization: %s" % description
-            yield eq_, normalize(input), output_
-            del eq_.description
-
-    def test_normalization_without_port(self):
-        """
-        normalize() and join_host_strings() omit port if omit_port given
-        """
-        eq_(
-            join_host_strings(*normalize('user@localhost', omit_port=True)),
-            'user@localhost'
-        )
-
-    def test_ipv6_host_strings_join(self):
-        """
-        join_host_strings() should use square brackets only for IPv6 and if port is given
-        """
-        eq_(
-            join_host_strings('user', '2001:DB8::1'),
-            'user@2001:DB8::1'
-        )
-        eq_(
-            join_host_strings('user', '2001:DB8::1', '1222'),
-            'user@[2001:DB8::1]:1222'
-        )
-        eq_(
-            join_host_strings('user', '192.168.0.0', '1222'),
-            'user@192.168.0.0:1222'
-        )
-
-    def test_nonword_character_in_username(self):
-        """
-        normalize() will accept non-word characters in the username part
-        """
-        eq_(
-            normalize('user-with-hyphens@someserver.org')[0],
-            'user-with-hyphens'
-        )
-
-    def test_at_symbol_in_username(self):
-        """
-        normalize() should allow '@' in usernames (i.e. last '@' is split char)
-        """
-        parts = normalize('user@example.com@www.example.com')
-        eq_(parts[0], 'user@example.com')
-        eq_(parts[1], 'www.example.com')
-
-    def test_normalization_of_empty_input(self):
-        empties = ('', '', '')
-        for description, input in (
-            ("empty string", ''),
-            ("None", None)
-        ):
-            template = "normalize() returns empty strings for %s input"
-            eq_.description = template % description
-            yield eq_, normalize(input), empties
-            del eq_.description
-
-    def test_host_string_denormalization(self):
-        username = _get_system_username()
-        for description, string1, string2 in (
-            ("Sanity check: equal strings remain equal",
-                'localhost', 'localhost'),
-            ("Empty username is same as get_system_username",
-                'localhost:22', username + '@localhost:22'),
-            ("Empty port is same as port 22",
-                'user@localhost', 'user@localhost:22'),
-            ("Both username and port",
-                'localhost', username + '@localhost:22'),
-            ("IPv6 address",
-                '2001:DB8::1', username + '@[2001:DB8::1]:22'),
-        ):
-            eq_.description = "Host-string denormalization: %s" % description
-            yield eq_, denormalize(string1), denormalize(string2)
-            del eq_.description
-
-    #
-    # Connection caching
-    #
-    @staticmethod
-    @with_fakes
-    def check_connection_calls(host_strings, num_calls):
-        # Clear Fudge call stack
-        # Patch connect() with Fake obj set to expect num_calls calls
-        patched_connect = patch_object('fabric.network', 'connect',
-            Fake('connect', expect_call=True).times_called(num_calls)
-        )
-        try:
-            # Make new cache object
-            cache = HostConnectionCache()
-            # Connect to all connection strings
-            for host_string in host_strings:
-                # Obtain connection from cache, potentially calling connect()
-                cache[host_string]
-        finally:
-            # Restore connect()
-            patched_connect.restore()
-
-    def test_connection_caching(self):
-        for description, host_strings, num_calls in (
-            ("Two different host names, two connections",
-                ('localhost', 'other-system'), 2),
-            ("Same host twice, one connection",
-                ('localhost', 'localhost'), 1),
-            ("Same host twice, different ports, two connections",
-                ('localhost:22', 'localhost:222'), 2),
-            ("Same host twice, different users, two connections",
-                ('user1@localhost', 'user2@localhost'), 2),
-        ):
-            TestNetwork.check_connection_calls.description = description
-            yield TestNetwork.check_connection_calls, host_strings, num_calls
-
-    def test_connection_cache_deletion(self):
-        """
-        HostConnectionCache should delete correctly w/ non-full keys
-        """
-        hcc = HostConnectionCache()
-        fake = Fake('connect', callable=True)
-        with patched_context('fabric.network', 'connect', fake):
-            for host_string in ('hostname', 'user@hostname',
-                'user@hostname:222'):
-                # Prime
-                hcc[host_string]
-                # Test
-                ok_(host_string in hcc)
-                # Delete
-                del hcc[host_string]
-                # Test
-                ok_(host_string not in hcc)
-
-
-    #
-    # Connection loop flow
-    #
-    @server()
-    def test_saved_authentication_returns_client_object(self):
-        cache = HostConnectionCache()
-        assert isinstance(cache[env.host_string], ssh.SSHClient)
-
-    @server()
-    @with_fakes
-    def test_prompts_for_password_without_good_authentication(self):
-        env.password = None
-        with password_response(PASSWORDS[env.user], times_called=1):
-            cache = HostConnectionCache()
-            cache[env.host_string]
-
-
-    @aborts
-    def test_aborts_on_prompt_with_abort_on_prompt(self):
-        """
-        abort_on_prompt=True should abort when prompt() is used
-        """
-        env.abort_on_prompts = True
-        prompt("This will abort")
-
-
-    @server()
-    @aborts
-    def test_aborts_on_password_prompt_with_abort_on_prompt(self):
-        """
-        abort_on_prompt=True should abort when password prompts occur
-        """
-        env.password = None
-        env.abort_on_prompts = True
-        with password_response(PASSWORDS[env.user], times_called=1):
-            cache = HostConnectionCache()
-            cache[env.host_string]
-
-    @with_fakes
-    @raises(NetworkError)
-    def test_connect_does_not_prompt_password_when_ssh_raises_channel_exception(self):
-        def raise_channel_exception_once(*args, **kwargs):
-            if raise_channel_exception_once.should_raise_channel_exception:
-                raise_channel_exception_once.should_raise_channel_exception = False
-                raise ssh.ChannelException(2, 'Connect failed')
-        raise_channel_exception_once.should_raise_channel_exception = True
-
-        def generate_fake_client():
-            fake_client = Fake('SSHClient', allows_any_call=True, expect_call=True)
-            fake_client.provides('connect').calls(raise_channel_exception_once)
-            return fake_client
-
-        fake_ssh = Fake('ssh', allows_any_call=True)
-        fake_ssh.provides('SSHClient').calls(generate_fake_client)
-        # We need the real exceptions here to preserve the inheritence structure
-        fake_ssh.SSHException = ssh.SSHException
-        fake_ssh.ChannelException = ssh.ChannelException
-        patched_connect = patch_object('fabric.network', 'ssh', fake_ssh)
-        patched_password = patch_object('fabric.network', 'prompt_for_password', Fake('prompt_for_password', callable = True).times_called(0))
-        try:
-            connect('user', 'localhost', 22, HostConnectionCache())
-        finally:
-            # Restore ssh
-            patched_connect.restore()
-            patched_password.restore()
-
-
-    @mock_streams('stdout')
-    @server()
-    def test_does_not_abort_with_password_and_host_with_abort_on_prompt(self):
-        """
-        abort_on_prompt=True should not abort if no prompts are needed
-        """
-        env.abort_on_prompts = True
-        env.password = PASSWORDS[env.user]
-        # env.host_string is automatically filled in when using server()
-        run("ls /simple")
-
-
-    @mock_streams('stdout')
-    @server()
-    def test_trailing_newline_line_drop(self):
-        """
-        Trailing newlines shouldn't cause last line to be dropped.
-        """
-        # Multiline output with trailing newline
-        cmd = "ls /"
-        output_string = RESPONSES[cmd]
-        # TODO: fix below lines, duplicates inner workings of tested code
-        prefix = "[%s] out: " % env.host_string
-        expected = prefix + ('\n' + prefix).join(output_string.split('\n'))
-        # Create, tie off thread
-        with settings(show('everything'), hide('running')):
-            result = run(cmd)
-            # Test equivalence of expected, received output
-            eq_(expected, sys.stdout.getvalue())
-            # Also test that the captured value matches, too.
-            eq_(output_string, result)
-
-    @server()
-    def test_sudo_prompt_kills_capturing(self):
-        """
-        Sudo prompts shouldn't screw up output capturing
-        """
-        cmd = "ls /simple"
-        with hide('everything'):
-            eq_(sudo(cmd), RESPONSES[cmd])
-
-    @server()
-    def test_password_memory_on_user_switch(self):
-        """
-        Switching users mid-session should not screw up password memory
-        """
-        def _to_user(user):
-            return join_host_strings(user, env.host, env.port)
-
-        user1 = 'root'
-        user2 = USER
-        with settings(hide('everything'), password=None):
-            # Connect as user1 (thus populating both the fallback and
-            # user-specific caches)
-            with settings(
-                password_response(PASSWORDS[user1]),
-                host_string=_to_user(user1)
-            ):
-                run("ls /simple")
-            # Connect as user2: * First cxn attempt will use fallback cache,
-            # which contains user1's password, and thus fail * Second cxn
-            # attempt will prompt user, and succeed due to mocked p4p * but
-            # will NOT overwrite fallback cache
-            with settings(
-                password_response(PASSWORDS[user2]),
-                host_string=_to_user(user2)
-            ):
-                # Just to trigger connection
-                run("ls /simple")
-            # * Sudo call should use cached user2 password, NOT fallback cache,
-            # and thus succeed. (I.e. p_f_p should NOT be called here.)
-            with settings(
-                password_response('whatever', times_called=0),
-                host_string=_to_user(user2)
-            ):
-                sudo("ls /simple")
-
-    @mock_streams('stderr')
-    @server()
-    def test_password_prompt_displays_host_string(self):
-        """
-        Password prompt lines should include the user/host in question
-        """
-        env.password = None
-        env.no_agent = env.no_keys = True
-        output.everything = False
-        with password_response(PASSWORDS[env.user], silent=False):
-            run("ls /simple")
-        regex = r'^\[%s\] Login password for \'%s\': ' % (env.host_string, env.user)
-        assert_contains(regex, sys.stderr.getvalue())
-
-    @mock_streams('stderr')
-    @server(pubkeys=True)
-    def test_passphrase_prompt_displays_host_string(self):
-        """
-        Passphrase prompt lines should include the user/host in question
-        """
-        env.password = None
-        env.no_agent = env.no_keys = True
-        env.key_filename = CLIENT_PRIVKEY
-        output.everything = False
-        with password_response(CLIENT_PRIVKEY_PASSPHRASE, silent=False):
-            run("ls /simple")
-        regex = r'^\[%s\] Login password for \'%s\': ' % (env.host_string, env.user)
-        assert_contains(regex, sys.stderr.getvalue())
-
-    def test_sudo_prompt_display_passthrough(self):
-        """
-        Sudo prompt should display (via passthrough) when stdout/stderr shown
-        """
-        TestNetwork._prompt_display(True)
-
-    def test_sudo_prompt_display_directly(self):
-        """
-        Sudo prompt should display (manually) when stdout/stderr hidden
-        """
-        TestNetwork._prompt_display(False)
-
-    @staticmethod
-    @mock_streams('both')
-    @server(pubkeys=True, responses={'oneliner': 'result'})
-    def _prompt_display(display_output):
-        env.password = None
-        env.no_agent = env.no_keys = True
-        env.key_filename = CLIENT_PRIVKEY
-        output.output = display_output
-        with password_response(
-            (CLIENT_PRIVKEY_PASSPHRASE, PASSWORDS[env.user]),
-            silent=False
-        ):
-            sudo('oneliner')
-        if display_output:
-            expected = """
-[%(prefix)s] sudo: oneliner
-[%(prefix)s] Login password for '%(user)s': 
-[%(prefix)s] out: sudo password:
-[%(prefix)s] out: Sorry, try again.
-[%(prefix)s] out: sudo password: 
-[%(prefix)s] out: result
-""" % {'prefix': env.host_string, 'user': env.user}
-        else:
-            # Note lack of first sudo prompt (as it's autoresponded to) and of
-            # course the actual result output.
-            expected = """
-[%(prefix)s] sudo: oneliner
-[%(prefix)s] Login password for '%(user)s': 
-[%(prefix)s] out: Sorry, try again.
-[%(prefix)s] out: sudo password: """ % {
-    'prefix': env.host_string,
-    'user': env.user
-}
-        eq_(expected[1:], sys.stdall.getvalue())
-
-    @mock_streams('both')
-    @server(
-        pubkeys=True,
-        responses={'oneliner': 'result', 'twoliner': 'result1\nresult2'}
-    )
-    def test_consecutive_sudos_should_not_have_blank_line(self):
-        """
-        Consecutive sudo() calls should not incur a blank line in-between
-        """
-        env.password = None
-        env.no_agent = env.no_keys = True
-        env.key_filename = CLIENT_PRIVKEY
-        with password_response(
-            (CLIENT_PRIVKEY_PASSPHRASE, PASSWORDS[USER]),
-            silent=False
-        ):
-            sudo('oneliner')
-            sudo('twoliner')
-        expected = """
-[%(prefix)s] sudo: oneliner
-[%(prefix)s] Login password for '%(user)s': 
-[%(prefix)s] out: sudo password:
-[%(prefix)s] out: Sorry, try again.
-[%(prefix)s] out: sudo password: 
-[%(prefix)s] out: result
-[%(prefix)s] sudo: twoliner
-[%(prefix)s] out: sudo password:
-[%(prefix)s] out: result1
-[%(prefix)s] out: result2
-""" % {'prefix': env.host_string, 'user': env.user}
-        eq_(sys.stdall.getvalue(), expected[1:])
-
-    @mock_streams('both')
-    @server(pubkeys=True, responses={'silent': '', 'normal': 'foo'})
-    def test_silent_commands_should_not_have_blank_line(self):
-        """
-        Silent commands should not generate an extra trailing blank line
-
-        After the move to interactive I/O, it was noticed that while run/sudo
-        commands which had non-empty stdout worked normally (consecutive such
-        commands were totally adjacent), those with no stdout (i.e. silent
-        commands like ``test`` or ``mkdir``) resulted in spurious blank lines
-        after the "run:" line. This looks quite ugly in real world scripts.
-        """
-        env.password = None
-        env.no_agent = env.no_keys = True
-        env.key_filename = CLIENT_PRIVKEY
-        with password_response(CLIENT_PRIVKEY_PASSPHRASE, silent=False):
-            run('normal')
-            run('silent')
-            run('normal')
-            with hide('everything'):
-                run('normal')
-                run('silent')
-        expected = """
-[%(prefix)s] run: normal
-[%(prefix)s] Login password for '%(user)s': 
-[%(prefix)s] out: foo
-[%(prefix)s] run: silent
-[%(prefix)s] run: normal
-[%(prefix)s] out: foo
-""" % {'prefix': env.host_string, 'user': env.user}
-        eq_(expected[1:], sys.stdall.getvalue())
-
-    @mock_streams('both')
-    @server(
-        pubkeys=True,
-        responses={'oneliner': 'result', 'twoliner': 'result1\nresult2'}
-    )
-    def test_io_should_print_prefix_if_ouput_prefix_is_true(self):
-        """
-        run/sudo should print [host_string] if env.output_prefix == True
-        """
-        env.password = None
-        env.no_agent = env.no_keys = True
-        env.key_filename = CLIENT_PRIVKEY
-        with password_response(
-            (CLIENT_PRIVKEY_PASSPHRASE, PASSWORDS[USER]),
-            silent=False
-        ):
-            run('oneliner')
-            run('twoliner')
-        expected = """
-[%(prefix)s] run: oneliner
-[%(prefix)s] Login password for '%(user)s': 
-[%(prefix)s] out: result
-[%(prefix)s] run: twoliner
-[%(prefix)s] out: result1
-[%(prefix)s] out: result2
-""" % {'prefix': env.host_string, 'user': env.user}
-        eq_(expected[1:], sys.stdall.getvalue())
-
-    @mock_streams('both')
-    @server(
-        pubkeys=True,
-        responses={'oneliner': 'result', 'twoliner': 'result1\nresult2'}
-    )
-    def test_io_should_not_print_prefix_if_ouput_prefix_is_false(self):
-        """
-        run/sudo shouldn't print [host_string] if env.output_prefix == False
-        """
-        env.password = None
-        env.no_agent = env.no_keys = True
-        env.key_filename = CLIENT_PRIVKEY
-        with password_response(
-            (CLIENT_PRIVKEY_PASSPHRASE, PASSWORDS[USER]),
-            silent=False
-        ):
-            with settings(output_prefix=False):
-                run('oneliner')
-                run('twoliner')
-        expected = """
-[%(prefix)s] run: oneliner
-[%(prefix)s] Login password for '%(user)s': 
-result
-[%(prefix)s] run: twoliner
-result1
-result2
-""" % {'prefix': env.host_string, 'user': env.user}
-        eq_(expected[1:], sys.stdall.getvalue())
-
-    @server()
-    def test_env_host_set_when_host_prompt_used(self):
-        """
-        Ensure env.host is set during host prompting
-        """
-        copied_host_string = str(env.host_string)
-        fake = Fake('raw_input', callable=True).returns(copied_host_string)
-        env.host_string = None
-        env.host = None
-        with settings(hide('everything'), patched_input(fake)):
-            run("ls /")
-        # Ensure it did set host_string back to old value
-        eq_(env.host_string, copied_host_string)
-        # Ensure env.host is correct
-        eq_(env.host, normalize(copied_host_string)[1])
-
-
-def subtask():
-    run("This should never execute")
-
-class TestConnections(FabricTest):
-    @aborts
-    def test_should_abort_when_cannot_connect(self):
-        """
-        By default, connecting to a nonexistent server should abort.
-        """
-        with hide('everything'):
-            execute(subtask, hosts=['nope.nonexistent.com'])
-
-    def test_should_warn_when_skip_bad_hosts_is_True(self):
-        """
-        env.skip_bad_hosts = True => execute() skips current host
-        """
-        with settings(hide('everything'), skip_bad_hosts=True):
-            execute(subtask, hosts=['nope.nonexistent.com'])
-
-    @server()
-    def test_host_not_in_known_hosts_exception(self):
-        """
-        Check reject_unknown_hosts exception
-        """
-        with settings(
-            hide('everything'), password=None, reject_unknown_hosts=True,
-            disable_known_hosts=True, abort_on_prompts=True,
-        ):
-            try:
-                run("echo foo")
-            except NetworkError as exc:
-                exp = "Server '[127.0.0.1]:2200' not found in known_hosts"
-                assert str(exc) == exp, "%s != %s" % (exc, exp)
-            else:
-                raise AssertionError("Host connected without valid "
-                                     "fingerprint.")
-
-
-@parallel
-def parallel_subtask():
-    run("This should never execute")
-
-class TestParallelConnections(FabricTest):
-    @aborts
-    def test_should_abort_when_cannot_connect(self):
-        """
-        By default, connecting to a nonexistent server should abort.
-        """
-        with hide('everything'):
-            execute(parallel_subtask, hosts=['nope.nonexistent.com'])
-
-    def test_should_warn_when_skip_bad_hosts_is_True(self):
-        """
-        env.skip_bad_hosts = True => execute() skips current host
-        """
-        with settings(hide('everything'), skip_bad_hosts=True):
-            execute(parallel_subtask, hosts=['nope.nonexistent.com'])
-
-
-class TestSSHConfig(FabricTest):
-    def env_setup(self):
-        super(TestSSHConfig, self).env_setup()
-        env.use_ssh_config = True
-        env.ssh_config_path = support("ssh_config")
-        # Undo the changes FabricTest makes to env for server support
-        env.user = env.local_user
-        env.port = env.default_port
-
-    def test_global_user_with_default_env(self):
-        """
-        Global User should override default env.user
-        """
-        eq_(normalize("localhost")[0], "satan")
-
-    def test_global_user_with_nondefault_env(self):
-        """
-        Global User should NOT override nondefault env.user
-        """
-        with settings(user="foo"):
-            eq_(normalize("localhost")[0], "foo")
-
-    def test_specific_user_with_default_env(self):
-        """
-        Host-specific User should override default env.user
-        """
-        eq_(normalize("myhost")[0], "neighbor")
-
-    def test_user_vs_host_string_value(self):
-        """
-        SSH-config derived user should NOT override host-string user value
-        """
-        eq_(normalize("myuser@localhost")[0], "myuser")
-        eq_(normalize("myuser@myhost")[0], "myuser")
-
-    def test_global_port_with_default_env(self):
-        """
-        Global Port should override default env.port
-        """
-        eq_(normalize("localhost")[2], "666")
-
-    def test_global_port_with_nondefault_env(self):
-        """
-        Global Port should NOT override nondefault env.port
-        """
-        with settings(port="777", use_ssh_config=False):
-            eq_(normalize("localhost")[2], "777")
-
-    def test_specific_port_with_default_env(self):
-        """
-        Host-specific Port should override default env.port
-        """
-        eq_(normalize("myhost")[2], "664")
-
-    def test_port_vs_host_string_value(self):
-        """
-        SSH-config derived port should NOT override host-string port value
-        """
-        eq_(normalize("localhost:123")[2], "123")
-        eq_(normalize("myhost:123")[2], "123")
-
-    def test_hostname_alias(self):
-        """
-        Hostname setting overrides host string's host value
-        """
-        eq_(normalize("localhost")[1], "localhost")
-        eq_(normalize("myalias")[1], "otherhost")
-
-    @with_patched_object(utils, 'warn', Fake('warn', callable=True,
-        expect_call=True))
-    def test_warns_with_bad_config_file_path(self):
-        # use_ssh_config is already set in our env_setup()
-        with settings(hide('everything'), ssh_config_path="nope_bad_lol"):
-            normalize('foo')
-
-    @server()
-    def test_real_connection(self):
-        """
-        Test-server connection using ssh_config values
-        """
-        with settings(
-            hide('everything'),
-            ssh_config_path=support("testserver_ssh_config"),
-            host_string='testserver',
-        ):
-            ok_(run("ls /simple").succeeded)
-
-
-class TestKeyFilenames(FabricTest):
-    def test_empty_everything(self):
-        """
-        No env.key_filename and no ssh_config = empty list
-        """
-        with settings(use_ssh_config=False):
-            with settings(key_filename=""):
-                eq_(key_filenames(), [])
-            with settings(key_filename=[]):
-                eq_(key_filenames(), [])
-
-    def test_just_env(self):
-        """
-        Valid env.key_filename and no ssh_config = just env
-        """
-        with settings(use_ssh_config=False):
-            with settings(key_filename="mykey"):
-                eq_(key_filenames(), ["mykey"])
-            with settings(key_filename=["foo", "bar"]):
-                eq_(key_filenames(), ["foo", "bar"])
-
-    def test_just_ssh_config(self):
-        """
-        No env.key_filename + valid ssh_config = ssh value
-        """
-        with settings(use_ssh_config=True, ssh_config_path=support("ssh_config")):
-            for val in ["", []]:
-                with settings(key_filename=val):
-                    eq_(key_filenames(), ["foobar.pub"])
-
-    def test_both(self):
-        """
-        Both env.key_filename + valid ssh_config = both show up w/ env var first
-        """
-        with settings(use_ssh_config=True, ssh_config_path=support("ssh_config")):
-            with settings(key_filename="bizbaz.pub"):
-                eq_(key_filenames(), ["bizbaz.pub", "foobar.pub"])
-            with settings(key_filename=["bizbaz.pub", "whatever.pub"]):
-                expected = ["bizbaz.pub", "whatever.pub", "foobar.pub"]
-                eq_(key_filenames(), expected)
diff -Nru fabric-1.14.0/tests/test_operations.py fabric-2.5.0/tests/test_operations.py
--- fabric-1.14.0/tests/test_operations.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/tests/test_operations.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,1140 +0,0 @@
-from __future__ import with_statement
-
-import os
-import re
-import shutil
-import sys
-
-from contextlib import nested
-from StringIO import StringIO
-
-from nose.tools import ok_, raises
-from fudge import patched_context, with_fakes, Fake
-from fudge.inspector import arg as fudge_arg
-from mock_streams import mock_streams
-from paramiko.sftp_client import SFTPClient  # for patching
-
-from fabric.state import env, output
-from fabric.operations import require, prompt, _sudo_prefix, _shell_wrap, \
-    _shell_escape
-from fabric.api import get, put, hide, show, cd, lcd, local, run, sudo, quiet
-from fabric.context_managers import settings
-from fabric.exceptions import CommandTimeout
-
-from fabric.sftp import SFTP
-from fabric.decorators import with_settings
-from utils import (eq_, aborts, assert_contains, eq_contents,
-                   with_patched_input, FabricTest)
-from server import server, FILES
-
-#
-# require()
-#
-
-
-def test_require_single_existing_key():
-    """
-    When given a single existing key, require() throws no exceptions
-    """
-    # 'version' is one of the default values, so we know it'll be there
-    require('version')
-
-
-def test_require_multiple_existing_keys():
-    """
-    When given multiple existing keys, require() throws no exceptions
-    """
-    require('version', 'sudo_prompt')
-
-
-@aborts
-def test_require_single_missing_key():
-    """
-    When given a single non-existent key, require() aborts
-    """
-    require('blah')
-
-
-@aborts
-def test_require_multiple_missing_keys():
-    """
-    When given multiple non-existent keys, require() aborts
-    """
-    require('foo', 'bar')
-
-
-@aborts
-def test_require_mixed_state_keys():
-    """
-    When given mixed-state keys, require() aborts
-    """
-    require('foo', 'version')
-
-
-@mock_streams('stderr')
-def test_require_mixed_state_keys_prints_missing_only():
-    """
-    When given mixed-state keys, require() prints missing keys only
-    """
-    try:
-        require('foo', 'version')
-    except SystemExit:
-        err = sys.stderr.getvalue()
-        assert 'version' not in err
-        assert 'foo' in err
-
-
-@aborts
-def test_require_iterable_provided_by_key():
-    """
-    When given a provided_by iterable value, require() aborts
-    """
-    # 'version' is one of the default values, so we know it'll be there
-    def fake_providing_function():
-        pass
-    require('foo', provided_by=[fake_providing_function])
-
-
-@aborts
-def test_require_noniterable_provided_by_key():
-    """
-    When given a provided_by noniterable value, require() aborts
-    """
-    # 'version' is one of the default values, so we know it'll be there
-    def fake_providing_function():
-        pass
-    require('foo', provided_by=fake_providing_function)
-
-
-@aborts
-def test_require_key_exists_empty_list():
-    """
-    When given a single existing key but the value is an empty list, require()
-    aborts
-    """
-    # 'hosts' is one of the default values, so we know it'll be there
-    require('hosts')
-
-
-@aborts
-@with_settings(foo={})
-def test_require_key_exists_empty_dict():
-    """
-    When given a single existing key but the value is an empty dict, require()
-    aborts
-    """
-    require('foo')
-
-
-@aborts
-@with_settings(foo=())
-def test_require_key_exists_empty_tuple():
-    """
-    When given a single existing key but the value is an empty tuple, require()
-    aborts
-    """
-    require('foo')
-
-
-@aborts
-@with_settings(foo=set())
-def test_require_key_exists_empty_set():
-    """
-    When given a single existing key but the value is an empty set, require()
-    aborts
-    """
-    require('foo')
-
-
-@with_settings(foo=0, bar=False)
-def test_require_key_exists_false_primitive_values():
-    """
-    When given keys that exist with primitive values that evaluate to False,
-    require() throws no exception
-    """
-    require('foo', 'bar')
-
-
-@with_settings(foo=['foo'], bar={'bar': 'bar'}, baz=('baz',), qux=set('qux'))
-def test_require_complex_non_empty_values():
-    """
-    When given keys that exist with non-primitive values that are not empty,
-    require() throws no exception
-    """
-    require('foo', 'bar', 'baz', 'qux')
-
-
-#
-# prompt()
-#
-
-def p(x):
-    sys.stdout.write(x)
-
-
-@mock_streams('stdout')
-@with_patched_input(p)
-def test_prompt_appends_space():
-    """
-    prompt() appends a single space when no default is given
-    """
-    s = "This is my prompt"
-    prompt(s)
-    eq_(sys.stdout.getvalue(), s + ' ')
-
-
-@mock_streams('stdout')
-@with_patched_input(p)
-def test_prompt_with_default():
-    """
-    prompt() appends given default value plus one space on either side
-    """
-    s = "This is my prompt"
-    d = "default!"
-    prompt(s, default=d)
-    eq_(sys.stdout.getvalue(), "%s [%s] " % (s, d))
-
-
-#
-# run()/sudo()
-#
-
-def test_sudo_prefix_with_user():
-    """
-    _sudo_prefix() returns prefix plus -u flag for nonempty user
-    """
-    eq_(
-        _sudo_prefix(user="foo", group=None),
-        "%s -u \"foo\" " % (env.sudo_prefix % env)
-    )
-
-
-def test_sudo_prefix_without_user():
-    """
-    _sudo_prefix() returns standard prefix when user is empty
-    """
-    eq_(_sudo_prefix(user=None, group=None), env.sudo_prefix % env)
-
-
-def test_sudo_prefix_with_group():
-    """
-    _sudo_prefix() returns prefix plus -g flag for nonempty group
-    """
-    eq_(
-        _sudo_prefix(user=None, group="foo"),
-        "%s -g \"foo\" " % (env.sudo_prefix % env)
-    )
-
-
-def test_sudo_prefix_with_user_and_group():
-    """
-    _sudo_prefix() returns prefix plus -u and -g for nonempty user and group
-    """
-    eq_(
-        _sudo_prefix(user="foo", group="bar"),
-        "%s -u \"foo\" -g \"bar\" " % (env.sudo_prefix % env)
-    )
-
-
-@with_settings(use_shell=True)
-def test_shell_wrap():
-    prefix = "prefix"
-    command = "command"
-    for description, shell, sudo_prefix, result in (
-        ("shell=True, sudo_prefix=None",
-            True, None, '%s "%s"' % (env.shell, command)),
-        ("shell=True, sudo_prefix=string",
-            True, prefix, prefix + ' %s "%s"' % (env.shell, command)),
-        ("shell=False, sudo_prefix=None",
-            False, None, command),
-        ("shell=False, sudo_prefix=string",
-            False, prefix, prefix + " " + command),
-    ):
-        eq_.description = "_shell_wrap: %s" % description
-        yield eq_, _shell_wrap(command, shell_escape=True, shell=shell, sudo_prefix=sudo_prefix), result
-        del eq_.description
-
-
-@with_settings(use_shell=True)
-def test_shell_wrap_escapes_command_if_shell_is_true():
-    """
-    _shell_wrap() escapes given command if shell=True
-    """
-    cmd = "cd \"Application Support\""
-    eq_(
-        _shell_wrap(cmd, shell_escape=True, shell=True),
-        '%s "%s"' % (env.shell, _shell_escape(cmd))
-    )
-
-
-@with_settings(use_shell=True)
-def test_shell_wrap_does_not_escape_command_if_shell_is_true_and_shell_escape_is_false():
-    """
-    _shell_wrap() does no escaping if shell=True and shell_escape=False
-    """
-    cmd = "cd \"Application Support\""
-    eq_(
-        _shell_wrap(cmd, shell_escape=False, shell=True),
-        '%s "%s"' % (env.shell, cmd)
-    )
-
-
-def test_shell_wrap_does_not_escape_command_if_shell_is_false():
-    """
-    _shell_wrap() does no escaping if shell=False
-    """
-    cmd = "cd \"Application Support\""
-    eq_(_shell_wrap(cmd, shell_escape=True, shell=False), cmd)
-
-
-def test_shell_escape_escapes_doublequotes():
-    """
-    _shell_escape() escapes double-quotes
-    """
-    cmd = "cd \"Application Support\""
-    eq_(_shell_escape(cmd), 'cd \\"Application Support\\"')
-
-
-def test_shell_escape_escapes_dollar_signs():
-    """
-    _shell_escape() escapes dollar signs
-    """
-    cmd = "cd $HOME"
-    eq_(_shell_escape(cmd), 'cd \$HOME')
-
-
-def test_shell_escape_escapes_backticks():
-    """
-    _shell_escape() escapes backticks
-    """
-    cmd = "touch test.pid && kill `cat test.pid`"
-    eq_(_shell_escape(cmd), "touch test.pid && kill \`cat test.pid\`")
-
-
-class TestCombineStderr(FabricTest):
-    @server()
-    def test_local_none_global_true(self):
-        """
-        combine_stderr: no kwarg => uses global value (True)
-        """
-        output.everything = False
-        r = run("both_streams")
-        # Note: the exact way the streams are jumbled here is an implementation
-        # detail of our fake SSH server and may change in the future.
-        eq_("ssttddoeurtr", r.stdout)
-        eq_(r.stderr, "")
-
-    @server()
-    def test_local_none_global_false(self):
-        """
-        combine_stderr: no kwarg => uses global value (False)
-        """
-        output.everything = False
-        env.combine_stderr = False
-        r = run("both_streams")
-        eq_("stdout", r.stdout)
-        eq_("stderr", r.stderr)
-
-    @server()
-    def test_local_true_global_false(self):
-        """
-        combine_stderr: True kwarg => overrides global False value
-        """
-        output.everything = False
-        env.combine_stderr = False
-        r = run("both_streams", combine_stderr=True)
-        eq_("ssttddoeurtr", r.stdout)
-        eq_(r.stderr, "")
-
-    @server()
-    def test_local_false_global_true(self):
-        """
-        combine_stderr: False kwarg => overrides global True value
-        """
-        output.everything = False
-        env.combine_stderr = True
-        r = run("both_streams", combine_stderr=False)
-        eq_("stdout", r.stdout)
-        eq_("stderr", r.stderr)
-
-
-class TestQuietAndWarnKwargs(FabricTest):
-    @server(responses={'wat': ["", "", 1]})
-    def test_quiet_implies_warn_only(self):
-        # Would raise an exception if warn_only was False
-        eq_(run("wat", quiet=True).failed, True)
-
-    @server()
-    @mock_streams('both')
-    def test_quiet_implies_hide_everything(self):
-        run("ls /", quiet=True)
-        eq_(sys.stdout.getvalue(), "")
-        eq_(sys.stderr.getvalue(), "")
-
-    @server(responses={'hrm': ["", "", 1]})
-    @mock_streams('both')
-    def test_warn_only_is_same_as_settings_warn_only(self):
-        eq_(run("hrm", warn_only=True).failed, True)
-
-    @server()
-    @mock_streams('both')
-    def test_warn_only_does_not_imply_hide_everything(self):
-        run("ls /simple", warn_only=True)
-        assert sys.stdout.getvalue() != ""
-
-
-class TestMultipleOKReturnCodes(FabricTest):
-    @server(responses={'no srsly its ok': ['', '', 1]})
-    def test_expand_to_include_1(self):
-        with settings(quiet(), ok_ret_codes=[0, 1]):
-            eq_(run("no srsly its ok").succeeded, True)
-
-
-slow_server = server(responses={'slow': ['', '', 0, 3]})
-slow = lambda x: slow_server(raises(CommandTimeout)(x))
-
-class TestRun(FabricTest):
-    """
-    @server-using generic run()/sudo() tests
-    """
-    @slow
-    def test_command_timeout_via_env_var(self):
-        env.command_timeout = 2 # timeout after 2 seconds
-        with hide('everything'):
-            run("slow")
-
-    @slow
-    def test_command_timeout_via_kwarg(self):
-        with hide('everything'):
-            run("slow", timeout=2)
-
-    @slow
-    def test_command_timeout_via_env_var_in_sudo(self):
-        env.command_timeout = 2 # timeout after 2 seconds
-        with hide('everything'):
-            sudo("slow")
-
-    @slow
-    def test_command_timeout_via_kwarg_of_sudo(self):
-        with hide('everything'):
-            sudo("slow", timeout=2)
-
-
-#
-# get() and put()
-#
-
-class TestFileTransfers(FabricTest):
-    #
-    # get()
-    #
-    @server(files={'/home/user/.bashrc': 'bash!'}, home='/home/user')
-    def test_get_relative_remote_dir_uses_home(self):
-        """
-        get('relative/path') should use remote $HOME
-        """
-        with hide('everything'):
-            # Another if-it-doesn't-error-out-it-passed test; meh.
-            eq_(get('.bashrc', self.path()), [self.path('.bashrc')])
-
-    @server(files={'/top/%a/%(/%()/%(x)/%(no)s/%(host)s/%d': 'yo'})
-    def test_get_with_format_chars_on_server(self):
-        """
-        get('*') with format symbols (%) on remote paths should not break
-        """
-        remote = '*'
-        with hide('everything'):
-            get(remote, self.path())
-
-    @server()
-    def test_get_single_file(self):
-        """
-        get() with a single non-globbed filename
-        """
-        remote = 'file.txt'
-        local = self.path(remote)
-        with hide('everything'):
-            get(remote, local)
-        eq_contents(local, FILES[remote])
-
-    @server(files={'/base/dir with spaces/file': 'stuff!'})
-    def test_get_file_from_relative_path_with_spaces(self):
-        """
-        get('file') should work when the remote path contains spaces
-        """
-        # from nose.tools import set_trace; set_trace()
-        with hide('everything'):
-            with cd('/base/dir with spaces'):
-                eq_(get('file', self.path()), [self.path('file')])
-
-    @server()
-    def test_get_sibling_globs(self):
-        """
-        get() with globbed files, but no directories
-        """
-        remotes = ['file.txt', 'file2.txt']
-        with hide('everything'):
-            get('file*.txt', self.tmpdir)
-        for remote in remotes:
-            eq_contents(self.path(remote), FILES[remote])
-
-    @server()
-    def test_get_single_file_in_folder(self):
-        """
-        get() a folder containing one file
-        """
-        remote = 'folder/file3.txt'
-        with hide('everything'):
-            get('folder', self.tmpdir)
-        eq_contents(self.path(remote), FILES[remote])
-
-    @server()
-    def test_get_tree(self):
-        """
-        Download entire tree
-        """
-        with hide('everything'):
-            get('tree', self.tmpdir)
-        leaves = filter(lambda x: x[0].startswith('/tree'), FILES.items())
-        for path, contents in leaves:
-            eq_contents(self.path(path[1:]), contents)
-
-    @server()
-    def test_get_tree_with_implicit_local_path(self):
-        """
-        Download entire tree without specifying a local path
-        """
-        dirname = env.host_string.replace(':', '-')
-        try:
-            with hide('everything'):
-                get('tree')
-            leaves = filter(lambda x: x[0].startswith('/tree'), FILES.items())
-            for path, contents in leaves:
-                path = os.path.join(dirname, path[1:])
-                eq_contents(path, contents)
-                os.remove(path)
-        # Cleanup
-        finally:
-            if os.path.exists(dirname):
-                shutil.rmtree(dirname)
-
-    @server()
-    def test_get_absolute_path_should_save_relative(self):
-        """
-        get(/x/y) w/ %(path)s should save y, not x/y
-        """
-        lpath = self.path()
-        ltarget = os.path.join(lpath, "%(path)s")
-        with hide('everything'):
-            get('/tree/subfolder', ltarget)
-        assert self.exists_locally(os.path.join(lpath, 'subfolder'))
-        assert not self.exists_locally(os.path.join(lpath, 'tree/subfolder'))
-
-    @server()
-    def test_path_formatstr_nonrecursively_is_just_filename(self):
-        """
-        get(x/y/z) nonrecursively w/ %(path)s should save y, not y/z
-        """
-        lpath = self.path()
-        ltarget = os.path.join(lpath, "%(path)s")
-        with hide('everything'):
-            get('/tree/subfolder/file3.txt', ltarget)
-        assert self.exists_locally(os.path.join(lpath, 'file3.txt'))
-
-    @server()
-    @mock_streams('stderr')
-    def _invalid_file_obj_situations(self, remote_path):
-        with settings(hide('running'), warn_only=True):
-            get(remote_path, StringIO())
-        assert_contains('is a glob or directory', sys.stderr.getvalue())
-
-    def test_glob_and_file_object_invalid(self):
-        """
-        Remote glob and local file object is invalid
-        """
-        self._invalid_file_obj_situations('/tree/*')
-
-    def test_directory_and_file_object_invalid(self):
-        """
-        Remote directory and local file object is invalid
-        """
-        self._invalid_file_obj_situations('/tree')
-
-    @server()
-    def test_nonexistent_glob_should_not_create_empty_files(self):
-        path = self.path()
-        with settings(hide('everything'), warn_only=True):
-            get('/nope*.txt', path)
-        assert not self.exists_locally(os.path.join(path, 'nope*.txt'))
-
-    @server()
-    def test_nonexistent_glob_raises_error(self):
-        try:
-            with hide('everything', 'aborts'):
-                get('/nope*.txt', self.path())
-        except SystemExit as e:
-            assert 'No such file' in e.message
-        else:
-            assert False
-
-    @server()
-    def test_get_single_file_absolutely(self):
-        """
-        get() a single file, using absolute file path
-        """
-        target = '/etc/apache2/apache2.conf'
-        with hide('everything'):
-            get(target, self.tmpdir)
-        eq_contents(self.path(os.path.basename(target)), FILES[target])
-
-    @server()
-    def test_get_file_with_nonexistent_target(self):
-        """
-        Missing target path on single file download => effectively a rename
-        """
-        local = self.path('otherfile.txt')
-        target = 'file.txt'
-        with hide('everything'):
-            get(target, local)
-        eq_contents(local, FILES[target])
-
-    @server()
-    @mock_streams('stderr')
-    def test_get_file_with_existing_file_target(self):
-        """
-        Clobbering existing local file should overwrite, with warning
-        """
-        local = self.path('target.txt')
-        target = 'file.txt'
-        with open(local, 'w') as fd:
-            fd.write("foo")
-        with hide('stdout', 'running'):
-            get(target, local)
-        assert "%s already exists" % local in sys.stderr.getvalue()
-        eq_contents(local, FILES[target])
-
-    @server()
-    def test_get_file_to_directory(self):
-        """
-        Directory as target path should result in joined pathname
-
-        (Yes, this is duplicated in most of the other tests -- but good to have
-        a default in case those tests change how they work later!)
-        """
-        target = 'file.txt'
-        with hide('everything'):
-            get(target, self.tmpdir)
-        eq_contents(self.path(target), FILES[target])
-
-    @server(port=2200)
-    @server(port=2201)
-    def test_get_from_multiple_servers(self):
-        ports = [2200, 2201]
-        hosts = map(lambda x: '127.0.0.1:%s' % x, ports)
-        with settings(all_hosts=hosts):
-            for port in ports:
-                with settings(
-                    hide('everything'), host_string='127.0.0.1:%s' % port
-                ):
-                    tmp = self.path('')
-                    local_path = os.path.join(tmp, "%(host)s", "%(path)s")
-                    # Top level file
-                    path = 'file.txt'
-                    get(path, local_path)
-                    assert self.exists_locally(os.path.join(
-                        tmp, "127.0.0.1-%s" % port, path
-                    ))
-                    # Nested file
-                    get('tree/subfolder/file3.txt', local_path)
-                    assert self.exists_locally(os.path.join(
-                        tmp, "127.0.0.1-%s" % port, 'file3.txt'
-                    ))
-
-    @server()
-    def test_get_from_empty_directory_uses_cwd(self):
-        """
-        get() expands empty remote arg to remote cwd
-        """
-        with hide('everything'):
-            get('', self.tmpdir)
-        # Spot checks -- though it should've downloaded the entirety of
-        # server.FILES.
-        for x in "file.txt file2.txt tree/file1.txt".split():
-            assert os.path.exists(os.path.join(self.tmpdir, x))
-
-    @server()
-    def _get_to_cwd(self, arg):
-        path = 'file.txt'
-        with hide('everything'):
-            get(path, arg)
-        host_dir = os.path.join(
-            os.getcwd(),
-            env.host_string.replace(':', '-'),
-        )
-        target = os.path.join(host_dir, path)
-        try:
-            assert os.path.exists(target)
-        # Clean up, since we're not using our tmpdir
-        finally:
-            shutil.rmtree(host_dir)
-
-    def test_get_to_empty_string_uses_default_format_string(self):
-        """
-        get() expands empty local arg to local cwd + host + file
-        """
-        self._get_to_cwd('')
-
-    def test_get_to_None_uses_default_format_string(self):
-        """
-        get() expands None local arg to local cwd + host + file
-        """
-        self._get_to_cwd(None)
-
-    @server()
-    def test_get_should_accept_file_like_objects(self):
-        """
-        get()'s local_path arg should take file-like objects too
-        """
-        fake_file = StringIO()
-        target = '/file.txt'
-        with hide('everything'):
-            get(target, fake_file)
-        eq_(fake_file.getvalue(), FILES[target])
-
-    @server()
-    def test_get_interpolation_without_host(self):
-        """
-        local formatting should work w/o use of %(host)s when run on one host
-        """
-        with hide('everything'):
-            tmp = self.path('')
-            # dirname, basename
-            local_path = tmp + "/%(dirname)s/foo/%(basename)s"
-            get('/folder/file3.txt', local_path)
-            assert self.exists_locally(tmp + "foo/file3.txt")
-            # path
-            local_path = tmp + "bar/%(path)s"
-            get('/folder/file3.txt', local_path)
-            assert self.exists_locally(tmp + "bar/file3.txt")
-
-    @server()
-    def test_get_returns_list_of_local_paths(self):
-        """
-        get() should return an iterable of the local files it created.
-        """
-        d = self.path()
-        with hide('everything'):
-            retval = get('tree', d)
-        files = ['file1.txt', 'file2.txt', 'subfolder/file3.txt']
-        eq_(map(lambda x: os.path.join(d, 'tree', x), files), retval)
-
-    @server()
-    def test_get_returns_none_for_stringio(self):
-        """
-        get() should return None if local_path is a StringIO
-        """
-        with hide('everything'):
-            eq_([], get('/file.txt', StringIO()))
-
-    @server()
-    def test_get_return_value_failed_attribute(self):
-        """
-        get()'s return value should indicate any paths which failed to
-        download.
-        """
-        with settings(hide('everything'), warn_only=True):
-            retval = get('/doesnt/exist', self.path())
-        eq_(['/doesnt/exist'], retval.failed)
-        assert not retval.succeeded
-
-    @server()
-    def test_get_should_not_use_windows_slashes_in_remote_paths(self):
-        """
-        sftp.glob() should always use Unix-style slashes.
-        """
-        with hide('everything'):
-            path = "/tree/file1.txt"
-            sftp = SFTP(env.host_string)
-            eq_(sftp.glob(path), [path])
-
-    @server()
-    @with_fakes
-    def test_get_use_sudo(self):
-        """
-        get(use_sudo=True) works by copying to a temporary path, downloading it and then removing it at the end
-        """
-        fake_run = Fake('_run_command', callable=True, expect_call=True).with_matching_args(
-            fudge_arg.startswith('cp -p "/etc/apache2/apache2.conf" "'), True, True, None
-        ).next_call().with_matching_args(
-            fudge_arg.startswith('chown username "'), True, True, None,
-        ).next_call().with_matching_args(
-            fudge_arg.startswith('chmod 400 "'), True, True, None,
-        ).next_call().with_matching_args(
-            fudge_arg.startswith('rm -f "'), True, True, None,
-        )
-        fake_get = Fake('get', callable=True, expect_call=True)
-
-        with hide('everything'):
-            with patched_context('fabric.operations', '_run_command', fake_run):
-                with patched_context(SFTPClient, 'get', fake_get):
-                    retval = get('/etc/apache2/apache2.conf', self.path(), use_sudo=True)
-                    # check that the downloaded file has the same name as the one requested
-                    assert retval[0].endswith('apache2.conf')
-
-    @server()
-    @with_fakes
-    def test_get_use_sudo_temp_dir(self):
-        """
-        get(use_sudo=True, temp_dir="/tmp") works by copying to /tmp/..., downloading it and then removing it at the end
-        """
-        fake_run = Fake('_run_command', callable=True, expect_call=True).with_matching_args(
-            fudge_arg.startswith('cp -p "/etc/apache2/apache2.conf" "/tmp/'), True, True, None,
-        ).next_call().with_matching_args(
-            fudge_arg.startswith('chown username "/tmp/'), True, True, None,
-        ).next_call().with_matching_args(
-            fudge_arg.startswith('chmod 400 "/tmp/'), True, True, None,
-        ).next_call().with_matching_args(
-            fudge_arg.startswith('rm -f "/tmp/'), True, True, None,
-        )
-        fake_get = Fake('get', callable=True, expect_call=True).with_args(
-            fudge_arg.startswith('/tmp/'), fudge_arg.any_value())
-
-        with hide('everything'):
-            with patched_context('fabric.operations', '_run_command', fake_run):
-                with patched_context(SFTPClient, 'get', fake_get):
-                    retval = get('/etc/apache2/apache2.conf', self.path(), use_sudo=True, temp_dir="/tmp")
-                    # check that the downloaded file has the same name as the one requested
-                    assert retval[0].endswith('apache2.conf')
-
-    #
-    # put()
-    #
-
-    @server()
-    def test_put_file_to_existing_directory(self):
-        """
-        put() a single file into an existing remote directory
-        """
-        text = "foo!"
-        local = self.mkfile('foo.txt', text)
-        local2 = self.path('foo2.txt')
-        with hide('everything'):
-            put(local, '/')
-            get('/foo.txt', local2)
-        eq_contents(local2, text)
-
-    @server()
-    def test_put_to_empty_directory_uses_cwd(self):
-        """
-        put() expands empty remote arg to remote cwd
-
-        Not a terribly sharp test -- we just get() with a relative path and are
-        testing to make sure they match up -- but should still suffice.
-        """
-        text = "foo!"
-        local = self.path('foo.txt')
-        local2 = self.path('foo2.txt')
-        with open(local, 'w') as fd:
-            fd.write(text)
-        with hide('everything'):
-            put(local)
-            get('foo.txt', local2)
-        eq_contents(local2, text)
-
-    @server()
-    def test_put_from_empty_directory_uses_cwd(self):
-        """
-        put() expands empty local arg to local cwd
-        """
-        text = 'foo!'
-        # Don't use the current cwd since that's a whole lotta files to upload
-        old_cwd = os.getcwd()
-        os.chdir(self.tmpdir)
-        # Write out file right here
-        with open('file.txt', 'w') as fd:
-            fd.write(text)
-        with hide('everything'):
-            # Put our cwd (which should only contain the file we just created)
-            put('', '/')
-            # Get it back under a new name (noting that when we use a truly
-            # empty put() local call, it makes a directory remotely with the
-            # name of the cwd)
-            remote = os.path.join(os.path.basename(self.tmpdir), 'file.txt')
-            get(remote, 'file2.txt')
-        # Compare for sanity test
-        eq_contents('file2.txt', text)
-        # Restore cwd
-        os.chdir(old_cwd)
-
-    @server()
-    def test_put_should_accept_file_like_objects(self):
-        """
-        put()'s local_path arg should take file-like objects too
-        """
-        local = self.path('whatever')
-        fake_file = StringIO()
-        fake_file.write("testing file-like objects in put()")
-        pointer = fake_file.tell()
-        target = '/new_file.txt'
-        with hide('everything'):
-            put(fake_file, target)
-            get(target, local)
-        eq_contents(local, fake_file.getvalue())
-        # Sanity test of file pointer
-        eq_(pointer, fake_file.tell())
-
-    @server()
-    @raises(ValueError)
-    def test_put_should_raise_exception_for_nonexistent_local_path(self):
-        """
-        put(nonexistent_file) should raise a ValueError
-        """
-        put('thisfiledoesnotexist', '/tmp')
-
-    @server()
-    def test_put_returns_list_of_remote_paths(self):
-        """
-        put() should return an iterable of the remote files it created.
-        """
-        p = 'uploaded.txt'
-        f = self.path(p)
-        with open(f, 'w') as fd:
-            fd.write("contents")
-        with hide('everything'):
-            retval = put(f, p)
-        eq_(retval, [p])
-
-    @server()
-    def test_put_returns_list_of_remote_paths_with_stringio(self):
-        """
-        put() should return a one-item iterable when uploading from a StringIO
-        """
-        f = 'uploaded.txt'
-        with hide('everything'):
-            eq_(put(StringIO('contents'), f), [f])
-
-    @server()
-    def test_put_return_value_failed_attribute(self):
-        """
-        put()'s return value should indicate any paths which failed to upload.
-        """
-        with settings(hide('everything'), warn_only=True):
-            f = StringIO('contents')
-            retval = put(f, '/nonexistent/directory/structure')
-        eq_(["<StringIO>"], retval.failed)
-        assert not retval.succeeded
-
-    @server()
-    def test_put_sends_all_files_with_glob(self):
-        """
-        put() should send all items that match a glob.
-        """
-        paths = ['foo1.txt', 'foo2.txt']
-        glob = 'foo*.txt'
-        remote_directory = '/'
-        for path in paths:
-            self.mkfile(path, 'foo!')
-
-        with hide('everything'):
-            retval = put(self.path(glob), remote_directory)
-        eq_(sorted(retval), sorted([remote_directory + path for path in paths]))
-
-    @server()
-    def test_put_sends_correct_file_with_globbing_off(self):
-        """
-        put() should send a file with a glob pattern in the path, when globbing disabled.
-        """
-        text = "globbed!"
-        local = self.mkfile('foo[bar].txt', text)
-        local2 = self.path('foo2.txt')
-        with hide('everything'):
-            put(local, '/', use_glob=False)
-            get('/foo[bar].txt', local2)
-        eq_contents(local2, text)
-
-    @server()
-    @with_fakes
-    def test_put_use_sudo(self):
-        """
-        put(use_sudo=True) works by uploading a the `local_path` to a temporary path and then moving it to a `remote_path`
-        """
-        fake_run = Fake('_run_command', callable=True, expect_call=True).with_matching_args(
-            fudge_arg.startswith('mv "'), True, True, None,
-        )
-        fake_put = Fake('put', callable=True, expect_call=True)
-
-        local_path = self.mkfile('foobar.txt', "baz")
-        with hide('everything'):
-            with patched_context('fabric.operations', '_run_command', fake_run):
-                with patched_context(SFTPClient, 'put', fake_put):
-                    retval = put(local_path, "/", use_sudo=True)
-                    # check that the downloaded file has the same name as the one requested
-                    assert retval[0].endswith('foobar.txt')
-
-    @server()
-    @with_fakes
-    def test_put_use_sudo_temp_dir(self):
-        """
-        put(use_sudo=True, temp_dir='/tmp/') works by uploading a file to /tmp/ and then moving it to a `remote_path`
-        """
-        # the sha1 hash is the unique filename of the file being downloaded. sha1(<filename>)
-        fake_run = Fake('_run_command', callable=True, expect_call=True).with_matching_args(
-            fudge_arg.startswith('mv "'), True, True, None,
-        )
-        fake_put = Fake('put', callable=True, expect_call=True)
-
-        local_path = self.mkfile('foobar.txt', "baz")
-        with hide('everything'):
-            with patched_context('fabric.operations', '_run_command', fake_run):
-                with patched_context(SFTPClient, 'put', fake_put):
-                    retval = put(local_path, "/", use_sudo=True, temp_dir='/tmp/')
-                    # check that the downloaded file has the same name as the one requested
-                    assert retval[0].endswith('foobar.txt')
-
-
-    #
-    # Interactions with cd()
-    #
-
-    @server()
-    def test_cd_should_apply_to_put(self):
-        """
-        put() should honor env.cwd for relative remote paths
-        """
-        f = 'test.txt'
-        d = '/empty_folder'
-        local = self.path(f)
-        with open(local, 'w') as fd:
-            fd.write('test')
-        with nested(cd(d), hide('everything')):
-            put(local, f)
-        assert self.exists_remotely('%s/%s' % (d, f))
-
-    @server(files={'/tmp/test.txt': 'test'})
-    def test_cd_should_apply_to_get(self):
-        """
-        get() should honor env.cwd for relative remote paths
-        """
-        local = self.path('test.txt')
-        with nested(cd('/tmp'), hide('everything')):
-            get('test.txt', local)
-        assert os.path.exists(local)
-
-    @server()
-    def test_cd_should_not_apply_to_absolute_put(self):
-        """
-        put() should not prepend env.cwd to absolute remote paths
-        """
-        local = self.path('test.txt')
-        with open(local, 'w') as fd:
-            fd.write('test')
-        with nested(cd('/tmp'), hide('everything')):
-            put(local, '/test.txt')
-        assert not self.exists_remotely('/tmp/test.txt')
-        assert self.exists_remotely('/test.txt')
-
-    @server(files={'/test.txt': 'test'})
-    def test_cd_should_not_apply_to_absolute_get(self):
-        """
-        get() should not prepend env.cwd to absolute remote paths
-        """
-        local = self.path('test.txt')
-        with nested(cd('/tmp'), hide('everything')):
-            get('/test.txt', local)
-        assert os.path.exists(local)
-
-    @server()
-    def test_lcd_should_apply_to_put(self):
-        """
-        lcd() should apply to put()'s local_path argument
-        """
-        f = 'lcd_put_test.txt'
-        d = 'subdir'
-        local = self.path(d, f)
-        os.makedirs(os.path.dirname(local))
-        with open(local, 'w') as fd:
-            fd.write("contents")
-        with nested(lcd(self.path(d)), hide('everything')):
-            put(f, '/')
-        assert self.exists_remotely('/%s' % f)
-
-    @server()
-    def test_lcd_should_apply_to_get(self):
-        """
-        lcd() should apply to get()'s local_path argument
-        """
-        d = self.path('subdir')
-        f = 'file.txt'
-        with nested(lcd(d), hide('everything')):
-            get(f, f)
-        assert self.exists_locally(os.path.join(d, f))
-
-    @server()
-    @mock_streams('stdout')
-    def test_stringio_without_name(self):
-        file_obj = StringIO(u'test data')
-        put(file_obj, '/')
-        assert re.search('<file obj>', sys.stdout.getvalue())
-
-    @server()
-    @mock_streams('stdout')
-    def test_stringio_with_name(self):
-        """If a file object (StringIO) has a name attribute, use that in output"""
-        file_obj = StringIO(u'test data')
-        file_obj.name = 'Test StringIO Object'
-        put(file_obj, '/')
-        assert re.search(file_obj.name, sys.stdout.getvalue())
-
-
-#
-# local()
-#
-
-# TODO: figure out how to mock subprocess, if it's even possible.
-# For now, simply test to make sure local() does not raise exceptions with
-# various settings enabled/disabled.
-
-def test_local_output_and_capture():
-    for capture in (True, False):
-        for stdout in (True, False):
-            for stderr in (True, False):
-                hides, shows = ['running'], []
-                if stdout:
-                    hides.append('stdout')
-                else:
-                    shows.append('stdout')
-                if stderr:
-                    hides.append('stderr')
-                else:
-                    shows.append('stderr')
-                with nested(hide(*hides), show(*shows)):
-                    d = "local(): capture: %r, stdout: %r, stderr: %r" % (
-                        capture, stdout, stderr
-                    )
-                    local.description = d
-                    yield local, "echo 'foo' >/dev/null", capture
-                    del local.description
-
-
-class TestRunSudoReturnValues(FabricTest):
-    @server()
-    def test_returns_command_given(self):
-        """
-        run("foo").command == foo
-        """
-        with hide('everything'):
-            eq_(run("ls /").command, "ls /")
-
-    @server()
-    def test_returns_fully_wrapped_command(self):
-        """
-        run("foo").real_command involves env.shell + etc
-        """
-        # FabTest turns use_shell off, we must reactivate it.
-        # Doing so will cause a failure: server's default command list assumes
-        # it's off, we're not testing actual wrapping here so we don't really
-        # care. Just warn_only it.
-        with settings(hide('everything'), warn_only=True, use_shell=True):
-            # Slightly flexible test, we're not testing the actual construction
-            # here, just that this attribute exists.
-            ok_(env.shell in run("ls /").real_command)
diff -Nru fabric-1.14.0/tests/test_parallel.py fabric-2.5.0/tests/test_parallel.py
--- fabric-1.14.0/tests/test_parallel.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/tests/test_parallel.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,93 +0,0 @@
-from __future__ import with_statement
-
-from fabric.api import run, parallel, env, hide, execute, settings
-
-from utils import FabricTest, eq_, aborts, mock_streams
-from server import server, RESPONSES, USER, HOST, PORT
-
-# TODO: move this into test_tasks? meh.
-
-class OhNoesException(Exception): pass
-
-
-class TestParallel(FabricTest):
-    @server()
-    @parallel
-    def test_parallel(self):
-        """
-        Want to do a simple call and respond
-        """
-        env.pool_size = 10
-        cmd = "ls /simple"
-        with hide('everything'):
-            eq_(run(cmd), RESPONSES[cmd])
-
-    @server(port=2200)
-    @server(port=2201)
-    def test_env_host_no_user_or_port(self):
-        """
-        Ensure env.host doesn't get user/port parts when parallel
-        """
-        @parallel
-        def _task():
-            run("ls /simple")
-            assert USER not in env.host
-            assert str(PORT) not in env.host
-
-        host_string = '%s@%s:%%s' % (USER, HOST)
-        with hide('everything'):
-            execute(_task, hosts=[host_string % 2200, host_string % 2201])
-
-    @server(port=2200)
-    @server(port=2201)
-    @aborts
-    def test_parallel_failures_abort(self):
-        with hide('everything'):
-            host1 = '127.0.0.1:2200'
-            host2 = '127.0.0.1:2201'
-
-            @parallel
-            def mytask():
-                run("ls /")
-                if env.host_string == host2:
-                    raise OhNoesException
-            
-            execute(mytask, hosts=[host1, host2])
-
-    @server(port=2200)
-    @server(port=2201)
-    @mock_streams('stderr') # To hide the traceback for now
-    def test_parallel_failures_honor_warn_only(self):
-        with hide('everything'):
-            host1 = '127.0.0.1:2200'
-            host2 = '127.0.0.1:2201'
-
-            @parallel
-            def mytask():
-                run("ls /")
-                if env.host_string == host2:
-                    raise OhNoesException
-
-            with settings(warn_only=True):
-                result = execute(mytask, hosts=[host1, host2])
-            eq_(result[host1], None)
-            assert isinstance(result[host2], OhNoesException)
-
-
-    @server(port=2200)
-    @server(port=2201)
-    def test_parallel_implies_linewise(self):
-        host1 = '127.0.0.1:2200'
-        host2 = '127.0.0.1:2201'
-
-        assert not env.linewise
-
-        @parallel
-        def mytask():
-            run("ls /")
-            return env.linewise
-
-        with hide('everything'):
-            result = execute(mytask, hosts=[host1, host2])
-        eq_(result[host1], True)
-        eq_(result[host2], True)
diff -Nru fabric-1.14.0/tests/test_project.py fabric-2.5.0/tests/test_project.py
--- fabric-1.14.0/tests/test_project.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/tests/test_project.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,177 +0,0 @@
-import unittest
-import os
-
-import fudge
-from fudge.inspector import arg
-
-from fabric.contrib import project
-
-
-class UploadProjectTestCase(unittest.TestCase):
-    """Test case for :func: `fabric.contrib.project.upload_project`."""
-
-    fake_tmp = "testtempfolder"
-
-
-    def setUp(self):
-        fudge.clear_expectations()
-
-        # We need to mock out run, local, and put
-
-        self.fake_run = fudge.Fake('project.run', callable=True)
-        self.patched_run = fudge.patch_object(
-                               project,
-                               'run',
-                               self.fake_run
-                           )
-
-        self.fake_local = fudge.Fake('local', callable=True)
-        self.patched_local = fudge.patch_object(
-                                 project,
-                                 'local',
-                                 self.fake_local
-                             )
-
-        self.fake_put = fudge.Fake('put', callable=True)
-        self.patched_put = fudge.patch_object(
-                               project,
-                               'put',
-                               self.fake_put
-                           )
-
-        # We don't want to create temp folders
-        self.fake_mkdtemp = fudge.Fake(
-                                'mkdtemp',
-                                expect_call=True
-                            ).returns(self.fake_tmp)
-        self.patched_mkdtemp = fudge.patch_object(
-                                   project,
-                                   'mkdtemp',
-                                   self.fake_mkdtemp
-                               )
-
-
-    def tearDown(self):
-        self.patched_run.restore()
-        self.patched_local.restore()
-        self.patched_put.restore()
-
-        fudge.clear_expectations()
-
-
-    @fudge.with_fakes
-    def test_temp_folder_is_used(self):
-        """A unique temp folder is used for creating the archive to upload."""
-
-        # Exercise
-        project.upload_project()
-
-
-    @fudge.with_fakes
-    def test_project_is_archived_locally(self):
-        """The project should be archived locally before being uploaded."""
-
-        # local() is called more than once so we need an extra next_call()
-        # otherwise fudge compares the args to the last call to local()
-        self.fake_local.with_args(arg.startswith("tar -czf")).next_call()
-
-        # Exercise
-        project.upload_project()
-
-
-    @fudge.with_fakes
-    def test_current_directory_is_uploaded_by_default(self):
-        """By default the project uploaded is the current working directory."""
-
-        cwd_path, cwd_name = os.path.split(os.getcwd())
-
-        # local() is called more than once so we need an extra next_call()
-        # otherwise fudge compares the args to the last call to local()
-        self.fake_local.with_args(
-            arg.endswith("-C %s %s" % (cwd_path, cwd_name))
-        ).next_call()
-
-        # Exercise
-        project.upload_project()
-
-
-    @fudge.with_fakes
-    def test_path_to_local_project_can_be_specified(self):
-        """It should be possible to specify which local folder to upload."""
-
-        project_path = "path/to/my/project"
-
-        # local() is called more than once so we need an extra next_call()
-        # otherwise fudge compares the args to the last call to local()
-        self.fake_local.with_args(
-            arg.endswith("-C path/to/my project")
-        ).next_call()
-
-        # Exercise
-        project.upload_project(local_dir=project_path)
-
-
-    @fudge.with_fakes
-    def test_path_to_local_project_no_separator(self):
-        """Local folder can have no path separator (in current directory)."""
-
-        project_path = "testpath"
-
-        # local() is called more than once so we need an extra next_call()
-        # otherwise fudge compares the args to the last call to local()
-        self.fake_local.with_args(
-            arg.endswith("-C . testpath")
-        ).next_call()
-
-        # Exercise
-        project.upload_project(local_dir=project_path)
-
-
-    @fudge.with_fakes
-    def test_path_to_local_project_can_end_in_separator(self):
-        """A local path ending in a separator should be handled correctly."""
-
-        project_path = "path/to/my"
-        base = "project"
-
-        # local() is called more than once so we need an extra next_call()
-        # otherwise fudge compares the args to the last call to local()
-        self.fake_local.with_args(
-            arg.endswith("-C %s %s" % (project_path, base))
-        ).next_call()
-
-        # Exercise
-        project.upload_project(local_dir="%s/%s/" % (project_path, base))
-
-
-    @fudge.with_fakes
-    def test_default_remote_folder_is_home(self):
-        """Project is uploaded to remote home by default."""
-
-        local_dir = "folder"
-
-        # local() is called more than once so we need an extra next_call()
-        # otherwise fudge compares the args to the last call to local()
-        self.fake_put.with_args(
-            "%s/folder.tar.gz" % self.fake_tmp, "folder.tar.gz", use_sudo=False
-        ).next_call()
-
-        # Exercise
-        project.upload_project(local_dir=local_dir)
-
-    @fudge.with_fakes
-    def test_path_to_remote_folder_can_be_specified(self):
-        """It should be possible to specify which local folder to upload to."""
-
-        local_dir = "folder"
-        remote_path = "path/to/remote/folder"
-
-        # local() is called more than once so we need an extra next_call()
-        # otherwise fudge compares the args to the last call to local()
-        self.fake_put.with_args(
-            "%s/folder.tar.gz" % self.fake_tmp, "%s/folder.tar.gz" % remote_path, use_sudo=False
-        ).next_call()
-
-        # Exercise
-        project.upload_project(local_dir=local_dir, remote_dir=remote_path)
-
diff -Nru fabric-1.14.0/tests/test_server.py fabric-2.5.0/tests/test_server.py
--- fabric-1.14.0/tests/test_server.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/tests/test_server.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,99 +0,0 @@
-"""
-Tests for the test server itself.
-
-Not intended to be run by the greater test suite, only by specifically
-targeting it on the command-line. Rationale: not really testing Fabric itself,
-no need to pollute Fab's own test suite. (Yes, if these tests fail, it's likely
-that the Fabric tests using the test server may also have issues, but still.)
-"""
-
-from nose.tools import eq_, ok_
-
-from fabric.network import ssh
-
-from server import FakeSFTPServer
-
-__test__ = False
-
-
-class AttrHolder(object):
-    pass
-
-
-def test_list_folder():
-    for desc, file_map, arg, expected in (
-        (
-            "Single file",
-            {'file.txt': 'contents'},
-            '',
-            ['file.txt']
-        ),
-        (
-            "Single absolute file",
-            {'/file.txt': 'contents'},
-            '/',
-            ['file.txt']
-        ),
-        (
-            "Multiple files",
-            {'file1.txt': 'contents', 'file2.txt': 'contents2'},
-            '',
-            ['file1.txt', 'file2.txt']
-        ),
-        (
-            "Single empty folder",
-            {'folder': None},
-            '',
-            ['folder']
-        ),
-        (
-            "Empty subfolders",
-            {'folder': None, 'folder/subfolder': None},
-            '',
-            ['folder']
-        ),
-        (
-            "Non-empty sub-subfolder",
-            {'folder/subfolder/subfolder2/file.txt': 'contents'},
-            "folder/subfolder/subfolder2",
-            ['file.txt']
-        ),
-        (
-            "Mixed files, folders empty and non-empty, in homedir",
-            {
-                'file.txt': 'contents',
-                'file2.txt': 'contents2',
-                'folder/file3.txt': 'contents3',
-                'empty_folder': None
-            },
-            '',
-            ['file.txt', 'file2.txt', 'folder', 'empty_folder']
-        ),
-        (
-            "Mixed files, folders empty and non-empty, in subdir",
-            {
-                'file.txt': 'contents',
-                'file2.txt': 'contents2',
-                'folder/file3.txt': 'contents3',
-                'folder/subfolder/file4.txt': 'contents4',
-                'empty_folder': None
-            },
-            "folder",
-            ['file3.txt', 'subfolder']
-        ),
-    ):
-        # Pass in fake server obj. (Can't easily clean up API to be more
-        # testable since it's all implementing 'ssh' interface stuff.)
-        server = AttrHolder()
-        server.files = file_map
-        interface = FakeSFTPServer(server)
-        results = interface.list_folder(arg)
-        # In this particular suite of tests, all results should be a file list,
-        # not "no files found"
-        ok_(results != ssh.SFTP_NO_SUCH_FILE)
-        # Grab filename from SFTPAttribute objects in result
-        output = map(lambda x: x.filename, results)
-        # Yield test generator
-        eq_.description = "list_folder: %s" % desc
-        yield eq_, set(expected), set(output)
-        del eq_.description
diff -Nru fabric-1.14.0/tests/test_state.py fabric-2.5.0/tests/test_state.py
--- fabric-1.14.0/tests/test_state.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/tests/test_state.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,52 +0,0 @@
-from nose.tools import eq_
-
-from fabric.state import _AliasDict
-
-
-def test_dict_aliasing():
-    """
-    Assigning values to aliases updates aliased keys
-    """
-    ad = _AliasDict(
-        {'bar': False, 'biz': True, 'baz': False},
-        aliases={'foo': ['bar', 'biz', 'baz']}
-    )
-    # Before
-    eq_(ad['bar'], False)
-    eq_(ad['biz'], True)
-    eq_(ad['baz'], False)
-    # Change
-    ad['foo'] = True
-    # After
-    eq_(ad['bar'], True)
-    eq_(ad['biz'], True)
-    eq_(ad['baz'], True)
-
-
-def test_nested_dict_aliasing():
-    """
-    Aliases can be nested
-    """
-    ad = _AliasDict(
-        {'bar': False, 'biz': True},
-        aliases={'foo': ['bar', 'nested'], 'nested': ['biz']}
-    )
-    # Before
-    eq_(ad['bar'], False)
-    eq_(ad['biz'], True)
-    # Change
-    ad['foo'] = True
-    # After
-    eq_(ad['bar'], True)
-    eq_(ad['biz'], True)
-
-
-def test_dict_alias_expansion():
-    """
-    Alias expansion
-    """
-    ad = _AliasDict(
-        {'bar': False, 'biz': True},
-        aliases={'foo': ['bar', 'nested'], 'nested': ['biz']}
-    )
-    eq_(ad.expand_aliases(['foo']), ['bar', 'biz'])
diff -Nru fabric-1.14.0/tests/test_tasks.py fabric-2.5.0/tests/test_tasks.py
--- fabric-1.14.0/tests/test_tasks.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/tests/test_tasks.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,618 +0,0 @@
-from __future__ import with_statement
-
-from fudge import Fake, patched_context, with_fakes
-import unittest
-from nose.tools import raises, ok_
-import random
-import sys
-
-import fabric
-from fabric.tasks import WrappedCallableTask, execute, Task, get_task_details
-from fabric.main import display_command
-from fabric.api import run, env, settings, hosts, roles, hide, parallel, task, runs_once, serial
-from fabric.exceptions import NetworkError
-
-from utils import eq_, FabricTest, aborts, mock_streams, support
-from server import server
-
-
-def test_base_task_provides_undefined_name():
-    task = Task()
-    eq_("undefined", task.name)
-
-@raises(NotImplementedError)
-def test_base_task_raises_exception_on_call_to_run():
-    task = Task()
-    task.run()
-
-class TestWrappedCallableTask(unittest.TestCase):
-    def test_passes_unused_args_to_parent(self):
-        args = [i for i in range(random.randint(1, 10))]
-
-        def foo(): pass
-        try:
-            WrappedCallableTask(foo, *args)
-        except TypeError:
-            msg = "__init__ raised a TypeError, meaning args weren't handled"
-            self.fail(msg)
-
-    def test_passes_unused_kwargs_to_parent(self):
-        random_range = range(random.randint(1, 10))
-        kwargs = dict([("key_%s" % i, i) for i in random_range])
-
-        def foo(): pass
-        try:
-            WrappedCallableTask(foo, **kwargs)
-        except TypeError:
-            self.fail(
-                "__init__ raised a TypeError, meaning kwargs weren't handled")
-
-    def test_allows_any_number_of_args(self):
-        args = [i for i in range(random.randint(0, 10))]
-        def foo(): pass
-        WrappedCallableTask(foo, *args)
-
-    def test_allows_any_number_of_kwargs(self):
-        kwargs = dict([("key%d" % i, i) for i in range(random.randint(0, 10))])
-        def foo(): pass
-        WrappedCallableTask(foo, **kwargs)
-
-    def test_run_is_wrapped_callable(self):
-        def foo(): pass
-        task = WrappedCallableTask(foo)
-        eq_(task.wrapped, foo)
-
-    def test_name_is_the_name_of_the_wrapped_callable(self):
-        def foo(): pass
-        foo.__name__ = "random_name_%d" % random.randint(1000, 2000)
-        task = WrappedCallableTask(foo)
-        eq_(task.name, foo.__name__)
-
-    def test_name_can_be_overridden(self):
-        def foo(): pass
-        eq_(WrappedCallableTask(foo).name, 'foo')
-        eq_(WrappedCallableTask(foo, name='notfoo').name, 'notfoo')
-
-    def test_reads_double_under_doc_from_callable(self):
-        def foo(): pass
-        foo.__doc__ = "Some random __doc__: %d" % random.randint(1000, 2000)
-        task = WrappedCallableTask(foo)
-        eq_(task.__doc__, foo.__doc__)
-
-    def test_dispatches_to_wrapped_callable_on_run(self):
-        random_value = "some random value %d" % random.randint(1000, 2000)
-        def foo(): return random_value
-        task = WrappedCallableTask(foo)
-        eq_(random_value, task())
-
-    def test_passes_all_regular_args_to_run(self):
-        def foo(*args): return args
-        random_args = tuple(
-            [random.randint(1000, 2000) for i in range(random.randint(1, 5))]
-        )
-        task = WrappedCallableTask(foo)
-        eq_(random_args, task(*random_args))
-
-    def test_passes_all_keyword_args_to_run(self):
-        def foo(**kwargs): return kwargs
-        random_kwargs = {}
-        for i in range(random.randint(1, 5)):
-            random_key = ("foo", "bar", "baz", "foobar", "barfoo")[i]
-            random_kwargs[random_key] = random.randint(1000, 2000)
-        task = WrappedCallableTask(foo)
-        eq_(random_kwargs, task(**random_kwargs))
-
-    def test_calling_the_object_is_the_same_as_run(self):
-        random_return = random.randint(1000, 2000)
-        def foo(): return random_return
-        task = WrappedCallableTask(foo)
-        eq_(task(), task.run())
-
-
-class TestTask(unittest.TestCase):
-    def test_takes_an_alias_kwarg_and_wraps_it_in_aliases_list(self):
-        random_alias = "alias_%d" % random.randint(100, 200)
-        task = Task(alias=random_alias)
-        self.assertTrue(random_alias in task.aliases)
-
-    def test_aliases_are_set_based_on_provided_aliases(self):
-        aliases = ["a_%d" % i for i in range(random.randint(1, 10))]
-        task = Task(aliases=aliases)
-        self.assertTrue(all([a in task.aliases for a in aliases]))
-
-    def test_aliases_are_None_by_default(self):
-        task = Task()
-        self.assertTrue(task.aliases is None)
-
-
-# Reminder: decorator syntax, e.g.:
-#     @foo
-#     def bar():...
-#
-# is semantically equivalent to:
-#     def bar():...
-#     bar = foo(bar)
-#
-# this simplifies testing :)
-
-def test_decorator_incompatibility_on_task():
-    from fabric.decorators import task, hosts, runs_once, roles
-    def foo(): return "foo"
-    foo = task(foo)
-
-    # since we aren't setting foo to be the newly decorated thing, its cool
-    hosts('me@localhost')(foo)
-    runs_once(foo)
-    roles('www')(foo)
-
-def test_decorator_closure_hiding():
-    """
-    @task should not accidentally destroy decorated attributes from @hosts/etc
-    """
-    from fabric.decorators import task, hosts
-    def foo():
-        print(env.host_string)
-    foo = task(hosts("me@localhost")(foo))
-    eq_(["me@localhost"], foo.hosts)
-
-
-
-#
-# execute()
-#
-
-def dict_contains(superset, subset):
-    """
-    Assert that all key/val pairs in dict 'subset' also exist in 'superset'
-    """
-    for key, value in subset.iteritems():
-        ok_(key in superset)
-        eq_(superset[key], value)
-
-
-class TestExecute(FabricTest):
-    @with_fakes
-    def test_calls_task_function_objects(self):
-        """
-        should execute the passed-in function object
-        """
-        execute(Fake(callable=True, expect_call=True))
-
-    @with_fakes
-    def test_should_look_up_task_name(self):
-        """
-        should also be able to handle task name strings
-        """
-        name = 'task1'
-        commands = {name: Fake(callable=True, expect_call=True)}
-        with patched_context(fabric.state, 'commands', commands):
-            execute(name)
-
-    @with_fakes
-    def test_should_handle_name_of_Task_object(self):
-        """
-        handle corner case of Task object referrred to by name
-        """
-        name = 'task2'
-        class MyTask(Task):
-            run = Fake(callable=True, expect_call=True)
-        mytask = MyTask()
-        mytask.name = name
-        commands = {name: mytask}
-        with patched_context(fabric.state, 'commands', commands):
-            execute(name)
-
-    @aborts
-    def test_should_abort_if_task_name_not_found(self):
-        """
-        should abort if given an invalid task name
-        """
-        execute('thisisnotavalidtaskname')
-
-    def test_should_not_abort_if_task_name_not_found_with_skip(self):
-        """
-        should not abort if given an invalid task name
-        and skip_unknown_tasks in env
-        """
-        env.skip_unknown_tasks = True
-        execute('thisisnotavalidtaskname')
-        del env['skip_unknown_tasks']
-
-    @with_fakes
-    def test_should_pass_through_args_kwargs(self):
-        """
-        should pass in any additional args, kwargs to the given task.
-        """
-        task = (
-            Fake(callable=True, expect_call=True)
-            .with_args('foo', biz='baz')
-        )
-        execute(task, 'foo', biz='baz')
-
-    @with_fakes
-    def test_should_honor_hosts_kwarg(self):
-        """
-        should use hosts kwarg to set run list
-        """
-        # Make two full copies of a host list
-        hostlist = ['a', 'b', 'c']
-        hosts = hostlist[:]
-        # Side-effect which asserts the value of env.host_string when it runs
-        def host_string():
-            eq_(env.host_string, hostlist.pop(0))
-        task = Fake(callable=True, expect_call=True).calls(host_string)
-        with hide('everything'):
-            execute(task, hosts=hosts)
-
-    def test_should_honor_hosts_decorator(self):
-        """
-        should honor @hosts on passed-in task objects
-        """
-        # Make two full copies of a host list
-        hostlist = ['a', 'b', 'c']
-        @hosts(*hostlist[:])
-        def task():
-            eq_(env.host_string, hostlist.pop(0))
-        with hide('running'):
-            execute(task)
-
-    def test_should_honor_roles_decorator(self):
-        """
-        should honor @roles on passed-in task objects
-        """
-        # Make two full copies of a host list
-        roledefs = {'role1': ['a', 'b', 'c']}
-        role_copy = roledefs['role1'][:]
-        @roles('role1')
-        def task():
-            eq_(env.host_string, role_copy.pop(0))
-        with settings(hide('running'), roledefs=roledefs):
-            execute(task)
-
-    @with_fakes
-    def test_should_set_env_command_to_string_arg(self):
-        """
-        should set env.command to any string arg, if given
-        """
-        name = "foo"
-        def command():
-            eq_(env.command, name)
-        task = Fake(callable=True, expect_call=True).calls(command)
-        with patched_context(fabric.state, 'commands', {name: task}):
-            execute(name)
-
-    @with_fakes
-    def test_should_set_env_command_to_name_attr(self):
-        """
-        should set env.command to TaskSubclass.name if possible
-        """
-        name = "foo"
-        def command():
-            eq_(env.command, name)
-        task = (
-            Fake(callable=True, expect_call=True)
-            .has_attr(name=name)
-            .calls(command)
-        )
-        execute(task)
-
-    @with_fakes
-    def test_should_set_all_hosts(self):
-        """
-        should set env.all_hosts to its derived host list
-        """
-        hosts = ['a', 'b']
-        roledefs = {'r1': ['c', 'd']}
-        roles = ['r1']
-        exclude_hosts = ['a']
-        def command():
-            eq_(set(env.all_hosts), set(['b', 'c', 'd']))
-        task = Fake(callable=True, expect_call=True).calls(command)
-        with settings(hide('everything'), roledefs=roledefs):
-            execute(
-                task, hosts=hosts, roles=roles, exclude_hosts=exclude_hosts
-            )
-
-    @mock_streams('stdout')
-    def test_should_print_executing_line_per_host(self):
-        """
-        should print "Executing" line once per host
-        """
-        def task():
-            pass
-        execute(task, hosts=['host1', 'host2'])
-        eq_(sys.stdout.getvalue(), """[host1] Executing task 'task'
-[host2] Executing task 'task'
-""")
-
-    @mock_streams('stdout')
-    def test_should_not_print_executing_line_for_singletons(self):
-        """
-        should not print "Executing" line for non-networked tasks
-        """
-        def task():
-            pass
-        with settings(hosts=[]): # protect against really odd test bleed :(
-            execute(task)
-        eq_(sys.stdout.getvalue(), "")
-
-    def test_should_return_dict_for_base_case(self):
-        """
-        Non-network-related tasks should return a dict w/ special key
-        """
-        def task():
-            return "foo"
-        eq_(execute(task), {'<local-only>': 'foo'})
-
-    @server(port=2200)
-    @server(port=2201)
-    def test_should_return_dict_for_serial_use_case(self):
-        """
-        Networked but serial tasks should return per-host-string dict
-        """
-        ports = [2200, 2201]
-        hosts = map(lambda x: '127.0.0.1:%s' % x, ports)
-        def task():
-            run("ls /simple")
-            return "foo"
-        with hide('everything'):
-            eq_(execute(task, hosts=hosts), {
-                '127.0.0.1:2200': 'foo',
-                '127.0.0.1:2201': 'foo'
-            })
-
-    @server()
-    def test_should_preserve_None_for_non_returning_tasks(self):
-        """
-        Tasks which don't return anything should still show up in the dict
-        """
-        def local_task():
-            pass
-        def remote_task():
-            with hide('everything'):
-                run("ls /simple")
-        eq_(execute(local_task), {'<local-only>': None})
-        with hide('everything'):
-            eq_(
-                execute(remote_task, hosts=[env.host_string]),
-                {env.host_string: None}
-            )
-
-    def test_should_use_sentinel_for_tasks_that_errored(self):
-        """
-        Tasks which errored but didn't abort should contain an eg NetworkError
-        """
-        def task():
-            run("whoops")
-        host_string = 'localhost:1234'
-        with settings(hide('everything'), skip_bad_hosts=True):
-            retval = execute(task, hosts=[host_string])
-        assert isinstance(retval[host_string], NetworkError)
-
-    @server(port=2200)
-    @server(port=2201)
-    def test_parallel_return_values(self):
-        """
-        Parallel mode should still return values as in serial mode
-        """
-        @parallel
-        @hosts('127.0.0.1:2200', '127.0.0.1:2201')
-        def task():
-            run("ls /simple")
-            return env.host_string.split(':')[1]
-        with hide('everything'):
-            retval = execute(task)
-        eq_(retval, {'127.0.0.1:2200': '2200', '127.0.0.1:2201': '2201'})
-
-    @with_fakes
-    def test_should_work_with_Task_subclasses(self):
-        """
-        should work for Task subclasses, not just WrappedCallableTask
-        """
-        class MyTask(Task):
-            name = "mytask"
-            run = Fake(callable=True, expect_call=True)
-        mytask = MyTask()
-        execute(mytask)
-
-    @server(port=2200)
-    @server(port=2201)
-    def test_nested_execution_with_explicit_ports(self):
-        """
-        nested executions should work with defined ports
-        """
-
-        def expect_host_string_port():
-            eq_(env.port, '2201')
-            return "bar"
-
-        def expect_env_port():
-            eq_(env.port, '2202')
-
-        def expect_per_host_config_port():
-            eq_(env.port, '664')
-            run = execute(expect_default_config_port, hosts=['some_host'])
-            return run['some_host']
-
-        def expect_default_config_port():
-            # uses `Host *` in ssh_config
-            eq_(env.port, '666')
-            return "bar"
-
-        def main_task():
-            eq_(env.port, '2200')
-
-            execute(expect_host_string_port, hosts=['localhost:2201'])
-
-            with settings(port='2202'):
-                execute(expect_env_port, hosts=['localhost'])
-
-            with settings(
-                use_ssh_config=True,
-                ssh_config_path=support("ssh_config")
-            ):
-                run = execute(expect_per_host_config_port, hosts='myhost')
-
-            return run['myhost']
-
-        run = execute(main_task, hosts=['localhost:2200'])
-
-        eq_(run['localhost:2200'], 'bar')
-
-
-class TestExecuteEnvInteractions(FabricTest):
-    def set_network(self):
-        # Don't update env.host/host_string/etc
-        pass
-
-    @server(port=2200)
-    @server(port=2201)
-    def test_should_not_mutate_its_own_env_vars(self):
-        """
-        internal env changes should not bleed out, but task env changes should
-        """
-        # Task that uses a handful of features which involve env vars
-        @parallel
-        @hosts('username@127.0.0.1:2200', 'username@127.0.0.1:2201')
-        def mytask():
-            run("ls /simple")
-        # Pre-assertions
-        assertions = {
-            'parallel': False,
-            'all_hosts': [],
-            'host': None,
-            'hosts': [],
-            'host_string': None
-        }
-        for key, value in assertions.items():
-            eq_(env[key], value)
-        # Run
-        with hide('everything'):
-            result = execute(mytask)
-        eq_(len(result), 2)
-        # Post-assertions
-        for key, value in assertions.items():
-            eq_(env[key], value)
-
-    @server()
-    def test_should_allow_task_to_modify_env_vars(self):
-        @hosts('username@127.0.0.1:2200')
-        def mytask():
-            run("ls /simple")
-            env.foo = "bar"
-        with hide('everything'):
-            execute(mytask)
-        eq_(env.foo, "bar")
-        eq_(env.host_string, None)
-
-
-class TestTaskDetails(unittest.TestCase):
-    def test_old_style_task_with_default_args(self):
-        """
-        __details__() should print docstr for old style task methods with default args
-        """
-        def task_old_style(arg1, arg2, arg3=None, arg4='yes'):
-            '''Docstring'''
-        details = get_task_details(task_old_style)
-        eq_("Docstring\n"
-            "Arguments: arg1, arg2, arg3=None, arg4='yes'",
-            details)
-
-    def test_old_style_task_without_default_args(self):
-        """
-        __details__() should print docstr for old style task methods without default args
-        """
-
-        def task_old_style(arg1, arg2):
-            '''Docstring'''
-        details = get_task_details(task_old_style)
-        eq_("Docstring\n"
-            "Arguments: arg1, arg2",
-            details)
-
-    def test_old_style_task_without_args(self):
-        """
-        __details__() should print docstr for old style task methods without args
-        """
-
-        def task_old_style():
-            '''Docstring'''
-        details = get_task_details(task_old_style)
-        eq_("Docstring\n"
-            "Arguments: ",
-            details)
-
-    def test_decorated_task(self):
-        """
-        __details__() should print docstr for method with any number and order of decorations
-        """
-        expected = "\n".join([
-            "Docstring",
-            "Arguments: arg1",
-            ])
-
-        @task
-        def decorated_task(arg1):
-            '''Docstring'''
-
-        actual = decorated_task.__details__()
-        eq_(expected, actual)
-
-        @runs_once
-        @task
-        def decorated_task1(arg1):
-            '''Docstring'''
-
-        actual = decorated_task1.__details__()
-        eq_(expected, actual)
-
-        @runs_once
-        @serial
-        @task
-        def decorated_task2(arg1):
-            '''Docstring'''
-
-        actual = decorated_task2.__details__()
-        eq_(expected, actual)
-
-    def test_subclassed_task(self):
-        """
-        __details__() should print docstr for subclassed task methods with args
-        """
-
-        class SpecificTask(Task):
-            def run(self, arg1, arg2, arg3):
-                '''Docstring'''
-        eq_("Docstring\n"
-            "Arguments: self, arg1, arg2, arg3",
-            SpecificTask().__details__())
-
-    @mock_streams('stdout')
-    def test_multiline_docstring_indented_correctly(self):
-        """
-        display_command() should properly indent docstr for old style task methods
-        """
-
-        def mytask(arg1):
-            """
-            This is a multi line docstring.
-
-            For reals.
-            """
-        try:
-            with patched_context(fabric.state, 'commands', {'mytask': mytask}):
-                display_command('mytask')
-        except SystemExit: # ugh
-            pass
-        eq_(
-            sys.stdout.getvalue(),
-"""Displaying detailed information for task 'mytask':
-
-    This is a multi line docstring.
-    
-    For reals.
-    
-    Arguments: arg1
-
-"""
-        )
diff -Nru fabric-1.14.0/tests/test_utils.py fabric-2.5.0/tests/test_utils.py
--- fabric-1.14.0/tests/test_utils.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/tests/test_utils.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,351 +0,0 @@
-from __future__ import with_statement
-
-import sys
-from unittest import TestCase
-
-from fudge import Fake, patched_context, with_fakes
-from fudge.patcher import with_patched_object
-from nose.tools import eq_, raises
-
-from fabric.state import output
-from fabric.utils import warn, indent, abort, puts, fastprint, error, RingBuffer
-from fabric import utils  # For patching
-from fabric.api import local, quiet
-from fabric.context_managers import settings, hide
-from fabric.colors import magenta, red
-from utils import mock_streams, aborts, FabricTest, assert_contains, \
-    assert_not_contains
-
-
-@mock_streams('stderr')
-@with_patched_object(output, 'warnings', True)
-def test_warn():
-    """
-    warn() should print 'Warning' plus given text
-    """
-    warn("Test")
-    eq_("\nWarning: Test\n\n", sys.stderr.getvalue())
-
-
-def test_indent():
-    for description, input_, output_ in (
-        ("Sanity check: 1 line string",
-            'Test', '    Test'),
-        ("List of strings turns in to strings joined by \\n",
-            ["Test", "Test"], '    Test\n    Test'),
-    ):
-        eq_.description = "indent(): %s" % description
-        yield eq_, indent(input_), output_
-        del eq_.description
-
-
-def test_indent_with_strip():
-    for description, input_, output_ in (
-        ("Sanity check: 1 line string",
-            indent('Test', strip=True), '    Test'),
-        ("Check list of strings",
-            indent(["Test", "Test"], strip=True), '    Test\n    Test'),
-        ("Check list of strings",
-            indent(["        Test", "        Test"], strip=True),
-            '    Test\n    Test'),
-    ):
-        eq_.description = "indent(strip=True): %s" % description
-        yield eq_, input_, output_
-        del eq_.description
-
-
-@aborts
-def test_abort():
-    """
-    abort() should raise SystemExit
-    """
-    abort("Test")
-
-class TestException(Exception):
-    pass
-
-@raises(TestException)
-def test_abort_with_exception():
-    """
-    abort() should raise a provided exception
-    """
-    with settings(abort_exception=TestException):
-        abort("Test")
-
-@mock_streams('stderr')
-@with_patched_object(output, 'aborts', True)
-def test_abort_message():
-    """
-    abort() should print 'Fatal error' plus exception value
-    """
-    try:
-        abort("Test")
-    except SystemExit:
-        pass
-    result = sys.stderr.getvalue()
-    eq_("\nFatal error: Test\n\nAborting.\n", result)
-
-def test_abort_message_only_printed_once():
-    """
-    abort()'s SystemExit should not cause a reprint of the error message
-    """
-    # No good way to test the implicit stderr print which sys.exit/SystemExit
-    # perform when they are allowed to bubble all the way to the top. So, we
-    # invoke a subprocess and look at its stderr instead.
-    with quiet():
-        result = local("python -m fabric.__main__ -f tests/support/aborts.py kaboom", capture=True)
-    # When error in #1318 is present, this has an extra "It burns!" at end of
-    # stderr string.
-    eq_(result.stderr, "Fatal error: It burns!\n\nAborting.")
-
-@mock_streams('stderr')
-@with_patched_object(output, 'aborts', True)
-def test_abort_exception_contains_separate_message_and_code():
-    """
-    abort()'s SystemExit contains distinct .code/.message attributes.
-    """
-    # Re #1318 / #1213
-    try:
-        abort("Test")
-    except SystemExit as e:
-        eq_(e.message, "Test")
-        eq_(e.code, 1)
-
-@mock_streams('stdout')
-def test_puts_with_user_output_on():
-    """
-    puts() should print input to sys.stdout if "user" output level is on
-    """
-    s = "string!"
-    output.user = True
-    puts(s, show_prefix=False)
-    eq_(sys.stdout.getvalue(), s + "\n")
-
-@mock_streams('stdout')
-def test_puts_with_unicode_output():
-    """
-    puts() should print unicode input
-    """
-    s = u"string!"
-    output.user = True
-    puts(s, show_prefix=False)
-    eq_(sys.stdout.getvalue(), s + "\n")
-
-
-@mock_streams('stdout')
-def test_puts_with_encoding_type_none_output():
-    """
-    puts() should print unicode output without a stream encoding
-    """
-    s = u"string!"
-    output.user = True
-    sys.stdout.encoding = None
-    puts(s, show_prefix=False)
-    eq_(sys.stdout.getvalue(), s + "\n")
-
-@mock_streams('stdout')
-def test_puts_with_user_output_off():
-    """
-    puts() shouldn't print input to sys.stdout if "user" output level is off
-    """
-    output.user = False
-    puts("You aren't reading this.")
-    eq_(sys.stdout.getvalue(), "")
-
-
-@mock_streams('stdout')
-def test_puts_with_prefix():
-    """
-    puts() should prefix output with env.host_string if non-empty
-    """
-    s = "my output"
-    h = "localhost"
-    with settings(host_string=h):
-        puts(s)
-    eq_(sys.stdout.getvalue(), "[%s] %s" % (h, s + "\n"))
-
-
-@mock_streams('stdout')
-def test_puts_without_prefix():
-    """
-    puts() shouldn't prefix output with env.host_string if show_prefix is False
-    """
-    s = "my output"
-    puts(s, show_prefix=False)
-    eq_(sys.stdout.getvalue(), "%s" % (s + "\n"))
-
-@with_fakes
-def test_fastprint_calls_puts():
-    """
-    fastprint() is just an alias to puts()
-    """
-    text = "Some output"
-    fake_puts = Fake('puts', expect_call=True).with_args(
-        text=text, show_prefix=False, end="", flush=True
-    )
-    with patched_context(utils, 'puts', fake_puts):
-        fastprint(text)
-
-
-class TestErrorHandling(FabricTest):
-    dummy_string = 'test1234!'
-
-    @with_patched_object(utils, 'warn', Fake('warn', callable=True,
-        expect_call=True))
-    def test_error_warns_if_warn_only_True_and_func_None(self):
-        """
-        warn_only=True, error(func=None) => calls warn()
-        """
-        with settings(warn_only=True):
-            error('foo')
-
-    @with_patched_object(utils, 'abort', Fake('abort', callable=True,
-        expect_call=True))
-    def test_error_aborts_if_warn_only_False_and_func_None(self):
-        """
-        warn_only=False, error(func=None) => calls abort()
-        """
-        with settings(warn_only=False):
-            error('foo')
-
-    def test_error_calls_given_func_if_func_not_None(self):
-        """
-        error(func=callable) => calls callable()
-        """
-        error('foo', func=Fake(callable=True, expect_call=True))
-
-    @mock_streams('stdout')
-    @with_patched_object(utils, 'abort', Fake('abort', callable=True,
-        expect_call=True).calls(lambda x: sys.stdout.write(x + "\n")))
-    def test_error_includes_stdout_if_given_and_hidden(self):
-        """
-        error() correctly prints stdout if it was previously hidden
-        """
-        # Mostly to catch regression bug(s)
-        stdout = "this is my stdout"
-        with hide('stdout'):
-            error("error message", func=utils.abort, stdout=stdout)
-        assert_contains(stdout, sys.stdout.getvalue())
-
-    @mock_streams('stdout')
-    @with_patched_object(utils, 'abort', Fake('abort', callable=True,
-        expect_call=True).calls(lambda x: sys.stdout.write(x + "\n")))
-    @with_patched_object(output, 'exceptions', True)
-    @with_patched_object(utils, 'format_exc', Fake('format_exc', callable=True,
-        expect_call=True).returns(dummy_string))
-    def test_includes_traceback_if_exceptions_logging_is_on(self):
-        """
-        error() includes traceback in message if exceptions logging is on
-        """
-        error("error message", func=utils.abort, stdout=error)
-        assert_contains(self.dummy_string, sys.stdout.getvalue())
-
-    @mock_streams('stdout')
-    @with_patched_object(utils, 'abort', Fake('abort', callable=True,
-        expect_call=True).calls(lambda x: sys.stdout.write(x + "\n")))
-    @with_patched_object(output, 'debug', True)
-    @with_patched_object(utils, 'format_exc', Fake('format_exc', callable=True,
-        expect_call=True).returns(dummy_string))
-    def test_includes_traceback_if_debug_logging_is_on(self):
-        """
-        error() includes traceback in message if debug logging is on (backwardis compatibility)
-        """
-        error("error message", func=utils.abort, stdout=error)
-        assert_contains(self.dummy_string, sys.stdout.getvalue())
-
-    @mock_streams('stdout')
-    @with_patched_object(utils, 'abort', Fake('abort', callable=True,
-        expect_call=True).calls(lambda x: sys.stdout.write(x + "\n")))
-    @with_patched_object(output, 'exceptions', True)
-    @with_patched_object(utils, 'format_exc', Fake('format_exc', callable=True,
-        expect_call=True).returns(None))
-    def test_doesnt_print_None_when_no_traceback_present(self):
-        """
-        error() doesn't include None in message if there is no traceback
-        """
-        error("error message", func=utils.abort, stdout=error)
-        assert_not_contains('None', sys.stdout.getvalue())
-
-    @mock_streams('stderr')
-    @with_patched_object(utils, 'abort', Fake('abort', callable=True,
-        expect_call=True).calls(lambda x: sys.stderr.write(x + "\n")))
-    def test_error_includes_stderr_if_given_and_hidden(self):
-        """
-        error() correctly prints stderr if it was previously hidden
-        """
-        # Mostly to catch regression bug(s)
-        stderr = "this is my stderr"
-        with hide('stderr'):
-            error("error message", func=utils.abort, stderr=stderr)
-        assert_contains(stderr, sys.stderr.getvalue())
-
-    @mock_streams('stderr')
-    def test_warnings_print_magenta_if_colorize_on(self):
-        with settings(colorize_errors=True):
-            error("oh god", func=utils.warn, stderr="oops")
-        # can't use assert_contains as ANSI codes contain regex specialchars
-        eq_(magenta("\nWarning: oh god\n\n"), sys.stderr.getvalue())
-
-    @mock_streams('stderr')
-    @raises(SystemExit)
-    def test_errors_print_red_if_colorize_on(self):
-        with settings(colorize_errors=True):
-            error("oh god", func=utils.abort, stderr="oops")
-        # can't use assert_contains as ANSI codes contain regex specialchars
-        eq_(red("\Error: oh god\n\n"), sys.stderr.getvalue())
-
-
-class TestRingBuffer(TestCase):
-    def setUp(self):
-        self.b = RingBuffer([], maxlen=5)
-
-    def test_append_empty(self):
-        self.b.append('x')
-        eq_(self.b, ['x'])
-
-    def test_append_full(self):
-        self.b.extend("abcde")
-        self.b.append('f')
-        eq_(self.b, ['b', 'c', 'd', 'e', 'f'])
-
-    def test_extend_empty(self):
-        self.b.extend("abc")
-        eq_(self.b, ['a', 'b', 'c'])
-
-    def test_extend_overrun(self):
-        self.b.extend("abc")
-        self.b.extend("defg")
-        eq_(self.b, ['c', 'd', 'e', 'f', 'g'])
-
-    def test_extend_full(self):
-        self.b.extend("abcde")
-        self.b.extend("fgh")
-        eq_(self.b, ['d', 'e', 'f', 'g', 'h'])
-
-    def test_plus_equals(self):
-        self.b += "abcdefgh"
-        eq_(self.b, ['d', 'e', 'f', 'g', 'h'])
-
-    def test_oversized_extend(self):
-        self.b.extend("abcdefghijklmn")
-        eq_(self.b, ['j', 'k', 'l', 'm', 'n'])
-
-    def test_zero_maxlen_append(self):
-        b = RingBuffer([], maxlen=0)
-        b.append('a')
-        eq_(b, [])
-
-    def test_zero_maxlen_extend(self):
-        b = RingBuffer([], maxlen=0)
-        b.extend('abcdefghijklmnop')
-        eq_(b, [])
-
-    def test_None_maxlen_append(self):
-        b = RingBuffer([], maxlen=None)
-        b.append('a')
-        eq_(b, ['a'])
-
-    def test_None_maxlen_extend(self):
-        b = RingBuffer([], maxlen=None)
-        b.extend('abcdefghijklmnop')
-        eq_(''.join(b), 'abcdefghijklmnop')
diff -Nru fabric-1.14.0/tests/test_version.py fabric-2.5.0/tests/test_version.py
--- fabric-1.14.0/tests/test_version.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/tests/test_version.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,24 +0,0 @@
-"""
-Tests covering Fabric's version number pretty-print functionality.
-"""
-
-from nose.tools import eq_
-
-import fabric.version
-
-
-def test_get_version():
-    get_version = fabric.version.get_version
-    for tup, short, normal, verbose in [
-        ((0, 9, 0, 'final', 0), '0.9.0', '0.9', '0.9 final'),
-        ((0, 9, 1, 'final', 0), '0.9.1', '0.9.1', '0.9.1 final'),
-        ((0, 9, 0, 'alpha', 1), '0.9a1', '0.9 alpha 1', '0.9 alpha 1'),
-        ((0, 9, 1, 'beta', 1), '0.9.1b1', '0.9.1 beta 1', '0.9.1 beta 1'),
-        ((0, 9, 0, 'release candidate', 1),
-            '0.9rc1', '0.9 release candidate 1', '0.9 release candidate 1'),
-        ((1, 0, 0, 'alpha', 0), '1.0a', '1.0 pre-alpha', '1.0 pre-alpha'),
-    ]:
-        fabric.version.VERSION = tup
-        yield eq_, get_version('short'), short
-        yield eq_, get_version('normal'), normal
-        yield eq_, get_version('verbose'), verbose
diff -Nru fabric-1.14.0/tests/transfer.py fabric-2.5.0/tests/transfer.py
--- fabric-1.14.0/tests/transfer.py	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/tests/transfer.py	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,257 @@
+try:
+    from invoke.vendor.six import StringIO
+except ImportError:
+    from six import StringIO
+
+from mock import Mock, call
+from pytest_relaxed import raises
+from pytest import skip  # noqa
+from paramiko import SFTPAttributes
+
+from fabric import Connection
+from fabric.transfer import Transfer
+
+
+# TODO: pull in all edge/corner case tests from fabric v1
+
+
+class Transfer_:
+    class init:
+        "__init__"
+
+        def requires_connection(self):
+            # Transfer() -> explodes
+            try:
+                Transfer()
+            except TypeError:
+                pass
+            else:
+                assert False, "Did not raise ArgumentError"
+            # Transfer(Connection()) -> happy, exposes an attribute
+            cxn = Connection("host")
+            assert Transfer(cxn).connection is cxn
+
+    class is_remote_dir:
+        def returns_bool_of_stat_ISDIR_flag(self, sftp_objs):
+            xfer, sftp = sftp_objs
+            # Default mocked st_mode is file-like (first octal digit is 1)
+            assert xfer.is_remote_dir("whatever") is False
+            # Set mode directory-ish (first octal digit is 4)
+            sftp.stat.return_value.st_mode = 0o41777
+            assert xfer.is_remote_dir("whatever") is True
+
+        def returns_False_if_stat_raises_IOError(self, sftp_objs):
+            xfer, sftp = sftp_objs
+            sftp.stat.side_effect = IOError
+            assert xfer.is_remote_dir("whatever") is False
+
+    class get:
+        class basics:
+            def accepts_single_remote_path_posarg(self, sftp_objs):
+                transfer, client = sftp_objs
+                transfer.get("file")
+                client.get.assert_called_with(
+                    localpath="/local/file", remotepath="/remote/file"
+                )
+
+            def accepts_local_and_remote_kwargs(self, sftp_objs):
+                transfer, client = sftp_objs
+                transfer.get(remote="path1", local="path2")
+                client.get.assert_called_with(
+                    remotepath="/remote/path1", localpath="/local/path2"
+                )
+
+            def returns_rich_Result_object(self, sftp_objs):
+                transfer, client = sftp_objs
+                cxn = Connection("host")
+                result = Transfer(cxn).get("file")
+                assert result.orig_remote == "file"
+                assert result.remote == "/remote/file"
+                assert result.orig_local is None
+                assert result.local == "/local/file"
+                assert result.connection is cxn
+                # TODO: timing info
+                # TODO: bytes-transferred info
+
+        class path_arg_edge_cases:
+            def local_None_uses_remote_filename(self, transfer):
+                assert transfer.get("file").local == "/local/file"
+
+            def local_empty_string_uses_remote_filename(self, transfer):
+                assert transfer.get("file", local="").local == "/local/file"
+
+            @raises(TypeError)
+            def remote_arg_is_required(self, transfer):
+                transfer.get()
+
+            @raises(ValueError)
+            def remote_arg_cannot_be_None(self, transfer):
+                transfer.get(None)
+
+            @raises(ValueError)
+            def remote_arg_cannot_be_empty_string(self, transfer):
+                transfer.get("")
+
+        class file_like_local_paths:
+            "file-like local paths"
+
+            def _get_to_stringio(self, sftp_objs):
+                transfer, client = sftp_objs
+                fd = StringIO()
+                result = transfer.get("file", local=fd)
+                # Note: getfo, not get
+                client.getfo.assert_called_with(
+                    remotepath="/remote/file", fl=fd
+                )
+                return result, fd
+
+            def remote_path_to_local_StringIO(self, sftp_objs):
+                self._get_to_stringio(sftp_objs)
+
+            def result_contains_fd_for_local_path(self, sftp_objs):
+                result, fd = self._get_to_stringio(sftp_objs)
+                assert result.remote == "/remote/file"
+                assert result.local is fd
+
+        class mode_concerns:
+            def setup(self):
+                self.attrs = SFTPAttributes()
+                self.attrs.st_mode = 0o100644
+
+            def preserves_remote_mode_by_default(self, sftp):
+                transfer, client, mock_os = sftp
+                # Attributes obj reflecting a realistic 'extended' octal mode
+                client.stat.return_value = self.attrs
+                transfer.get("file", local="meh")
+                # Expect os.chmod to be called with the scrubbed/shifted
+                # version of same.
+                mock_os.chmod.assert_called_with("/local/meh", 0o644)
+
+            def allows_disabling_remote_mode_preservation(self, sftp):
+                transfer, client, mock_os = sftp
+                client.stat.return_value = self.attrs
+                transfer.get("file", local="meh", preserve_mode=False)
+                assert not mock_os.chmod.called
+
+    class put:
+        class basics:
+            def accepts_single_local_path_posarg(self, sftp_objs):
+                transfer, client = sftp_objs
+                transfer.put("file")
+                client.put.assert_called_with(
+                    localpath="/local/file", remotepath="/remote/file"
+                )
+
+            def accepts_local_and_remote_kwargs(self, sftp_objs):
+                transfer, sftp = sftp_objs
+                # NOTE: default mock stat is file-ish, so path won't be munged
+                transfer.put(local="path2", remote="path1")
+                sftp.put.assert_called_with(
+                    localpath="/local/path2", remotepath="/remote/path1"
+                )
+
+            def returns_rich_Result_object(self, transfer):
+                cxn = Connection("host")
+                result = Transfer(cxn).put("file")
+                assert result.orig_remote is None
+                assert result.remote == "/remote/file"
+                assert result.orig_local == "file"
+                assert result.local == "/local/file"
+                assert result.connection is cxn
+                # TODO: timing info
+                # TODO: bytes-transferred info
+
+        class remote_end_is_directory:
+            def appends_local_file_basename(self, sftp_objs):
+                xfer, sftp = sftp_objs
+                sftp.stat.return_value.st_mode = 0o41777
+                xfer.put(local="file.txt", remote="/dir/path/")
+                sftp.stat.assert_called_once_with("/dir/path/")
+                sftp.put.assert_called_with(
+                    localpath="/local/file.txt",
+                    remotepath="/dir/path/file.txt",
+                )
+
+            class file_like_local_objects:
+                def name_attribute_present_appends_like_basename(
+                    self, sftp_objs
+                ):
+                    xfer, sftp = sftp_objs
+                    sftp.stat.return_value.st_mode = 0o41777
+                    local = StringIO("sup\n")
+                    local.name = "sup.txt"
+                    xfer.put(local, remote="/dir/path")
+                    sftp.putfo.assert_called_with(
+                        fl=local, remotepath="/dir/path/sup.txt"
+                    )
+
+                @raises(ValueError)
+                def no_name_attribute_raises_ValueError(self, sftp_objs):
+                    xfer, sftp = sftp_objs
+                    sftp.stat.return_value.st_mode = 0o41777
+                    local = StringIO("sup\n")
+                    xfer.put(local, remote="/dir/path")
+
+        class path_arg_edge_cases:
+            def remote_None_uses_local_filename(self, transfer):
+                assert transfer.put("file").remote == "/remote/file"
+
+            def remote_empty_string_uses_local_filename(self, transfer):
+                assert transfer.put("file", remote="").remote == "/remote/file"
+
+            @raises(ValueError)
+            def remote_cant_be_empty_if_local_file_like(self, transfer):
+                transfer.put(StringIO())
+
+            @raises(TypeError)
+            def local_arg_is_required(self, transfer):
+                transfer.put()
+
+            @raises(ValueError)
+            def local_arg_cannot_be_None(self, transfer):
+                transfer.put(None)
+
+            @raises(ValueError)
+            def local_arg_cannot_be_empty_string(self, transfer):
+                transfer.put("")
+
+        class file_like_local_paths:
+            "file-like local paths"
+
+            def _put_from_stringio(self, sftp_objs):
+                transfer, client = sftp_objs
+                fd = StringIO()
+                result = transfer.put(fd, remote="file")
+                # Note: putfo, not put
+                client.putfo.assert_called_with(
+                    remotepath="/remote/file", fl=fd
+                )
+                return result, fd
+
+            def remote_path_from_local_StringIO(self, sftp_objs):
+                self._put_from_stringio(sftp_objs)
+
+            def local_FLOs_are_rewound_before_putting(self, transfer):
+                fd = Mock()
+                fd.tell.return_value = 17
+                transfer.put(fd, remote="file")
+                seek_calls = fd.seek.call_args_list
+                assert seek_calls, [call(0) == call(17)]
+
+            def result_contains_fd_for_local_path(self, sftp_objs):
+                result, fd = self._put_from_stringio(sftp_objs)
+                assert result.remote == "/remote/file"
+                assert result.local is fd
+
+        class mode_concerns:
+            def preserves_local_mode_by_default(self, sftp):
+                transfer, client, mock_os = sftp
+                # This is a realistic stat for 0o644
+                mock_os.stat.return_value.st_mode = 33188
+                transfer.put("file")
+                client.chmod.assert_called_with("/remote/file", 0o644)
+
+            def allows_disabling_local_mode_preservation(self, sftp_objs):
+                transfer, client = sftp_objs
+                transfer.put("file", preserve_mode=False)
+                assert not client.chmod.called
diff -Nru fabric-1.14.0/tests/_util.py fabric-2.5.0/tests/_util.py
--- fabric-1.14.0/tests/_util.py	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/tests/_util.py	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,96 @@
+from contextlib import contextmanager
+import os
+import re
+import sys
+
+from invoke.vendor.lexicon import Lexicon
+from pytest_relaxed import trap
+
+from fabric import Connection as Connection_, Config as Config_
+from fabric.main import make_program
+from paramiko import SSHConfig
+
+
+support = os.path.join(os.path.abspath(os.path.dirname(__file__)), "_support")
+config_file = os.path.abspath(os.path.join(support, "config.yml"))
+
+# TODO: move invoke's support_path + load + etc somewhere importable? or into
+# pytest-relaxed, despite it not being strictly related to that feature set?
+# ugh
+@contextmanager
+def support_path():
+    sys.path.insert(0, support)
+    try:
+        yield
+    finally:
+        sys.path.pop(0)
+
+
+def load(name):
+    with support_path():
+        imported = __import__(name)
+        return imported
+
+
+# TODO: this could become a fixture in conftest.py, presumably, and just yield
+# stdout, allowing the tests themselves to assert more naturally
+@trap
+def expect(invocation, out, program=None, test="equals"):
+    if program is None:
+        program = make_program()
+    program.run("fab {}".format(invocation), exit=False)
+    output = sys.stdout.getvalue()
+    if test == "equals":
+        assert output == out
+    elif test == "contains":
+        assert out in output
+    elif test == "regex":
+        assert re.match(out, output)
+    else:
+        err = "Don't know how to expect that <stdout> {} <expected>!"
+        assert False, err.format(test)
+
+
+# Locally override Connection, Config with versions that supply a dummy
+# SSHConfig and thus don't load any test-running user's own ssh_config files.
+# TODO: find a cleaner way to do this, though I don't really see any that isn't
+# adding a ton of fixtures everywhere (and thus, opening up to forgetting it
+# for new tests...)
+class Config(Config_):
+    def __init__(self, *args, **kwargs):
+        wat = "You're giving ssh_config explicitly, please use Config_!"
+        assert "ssh_config" not in kwargs, wat
+        # Give ssh_config explicitly -> shorter way of turning off loading
+        kwargs["ssh_config"] = SSHConfig()
+        super(Config, self).__init__(*args, **kwargs)
+
+
+class Connection(Connection_):
+    def __init__(self, *args, **kwargs):
+        # Make sure we're using our tweaked Config if none was given.
+        kwargs.setdefault("config", Config())
+        super(Connection, self).__init__(*args, **kwargs)
+
+
+def faux_v1_env():
+    # Close enough to v1 _AttributeDict...
+    # Contains a copy of enough of v1's defaults to prevent us having to do a
+    # lot of extra .get()s...meh
+    return Lexicon(
+        always_use_pty=True,
+        forward_agent=False,
+        gateway=None,
+        host_string="localghost",
+        key_filename=None,
+        no_agent=False,
+        password=None,
+        port=22,
+        ssh_config_path=None,
+        # Used in a handful of sanity tests, so it gets a 'real' value. eh.
+        sudo_password="nope",
+        sudo_prompt=None,
+        timeout=None,
+        use_ssh_config=False,
+        user="localuser",
+        warn_only=False,
+    )
diff -Nru fabric-1.14.0/tests/util.py fabric-2.5.0/tests/util.py
--- fabric-1.14.0/tests/util.py	1970-01-01 01:00:00.000000000 +0100
+++ fabric-2.5.0/tests/util.py	2019-08-06 23:57:28.000000000 +0100
@@ -0,0 +1,23 @@
+"""
+Tests testing the fabric.util module, not utils for the tests!
+"""
+
+from mock import patch
+
+from fabric.util import get_local_user
+
+
+# Basically implementation tests, because it's not feasible to do a "real" test
+# on random platforms (where we have no idea what the actual invoking user is)
+class get_local_user_:
+    @patch("getpass.getuser")
+    def defaults_to_getpass_getuser(self, getuser):
+        "defaults to getpass.getuser"
+        get_local_user()
+        getuser.assert_called_once_with()
+
+    @patch("getpass.getuser", side_effect=KeyError)
+    def KeyError_means_SaaS_and_thus_None(self, getuser):
+        assert get_local_user() is None
+
+    # TODO: test for ImportError+win32 once appveyor is set up as w/ invoke
diff -Nru fabric-1.14.0/tests/utils.py fabric-2.5.0/tests/utils.py
--- fabric-1.14.0/tests/utils.py	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/tests/utils.py	1970-01-01 01:00:00.000000000 +0100
@@ -1,196 +0,0 @@
-from __future__ import with_statement
-
-from contextlib import contextmanager
-from fudge.patcher import with_patched_object
-from functools import partial
-from types import StringTypes
-import copy
-import getpass
-import os
-import re
-import shutil
-import sys
-import tempfile
-
-from fudge import Fake, patched_context, clear_expectations
-from nose.tools import raises
-
-from fabric.state import env, output
-from fabric.sftp import SFTP
-from fabric.network import to_dict
-
-from server import PORT, PASSWORDS, USER, HOST
-from mock_streams import mock_streams
-
-
-class FabricTest(object):
-    """
-    Nose-oriented test runner which wipes state.env and provides file helpers.
-    """
-    def setup(self):
-        # Clear Fudge mock expectations
-        clear_expectations()
-        # Copy env, output for restoration in teardown
-        self.previous_env = copy.deepcopy(env)
-        # Deepcopy doesn't work well on AliasDicts; but they're only one layer
-        # deep anyways, so...
-        self.previous_output = output.items()
-        # Allow hooks from subclasses here for setting env vars (so they get
-        # purged correctly in teardown())
-        self.env_setup()
-        # Temporary local file dir
-        self.tmpdir = tempfile.mkdtemp()
-
-    def set_network(self):
-        env.update(to_dict('%s@%s:%s' % (USER, HOST, PORT)))
-
-    def env_setup(self):
-        # Set up default networking for test server
-        env.disable_known_hosts = True
-        self.set_network()
-        env.password = PASSWORDS[USER]
-        # Command response mocking is easier without having to account for
-        # shell wrapping everywhere.
-        env.use_shell = False
-
-    def teardown(self):
-        env.clear() # In case tests set env vars that didn't exist previously
-        env.update(self.previous_env)
-        output.update(self.previous_output)
-        shutil.rmtree(self.tmpdir)
-        # Clear Fudge mock expectations...again
-        clear_expectations()
-
-    def path(self, *path_parts):
-        return os.path.join(self.tmpdir, *path_parts)
-
-    def mkfile(self, path, contents):
-        dest = self.path(path)
-        with open(dest, 'w') as fd:
-            fd.write(contents)
-        return dest
-
-    def exists_remotely(self, path):
-        return SFTP(env.host_string).exists(path)
-
-    def exists_locally(self, path):
-        return os.path.exists(path)
-
-
-def password_response(password, times_called=None, silent=True):
-    """
-    Context manager which patches ``getpass.getpass`` to return ``password``.
-
-    ``password`` may be a single string or an iterable of strings:
-
-    * If single string, given password is returned every time ``getpass`` is
-      called.
-    * If iterable, iterated over for each call to ``getpass``, after which
-      ``getpass`` will error.
-
-    If ``times_called`` is given, it is used to add a ``Fake.times_called``
-    clause to the mock object, e.g. ``.times_called(1)``. Specifying
-    ``times_called`` alongside an iterable ``password`` list is unsupported
-    (see Fudge docs on ``Fake.next_call``).
-
-    If ``silent`` is True, no prompt will be printed to ``sys.stderr``.
-    """
-    fake = Fake('getpass', callable=True)
-    # Assume stringtype or iterable, turn into mutable iterable
-    if isinstance(password, StringTypes):
-        passwords = [password]
-    else:
-        passwords = list(password)
-    # Optional echoing of prompt to mimic real behavior of getpass
-    # NOTE: also echo a newline if the prompt isn't a "passthrough" from the
-    # server (as it means the server won't be sending its own newline for us).
-    echo = lambda x, y: y.write(x + ("\n" if x != " " else ""))
-    # Always return first (only?) password right away
-    fake = fake.returns(passwords.pop(0))
-    if not silent:
-        fake = fake.calls(echo)
-    # If we had >1, return those afterwards
-    for pw in passwords:
-        fake = fake.next_call().returns(pw)
-        if not silent:
-            fake = fake.calls(echo)
-    # Passthrough times_called
-    if times_called:
-        fake = fake.times_called(times_called)
-    return patched_context(getpass, 'getpass', fake)
-
-
-def _assert_contains(needle, haystack, invert):
-    matched = re.search(needle, haystack, re.M)
-    if (invert and matched) or (not invert and not matched):
-        raise AssertionError("r'%s' %sfound in '%s'" % (
-            needle,
-            "" if invert else "not ",
-            haystack
-        ))
-
-assert_contains = partial(_assert_contains, invert=False)
-assert_not_contains = partial(_assert_contains, invert=True)
-
-
-def line_prefix(prefix, string):
-    """
-    Return ``string`` with all lines prefixed by ``prefix``.
-    """
-    return "\n".join(prefix + x for x in string.splitlines())
-
-
-def eq_(result, expected, msg=None):
-    """
-    Shadow of the Nose builtin which presents easier to read multiline output.
-    """
-    params = {'expected': expected, 'result': result}
-    aka = """
-
---------------------------------- aka -----------------------------------------
-
-Expected:
-%(expected)r
-
-Got:
-%(result)r
-""" % params
-    default_msg = """
-Expected:
-%(expected)s
-
-Got:
-%(result)s
-""" % params
-    if (repr(result) != str(result)) or (repr(expected) != str(expected)):
-        default_msg += aka
-    assert result == expected, msg or default_msg
-
-
-def eq_contents(path, text):
-    with open(path) as fd:
-        eq_(text, fd.read())
-
-
-def support(path):
-    return os.path.join(os.path.dirname(__file__), 'support', path)
-
-fabfile = support
-
-
-@contextmanager
-def path_prefix(module):
-    i = 0
-    sys.path.insert(i, os.path.dirname(module))
-    yield
-    sys.path.pop(i)
-
-
-def aborts(func):
-    return raises(SystemExit)(mock_streams('stderr')(func))
-
-
-def _patched_input(func, fake):
-    return func(sys.modules['__builtin__'], 'raw_input', fake)
-patched_input = partial(_patched_input, patched_context)
-with_patched_input = partial(_patched_input, with_patched_object)
diff -Nru fabric-1.14.0/.travis.yml fabric-2.5.0/.travis.yml
--- fabric-1.14.0/.travis.yml	2017-08-25 22:41:36.000000000 +0100
+++ fabric-2.5.0/.travis.yml	2019-08-06 23:57:28.000000000 +0100
@@ -1,38 +1,87 @@
 language: python
+sudo: required
+dist: trusty
+cache:
+  directories:
+    - $HOME/.cache/pip
 python:
-  - "2.6"
   - "2.7"
+  - "3.4"
+  - "3.5"
+  - "3.6"
+  - "pypy"
+  - "pypy3"
+matrix:
+  # pypy3 (as of 2.4.0) has a wacky arity issue in its source loader. Allow it
+  # to fail until we can test on, and require, PyPy3.3+. See
+  # pyinvoke/invoke#358.
+  # NOTE: both pypy flavors are weirdly unstable on Travis nowadays, even
+  # pre-test-run.
+  allow_failures:
+    - python: pypy
+    - python: pypy3
+  # Disabled per https://github.com/travis-ci/travis-ci/issues/1696
+  # fast_finish: true
 install:
-  # Build/test dependencies
-  - pip install -r requirements.txt
-  # Get fab to test fab
+  # TODO: real test matrix with at least some cells combining different invoke
+  # and/or paramiko versions, released versions, etc
+  # Invoke from master for parity
+  - "pip install -e git+https://github.com/pyinvoke/invoke#egg=invoke"
+  # And invocations, ditto
+  - "pip install -e git+https://github.com/pyinvoke/invocations#egg=invocations"
+  # Paramiko ditto
+  - "pip install -e git+https://github.com/paramiko/paramiko#egg=paramiko"
+  # Self
   - pip install -e .
-  # Deal with issue on Travis builders re: multiprocessing.Queue :(
-  - "sudo rm -rf /dev/shm && sudo ln -s /run/shm /dev/shm"
-  - "pip install jinja2"
+  # Limit setuptools as some newer versions have Issues(tm). This needs doing
+  # as its own step; trying to do it via requirements.txt isn't always
+  # sufficient.
+  - pip install "setuptools<34"
+  # Dev requirements
+  # TODO: follow invoke and split it up a bit so we're not pulling down
+  # conflicting or unused-by-travis deps?
+  - pip install -r dev-requirements.txt
+  # Sanity test of the Invoke layer, if that's busted everything is
+  - inv --list
+  # Sanity test of Fabric itself
+  - fab --version
 before_script:
+  # Create 'sudouser' w/ sudo password & perms on Travis' homedir
+  - inv travis.make-sudouser
   # Allow us to SSH passwordless to localhost
-  - ssh-keygen -f ~/.ssh/id_rsa -N ""
-  - cp ~/.ssh/{id_rsa.pub,authorized_keys}
-  # Creation of an SSH agent for testing forwarding
-  - eval $(ssh-agent)
-  - ssh-add
+  - inv travis.make-sshable
 script:
-  # Normal tests
-  - fab test
-  # Integration tests
-  - fab -H localhost test:"--tests\=integration"
-  # Build docs; www first without warnings so its intersphinx objects file
-  # generates. Then docs (with warnings->errors), then www again (also w/
-  # warnings on.) FUN TIMES WITH CIRCULAR DEPENDENCIES.
-  - invoke www
-  - invoke docs -o -W
-  - invoke www -c -o -W
+  # Fast syntax check failures for more rapid feedback to submitters
+  # (Travis-oriented metatask that version checks Python, installs, runs.)
+  - inv travis.blacken
+  # I have this in my git pre-push hook, but contributors probably don't
+  - flake8
+  # Execute full test suite + coverage, as the new sudo-capable user
+  - inv travis.sudo-coverage
+  # Execute integration tests too. TODO: merge under coverage...somehow
+  # NOTE: this also runs as the sudo-capable user, even if it's not necessarily
+  # doing any sudo'ing itself - the sudo-capable user is also the ssh-able
+  # user...
+  - inv travis.sudo-run "inv integration"
+  # Websites build OK? (Not on PyPy3, Sphinx is all "who the hell are you?" =/
+  - "if [[ $TRAVIS_PYTHON_VERSION != 'pypy3' ]]; then inv sites www.doctest docs.doctest; fi"
+  # Did we break setup.py?
+  - inv travis.test-installation --package=fabric --sanity="fab --version"
+  # Test distribution builds.
+  - inv travis.test-packaging --package=fabric --sanity="fab --version"
+  # Again, but as 'fabric2'
+  - rm -rf tmp
+  - pip uninstall -y fabric
+  - "PACKAGE_AS_FABRIC2=yes inv travis.test-packaging --package=fabric2 --sanity=\"fab2 --version\""
+  - inv sanity-test-from-v1
+after_success:
+  # Upload coverage data to codecov
+  - codecov
 notifications:
   irc:
     channels: "irc.freenode.org#fabric"
     template:
-      - "%{repository}@%{branch}: %{message} (%{build_url})"
+      - "%{repository_name}@%{branch}: %{message} (%{build_url})"
     on_success: change
     on_failure: change
   email: false
